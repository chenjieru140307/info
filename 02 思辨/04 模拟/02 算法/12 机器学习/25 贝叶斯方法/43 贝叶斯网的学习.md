

## 学习

若网络结构已知，即属性间的依赖关系已知，则贝叶斯网的学习过程相对简单，只需通过对训练样本“计数”，估计出每个结点的条件概率表即可.

但在 现实应用中我们往往并不知晓网络结构，于是，贝叶斯网学习的首要任务就是 根据训练数据集来找出结构最“恰当”的贝叶斯网.

“评分搜索”是求解这一 问题的常用办法。具体来说，我们先定义一个评分函数(score function)，以此来 评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优 的贝叶斯网。显然，评分函数引入了关于我们希望获得什么样的贝叶斯网的归 纳偏好.

常用评分函数通常基于信息论准则，此类准则将学习问题看作一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时 编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需 的字节长度.

对贝叶斯网学习而言，模型就是一个贝叶斯网，同时，每个贝叶斯网描述了一个在训练数据上的概率分布，自有一套编码机制能使那些经常出现的样本有更短的编码。于是，我们应选择那个综合编码长度(包括描述网络 和编碍数据)最短的贝叶斯网，这就是“最小描述长度” (Minimal Description Length，简称 MDL)准则.


给定训练集 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$ ，贝叶斯网 $B=\langle G, \Theta\rangle$  在 $D$ 上的评分函数可写为

$$
s(B | D)=f(\theta)|B|-L L(B | D)\tag{7.28}
$$

其中， $|B|$ 是贝叶斯网的参数个数;$f(\theta)$ 表示描述每个参数 $\theta$ 所需的字节数；而

$$
L L(B | D)=\sum_{i=1}^{m} \log P_{B}\left(\boldsymbol{x}_{i}\right)\tag{7.29}
$$

是贝叶斯网 $B$ 的对数似然。显然，式(7.28)的第一项是计算编码贝叶斯网 $B$ 所需的字节数，第二项是计算 $B$ 所对应的概率分布 $P_B$ 需多少字节来描述 $D$ 。于是，学习任务就转化为一个优化任务，即寻找一个贝叶斯网 $B$ 使评分函数 $s(B|D)$ 最小.

若 $f(\theta)=1$ ，即每个参数用 1 字节描述，则得到 AIC (Akaike Information Criterion)评分函数

$$
\operatorname{AIC}(B | D)=|B|-L L(B | D)\tag{7.30}
$$


若 $f(\theta)=\frac{1}{2}log\,m$ ，即每个参数用 $\frac{1}{2}log\,m$ 字节描述，则得到 BIC (Bayesian Information Criterion)评分函数

$$
\operatorname{BIC}(B | D)=\frac{\log m}{2}|B|-L L(B | D)\tag{7.31}
$$

显然，若 $f(\theta)=0$ ，即不计算对网络进行编码的长度，则评分函数退化为负对数似然，相应的，学习任务退化为极大似然估计.

不难发现，若贝叶斯网 $B=\langle G,\Theta\rangle$ 的网络结构 $G$ 固定，则评分函数 $s(B|D)$ 的第一项为常数。此时，最小化 $s(B|D)$ 等价于对参数 $\Theta$ 的极大似然 估计。由式(7.29)和(7.26)可知，参数 $\theta_{x_i|\pi_i}$ 能直接在训练数据 $D$ 上通过经验估计获得，即

$$
\theta_{x_{i} | \pi_{i}}=\hat{P}_{D}\left(x_{i} | \pi_{i}\right)\tag{7.32}
$$


其中 $\hat{P}_D(\cdot )$ 是 $D$ 上的经验分布。因此，为了最小化评分函数 $s(B|D)$ ，只需对网 络结构进行搜索，而候选结构的最优参数可直接在训练集上计算得到.


不幸的是，从所有可能的网络结构空间搜索最优贝叶斯网结构是一个 NP 难问题，难以快速求解。有两种常用的策略能在有限时间内求得近似解：

- 第一种是贪心法，例如从某个网络结构出发，每次调整一条边(增加、删除或调整方 向)，直到评分函数值不再降低为止。
- 第二种是通过给网络结构施加约束来削减 搜索空间，例如将网络结构限定为树形结构等.





# 相关

- 《机器学习》周志华
