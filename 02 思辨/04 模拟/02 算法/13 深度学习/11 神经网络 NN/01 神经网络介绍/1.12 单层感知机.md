


## 单层感知机

感知机 Perceptron：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180626/HH2jbcD06F.png?imageslim">
</p>

感知机能容易地实现逻辑与、或、非运算。


注意到 $y=f(\sum_i w_ix_i-\theta)$ ，假定 $f$ 是阶跃函数，有

- “与”($x_1\wedge x_2$): 令 $w_1=w_2=1$，$\theta=2$ ，则 $y=f(1\cdot x_1+1\cdot x_2 -2)$ ，仅在 $x_1=x_2=1$ 时，$y=1$ 。
- “或”($x_1\vee x_2$): 令 $w_1=w_2=1$，$\theta=0.5$ ，则 $y=f(1\cdot x_1+1\cdot x_2 -0.5)$ ，当 $x_1=1$ 或 $x_2=1$ 时，$y=1$ 。
- “非”($\neg x_1$): 令 $w_1=-0.6$，$w_2=0$，$\theta=-0.5$ ，则 $y=f(-0.6\cdot x_1+0\cdot x_2 +0.5)$ ，当 $x_1=1$ 时，y=0；当 x_1=0 时，$y=1$ 。



更一般地，给定训练数据集，权重购 $w_i(i=1,2,\cdots ,n)$ 以及阈值 $\theta$  可通过学习得到。阈值 $\theta$ 可看作一个固定输入为-1.0的 “哑结点”(dummy node)，所对应的连接权重 $w_{n+1}$ 这样，权重和阈值的学习就可统一为权重的学习。感知机学习规则非常简单，对训练样例 $(\boldsymbol{x}, y)$，若当前感知机的输出为 $\hat{y}$ 则感知机权重将这样调整：
$$
w_{i} \leftarrow w_{i}+\Delta w_{i}\tag{5.1}
$$
$$
\Delta w_{i}=\eta(y-\hat{y}) x_{i}\tag{5.2}
$$


> $$\Delta w_i = \eta(y-\hat{y})x_i$$
> [推导]：此处感知机的模型为：
> $$y=f(\sum_{i} w_i x_i - \theta)$$
> 将 $\theta$ 看成哑结点后，模型可化简为：
> $$y=f(\sum_{i} w_i x_i)=f(\boldsymbol w^T \boldsymbol x)$$
> 其中 $f$ 为阶跃函数。<br>根据《统计学习方法》§2可知，假设误分类点集合为 $M$，$\boldsymbol x_i \in M$ 为误分类点，$\boldsymbol x_i$ 的真实标签为 $y_i$，模型的预测值为 $\hat{y_i}$，对于误分类点 $\boldsymbol x_i$ 来说，此时 $\boldsymbol w^T \boldsymbol x_i \gt 0,\hat{y_i}=1,y_i=0$ 或 $\boldsymbol w^T \boldsymbol x_i \lt 0,\hat{y_i}=0,y_i=1$，综合考虑两种情形可得：
> $$(\hat{y_i}-y_i)\boldsymbol w \boldsymbol x_i>0$$
> 所以可以推得损失函数为：
> $$L(\boldsymbol w)=\sum_{\boldsymbol x_i \in M} (\hat{y_i}-y_i)\boldsymbol w \boldsymbol x_i$$
> 损失函数的梯度为：
> $$\nabla_w L(\boldsymbol w)=\sum_{\boldsymbol x_i \in M} (\hat{y_i}-y_i)\boldsymbol x_i$$
> 随机选取一个误分类点 $(\boldsymbol x_i,y_i)$，对 $\boldsymbol w$ 进行更新：
> $$\boldsymbol w \leftarrow \boldsymbol w-\eta(\hat{y_i}-y_i)\boldsymbol x_i=\boldsymbol w+\eta(y_i-\hat{y_i})\boldsymbol x_i$$
> 显然式 5.2为 $\boldsymbol w$ 的第 $i$ 个分量 $w_i$ 的变化情况

其中 $\eta$ 称为学习率(learning rate)。从式(5.1)可看出，若感知机对训练 样例 $(\boldsymbol{x}, y)$ 预测正确，即 $\hat{y}=y$，则感知机不发生变化，否则将根据错误的程度进行权重调整。

需注意的是，感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元(functional neuron)，其学习能力非常有限。

事实上，上述与、或、 非问题都是线性可分(linearly separable)的问题。

若两类模式是线性可分的，即存在一个线性超平面能将它们分开，如图 5.4(a)-(c)所示，则感知机的学习过程一定会收敛(converge)而求得适当的权向量 $\boldsymbol{w}=\left(w_{1} ; w_{2} ; \dots ; w_{n+1}\right)$；否则感知机学习过程将会发生振荡(fluctuation)，$\boldsymbol{w}$ 难以稳定下来，不能求得合适解，例如感知机甚至不能解决如图 5.4(d)所示的异或这样简单的非线性可分问题。<span style="color:red;">嗯</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180626/EdKdbB6djE.png?imageslim">
</p>
