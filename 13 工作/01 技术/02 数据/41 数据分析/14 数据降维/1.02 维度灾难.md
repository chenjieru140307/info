
# 维度灾难

主要是两点：

- 维度增加意味着需要更多样本，不然样本分布过于稀疏。
- 许多算法需要计算样本间距离，高维空间使得内积的计算不再容易。

## 举个例子

对于 K 近邻来说，它需要任意测试样本 $\boldsymbol{x}$ 附近任意小的 $\delta$ 各距离范围内要总能找到一个训练样本，即训练样本的采样密度足够大，或称为 “密采样” (dense sample)。

然而，这个假设在现实任务中通常很难满足，<span style="color:red;">是的。</span>

例如若 $\delta = 0.001$ ，仅考虑单个属性，则仅需 $1000$ 个样本点平均分布在归一化后的属性取值范围内，即可使得任意测试样本在其附近 $0.001$ 距离范围内总能找到一个训练样本，此时最近邻分类器的错误率不超过贝叶斯最优分类器的错误率的两倍。

然而，这仅是属性维数为 $1$ 的情形，若有更多的属性，则情况会发生显著变化。例如假定属性维数为 $20$，若要求样本满足密采样条件，则至少需 $(10^3)^{20}=10^{60}$ 个样本。**现实应用中属性维数经常成千上万，要满足密采样条件所需的样本数目是无法达到的天文数字。** 此外，许多学习方法都涉及距离计算，而高维空间会给距离计算带来很大的麻烦，例如**当维数很高时甚至连计算内积都不再容易**<span style="color:red;">为什么？</span>。


事实上，在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，被称为“维数灾难” (curse of dimensionality)。

缓解维数灾难的一个重要途径是降维(dimension reduction)，亦称“维数约简”，即通过某种数学变换将原始高维属性空间转变为一个低维“子空间” (subspace)，在这个子空间中样本密度大幅提高，距离计算也变得更为容 易。
