
4.2 熟悉你的数据
尽管我们强烈反对，但是你仍然可以在不了解数据时就建立模型；这样的行为可能会占用你更长的时间，并且生成的模型质量可能也不太理想，但是它是可行的。在本节中，我们将使用从 http://packages.revolutionanalytics.com/datasets/ccFraud.csv下载的数据集。我们不会改变数据集本身，但是它是用 GZipped 压缩并上传到 http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz的。请先下载文件并且和包含本章的笔记本保存在同一个文件夹中。
数据集的开头如下：
因此，任何严谨的数据科学家或者数据建模师都会在开始任何建模之前先熟悉数据集。第一件事通常是，从一些描述性统计开始，找到我们正在处理的问题的感觉。
4.2.1 描述性统计
按照最简单的意义，描述性统计会告诉你关于数据集的基本信息：你的数据集中有多少个非缺失的观测数据、列的平均值和标准偏差、还有最小值和最大值。
但是，第一件事首先是——加载数据并且转换为 Spark DataFrame：
首先，我们只加载需要的模块。pyspark.sql.types显示了所有我们可以使用的数据类型，如 IntegerType（）或者 FloatType（）。一个完整的可用类型完整列表查看 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types。
下一步，读取数据并使用.filter（……）方法删除标题行。接下来是以每个逗号分割出行（因为这是一个.csv文件）并且将每个元素转换成整型（integer）：
然后，我们创建自己的 DataFrame 模式：
最后，创建了我们的 DataFrame：
创建了 fraud_df DataFrame之后，我们可以计算出数据集的基本描述统计。不过，你需要记住，尽管我们所有的特征本质上都是数字的，但是有些特征却是按类别的（例如性别（gender）或者州（state））。
这是我们的 DataFrame 模式：


图示如下：
同样，没有信息可以从计算 custId 列的平均值和标准差中获取，所以我们还不会这么做。
要更好地理解分类列，我们会利用.groupby（……）方法计算这些列值的使用频率。在这个例子中，我们计算性别列的频率。
这段代码的输出结果如下：
正如你所见，我们正在处理一个特别失衡的数据集。你期待看到的是男女性别平等的分布。这超出了本章的范围，但是如果我们正在建立一个统计模型，你需要注意这些偏差。你可以再读读这篇文章 http://www.va.gov/VETDATA/docs/SurveysAndStudies/SAMPLE_WEIGHT.pdf。
对于真正的数值特征，我们可以使用.describe（）方法：
.show（）方法生成以下结果：
即使是从这些相对较少的数字中，我们都能发现不少东西：
·所有的特征都呈正偏态；最大值是平均值的多倍。
·变异系数（均值与标准差之比）非常高（接近或者大于 1），意味着这是一个广泛的观测数据。
如何检查偏度（我们仅对“平衡”特征进行检查）：
这段代码生成结果如下：
列表中的聚合函数包括：avg（）、count（）、countDistinct（）、first（）、kurtosis（）、max（）、mean（）、min（）、skewness（）、stddev（）、stddev_pop（）、stddev_samp（）、sum（）、sumDistinct（）、var_pop（）、var_samp（）和 variance（）。
4.2.2 相关性
特征之间相互关系的另一个非常有用的度量是相关性。你的模型通常只包括那些与你的目标高度相关的特征。然而，检查特征之间的相关性几乎是同样重要的，包括它们之间高度相关的特征（即共性（collinear）），可能会导致模型的不可预知行为或者可能进行不必要地复

杂化。在我的 Practical Data Analysis Cookbook，Packt Publishing中，更多地谈论到了多重共线性（multicollinearity）（https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook），第 5 章 Identifying and tackling multicollinearity节。
只要你的数据是 DataFrame 形式，运用 PySpark 计算相关性非常容易。唯一的困难是，这时.corr（……）方法支持 Pearson 相关性系数，并且只能计算两两相关性，如以下：
为了创建一个相关矩阵，可以使用以下脚本：
这段代码创建的输出结果如下：
正如你所见，信用卡欺诈数据集中的数值型特征间的相关性几乎是不存在的。因此所有可以在我们模型中使用的这些特征，在解释我们的目标方面都是具有统计学意义的。
检查了相关性，现在可以继续在视觉上检测我们的数据了。
