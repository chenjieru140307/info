
所有数据都是未经处理的，不管数据从何而来，你可以认为：这些数据来自你的同事、来自一个监控你的环境的遥测系统、来自你从网上下载的数据集或者其他的来源。直到你自己测试并且证明了你的数据处于干净过滤的状态（我们可以在 1 秒内达到干净状态），都不应该信任这些数据，也不该用这些数据来建模。
你的数据可以是重复数据、未观测数据和异常数据（离群值），可以有不存在的地址、错误的电话号码和区号、不准确的地理坐标、错误的日期、不正确的标签、大小写字母混乱、尾随空格以及许多其他更细小的问题。不管你是数据科学家还是数据工程师，你的工作就是清理数据，这样你才可以建立起一个统计或者机器学习的模型。
如果没有发现上述问题，那么在技术上你的数据集可以被视为干净的。但是，为了清理数据集进行建模，你还需要检查数据特征的分布并且确认它们符合预定义的标准。
作为一个数据科学家，你可以预料到要花费 80％～90％的时间整理数据并且熟悉数据所有的特征。本章将利用 Spark 的功能，引导你完成这一段过程。
