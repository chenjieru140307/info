
# 分词、词性标注和关键字提取

介绍：

- 中文分词
  - 中文分词(Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。
    - 分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。
    - 英文是不需要做分词的。
  - 中文分词是其他中文信息处理的基础，
    - 很多项目，第一步都需要做分词。
    - 搜索引擎、机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。
- 词性标注
  - 词性标注（Part-of-Speech tagging 或 POS tagging)，又称词类标注或者简称标注，
  - 是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或其他词性的过程。
  - 在汉语中，词性标注比较简单，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。
    - 据说，只需选取最高频词性，即可实现80%准确率的中文词性标注程序。
  - 不同的工具词性标注不一定一样。所以混合使用不同的工具时候，需要查下对应的表示。jieba 的和 hanlp 和 standfordnlp 可能有些不同。
    - 以下是 jieba 标注：这个版本可能有点旧了。
      - Ag 形语素
        - 形容词性语素。形容词代码为 a，语素代码ｇ前面置以A。
      - a 形容词
        - 取英语形容词 adjective 的第1个字母。
      - ad 副形词
        - 直接作状语的形容词。形容词代码 a 和副词代码 d 并在一起。
      - an 名形词
        - 具有名词功能的形容词。形容词代码 a 和名词代码 n 并在一起。
      - b 区别词
        - 取汉字“别”的声母。
      - c 连词
        - 取英语连词 conjunction的第 1 个字母。
      - dg 副语素
        - 副词性语素。副词代码为 d，语素代码 g 前面置以 D。
      - d 副词
        - 取 adverb 的第2个字母，因其第 1 个字母已用于形容词。
      - e 叹词
        - 取英语叹词 exclamation 的第1个字母。
      - f 方位词
        - 取汉字“方”
      - g 语素
        - 绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母。
      - h 前接成分
        - 取英语 head的第1个字母。
      - i 成语
        - 取英语成语 idiom的第1个字母。
      - j 简称略语
        - 取汉字“简”的声母。
      - k 后接成分
      - l 习用语
        - 习用语尚未成为成语，有点“临时性”，取“临”的声母。
      - m 数词
        - 取英语 numeral的第3个字母，n，u已有他用。
      - Ng 名语素
        - 名词性语素。名词代码为 n，语素代码ｇ前面置以N。
      - n 名词
        - 取英语名词 noun的第1个字母。
      - nr 人名
        - 名词代码 n和“人(ren)”的声母并在一起。
      - ns 地名
        - 名词代码 n和处所词代码s并在一起。
      - nt 机构团体
        - “团”的声母为 t，名词代码n和t并在一起。
      - nz 其他专名
        - “专”的声母的第 1 个字母为 z，名词代码 n 和 z 并在一起。
      - o 拟声词
        - 取英语拟声词 onomatopoeia 的第 1 个字母。
      - p 介词
        - 取英语介词 prepositional 的第 1 个字母。
      - q 量词
        - 取英语 quantity 的第 1 个字母。
      - r 代词
        - 取英语代词 pronoun的第2个字母,因p已用于介词。
      - s 处所词
        - 取英语 space的第1个字母。
      - tg 时语素
        - 时间词性语素。时间词代码为 t,在语素的代码g前面置以T。
      - t 时间词
        - 取英语 time的第1个字母。
      - u 助词
        - 取英语助词 auxiliary
      - vg 动语素
        - 动词性语素。动词代码为 v。在语素的代码g前面置以V。
      - v 动词
        - 取英语动词 verb的第一个字母。
      - vd 副动词
        - 直接作状语的动词。动词和副词的代码并在一起。
      - vn 名动词
        - 指具有名词功能的动词。动词和名词的代码并在一起。
      - w 标点符号
      - x 非语素字
        - 非语素字只是一个符号，字母 x通常用于代表未知数、符号。
      - y 语气词
        - 取汉字“语”的声母。
      - z 状态词
        - 取汉字“状”的声母的前一个字母。
      - un 未知词
        - 不可识别词及用户自定义词组。取英文Unkonwn首两个字母。(非北大标准，CSW分词中定义)
    - 举例：
      - 中文/n 分词/n 是/v 其他/p 中文/n (信息,n) 处理/v 的 基础，
- 命名实体识别
  - 命名实体识别（Named Entity Recognition，简称 NER），又称作 “专名识别”。
  - 是指识别文本中具有特定意义的实体，主要包括 人名、地名、机构名、专有名词等。一般为：
    - 三大类：实体类、时间类和数字类
    - 七小类：人名、机构名、地名、时间、日期、货币和百分比 
      - 如：上海市徐汇区 和 上海徐家汇。
  - 注意：
    - 在不同的顷目中，命名实体类别具有不同的定义。
      - 比如：医疗的命名实体识别系统中：需要识别 病名，症状，药品，手术，细菌 等

## 分词

- 当分词工具分词不准确时，该怎么办？
  - 加载自定义字典。
  - 或者可以使用正则匹配的方式对一些比如 xx% 这样的形式先进行划分。


举例：

```py
# -*- coding=utf8 -*-
import jieba
import re
import hanlp

# jieba 字典
jieba.load_userdict("dict.txt")


# hanlp 字典
from hanlp.common.trie import Trie
trie = Trie()
trie.update({'奥沙利铂': 'test1',
             '氟尿嘧啶单药': 'test2',
             'Cox模型': 'test3',
             'TNM分期': 'test4',
             '||期': 'test5'})

def split_sents(text: str, trie: Trie):
    words = trie.parse_longest(text)
    sents = []
    pre_start = 0
    offsets = []
    for word, value, start, end in words:
        if pre_start != start:
            sents.append(text[pre_start: start])
            offsets.append(pre_start)
        pre_start = end
    if pre_start != len(text):
        sents.append(text[pre_start:])
        offsets.append(pre_start)
    return sents, offsets, words

def merge_parts(parts, offsets, words):
    items = [(i, p) for (i, p) in zip(offsets, parts)]
    items += [(start, [word]) for (word, value, start, end) in words]
    # In case you need the tag, use the following line instead
    # items += [(start, [(word, value)]) for (word, value, start, end) in words]
    return [each for x in sorted(items) for each in x[1]]

tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
tokenizer = hanlp.pipeline() \
    .append(split_sents, output_key=('parts', 'offsets', 'words'), trie=trie) \
    .append(tokenizer, input_key='parts', output_key='tokens') \
    .append(merge_parts, input_key=('tokens', 'offsets', 'words'), output_key='merged')


# 将按照 flag 分开的 list，与 flag 原先对应的原文 list，merge 起来。
def merge_two_list(a, b):
    c = []
    len_a, len_b = len(a), len(b)
    minlen = min(len_a, len_b)
    for i in range(minlen):
        c.append(a[i])
        c.append(b[i])

    if len_a > len_b:
        for i in range(minlen, len_a):
            c.append(a[i])
    else:
        for i in range(minlen, len_b):
            c.append(b[i])
    return c


if __name__ == "__main__":
    fp = open("text.txt", "r", encoding="utf8")
    fo_jieba = open("result_cut_jieba.txt", "w", encoding="utf8")
    fo_hanlp = open("result_cut_hanlp.txt", "w", encoding="utf8")


    regex1 = u'(?:[^\u4e00-\u9fa5（）*&……%￥$，,。.@! ！]){1,5}期' # 用来匹配 xx期
    regex2 = r'(?:[0-9]{1,3}[.]?[0-9]{1,3})%'  # 用来匹配 xx%
    p1 = re.compile(regex1)
    p2 = re.compile(regex2)


    def reg_before(line=None):
        matches1 = p1.findall(line)
        if matches1:
            regex_re1 = matches1
            line = p1.sub("FLAG1", line)  # 使用 FLAG1 替换找到的 xx期
        matches2 = p2.findall(line)
        if matches2:
            line = p2.sub("FLAG2", line)  # 使用 FLAG2 替换找到的 xx%
        return line, matches1, matches2

    # 将 flag 替换为原来的内容
    def reg_after(result=None, matches1=None, matches2=None):
        if "FLAG1" in result:
            result = result.split("FLAG1")
            result = merge_two_list(result, matches1)
            result = "".join(result)
        if "FLAG2" in result:
            result = result.split("FLAG2")
            result = merge_two_list(result, matches2)
            result = "".join(result)
        return result

    for line in fp.readlines():
        line, matches1, matches2=reg_before(line)

        words = jieba.cut(line)
        result = " ".join(words)
        result=reg_after(result,matches1,matches2)
        fo_jieba.write(result)

        words=tokenizer(line)["merged"]
        result=" ".join(words)
        result=reg_after(result,matches1,matches2)
        fo_hanlp.write(result)
    fo_jieba.close()
    fo_hanlp.close()
```

其中：

dict.txt：

```txt
奥沙利铂
氟尿嘧啶单药
Cox模型
TNM分期
65%
25%
10%
||期
II期
III期
ii期
```


text.txt：

```txt
恶性肿瘤的分期越高，患者预后越差。通过对肿瘤不同恶性程度的划分，TNM分期在预测预后方面不断完善。
但是有国外研究发现，部分结肠癌的预后并不能完全按照一般的分期阶梯进行预测。
TNM分期不太能明确地区分||期和III期结肠癌患者的预后ii期，特别是在接受辅助化疗的患者中，他们的5年总生存期在50.1%-9.8%。
此外已知影响结肠癌生存的患者或疾病特征，包括年龄、性别、原发疾病位置、肿瘤分级、阳性淋巴结数目（LNs）、接受检查的
LNs数目、淋巴管和周围神经浸润、肠道梗阻或穿孔、以及辅助治疗（氟尿嘧啶单药或联合奥沙利铂），并未直接纳入TNM分期系统。
在多变量模型中，分子标记的微卫星不稳定性（MSI）和BRAF或KRAS基因突变联合详细的临床病理学诠释可以多大程度改善预后评估
目前尚不清楚。近期，发表在Annals of oncology杂志上的一项回顾性研究，在TNM分期系统基础上联合汇集的标志物对II期和III期
结肠癌总生存期进行预测。
研究人员将缺失随机数据插补后，利用3期辅助化疗试验（n=3016）—N0147（NCT00079274）和PETACC3（NCT00026273）—产生的患者亚组
聚集构建了一个5年总生存期多变量Cox模型，随后在剩余的临床试验样本（n=1499）中进行内部验证，并且还在不同人群队列中外部分析，
包括接受化疗（n=949）或者未接受化疗（n=1080）的结肠癌患者，以及没有治疗注释患者。
研究分析发现：
在根据临床试验队列和观察性研究做出的多变量模型中，TMN分期，MSI和BRAFV600E基因突变状态仍然是独立预后因素。
单纯TNM模型的一致性指数（Concordance-indices）为0.61-0.68，而增加分子标记物、临床病理特征和所有协变量后的
一致性指数分别增加至0.63-0.71、0.65-0.73和0.66-0.74。
在有完整注释的验证队列中，单独TNM的综合时间依赖AUC值为0.64，纳入临床病理特征联合或不联合分子标记物的AUC增加为0.67。
在接受辅助化疗的患者队列中，通过TNM、临床病理特征和分子标记物的方差（R2）相对比例平均值分别为65%、25%和10%。
因此，将MSI、BRAFV600E和KRAS基因突变状态纳入TNM分期系统的总生存期模型可以提高精确预测II期和III期结肠癌患者的能力，
而且包括临床病理特征的多变量模型中会增加预测准确性，特别是需要接受化疗的患者。
```

输出：

result_cut_jieba.txt：

```txt
恶性肿瘤 的 分期 越高 ， 患者 预后 越差 。 通过 对 肿瘤 不同 恶性 程度 的 划分 ， TNM分期 在 预测 预后 方面 不断完善 。 
但是 有 国外 研究 发现 ， 部分 结肠癌 的 预后 并 不能 完全 按照 一般 的 分期 阶梯 进行 预测 。 
TNM分期 不太能 明确 地区 分 ||期 和 III期 结肠癌 患者 的 预后 ii期 ， 特别 是 在 接受 辅助 化疗 的 患者 中 ， 他们 的 5 年 总 生存期 在 50.1% - 9.8% 。 
此外 已知 影响 结肠癌 生存 的 患者 或 疾病 特征 ， 包括 年龄 、 性别 、 原发 疾病 位置 、 肿瘤 分级 、 阳性 淋巴结 数目 （ LNs ） 、 接受 检查 的 
LNs 数目 、 淋巴管 和 周围神经 浸润 、 肠道 梗阻 或 穿孔 、 以及 辅助 治疗 （ 氟尿嘧啶单药 或 联合 奥沙利铂 ） ， 并未 直接 纳入 TNM分期 系统 。 
在 多 变量 模型 中 ， 分子 标记 的 微卫星 不稳定性 （ MSI ） 和 BRAF 或 KRAS 基因突变 联合 详细 的 临床 病理学 诠释 可以 多 大程度 改善 预后 评估 
目前 尚 不 清楚 。 近期 ， 发表 在 Annals   of   oncology 杂志 上 的 一项 回顾性 研究 ， 在 TNM分期 系统 基础 上 联合 汇集 的 标志物 对 II期 和 III期 
结肠癌 总 生存期 进行 预测 。 
研究 人员 将 缺失 随机 数据 插补后 ， 利用 3期 辅助 化疗 试验 （ n = 3016 ） — N0147 （ NCT00079274 ） 和 PETACC3 （ NCT00026273 ） — 产生 的 患者 亚组 
聚集 构建 了 一个 5 年 总 生存期 多 变量 Cox模型 ， 随后 在 剩余 的 临床试验 样本 （ n = 1499 ） 中 进行 内部 验证 ， 并且 还 在 不同 人群 队列 中 外部 分析 ， 
包括 接受 化疗 （ n = 949 ） 或者 未 接受 化疗 （ n = 1080 ） 的 结肠癌 患者 ， 以及 没有 治疗 注释 患者 。 
研究 分析 发现 ： 
在 根据 临床试验 队列 和 观察 性 研究 做出 的 多 变量 模型 中 ， TMN 分期 ， MSI 和 BRAFV600E 基因突变 状态 仍然 是 独立 预后 因素 。 
单纯 TNM 模型 的 一致性 指数 （ Concordance - indices ） 为 0.61 - 0.68 ， 而 增加 分子 标记 物 、 临床 病理 特征 和 所有 协 变量 后 的 
一致性 指数 分别 增加 至 0.63 - 0.71 、 0.65 - 0.73 和 0.66 - 0.74 。 
在 有 完整 注释 的 验证 队列 中 ， 单独 TNM 的 综合 时间 依赖 AUC 值为 0.64 ， 纳入 临床 病理 特征 联合 或 不 联合 分子 标记 物 的 AUC 增加 为 0.67 。 
在 接受 辅助 化疗 的 患者 队列 中 ， 通过 TNM 、 临床 病理 特征 和 分子 标记 物 的 方差 （ R2 ） 相对 比例 平均值 分别 为 65% 、 25% 和 10% 。 
因此 ， 将 MSI 、 BRAFV600E 和 KRAS 基因突变 状态 纳入 TNM分期 系统 的 总 生存期 模型 可以 提高 精确 预测 II期 和 III期 结肠癌 患者 的 能力 ， 
而且 包括 临床 病理 特征 的 多 变量 模型 中 会 增加 预测 准确性 ， 特别 是 需要 接受 化疗 的 患者 。
```

result_cut_hanlp.txt：

```txt
恶性肿瘤 的 分期 越 高 ， 患者 预后 越 差 。 通过 对 肿瘤 不同 恶性 程度 的 划分 ， TNM分期 在 预测 预后 方面 不断 完善 。 
但是 有 国外 研究 发现 ， 部分 结肠癌 的 预后 并 不 能 完全 按照 一般 的 分期 阶梯 进行 预测 。 
TNM分期 不 太 能 明确 地区 分 ||期 和 III期 结肠癌 患者 的 预后 ii期 ， 特别 是 在 接受 辅助 化疗 的 患者 中 ， 他们 的 5 年 总 生存期 在 50.1% - 9.8% 。 
此外 已 知 影响 结肠癌 生存 的 患者 或 疾病 特征 ， 包括 年龄 、 性别 、 原发 疾病 位置 、 肿瘤 分级 、 阳性 淋巴 结 数目 （ LNs ） 、 接受 检查 的 
LNs 数目 、 淋巴管 和 周围 神经 浸润 、 肠道 梗阻 或 穿孔 、 以及 辅助 治疗 （ 氟尿嘧啶单药 或 联合 奥沙利铂 ） ， 并 未 直接 纳入 TNM分期 系统 。 
在 多 变量 模型 中 ， 分子 标记 的 微卫星 不 稳定性 （ MSI ） 和 BRAF 或 KRAS 基因 突变 联合 详细 的 临床 病理学 诠释 可以 多 大 程度 改善 预后 评估 
目前 尚 不 清楚 。 近期 ， 发表 在 Annals   of   oncology 杂志 上 的 一 项 回顾性 研究 ， 在 TNM分期 系统 基础 上 联合 汇集 的 标志物 对 II期 和 III期 
结肠癌 总 生存期 进行 预测 。 
研究 人员 将 缺失 随机 数据 插补 后 ， 利用 3期 辅助 化疗 试验 （ n = 3016 ） — N0147 （ NCT00079274 ） 和 PETACC3 （ NCT00026273 ） — 产生 的 患者 亚组 
聚集 构建 了 一个 5 年 总 生存期 多 变量 Cox模型 ， 随后 在 剩余 的 临床 试验 样本 （ n = 1499 ） 中 进行 内部 验证 ， 并且 还 在 不同 人群队 列 中外部 分析 ， 
包括 接受 化疗 （ n =949 ） 或者 未 接受 化疗 （ n = 1080 ） 的 结肠癌 患者 ， 以及 没有 治疗 注释 患者 。 
研究 分析 发现 ： 
在 根据 临床 试验 队列 和 观察性 研究 做出 的 多变量 模型 中 ， TMN 分期 ， MSI 和 BRAFV600E 基因 突变 状态 仍然 是 独立 预后 因素 。 
单纯 TNM 模型 的 一致性 指数 （ Concordance - indices ） 为 0.61 - 0.68 ， 而 增加 分子 标记物 、 临床 病理 特征 和 所有 协变量 后 的 
一致性 指数 分别 增加 至 0.63 - 0.71 、 0.65 - 0.73 和 0.66 - 0.74 。 
在 有 完整 注释 的 验证 队列 中 ， 单独 TNM 的 综合 时间 依赖 AUC值 为 0.64 ， 纳入 临床 病理 特征 联合 或 不 联合 分子 标记物 的 AUC 增加 为 0.67 。 
在 接受 辅助 化疗 的 患者 队列 中 ， 通过 TNM 、 临床 病理 特征 和 分子 标记物 的 方差 （ R2 ） 相对 比例 平均值 分别 为 65% 、 25% 和 10% 。 
因此 ， 将 MSI 、 BRAFV600E 和 KRAS 基因 突变 状态 纳入 TNM分期 系统 的 总生 存期 模型 可以 提高 精确 预测 II期 和 III期 结肠癌 患者 的 能力 ， 
而且 包括 临床 病理 特征 的 多变量 模型 中 会 增加 预测 准确性 ， 特别 是 需要 接受 化疗 的 患者 。
```


说明：

- 没有使用字典前，“氟尿嘧啶 单药 或 联合 奥沙利 铂” 被分开了。使用字典后，分为：“氟尿嘧啶单药 或 联合 奥沙利铂”。



**使用 suggest_freq 来使得字典内的词频较高：**

```py
import jieba
jieba.load_userdict("dict.txt")
# jieba.suggest_freq('确与', tune=True)
[jieba.suggest_freq(line.strip(), tune=True) for line in open("dict.txt", 'r', encoding='utf8')]

if __name__ == "__main__":
    string = "测试正确与否。"
    words=list(jieba.cut(string))
    print(words)
```

其中，

dict.txt ：

```txt
确与
```

输出：

```txt
['测试', '正确', '与否', '。']
```


疑问:

- 为什么这个没有把 确与 划分到一起？


**解决 数据库管理 与 数据库 这类问题**

可以将字典按照字符长度进行排序。

（是这样吗？这个地方举的例子时老的 java 的例子，想知道现在的 jieba 和 hanlp 怎么处理这个）


## 词性标注 

使用词性标注对简历进行过滤：


- 使用词性对一些语句进行过滤。
- 对时间数字进行过滤


举例：

```py
import hanlp
import re

tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)

keep_pos = "q,qg,qt,qv,s,t,tg,g,gb,gbc,gc,gg,gm,gp,m,mg,Mg,mq,n,an,vn,ude1,nr,ns,nt,nz,nb,nba,nbc,nbp,nf,ng,nh,nhd,o,nz,nx,ntu,nts,nto,nth,ntch,ntcf,ntcb,ntc,nt,nsf,ns,nrj,nrf,nr2,nr1,nr,nnt,nnd,nn,nmc,nm,nl,nit,nis,nic,ni,nhm,nhd"
keep_pos_nouns = set(keep_pos.split(","))
keep_pos_v = "v,vd,vg,vf,vl,vshi,vyou,vx,vi"
keep_pos_v = set(keep_pos_v.split(","))
keep_pos_p = {'p', 'pbei', 'pba'}
drop_pos_set = {'xu', 'xx', 'y', 'yg', 'wh', 'wky', 'wkz', 'wp', 'ws',
                'wyy', 'wyz', 'wb', 'u', 'ud', 'ude1', 'ude2',
                'ude3', 'udeng', 'udh', 'p', 'rr'}
han_pattern = re.compile(r'[^\dA-Za-z\u3007\u4E00-\u9FCB\uE815-\uE864]+')


def seg_sentences_with_filter(sentence):
    split = tokenizer(sentence)
    tag = tagger(split)
    pair_list = [x for x in zip(split, tag)]
    result = [x[0] for x in pair_list if x[0] != " " and x[1].lower() not in drop_pos_set]
    return result

if __name__ == "__main__":
    fp = open("text.txt", 'r', encoding='utf8')
    fout = open("out.txt", 'w', encoding='utf8')
    for line in fp:
        line = line.strip()
        if len(line) > 0:
            fout.write(' '.join(seg_sentences_with_filter(line)) + "\n")
    fout.close()
```

原文：

text.txt：

```txt
自我评价

本人诚实正直，对工作认真负责，吃苦耐劳，善于创新，敢于迎接挑战及承担责任，富有工作热情，乐业敬业，善于与人沟通。营造和谐的工作氛围，注重人性化管理，能带动下属充分发挥团队合作精神，为公司创造效益！

 

求职意向

到岗时间：一个月之内

工作性质：全职

希望行业：通信/电信运营

目标地点：广州

期望月薪：面议/月

目标职能：数据分析专员

 

工作经验

2014/11 — 2015/9：XX有限公司[10个月]

所属行业：通信/电信运营

数据部　　数据分析专员

1.   数据库日常简单维护，熟悉SQL查询语句。

2.   数据分析，协助客户定位网络疑问问题。

3.   投诉建模，通过匹配大量的投诉用户及其上网行为，分析其可能投诉的原因并进行建模。

 

2013/5 — 2014/10：XX有限公司[1年5个月]

所属行业：通信/电信运营

数据部　　数据分析专员

1.   日常办公用品采购，基站租赁合同处理及工程物资采购。

2.   ERP项目支出入账及物资装配，投诉工单处理，通信基站故障处理。

3.   按排会议室，会议记要记录及整理，公文编辑分发。
```


输出：

out.txt：

```txt
 自我 评价
本人 诚实 正直 ， 工作 认真 负责 ， 吃苦耐劳 ， 善于 创新 ， 敢于 迎接 挑战 及 承担 责任 ， 富有 工作 热情 ， 乐业敬业 ， 善于 人 沟通 。 营造 和谐 的 工作 氛围 ， 注重 人性化 管理 ， 能 带动 下属 充分 发挥 团队 合作 精神 ， 公司 创造 效益 ！
求职 意向
到岗 时间 ： 一个 月 之内
工作 性质 ： 全职
希望 行业 ： 通信 / 电信 运营
目标 地点 ： 广州
期望 月薪 ： 面议 / 月
目标 职能 ： 数据 分析 专员
工作 经验
2014/11 — 2015/9 ：XX 有限公司 [ 10 个 月 ]
所属 行业 ： 通信 / 电信 运营
数据部 　 　 数据 分析 专员
1.     数据库 日常 简单 维护 ， 熟悉 SQL 查询 语句 。
2.     数据 分析 ， 协助 客户 定位 网络 疑问 问题 。
3.     投诉 建模 ， 匹配 大量 的 投诉 用户 及 其 上网 行为 ， 分析 其 可能 投诉 的 原因 并 进行 建模 。
2013/5 — 2014/10 ： XX 有限公司 [ 1 年 5 个 月 ]
所属 行业 ： 通信 / 电信 运营
数据部 　 　 数据 分析 专员
1.     日常 办公 用品 采购 ， 基站 租赁 合同 处理 及 工程 物资 采购 。
2.     ERP 项目 支出 入账 及 物资 装配 ， 投诉 工单 处理 ， 通信 基站 故障 处理 。
3.     排 会议室 ， 会议记 要 记录 及 整理 ， 公文 编辑 分发 。
```


## 命名实体识别

3、5人名、地名、机构名等关键命名实体识别


海量的 JD 和 海量的简历，怎么进行匹配。


```py
import os
import re
import nltk, json
import hanlp
import re
from itertools import chain

tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)


def get_stanford_ner_nodes(parent):
    date = ''
    org = ''
    loc = ''
    for node in parent:
        if type(node) is nltk.Tree:
            if node.label() == 'DATE':
                date = date + " " + ''.join([i[0] for i in node])
            elif node.label() == 'ORGANIZATIONL':
                org = org + " " + ''.join([i[0] for i in node])
            elif node.label() == 'LOCATION':
                loc = loc + " " + ''.join([i[0] for i in node])
    if len(date) > 0 or len(org) > 0 or len(loc) > 0:
        return {'date': date, 'org': org, 'loc': loc}
    else:
        return {}


def grammer_parse(raw_sentence=None):
    if len(raw_sentence.strip()) < 1:
        return False
    recognized = recognizer(list(raw_sentence))
    ner_pair_list = [tuple(x[0:2]) for x in recognized]

    grammer_dict = {
        'ner_drop': r"""
        DATE:{<DATE>+<MISC>?<DATE>*}
        {<DATE>+<MISC>?<DATE>*}
        {<DATE>+}
        {<TIME>+}
        ORGANIZATIONL:{<ORGANIZATION>+}
        LOCATION:{<LOCATION|STATE_OR_PROVINCE|CITY|COUNTRY>+}
        """
    }
    ner_rop_rp = nltk.RegexpParser(grammer_dict['ner_drop'])
    ner_drop_result = ner_rop_rp.parse(ner_pair_list)
    stanford_keep_drop_dict = get_stanford_ner_nodes(ner_drop_result)
    return stanford_keep_drop_dict


if __name__ == "__main__":
    with open('text.txt', 'r', encoding='utf-8') as f:
        tree_list = [grammer_parse(line.strip()) for line in f if len(line.strip()) > 0]
    print(tree_list)
    with open('out.txt', 'w', encoding='utf-8') as f:
        for d in tree_list:
            if len(d) > 0:
                f.write(json.dumps(d, skipkeys=False,
                                   ensure_ascii=False,
                                   check_circular=True,
                                   allow_nan=True,
                                   cls=None,
                                   indent=4,
                                   separators=None,
                                   default=None,
                                   sort_keys=False))
```

其中：

text.txt：

```txt
2017年5月带领欧洲和国内的双边项目团队在国外工厂和本土工厂间进行新产品和模具项目的国际转移。搭建外国人成员和中国人成员间的沟通平台，通过定期的电视电话会议组织欧洲团队和中国团队的项目交流。循序渐进地让双方理解不同国家间的文化差异，从而在推进项目的节奏上取得同步。
制定组织规则，约束外国团队和中国团队。如欧洲成员休暑假必须预先知会项目组并安排妥当工作交接手续，不得对项目造成人为影响。
由于中国成员长在面对外国人时缺乏自信，因此培训中国团队成员面对外国人的平等意识，提升其自信心和就事论事一切从工作角度看问题的观念是我的其中一项重要的工作内容。
分析产品成本结构，优化产品设计，推动综合成本降低。
开发ODMOEM供应商，引进新产品。产品成本分析，供应商报价询盘，竞标。价格，交货期，付款条件及运输条款等的谈判。推进本地化供应。
模具价格预算,基于项目寿命和年需求量作出产能分析。
成功的实施了模具,五金塑胶组件,工艺设备的本地化生产,本地化比例由原来的15%增长到100%。
确定模具的设计规格(模穴数,模具寿命,模具结构,分模线,入水方式,行位结构,表面处理,产品功能要求),定义技术要求和设计方案。
模具制造,改模，注塑工艺设定，装配工艺设计，新产品试产,模具设备维修,SPC控制,日常生产的技术支持
注射条件的定义和改进,注塑机参数制定和调整。
提供其他技术支持。
每年300套新模具开发，2500套模具生产维护和改模。
```

输出：

out.txt：

```txt
```


说明：

- 没有使用 stanford nlp，它运行到要创建 host 的时候，就停住了。可能是内存不够，或者别的原因。
- 但是使用 hanlp 好像也是有问题的，hanlp 好像只能识别出 国家 人名 等。没法识别出别的，比如时间，
- 这个程序有问题，输出的是空白。
- 应该的输出：
  - 使用 stanford nlp 命名识别出的内容应该是类似：
    <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200712/2SMnMXGn0fLf.png?imageslim">
    </p>
  - 然后使用 nltk.RegexpParser 的规则，对识别出的内容中 DATA 进行合并。
  - 这时候就形成了一个 tree:
    <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200712/KUkNIyFrPOhm.png?imageslim">
    </p>
  - 可以使用 ner_drop_result.draw() 把 tree 画出来：
    <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200712/lli0tbCbMlSH.png?imageslim">
    </p>
  - get_stanford_ner_nodes 函数就是为了遍历树，吧 data org loc 提取出来。
  - 应该的 out.txt 为：
    ```txt
    {
        "date": " 2017年5月",
        "org": "",
        "loc": " 欧洲 欧洲 中国"
    }{
        "date": "",
        "org": "",
        "loc": " 中国 欧洲"
    }{
        "date": "",
        "org": "",
        "loc": " 中国 中国"
    }{
        "date": "",
        "org": " SPC",
        "loc": ""
    }
    ```

  
## TextRank

（为什么这个放在这里？）

TextRank：

- 介绍：
  - 类似于 PageRank 的思想，将文本中的 语法 单元 视作图中的节点。
    - 如果两个语法单元存在一定语法关系（例如共现），则这两个语法单元在图中就会有一条边相互连接。
    - 通过一定的迭代次数，最终不同的节点会有不同的权重，权重高的语法单元可以作为关键词。
      - 节点的权重不仅依赖于它的入度结点，还依赖于这些入度结点的权重，入度结点越多，入度结点的权重越大，说明这个结点的权重越高；
- 作用：
  - 可以用来提取关键词。
- 优点：
  - 计算简单，计算速度快
  - 不用标注样本。
- TextRank 迭代计算
  - 公式：
    $$W S\left(V_{i}\right)=(1-d)+d * \sum_{V_{j} \in I n\left(V_{i}\right)} \frac{w_{j i}}{\sum_{V_{k} \in O u t\left(V_{j}\right)} w_{j k}} W S\left(V_{j}\right)$$
  - 说明：
    - $W S\left(V_{i}\right)$ 是节点的权重。
    - $w_{ji}$ 为任两点 $V_i$ , $V_j$ 之间连接的次数。
      - 注意：$w_{ji}$ 不是可变的权重，而是从样本中统计得到的次数。
    - 对于一个给定的点 $V_i$, $In(V_i)$ 为指向该点的点集合 , $Out(V_i)$ 为点 $Vi$ 指向的点集合。
    - $d$ 为阻尼系数, 取值范围为 0 到 1, 代表从图中某一特定点指向其他任意点的概率, 一般取值为 0.85。
    - 使用 TextRank 算法计算图中各点的得分时, 需要给图中的点指定任意的初值, 并递归计算直到收敛, 即图中任意一点的误差率小于给定的极限值时就可以达到收敛, 一般该极限值取 0.0001。
- 算法通用流程：**应用到关键短语抽取：**
  - 01 预处理，首先进行分词和词性标注，将单个 word 作为结点添加到图中；
  - 02 设置语法过滤器，将通过语法过滤器的词汇添加到图中；出现在一个窗口中的词汇之间相互形成一条边；
    - 窗口即滑动窗口。
  - 03 基于上述公式，迭代直至收敛；一般迭代 20-30 次，迭代阈值设置为 0.0001；
  - 04 根据顶点的分数降序排列，并输出指定个数的词汇作为可能的关键词；
  - 05 后处理，如果两个词汇在文本中前后连接，那么就将这两个词汇连接在一起，作为关键短语；





jieba textrank.py 源码分析：

```py
from __future__ import absolute_import, unicode_literals
import sys
from operator import itemgetter
from collections import defaultdict
import jieba.posseg
from .tfidf import KeywordExtractor
from .._compat import *


class UndirectWeightedGraph:
    d = 0.85

    def __init__(self):
        self.graph = defaultdict(list)

    def addEdge(self, start, end, weight):
        # use a tuple (start, end, weight) instead of a Edge object
        self.graph[start].append((start, end, weight))
        self.graph[end].append((end, start, weight))

    def rank(self):
        ws = defaultdict(float)
        outSum = defaultdict(float)

        wsdef = 1.0 / (len(self.graph) or 1.0)
        for n, out in self.graph.items():
            ws[n] = wsdef # 对每个结点赋予相同的权重
            outSum[n] = sum((e[2] for e in out), 0.0) # 计算 与该节点，相连的边 上面的次数的和。注意，这个是次数的和，不是权重的和。

        # this line for build stable iteration
        sorted_keys = sorted(self.graph.keys())
        for x in xrange(10):  # 10 iters
            for n in sorted_keys:
                s = 0
                for e in self.graph[n]: # 遍历从 start 开始的 边列表。
                    s += e[2] / outSum[e[1]] * ws[e[1]] # e[2] 为边上统计得到的次数，outSum[e[1]] 为 end 这个点的出度，ws[e[1]] 为 start 这个点的权重
                ws[n] = (1 - self.d) + self.d * s

        (min_rank, max_rank) = (sys.float_info[0], sys.float_info[3]) # 限定最大最小值

        for w in itervalues(ws):
            if w < min_rank:
                min_rank = w
            if w > max_rank:
                max_rank = w
        # 归一化
        for n, w in ws.items():
            # to unify the weights, don't *100.
            ws[n] = (w - min_rank / 10.0) / (max_rank - min_rank / 10.0)

        return ws


class TextRank(KeywordExtractor):

    def __init__(self):
        self.tokenizer = self.postokenizer = jieba.posseg.dt
        self.stop_words = self.STOP_WORDS.copy()
        self.pos_filt = frozenset(('ns', 'n', 'vn', 'v'))
        self.span = 5

    def pairfilter(self, wp):
        return (wp.flag in self.pos_filt and len(wp.word.strip()) >= 2
                and wp.word.lower() not in self.stop_words)

    def textrank(self, sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'), withFlag=False):
        """
        Extract keywords from sentence using TextRank algorithm.
        Parameter:
            - topK: return how many top keywords. `None` for all possible words.
            - withWeight: if True, return a list of (word, weight);
                          if False, return a list of words.
            - allowPOS: the allowed POS list eg. ['ns', 'n', 'vn', 'v'].
                        if the POS of w is not in this list, it will be filtered.
            - withFlag: if True, return a list of pair(word, weight) like posseg.cut
                        if False, return a list of words
        """
        self.pos_filt = frozenset(allowPOS)
        g = UndirectWeightedGraph() # 无向有权图
        cm = defaultdict(int) # 共现词典
        words = tuple(self.tokenizer.cut(sentence)) # 分词
        for i, wp in enumerate(words):
            if self.pairfilter(wp):
                # 对于满足过滤条件的词 i ，遍历之后窗口范围内的词
                for j in xrange(i + 1, i + self.span):
                    if j >= len(words): # 词 j 不能超出整个句子
                        break
                    if not self.pairfilter(words[j]): # 词 j 不满足过滤条件，则跳过
                        continue
                    # 将词 i 和词 j 作为key，出现的次数作为 value，添加到共现词典中
                    if allowPOS and withFlag:
                        cm[(wp, words[j])] += 1
                    else:
                        cm[(wp.word, words[j].word)] += 1
        # 遍历共现词典的每个元素，将词 i，词 j 作为一条边起始点和终止点，共现的次数作为边的权重，添加到无向有权图中。
        for terms, w in cm.items():
            g.addEdge(terms[0], terms[1], w)
        # 运行textrank算法
        nodes_rank = g.rank()
        # 根据指标值进行排序
        if withWeight:
            tags = sorted(nodes_rank.items(), key=itemgetter(1), reverse=True)
        else:
            tags = sorted(nodes_rank, key=nodes_rank.__getitem__, reverse=True)
        # 输出topK个词作为关键词
        if topK:
            return tags[:topK]
        else:
            return tags

    extract_tags = textrank
```


- 说明：
  - 流程：
    - 首先定义一个无向有权图。
    - 然后对句子进行分词。
    - 遍历分词结果，得到 共现 字典：
      - 将不满足如下过滤条件的词去除：词性在词性过滤集合中，并且词的长度大于等于2，并且词不是停用词。
      - 再次遍历过滤后的结果，对于某个词 $i$ ，将它之后窗口长度为 $5$ 范围内的词 $j$ 作为一对，在遍历的过程中，统计这种词对出现的次数 $num$，遍历后，得到一个 以 $(i,j)$ 为 key，出现的次数 $num$ 作为 value 的字典。
        - 注意：$(i,j)$ 与 $(j,i)$ 为不同的词对，分开计数。
    - 遍历词对字典，添加入无向有权图：
      - 遍历字典，对于每个元素，$key = (i,j)$，$value = num$，将词 $i$ 作为一条边的起始点，将词 $j$ 作为一条边的终点，将次数 $num$ 作为这条边的值，添加到 之前定义的无向有权图中。
    - 对这个无向有权图迭代 textrank 算法。
      - 经过若干次迭代后，算法收敛。
    - 这时，每个词都对应一个指标值。此时，如果设置了权重标志位，则根据指标值值对无向有权图中的词进行降序排序，最后输出topK个词作为关键词；
  - TextRank
    - TextRank 是为 TextRank 算法抽取关键词所定义的类。
    - 类在初始化时，默认加载了
      - 分词函数和词性标注函数 `self.tokenizer = self.postokenizer = jieba.posseg.dt`
      - 停用词表 `self.stop_words = self.STOP_WORDS.copy()`
      - 词性过滤集合 `self.pos_filt = frozenset(('ns', 'n', 'vn', 'v'))`
        - (("ns", "n", "vn", "v")) 表示词性为地名、名词、动名词、动词。这些词性是准备保存的。
      - 窗口 `self.span = 5`
  - UndirectWeightedGraph
    - UndirectWeightedGraph 类包括了无向有权图的的定义及实现。
    - 从 `__init__` 可知，所谓的无向有权图就是一个词典，词典的 key 是后续要添加的词，词典的 value，则是一个由（起始点，终止点，边的权重）构成的三元组所组成的列表，表示以这个词作为起始点的所有的边。
    - `addEdge` 函数
      - 用来向无向有权图添加边。因为是无向图，所以我们需要依次将 start 作为起始点，end 作为终止点，然后再将 start 作为终止点，end 作为起始点。
      - 注意：
        - 如果 共现字典中有 $((i,j),5)$ 和 $((j,i),3)$ 两个 item，它们表明的是，有 $5$ 个  $i,j$ 这样的 pair ，有 $3$ 个 $j,i$ 这样的 pair。但是，当我们用 addEdge 函数对边进行添加时，表现为：对 $i$ 节点，添加了 $(i,j,5)$、$(i,j,3)$ 这两个 item，对于 $j$ 节点，添加了 $(j,i,5)$、$(j,i,3)$ 这两个节点。
        - 这样的话，其实，i 节点的 list 中包含了以它为目标的，和 从它出来的。
    - `rank` 函数
      - 用来执行 textrank 算法的迭代。
      - 流程：
        - 首先对每个结点赋予相同的权重，以及计算出该结点的所有出度的次数之和；
          - 注意：`sum((e[2] for e in out), 0.0)` 由于是无向的，所以是把 从它出来，和，到它哪里的，都加起来，作为它的出度。它的出度与入度相同。
        - 然后迭代若干次，以确保得到稳定的结果；
          - 在每一次迭代中，依次遍历每个结点；
            - 对于结点 $n$，
              - 首先根据无向有权图得到结点 $n$ 的所有入度结点
                - 对于无向有权图，入度结点与出度结点是相同的，都是与结点 $n$ 相连的结点，
              - 在前面我们已经计算出这个入度结点的所有出度的次数，而 它对于结点 $n$ 的权值的贡献=它与结点$n$的共现次数 $\times$  它本身的权值/ 这个结点的所有出度的次数 。
            - 将各个入度结点得到的权值相加，再乘以一定的阻尼系数，即可得到结点 $n$ 的权值。
        - 迭代完成后，对权值进行归一化，并返回各个结点及其对应的权值。


举例：

```py
from jieba import analyse

textrank = analyse.textrank
text = "非常线程是程序执行时的最小单位，它是进程的一个执行流，\ 是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\ 线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\ 线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\ 同样多线程也可以实现并发操作，每个请求分配一个线程来处理。"
keywords = textrank(text, topK=10, withWeight=True, allowPOS=('ns', 'n'))
print(keywords)
words = [keyword for keyword, w in keywords if w > 0.2]
print(' '.join(words))
```

输出：

```txt
[('线程', 1.0), ('进程', 0.7718555730764052), ('单位', 0.7628257229134912), ('基本', 0.6163056904534447), ('调度', 0.48460708891084064), ('分派', 0.4709678926359732), ('局部变量', 0.373179706115229), ('堆栈', 0.3714344364616641), ('资源', 0.3373493149010804), ('程序执行', 0.3354947586849095)]
线程 进程 单位 基本 调度 分派 局部变量 堆栈 资源 程序执行
```
