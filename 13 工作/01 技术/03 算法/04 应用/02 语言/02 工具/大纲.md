# 大纲


常用类库：

- numpy
- NLTK
  - Natural Language Toolkit，自然语言处理工具包，在NLP领域中，最常使用的一个Python库。
  - `pip install nltk`
  - 使用：
    - 对句法树的树形结构进行遍历，提取树的主干，叶子节点
    - 定义文法，语法。
    - 可以进行逻辑推理，lambda 演算，一阶逻辑等。
- Gensim
  - Gensim 是一个占内存低，接口简单，免费的Python库，它可以用来从文档中自劢提取语义主题。比 NLTK 高级一点。
  - 它包含了很多非监督学习算法如：
    - TF/IDF，
    - 潜在语义分析（Latent Semantic Analysis，LSA）、
    - 隐含狄利克雷分配（Latent Dirichlet Allocation，LDA），
    - 层次狄利克雷过程（ Hierarchical Dirichlet Processes ，HDP ）
      - 在做 LDA 前，需要确定主题数量，可以使用 HDP 确定主题的数量。
    - 等。
  - Gensim 支持 Word2Vec,Doc2Vec 等模型。
    - word2vec 应用非常广泛。可以经常使用 gensim 训练 word2vec 
  - `pip install gensim`
- Tensorflow
  - TensorFlow 是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个戒多个CPU（戒GPU），服务器，移劢设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用性使其也可广泛用于其他计算领域。
  - `Pip install tensorflow` 或 `Pip install tf-nightly-gpu`/cpu 后一个版本可调试。（是这样吗？什么时候用到？）
  - 疑问：
    - tensorflow 现在还需要与 spark 结合吗？还是可以代替 spark？
- jieba
  - “结巴”中文分词：是广泛使用的中文分词工具，具有以下特点：
    - 三种分词模式：精确模式，全模式和搜索引擎模式
    - 词性标注和返回词语在原文的起止位置（ Tokenize）
    - 可加入自定义字典
      - 对于生僻的词，可以加入字典进行分词。
    - 支持多种语言，支持简体繁体
  - 地址：https://github.com/fxsjy/jieba
- Stanford NLP
  - Stanford NLP提供了一系列自然语言分析工具。它能够给出基本的词形，词性，不管是公司名还是人名等，格式化的日期，时间，量词，并且能够标记句子的结构，语法形式和字词依赖，指明那些名字指向同样的实体，指明情绪，提取发言中的开放关系等。
  - 功能：
    - 一个集成的语言分析工具集；
    - 进行快速，可靠的任意文本分析；
    - 整体的高质量的文本分析;
    - 支持多种主流语言;
    - 多种编程语言的易用接口;
    - 方便的简单的部署web服务。
  - 支持中文。
- Hanlp
  - HanLP是由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。
  - 功能：中文分词 词性标注 命名实体识别 依存句法分析 关键词提取新词发现 短语提取 自动摘要 文本分类 拼音简繁
  - 比 jieba 功能要强一些。

