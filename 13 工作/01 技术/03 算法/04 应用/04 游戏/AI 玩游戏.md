
作者： Vino Wan @ Tencent Game AI Research Center (TGAIRC)

本文内容包含（但不限于）以下章节：

Chapter 3 Playing Games

本书英文版： Artificial Intelligence and Games - A Springer Textbook4



## 利用AI玩游戏

## 为何使用AI来玩游戏

选择AI来玩游戏之前，我们需要明确两个问题：

1. AI的目标是为了获胜，还是为了提高游戏体验？

在棋牌类游戏或者FPS类游戏中，提供一个高性能的AI能增加游戏的挑战性；而在另一些游戏中，比如模拟人生和MineCraft等，这些游戏并没有一个明确的获胜条件，玩家在这些游戏中主要目标也不是获胜，而是为了诸如消磨时间、放松心情或者是交友的目标，那么在这些游戏中就需要一个能够优化游戏体验的AI。

1. AI的目标是为了模拟人类角色（Player Character，后面简称为PC）还是为了模拟非人类角色（Non Player Character，后面简称为NPC）？

有一些游戏希望AI能够模拟人类角色（PC），比如在CS中有一些角色可以由AI来控制，而这些游戏使用AI的目的可能是人数不够，需要AI来补充人数，也可能是为了故意输给玩家，而又不让玩家感知角色是AI控制的。而在另一些游戏中，比如很多角色扮演类游戏（Role-Playing Game）中就存在大量的NPC，这些NPC需要AI来使得游戏更加流畅的运行，提高游戏的体验。

所以对AI的需求可以划分为如下图所示的4个类别：

![img](https://pic4.zhimg.com/80/v2-110cdc98214cbf0225707f6fcf92c4a7_hd.jpg)



## 游戏设计和AI设计的考量

游戏本身的特征以及AI算法的特征决定了如何在游戏中选择合适的AI。

按照Characteristics of Games中游戏划分的方法，游戏可以按照可观察性（observability）、随机性（stochasticity）以及时间粒度（time granularity）这三个维度划分成如下图所示的类别：



![img](https://pic2.zhimg.com/80/v2-a3bc74764d5ee4a5408a4ebe23c34f81_hd.jpg)



而这些游戏特征又会影响对AI算法的选择。比如在具有随机性的游戏中就不能使用MCTS算法，即使是增强学习也需要对模型做一些调整；而完美信息和非完美信息的建模方法也有很大的不同，比如在非完美信息情况下，就需要通过ISMCTS（Information Set Monte Carlo Tree Search）的方案来做树搜索；有些游戏对实时性要求不高，就可以使用基于规划的方法，比如树搜索等，而对实时性要求高的游戏就必须尽快的返回评估结果，所以也就无法使用基于规划的方法。

AI算法本身的一些特性也会影响对AI算法的选择。比如算法的输入是结构化的特征还是原始的视觉输入，显然原始的视觉输入对算法更加具有挑战；再比如游戏的仿真成本高不高，如果不高的话就可以使用tree-search的方法（例如MCTS），如果仿真成本很高，那就只能使用一些online的方法（例如A3C和DQN等）；

## AI如何玩游戏

利用AI玩游戏主要有4种方法：

- 基于规划的方法（Planning-Based Approaches）
- 基于增强学习的方法
- 基于监督学习的方法
- 混合方法

## 基于规划的方法

基于规划的方法一般都是通过探索未来的状态空间来决定当前的动作，这种方法一般都需要有一个仿真器来计算未来的状态。基于规划的方法又可以分为：

- 经典的树搜索方法：包括在游戏中用于寻路的A*算法和在对抗游戏（比如棋牌）中使用的minmax算法
- 随机树搜索方法：最常见的随机树搜索方法就是MCTS以及在非完美信息情况下使用的ISMCTS
- 进化规划（Evolutionary Planning）方法：利用进化算法来优化AI的动作序列以达到最大化效能函数（utility function）的目的。相比树搜索的方法，这种方法更加适合用于动作空间比较大的场景。
- 基于符号表示的规划方法：有些游戏可以使用一阶符号逻辑来表示事件、状态和动作之间的转换，然后就可以基于这写符号逻辑表示来对游戏进行规划。

## 增强学习

增强学习的方法把游戏建模称为蒙特卡洛决策过程（Monte Carlo Decision Process）。增强学习方法需要较长的训练时间，但在预测时的时间很短，这些特点正好和基于规划的方法相反。以下主要讨论两类增强学习方法：

- 经典的增强学习方法以及深度增强学习方法：

最初经典的增强学习算法在一些简单的棋牌类游戏取得了很好的效果，但是对于更加复杂的一些游戏，经典的增强学习算法难以遍历所有的状态-动作组合，这时就需要通过近似函数模型（function approximator）来压缩状态表示，而神经网络（Neural Network）就是一个非常好的近似函数模型。由于Reward的稀疏性，Agent可能很长时间都无法收到新的reward值，这就会导致近似函数模型在不同状态下始终只有一个输出值，这样的话模型就会退化，这就是所谓的“灾难性遗忘”（catastrophic forgetting）

- 基于进化算法的增强学习方法：

基于进化算法的增强学习算法一般通过遗传算法或者进化算法来进化神经网络的权重和拓扑结构，这种方法在一些简单的游戏取得了不错的成绩，但是由于维度诅咒（curse of dimensionality）的原因，这种随机搜索的方法只能处理输入维度较少（一般来说不多于50维）的场景。

## 监督学习

一般的监督学习方法就是通过使用人类的轨迹，利用近似函数模型（function approximator）学习这些样本，学习到模型之后就可以用来预测某一个状态的输出目标。但是如下图所示，人类的样本覆盖的分布范围只占整个分布的很小一部分，为了填补这些空白，可以有很多方法，比如最简单的DAgger（Dataset Aggregation），核心思想就是当出现新的状态，就由人类专家来给出目标值。



![img](https://pic3.zhimg.com/80/v2-fc902e3673aacb025b4125f6d61f86c6_hd.jpg)



## 混合方法

在很多游戏中也常常会把之前讨论的三种方法结合起来一起使用，比如有一种动态脚本的方法，结合了基于脚本的规则和增强学习方法，能够基于当前的状态动态的选择执行的脚本，这种方法被应用到很多格斗类游戏和即时战略类游戏中。

## AI怎么玩游戏？

接下来我们将按照游戏的类别来讨论AI是如何玩游戏的

## 棋类游戏

由于棋类游戏具有状态表示简单并且具有完美信息，所以早起的AI研究有很多都是针对棋类游戏的。棋类游戏主要研究方向是对抗性规划（adversarial planning），即在博弈中如何选择最佳的动作。由于棋类游戏通常具有离散化的状态和动作，所以非常适合使用树搜索的方法。对于一些简单的棋类游戏可以直接使用minmax树搜索结合剪枝来进行树搜索，而对于一些比较复杂的棋类游戏，比如最近去的突破的围棋，就需要通过MCTS（Monte Carlo Tree Search）来进行树搜索。

## 牌类游戏

与棋类游戏不同，大部分的牌类游戏都是非完美信息的，所以牌类游戏主要的研究方向就是在非完美信息条件下的对抗性规划，即在包含隐藏信息的条件下选择最佳的动作。通常这类问题都是通过反事实遗憾最小化（Counterfactual Regret Minimization）来实现的，这个算法通过计算每次实际采取动作的收益和最佳动作收益之差，即遗憾（regret），然后迭代的最小化这个遗憾值，最终收敛到一个最优的策略。这个算法在一些比较简单的牌类游戏中取得了较好的效果，但是对于德州扑克这种比较复杂的牌类游戏，还是需要在原有的基础上做一些优化，比如DeepStack就是在原有的CRF算法的基础上借鉴了AlphaGo的思想，增加了树搜索以及状态评估模型。

## 经典的竞技类游戏

经典的竞技类游戏平台，比如Atari 2600，包含了多个种类的游戏，也同时对AI提出了不同的能力要求。比如一些体育类的游戏主要考验的是反应的能力，而超级马里奥这样的游戏又主要考验对游戏中的事件以及轨迹的提前判断能力，还有像吃豆人这种考验在迷宫中导航的能力，甚至有祖玛的复仇这种考验长期的规划以及记忆隐藏的状态能力。

大部分的经典竞技类游戏目前已经很好的得到了结果，比如2015年DeepMind提出的DQN方法以及它后续的一系列变种已经在很多Atari游戏上取得了和人类水平相当甚至是超过人类水平的能力，但是也有一些这些方法表现的就非常差，比如吃豆人游戏，在缺乏向前搜索的情况下，经典的增强学习方法表现都不好，只有在具备Planning能力的AI才能取得平均人类的水平，不过最近Microsoft提出了一种混合reward的架构，通过把复杂的reward函数分解为不同的部分，然后由不同的agent分别学习，最后把各个Agent的Q函数聚合称为一个最终的结果。

目前，经典的竞技类游戏有很多测试床：

- 超级马里奥：主要包括设计AI能力更高、AI表现更像人以及生成游戏关卡
- ALE平台：主要包括Atari的很多游戏，DeepMind的很多研究都是基于ALE平台
- 通用游戏AI（General Video Game AI）：主要目标是设计一个AI能够完成多种游戏

## 策略游戏

策略类游戏一般可以分为“回合制”以及“实时”的，“文明”就是典型的回合制游戏，而“星际争霸”就是典型的实时策略游戏。策略类游戏的主要挑战就是需要同时对多个游戏单元（比如骑士、坦克等）作出规划（planning），导致动作空间巨大。与此同时，策略类游戏的每一个动作的影响都很长远，导致游戏的规划深度非常的长。这两个特点导致策略类游戏planning的复杂度远远超过棋牌类游戏，并且对于即时战略类游戏（Real-Time Strategy）需要能够实时的planning，并选择最合适的动作。所以为了能够处理这么复杂的搜索场景，必须对问题作出简化。

星际争霸是一个典型的即时战略类游戏（Real-Time Strategy），并且在玩家中非常流行，目前也有多个星际争霸的研究环境，包括基于星际1的BWAPI以及TorchCraft和基于星际2的SC2LE。并且还有多个星际争霸的AI机器人比赛，比如IEEE CIG和AIIDE。

由于星际争霸非常复杂，一般来说，会把这个问题分为三个层次：战略、战术以及微操，如下图所示。大部分研究都只能解决其中的一个部分，目前为止还没有一个AI能够达到人类的平均水平。



![img](https://pic1.zhimg.com/80/v2-ed86d87294558af812f8c352e7c998ac_hd.jpg)



在微观层面，需要解决每个游戏单元具体执行的动作。目前的研究还大多数集中在两只队伍面对面对战的场景，而建筑物建造、科技研究以及战争迷雾这些都没有涉及。早期研究有一些非机器学习方法，比如势能场和模糊逻辑，应用在这个场景并取得了一定的成功，后来出现了很多基于增强学习的方法，比如为每一个游戏单元学习一个单独的Q函数，还有一些利用forward model的基于planning的方法，也取得一定的成功。总体来说，目前大多数的研究还只能解决比较简单的场景，尚未有突破性的进展。

在宏观层面，需要根据对手的动作来动态的选择策略。目前大部分的AI机器人都没有实现多个策略，更不用说动态的选择策略了。因为星际争霸是非完美信息游戏，要判断对手的策略只能基于有限的可观察数据，要做到这一点是非常困难的。

## 赛车游戏

赛车游戏一般需要玩家驾驶赛车在尽可能短的时间内跑完赛道，有些简单的游戏只需要控制赛车的方向，而一些复杂的游戏还需要玩家考虑换挡、离合以及氮气等等。在不考虑其他玩家的情况下，简单的动作只需要短期规划即可，而复杂的动作则需要考虑长远一些，而在考虑其他玩家的情况下，还需要考虑玩家之间的对抗策略。

当前赛车游戏的研究环境主要是TORCS，并且每年都有举办一次比赛Simulated Car Racing Championship，目前AI已经可以达到人类高手水平。从参赛的AI机器人采用的技术来看，赛车游戏AI的主要方法包括：

- 非机器学习类方法：比如将赛道分成很多段，然后在人类赛道轨迹中需要和当前段最相近的，最后按照这个最相近的段中人类执行的动作来运行

- 机器学习类方法：

- - 增强学习方法：将赛车比赛建模成为MDP问题，根据比赛时间赋予Reward，然后根据当前状态（可以是结构化输入也可以是纯图片输入）判断最优的动作
  - 进化算法：根据领域知识为赛车建立模型，并使用进化算法调优模型参数。



## 第一人称射击游戏

第一人称射击游戏（后面简称为FPS游戏）是一个快节奏的游戏，强调人或者AI的反应能力，并且需要具备在复杂的3D环境下的导航和需要目标的能力，在多人场景下，还需要具备团队协作能力，这些都对AI提出了很多挑战。

目前FPS游戏主要有两个AI机器人比赛：

- 2K BotPrize：基于Unreal Tournament 2004的AI机器人比赛，比赛的主要目标是让AI更加像人，并让其他玩家感觉不到是AI在和他们玩游戏。2014年获胜的团队主要是采用进化算法来优化多目标函数的反复。
- VizDoom：基于DOOM的AI机器人比赛，比赛分为三个场景：采集加血包、迷宫导航以及deathmatch比赛模式，比赛的主要目标是提高AI的表现能力，目前大多数研究方法都是基于Deep Reinforcement Learning，但也有基于进化算法的研究工作。

## 其他游戏

还有其他一些非典型的游戏AI，比如严肃游戏（Serious Games）中的AI，主要目的是为了科学、工程、数学或者是心理类的互动，具体应用的技术也和游戏的内容和方向有关。还有一种文本冒险类游戏（text-based adventure games）需要通过自然语言处理技术来理解用户的输入，并且根据游戏内容生成输出，这样就可以和用户产生互动。还有一些诸如因果类游戏（Casual Games），比如愤怒的小鸟和植物大战僵尸等，和格斗类游戏以及沙箱类游戏最近都引起了比较多的关注，也有一些专门针对这些游戏的研究，但主要方法和思路和之前的游戏都差不多，在此就不在赘述。


# 相关

- [游戏人工智能 读书笔记（十）利用AI玩游戏](https://zhuanlan.zhihu.com/p/38541836)
