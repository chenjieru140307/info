

# 行为决策

行为决策层在整个无人车规划控制软件系统中扮演着“副驾驶”的角色。这个层面汇集了所有重要的车辆周边信息，不仅包括了无人车本身的当前位置、速度、朝向，以及所处车道，还收集了无人车一定距离以内所有重要的感知相关的障碍物信息。行为决策层需要解决的问题，就是在知晓这些信息的基础上，决定无人车的行驶策略。这些信息具体包括以下几点。

- （1）所有的路由寻径结果：比如无人车为了到达目的地，需要进入的车道是什么（target lane）。
- （2）无人车的当前自身状态：车的位置、速度、朝向，以及当前主车所在的车道、按照寻径路由需要进入的下一个车道等。
- （3）无人车的历史信息：在上一个行为决策周期，无人车所做出的决策是什么？是跟车、停车、转弯或者是换道？
- （4）无人车周边的障碍物信息：无人车周边一定距离范围内的所有障碍物信息。例如周边的车辆所在的车道，邻近的路口有哪些车辆，它们的速度、位置如何，以及在一个较短的时间内它们的意图和预测的轨迹，周边是否有自行车或者行人，以及他们的位置、速度、轨迹等。
- （5）无人车周边的交通标识信息：一定范围内的Lane的变化情况。比如路由寻径的结果是在Lane 1的纵向位移10m处换道进入对应的相邻Lane 2的纵向位移20m处，那么Lane 1的合法的纵向位移换道空间是多大？比如从一个直行Lane行驶结束，需要进入下一个左转Lane，两条Lane的交界处是否有红绿灯或者人行道？
- （6）当地的交通规则：例如道路限速，是否可以红灯右拐等。

无人车的行为决策模块，就是要在上述所有信息的基础上，做出如何行驶的决策。可以看出，无人车的行为决策模块是一个信息汇聚的地方。由于需要考虑如此多种不同类型的信息及受到非常本地化的交规限制，行为决策问题往往很难用一个单纯的数学模型解决。

往往更适合行为决策模块的解决方法，是利用一些软件工程的先进观念来设计一些规则系统。

例如在DARPA无人车竞赛中，Stanford的无人车系统“Junior”利用一系列cost设计和有限状态机（Finite State Machine）来设计无人车的轨迹和操控指令。类似地，CMU的无人车系统“Boss”通过计算分析Lane之间的空隙（Gap），并且按照一定规则和一些预设的阈值比较决定换道这一行为的触发。其他很多的参赛系统如Odin和Virginia Tech也都利用了规则引擎来决定无人车的驾驶行为。Carolo团队则是结合了规则引擎和行为模型，建立了一个混合的无人车决策系统。

随着对无人车研究兴趣的广泛发展和研究的深入，越来越多的研究结果开始使用一些Bayesian模型对无人车行为进行建模。其中MDP（Markov Decision Process）和POMDP（Partially Observable Markov Decision Process）都是在学术界最为流行的无人车行为决策建模方法，我们将在本节简单介绍几种基于MDP的无人车行为决策方式。虽然MDP类的非deterministic概率模型在学术界渐渐流行，但笔者从工业界的实际应用经验出发，认为基于规则的决定性（Deterministic）行为决策系统仍然是目前工业界的主流。

本节将介绍一种利用分治（Divide and Conquer）思想来设计的基于规则的行为决策实现。事实上，如果能够用先进的软件工程实现结合交规和周边路况的行为决策，作者认为Deterministic的规则系统甚至可能在安全可靠性上优于基于概率模型的实现方式。设想实际人类驾驶员是如何按照一个固定的路线从A点开到B点。因为交通规则是明确并且是可以具体执行的，所以作者认为在宏观层面的驾驶行为，在给定的周边路况下，按照交规要求和自身的意图，可以看成是完全基于规则的决定性行为。

## 1 有限状态马尔可夫决策过程

一个马尔可夫决策过程，由下面的五元组定义：$\left(\mathrm{S}, \mathrm{A}, \mathrm{P}_{\mathrm{a}}, \mathrm{R}_{\mathrm{a}}, \mathrm{r}\right)$ 。

- （1）S代表了无人车所处的有限的状态空间，状态空间的划分可以结合无人车当前位置及其在地图上的场景进行设计：例如在位置维度可以考虑将无人车按照当前所处的位置划分成等距离的格子；参考地图的场景，可以将无人车所处的车道和周边道路情况归纳到有限的抽象状态中。
- （2）A代表了无人车的行为决策空间，即无人车在任何状态下的所有行为空间的集合：例如，可能的状态空间包括当前Lane跟车（Follow）、换道（Change Lane）、左/右转（Turn Left/Right）、路口的先后关系（Yield/Overtake）、遇到行人或者红绿灯时的停车（Stop）等。
- （3）$P_{a}\left(s, s^{\prime}\right)=P\left(s^{\prime} | s, a\right)$ 是一个条件概率，代表了无人车在状态s和动作a下，到达下一个状态s'的概率。
- （4）$\mathrm{R}_{\mathrm{a}}\left(\mathrm{s}, \mathrm{s}^{\prime}\right)$ 是一个激励函数（Reward），代表了无人车在动作 $a$ 下，从状态到状态 $s'$ 所得到的激励。该激励函数的设计可以考虑安全性、舒适性，以及下游动作规划（Motion Planning）执行难度等因素综合设计。
- （5）$\gamma \in(0,1)$ 是激励的衰减因子，下一个时刻的激励便按照这个因子进行衰减；在任何一个时间，当前的激励系数为1，下一个时刻的激励系数为 $\gamma$，下两个时刻的激励系数为 $\gamma^2$，依此类推。其含义是当前的激励总是比未来的激励重要。<span style="color:red;">$\gamma^2$ 这个地方可能印错了，确认下。</span>

无人车行为决策层面需要解决的问题，在上述MDP的定义下，可以正式描述为寻找一个最优“策略”，记为 $\pi: S \rightarrow A$。在任意给定的状态S下，策略会决定产生一个对应的行为 $a=\pi(s)$。当策略确定后，整个MDP的行为可以看成是一个马尔可夫链。行为决策的策略 $\pi$ 的选取目标是优化从当前时间点开始到未来的累积激励（如果Reward是随机变量，则优化累积Reward的期望）：

$$
\sum_{t=0}^{\infty} \gamma^{t} R_{a_{t}}\left(s_{t}, s_{t+1}\right)
$$

其中 action 是由策略 $\pi$ 产生 $a=\pi(s)$。

在上述马尔可夫决策过程定义下，最优策略 $\pi$ 通常可以用动态编程（Dynamic Programming）的方法求解。假设转移矩阵$P$和激励分布$R$已知，最优策略的求解通常都是基于不断计算和存储如下两个基于状态$s$的数组：

$$
\begin{array}{l}{\pi\left(s_{t}\right) \leftarrow \underset{a}{\operatorname{argmax}}\left\{\sum_{s_{t+1}} P_{a}\left(s_{t}, s_{t+1}\right)\left(R_{a}\left(s_{t}, s_{t+1}\right)+\gamma V\left(s_{t+1}\right)\right)\right\}} \\ {V\left(s_{t}\right) \leftarrow \sum_{s_{t+1}} P_{\pi\left(s_{t}\right)}\left(s_{t}, s_{t+1}\right)\left(R_{\pi\left(s_{t}\right)}\left(s_{t}, s_{t+1}\right)+\gamma V\left(s_{t+1}\right)\right)}\end{array}
$$


其中数组 $\mathrm{V}\left(\mathrm{s}_{\mathrm{t}}\right)$ 代表了未来衰减叠加的累积（期望）激励，$\pi\left(s_{t}\right)$ 代表需要求解的策略。具体的求解过程可以是在所有可能的状态 $s$ 和 $s'$ 之间进行重复迭代计算，直到二者收敛为止。更进一步，在Bellman的Value Iteration算法中，$\pi\left(s_{t}\right)$ 不需要进行显式的计算，而是可以将其必要的计算包括在  $V\left(s_{t}\right)$ 的计算中，因此可以得到如下的Value Iteration的单步迭代计算：

$$
V_{i+1}(s) \leftarrow \max _{a}\left\{\sum_{s^{\prime}} P_{a}\left(s, s^{\prime}\right)\left(R_{a}\left(s, s^{\prime}\right)+\gamma V_{i}\left(s^{\prime}\right)\right)\right\}
$$

其中 $i$ 代表迭代步骤，在 $i=0$ 时使用一个初始猜测 $V_{0}(s)$ 开始迭代，直到 $V(s)$ 的计算趋于稳定为止。由于利用MDP建模解决无人车行为决策的方法比较多样，本书在这里不再赘述所有的基于马尔可夫决策过程的行为决策方法，读者可以参考参考文献中的来了解具体的状态空间、动作空间，以及转移概率和Reward函数的实现举例。需要强调的是，利用MDP解决无人车行为决策的最关键部分在于激励函数R的设计。在设计这一Reward函数时，需要尽可能考虑如下因素。

- （1）到达目的地：“鼓励”无人车按照既定的路由寻径路线行进到达目的地，也就是说，如果选择的动作 $a=\pi(s)$ 会使得无人车有可能偏离既定的路由寻径路线，那么应当给予对应的惩罚。
- （2）安全性和避免碰撞：按照前文所述，如果将无人车周边的空间划分成等间距的方格，那么远离可能有碰撞的方格应当得到奖励，接近碰撞发生时，应当加大惩罚。
- （3）乘坐的舒适性和下游执行的平滑性（smoothness）：这两个因素往往是一致的。乘坐的舒适往往意味着安全顺畅的操作。例如从某一个速度状态到一个比较接近的速度状态的 $a=\pi(s)$，其cost应该较小；反之，如果猛打方向盘或者猛然加速，这个 $a=\pi(s)$ 的action，对应的cost就应该比较高（负向Reward）。

正是因为利用马尔可夫概率模型的MDP需要如此细致地设计诸如状态空间、转移概率和激励函数等参数，作者认为基于规则的宏观行为决策系统是一种更可靠的设计，下面我们介绍一种利用场景分割无人车周边环境，通过构建子场景并加以Rule运用的行为决策系统设计。

## 2 基于场景划分和规则的行为决策设计

这里我们介绍一种基于规则的无人车行为决策层的设计，其核心思想是利用分治的原则将无人车周边的场景进行划分。在每个场景中，独立运用对应的规则来计算无人车对每个场景中元素的决策行为，再将所有划分的场景的决策进行综合，得出一个最后综合的总体行为决定。我们先引入几个重要概念：综合行为决策（Synthetic Decision）、个体行为决策（Individual Decision），以及场景（Scenario）。

1．综合决策

综合的行为决策代表无人车行为决策层面的整体最高层的决策，例如按照当前Lane跟车保持车距行驶，换道至左/右相邻Lane，立刻停车到某一停止线后等；作为最高层面的综合决策，其所决策的指令状态空间定义，需要和下游的动作规划（Motion Planning）协商一致，使得做出的综合决策指令是下游可以直接用来执行规划出路线轨迹（trajectory）的。为了便于下游直接执行，综合决策的指令集往往带有具体的指令参数数据。


下表中列出了一些综合决策的指令集定义及其可能的参数数据。例如，当综合决策是在当前车道跟车行驶（Follow）时，传给下游动作规划的不仅是跟车这一宏观指令，还包含如下参数数据：前方需要跟车的车辆的id（一般从感知输出获得），跟车需要保持的车速（当前车道限速和前车车速之间较小值），以及需要和前车保持的距离（例如前车尾部向后3m）等。下游的动作规划基于宏观综合决定及伴随指令传来的参数数据，结合地图信息（如车道形状）等，便可以直接规划出安全无碰撞的行驶路线。

表7-1　行为决策中的综合决策及其参数

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200210/kCmVYefOiYaJ.png?imageslim">
</p>


2．个体决策

与综合决策相对应的是个体决策。在本节开始处我们便提出过，行为决策层面是所有信息汇聚的地方。因此，最终的综合决策必须是考虑了所有重要的信息元素后得出的。这里，我们提出对所有重要的行为决策层面的输入个体，都产生一个个体决策。这里的个体，可以是感知输出的路上车辆和行人，也可以是结合了地图元素的抽象个体，比如红绿灯或者人行横道对应的停止线等。事实上，最终的综合决策是先经过场景的划分，产生每个场景下的个体决策，再综合考虑归纳这些个体决策才得到最终的综合决策。

个体决策和综合决策相似的地方是除了其指令集本身外，个体决策也带有参数数据。个体决策不仅是产生最后的综合决策的元素，而且也和综合决策一起被传递给下游动作规划模块。这种设计虽然传递了更多的数据，但作者根据工业界的经验认为，传递作为底层决策元素的个体决策能够非常有效地帮助下层模块更有效地实现路径规划。同时，当需要调试解决问题时，传递过来的个体决策能够大大提高调试的效率。



表7-2列出了一些典型的个体决策及其可能的附带参数数据。例如，在做出针对某个感知物体X的超车这一个体决策时，附带的参数数据包括超车的距离和时间限制。距离代表本车车身至少要超过物体X的车头的最小距离，同样，时间代表这段超车安全距离至少要对应物体X行驶一个最小安全时间间隔。注意这种超车个体决策，往往发生在两车轨迹有所交互的场景中。典型的场景包括换道和路口的先行后行。下面我们会结合红绿灯路口右转这一具体例子，描述如何结合分割场景产生不同的个体决策，并最终融合成综合决策输出。

表7-2　行为决策中的个体决策及其参数

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200210/QSEdOApfC6kf.png?imageslim">
</p>

3．场景划分构建和系统设计

个体决策的产生依赖于场景的构建。这里我们可以将场景理解成一系列具有相对独立意义的无人车周边环境的划分。利用这种分而治之思想的场景划分，我们将无人车行为决策层面汇聚的众多无人车主车周边属于不同类别的信息元素，聚类到不同的富有实际意义的场景实体中。在每个场景实体中，我们通过交规，并结合主车的意图，计算出对于每个信息元素的个体决策，再通过一系列准则和必要的运算把这些个体决策最终综合输出给下游。

图7.7（a）和图7.7（b）所示为两个非常典型的场景划分。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200210/XAsqEi6RggBI.png?imageslim">
</p>

图7-7　行为决策场景

在图7.7（a）中，车辆a和d出现在“左侧车道”这一场景①中。此时无人车主车的意图是向左换道。在计算了主车相对a和d的位置和速度后，“左侧车道”这一场景计算的结果是需要让a车先通过，然后在d车之前进行换道；与此同时，一个相对独立的场景是“前车”场景②，此时主车虽然在考虑向左换道，但仍然需要注意当前车道的前车，所以场景②对前车仍然做出了对车辆b需要注意这个个体决策；相对主车当前意图而言，右侧车道场景③和后方车辆场景④和当前的主车轨迹没有冲突，所以可以安全做出对车辆c和e的Ignore决策。

值得一提的是类似前方后方车辆，两侧车道这些场景是基本的场景。有一些场景的基本元素本身就可以是这些基本场景。图7.7（b）中给出了“路口”这么一个“复合场景”。可以看出，我们的场景定义是分层次的（Layered）。每个层次中间的场景是互相独立构建的。其中主车可以认为是最基本的底层场景，其他所有场景的构建都需要先以无人车主车在哪里这么一个基本场景为基础；在此之上的第一层场景包括红绿灯、前后方车辆，以及左右两侧车道车辆等；如图7.7（b）所示中的路口场景，是第二层的复合场景。其中的元素包括第一层的人行横道、红绿灯，以及主车等场景。结合这些场景，路口场景本身中的元素是车辆a和b。假设此时无人车的意图是右转，路口红灯可以右转但由于没有道路优先权需要避让其他车辆，此时感知发现一个行人在人行横道的场景横穿马路，那么结合所有这些场景元素和意图，得到的最终指令是针对行人在人行横道前停车。

综上所述，每个场景模块利用自身的业务逻辑（Business Logic）来计算其不同元素个体的决策。通过场景的复合，以及最后对所有个体的综合决策考虑，无人车得到的最终行为决策需要是最安全的决策。这里的一个问题是会不会出现不同场景对同一个物体（例如某个车辆）通过各自独立的规则计算出矛盾的决策？从场景的划分可以看出，本身一个物体出现在不同场景里的概率是很小的。事实上，我们提出的这种场景划分的方法本身就尽可能避免了这一情况的出现。即使这种矛盾出现，在图7-8所示的系统框架的中间层，也会对所有的个体决策进行汇总和安全无碰撞的验证。

整个行为决策层面的框架和运行流程如图7-8所示。首先是结合主车信息、地图数据及感知结果构建不同层次的场景。在路由寻径的指引下，每个场景结合自身的规则（往往是交规或者安全避让优先），计算出属于每个场景物体的个体决策。在所有的个体决策计算完毕后，虽然发生的概率极其微小，但我们还是会检查有无冲突的个体决策。在对冲突的个体决策进行冲突解决（往往是优先避让）后，我们会在一个统一的时空里，推演预测当前的所有个体决策能否汇总成一个安全行驶无碰撞的综合决策。如果这样的安全无碰撞综合决策存在，我们便将其和个体决策一起输出给下层的动作规划模块计算具体从当前位置到下一个位置的时空轨迹。

基于规则和场景的行为决策模块系统框架和流程：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200210/Apm1bjlQj3RI.png?imageslim">
</p>



