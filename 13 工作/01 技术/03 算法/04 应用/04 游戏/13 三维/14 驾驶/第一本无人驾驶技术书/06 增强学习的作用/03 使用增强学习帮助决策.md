
# 使用增强学习帮助决策

现有的深度增强学习解决的问题中我们执行的行为一般只对环境有短期影响。例如，在Atari的赛车游戏中，我们只需要控制赛车的方向和速度让赛车沿着跑道行驶，并且躲避其他赛车就可以获得最优的策略，但是对于更复杂决策的情景，我们无法只通过短期的奖励得到最优的策略，一个典型的例子是走迷宫。

在走迷宫这个任务中，判断一个行为是否是最优的行为无法从短期的奖励得到，只有当最终走到终点时才能得到奖励。在这种情况下，直接学习出正确的Q函数是非常困难的。我们只有结合基于搜索的算法和基于增强学习的算法才能有效地解决这类问题。

基于搜索的算法一般是通过搜索树实现的。搜索树既可以解决一个玩家在环境中探索的问题（例如走迷宫），也可以解决多个玩家竞争的问题（例如围棋）。我们以围棋为例，讲解搜索树的基本概念。围棋游戏有两个玩家，分别由白子和黑子代表。一个围棋棋盘中线的交叉点是可以下子的地方。两个玩家分别在棋盘上下白子和黑子，一旦一片白子或黑子被相反的颜色的子包围，那么这片子就会被提掉，重新成为空白的区域。游戏的最后，当所有的空白区域都被占领或者包围时，占领和包围的区域比较大的一方获胜。

在围棋这个游戏中，我们从环境中得到的观测 $s_t$ 是棋盘的状态，也就是白子和黑子的分布。我们执行的行为是所下的白子或者黑子的位置。我们最后得到的奖励可以根据游戏是否取胜得到。取胜的一方得到的奖励是$+1$，失败的一方得到的奖励是$−1$。

这个游戏的进程可以通过如图所示的搜索树表示。

搜索树算法示例：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200209/AIeSeC4wsks1.png?imageslim">
</p>

搜索树中的每个节点对应着一种棋盘的状态。每一条边对应着一个可能的行为。黑棋先行，树的根节点对应的是棋盘的初始状态s0。$a_1$和$a_2$对应黑棋的两种可能的落子位置（在实际的围棋中，可能的行为远比两种多），每个行为$a_i$对应着一个新的棋盘的状态$si_1$。接下来该白棋走，白棋同样有两种走法 $b1$和$b2$，对于每个棋盘的状态$si_1$，两种不同的走法又会生成两种不同的状态。如此往复，一直到游戏结束，我们就可以在游戏的叶子节点中获得游戏结束时黑棋获得的奖励。我们可以通过这些奖励获得最佳的状态。


通过这个搜索树，如果给定黑棋和白棋的策略 $\pi=[\pi 1, \pi 2]$ ，我们可以定义黑棋的值函数为黑棋在双方分别执行策略 $\pi 1$ 和 $\pi 2$ 时，黑棋最终能获得奖励的期望值。

$$
v_{\pi}(s)=\mathrm{E}_{\pi}\left[G_{t} | S_{t}=s\right]
$$

黑棋需要寻找的最优策略需要最优化最坏的情况下黑棋所能得到的奖励。我们定义这个值函数为最小最大值函数。黑棋的最优策略就是能够达到这个值函数的策略 $\pi 1$。

$$
v(s)=\max _{\pi 1} \min _{\pi 2} v_{\pi}(s)
$$


如果能够穷举搜索树的每个节点，那么我们可以很容易地用递归的方式计算出最小最大值函数和黑棋的最优策略。但是，在实际的围棋情况中，每一步黑棋和白棋可以采用的行为的个数非常多，而搜索树的节点的数目是随着树的深度指数增长的。因此，我们无法枚举所有的节点计算出准确的最小最大值函数，而只能通过学习 $v(s ; w) \sim v(s)$ 作为近似最小最大值函数。我们可以通过两种方法使用这个近似函数。首先，我们可以使用这个近似函数确定搜索的优先级。对于一个节点，白棋或者黑棋有可能有多种走法，我们应该优先搜索产生最小最大值函数比较高的节点的行为，因为在实际的游戏中，真实的玩家一般会选择这些相对比较好的行为。其次，我们可以使用这个近似函数估计非叶子节点的最小最大值。如果这些节点的最小最大值非常低，那么这些节点几乎不可能对应着最优的策略。我们再搜索时也不用考虑到这些节点。

因此，我们的主要问题是如何学习到近似最小最大值函数 $v(s ; w)$。我们可以使用两个我们学习到的围棋算法自己和自己玩围棋游戏，然后通过增强学习的算法更新近似最小最大值函数的参数$w$。在玩完了一局游戏之后，我们可以使用类似REINFORCE算法的更新方式：

$$
\nabla w=\alpha\left[G_{t}-\mathrm{V}\left(s_{t} ; w\right)\right] \nabla_{w} \mathcal{V}\left(s_{t} ; w\right)
$$

在这个式子中，$G_t$表示的是在$t$时刻之后获得的奖励。因为在围棋这个游戏中，我们只在最后时刻获得奖励，所以$G_t$对应的是最后获得的奖励。我们也可以使用类似Q-Learning的方式用TD误差更新参数。

$$
\nabla w=\alpha\left[v\left(s_{t+1} ; w\right)-v\left(s_{t} ; w\right)\right] \nabla_{w} v\left(s_{t} ; w\right)
$$

在围棋这个游戏中，我们只在最后时刻获得奖励，一般使用REINFORCE算法的更新方式的效果比较好。在学习出一个好的近似最小最大值函数之后，我们可以大大地加快搜索的效率，这和人学习围棋的过程类似。人在学习围棋的过程中会对特定的棋行形成感觉，能够一眼就判断出棋行的好坏，而不用对棋的发展进行推理。这就是通过学习近似最小最大值函数加速搜索的过程。

通过学习近似最小最大值函数，Google DeepMind在围棋领域取得了突飞猛进的进展。在2016年3月进行的比赛中，DeepMind的AlphaGo以4比1的比分战胜了围棋世界冠军李世石。AlphaGo的核心算法就是利用历史棋局和自己对弈，从而学习近似最小最大值函数的算法。AlphaGo的成功充分展示了增强学习和搜索的结合使用在解决涉及长期规划问题的潜力。

需要注意的是，现有的将增强学习和搜索结合的算法只能用于确定性的环境中。确定性的环境中给定一个观测和一个行为，下一个观测是确定的，并且这个转移函数是已知的。在环境非确定，并且转移函数未知的情况下，如何将增强学习和搜索结合是增强学习领域中没有解决的问题。

