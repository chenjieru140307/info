

## Optical Flow和立体视觉

<span style="color:red;">还是没有明白怎么做到的。</span>

物体的识别与跟踪，以及车辆本身的定位都离不开底层的Optical Flow与立体视觉技术。

在计算机视觉领域，Optical Flow是图片序列或者视频中像素级的密集对应关系，例如在每个像素上估算一个二维的偏移矢量，得到的Optical Flow以二维的矢量场表示[5]。立体视觉则是从两个或更多的视角得到的图像中建立对应关系。这两个问题有高度的相关性，一个是基于单个摄像头在连续时刻的图像，另一个是基于多个摄像头在同一时刻的图片。解决这类问题时有两个基本假设。

- 不同图像中的对应点都来自物理世界中同一点的成像，所以“外观”相似。
- 不同图像中的对应点集合的空间变换基本满足刚体条件，或者说空间上分割为多个刚体的运动。从这个假设我们自然得到Optical Flow的二维矢量场是片状平滑的结论。

2016年6月，美国拉斯维加斯召开的CVRP大会上，Urtasun教授和她的学生改进了深度学习中的Siamese网络，用一个内积层代替了拼接层，把处理一对图片的时间从一分钟左右降低到不到一秒。<span style="color:red;">补充了解下。</span>

如下图所示，这个Siamese结构的深度神经网络分左右两部分，分别是一个多层的卷积神经网络和两个卷积神经网络共享网络权重。Optical Flow的偏移矢量估计问题转化为一个分类问题，输入是两个9×9的图片块，输出是128或者256个可能的偏移矢量y。通过从已知偏移矢量的图片对中抽取的图片块输入到左右两个卷积神经网络，然后最小化cross-entropy（如下面公式所示）。

Siamese结构的深度神经网络分层示意：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200208/85M7EGkNupV2.png?imageslim">
</p>


$$
\min _{w}\left[-\sum_{i, y_{i}} P_{g t}\left(y_{i}\right) \log _{P_{i}}\left(y_{i}, w\right)\right].
$$

我们可以用监督学习的方法训练整个神经网络。

- $i$ 是像素的指标。
- $yi$ 是像素 $i$ 可能的偏移矢量。
- $P_{gt}$ 是一个平滑过的目标分布，用来给一两个像素的预估误差反馈一个非 $0$ 的概率，$gt$表示ground truth。
- $p_i(y_i，w)$ 是神经网络输出的给定$w$时$y_i$的概率。


在 KITTI 的stereo 2012数据集上，这样的一个算法可以在 0.34 秒的时间里完成计算，并达到相当出色的精度，偏移估计误差在 3～4 像素左右，对大于 3 像素的偏移估计误差在8.61像素，表现优于其他低速度的算法。

在得到每个像素上 $y_i$ 的分布后，我们还需要加入空间上的平滑约束，本节试验了三种方法。


- 最简单直接的5×5窗口平均。
- 加入了相邻像素y一致性的半全局块匹配（semi global block matching）。
- 超像素+3维斜面。

这些平滑方法一起，能把偏移估计的误差再降低大约50％，这样就得到了一个比较准确的2位偏移矢量场。基于它，我们能够得到下图所示场景的三维深度/距离估计。这样的信息对无人车自动驾驶非常重要。

深度信息图示意：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200208/hzgoivhnjXgS.png?imageslim">
</p>
