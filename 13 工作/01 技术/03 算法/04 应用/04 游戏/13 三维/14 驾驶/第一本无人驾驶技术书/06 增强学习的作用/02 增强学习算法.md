
# 增强学习算法

增强学习中的每个时刻 $t \in\{0,1,2, \ldots\}$ 中，我们的算法和环境通过执行行为 $a_t$ 进行交互，可以得到观测$s_t$和奖励 $r_t$。

一般情况下，我们假设环境是存在马尔科夫性质的，也就是环境的变化完全可以通过状态转移概率 $Pass' = Pr \left\{s_{t}+1=s^{\prime} | s_{t}=s, \quad a_{t}=a\right\}$ 刻画出来。也就是说，环境下一时刻的观测值与给定当前时刻的观测值和行为，以及与之前所有时刻的观测值和行为都没有关系。而环境早 $t+1$ 时刻返回的奖励在当前状态和行为确定下的期望可以表示为：$Ras =\mathrm{E}\left\{\mathrm{r}_{\mathrm{t+1}} | \mathrm{s}_{\mathrm{t}}=\mathrm{s}, \mathrm{a}_{\mathrm{t}}=\mathrm{a}\right\}$．增强学习算法在每一个时刻执行行为的策略可以通过概率 $\pi(s, a, \theta)=P_{r}\left\{a_{t}=a | s_{t}=s ; \theta\right\}$ 表示，其中 $\theta$ 是需要学习的策略的参数。我们需要学习到最优的增强学习策略，也就是学习到能够取得最高奖励的策略。


$$
\rho(\pi)=\mathrm{E}\left\{\sum_{t=1}^{\infty} \gamma^{t-1} r_{t} | s_{0}, \pi\right\}
$$


其中 $\gamma$ 是增强学习中的折扣系数，用来表示在之后时刻得到的奖励的折扣。同样的奖励，获得的时刻越早，增强学习系统所感受到的奖励越高。

同时，我们可以按照如下方式定义 $Q$ 函数。$Q$函数 $Qpi (s, a)$表示的是在状态为$s$，执行行为$a$之后的时刻都使用策略$\pi$选择行为能够得到的奖励。我们能够学习到准确的$Q$函数，那么使$Q$函数最高的行为就是最优的行为。


$$
\mathrm{Q}_{\pi}(s, a)=\mathrm{E}\left\{\sum_{k=1}^{\infty} \gamma^{k-1} \mathrm{r}_{t+k} | s_{t}=s, a_{t}=a, \pi\right\}=\mathrm{E}_{s^{\prime}}\left[r+\gamma \mathrm{Q}_{\pi}\left(s^{\prime}, a^{\prime}\right) | s, a, \pi\right]
$$

增强学习的目的就是在给定的任意环境下，通过对环境进行探索，学习到最佳的策略函数 $\pi$ 最大化 $\rho(\pi)$。下面的章节中我们会简单介绍常用的增强学习的算法，包括REINFORCE算法和Deep Q-Learning算法。

## 1 REINFORCE算法

REINFORCE算法是最简单的增强学习算法。

REINFORCE算法的基本思想通过在环境里执行当前的策略直到一个回合结束（比如游戏结束），根据得到的奖励可以计算出当前策略的梯度。我们可以用这个梯度更新当前的策略得到新的策略。在下面的回合，我们再用新的策略重复这个过程，一直到计算出的梯度足够小为止。最后得到的策略就是最优策略。

假设我们当前的策略的概率是 $\pi \theta(x)=\operatorname{Pr}\left\{a_{t}=a | s_{t}=s ; \theta\right\}$（ $\theta$ 是策略的参数）。每个回合，算法实际执行的行为 $a_t$ 是按照概率 $\pi(x)$ 采样得到的。算法在当前回合的时刻 $t$ 获得的奖励用 $r_t$表示。那么，策略的梯度可以通过以下公式计算。


$$
\nabla_{\theta} \rho(\pi)=\sum_{t=1}^{T} \nabla_{\theta} \log \left[\pi\left(a_{t} | s_{t} ; \theta\right)\right] R_{t}
$$

其中 $\pi\left(a_{t} | s_{t}; \theta\right)$ 是策略在观测到 $s_{t}$时选择$a_{t}$的概率。$R_{t}=\sum T_{t}^{\prime}=t \gamma_{t}^{\prime}-t r_{t}^{\prime}$ 是算法在采取了当前策略之后获得的总的折扣后的奖励。为了减少预测出的梯度的方差。我们一般会使用 $\left(\mathrm{R}_{\mathrm{t}}-\mathrm{b}_{\mathrm{t}}\right)$ 代替 $\mathrm{R}_{\mathrm{t}}$。$\mathrm{b}_{\mathrm{t}}$一般等于 $\mathrm{E}_{\pi}\left[\mathrm{R}_{\mathrm{t}}\right]$，也就是当前t时刻的环境下使用策略 $\pi$ 之后能获得的折扣后奖励的期望。

计算出方差之后，我们可以使用 $\theta=\theta+\theta \rho(\pi)$ 更新参数得到新的策略。

REINFORCE算法的核心思想是通过从环境中获得的奖励判断执行的行为的好坏。如果一个行为执行之后获得的奖励比较高，那么算出的梯度也会比较高，这样在更新后的策略中该行为被采样到的概率也会比较高。反之，对于执行之后获得奖励比较低的行为，因为计算出的梯度低，更新后的策略中该行为被采样到的概率也会比较低。通过在这个环境中反复地执行各种行为，REINFORCE可以大致准确地估计出各个行为的正确梯度，从而对策略中各个行为的采样概率做出相应的调整。

作为最简单的采样算法，REINFORCE算法得到了广泛的应用，例如学习视觉的注意力机制和学习序列模型的预测策略都用到了REINFORCE算法。事实证明，在模型相对简单、环境的随机性不强的环境下，REINFORCE算法可以达到很好的效果。

但是，REINFORCE算法也存在它的问题。首先，在REINFORCE算法中，执行了一个行为之后的所有奖励都被认为是因为这个行为产生的。这显然是不合理的。虽然在执行了策略足够多的次数然后对计算出的梯度进行平均之后，REINFORCE有很大的概率计算出正确的梯度，但在实际中，出于效率的考虑，同一个策略在更新之前不可能在环境中执行太多次。在这种情况下，REINFORCE计算出的梯度有可能会有比较大的误差。其次，REINFROCE算法有可能会收敛到一个局部最优点中。如果我们已经学到了一个策略，这个策略中大部分的行为都是以近似1的概率采样到的，那么即使这个策略不是最优的，REINFORCE算法也很难学习到如何改进这个策略，因为我们完全没有执行其他采样概率为0的行为，无法知道这些行为的好坏。最后，REINFORCE算法只有在环境存在回合概念时才能够使用。如果环境不存在回合的概念，REINFORCE算法将无法使用。<span style="color:red;">是的。</span>

最近，DeepMind提出了使用Deep Q-learning算法的学习策略，克服了REINFORCE算法的缺点，在Atari游戏学习这样的复杂的任务中取得了令人惊喜的效果。

## 2 Deep Q-Learning

Deep Q-Learning是一种基于Q函数的增强学习算法。该算法对于复杂的、每步行为之间存在较强相关性的环境有很好的学习效果。

Deep Q-Learning的学习算法的基础是Bellman公式[4]。我们在6.2.1节已经介绍了Q函数的定义。

如果我们学习到了最优的行为对应的Q函数 $\mathrm{Q}(\mathrm{s}, \mathrm{a})$ ，那么这个函数应该满足下面的Bellman公式。

$$
\left.\mathrm{Q}(s, a)=\mathrm{E}_{s^{\prime}}\left[r+\gamma \max _{a}^{\prime} \mathrm{Q}(s, a)\right| s, a\right]
$$

另外，如果我们学习到了最优的行为对应的Q函数 $\mathrm{Q}(\mathrm{s}, \mathrm{a})$，那么我们在每一时刻得到了观察 $s_t$ 之后，可以选择使得  $\mathrm{Q}(\mathrm{s}, \mathrm{a})$ 最高的行为作为执行的行为 $a_t$。

我们可以用一个神经网络计算Q函数，用 $\mathrm{Q}(\mathrm{s}, \mathrm{a} ; \mathrm{w})$ 来表示，其中$w$是神经网络的参数。我们希望我们学习出来的Q函数满足Bellman公式，因此可以定义下面的损失函数。这个函数的Bellman公式的L2误差如下。


$$
\mathrm{L}(w)=\mathrm{E}\left\{\left[r+\gamma \max_{\mathrm{a'}} \mathrm{Q}\left(s^{\prime}, a^{\prime} ; w\right)-\mathrm{Q}(s, a ; w)\right] 2\right\}
$$


其中 $r$ 是在 $s$ 的观测执行行为$a$后得到的奖励，$s′$是执行行为$a$之后下一个时刻的观测。这个公式的前半部分 $r+\gamma \max _{\mathrm{a'}} \mathrm{Q}\left(s^{\prime}, a^{\prime}, w\right)$ 也被称为目标函数。我们希望预测出的$Q$函数能够和通过这个时刻得到的奖励和下个时刻状态得到的目标函数尽可能接近。通过这个损失函数，我们可以计算出如下梯度：

$$
\partial \mathrm{L}(w) \partial w=\mathrm{E}\left\{\left[r+\gamma \max _{\mathrm{a}}^{\prime} \mathrm{Q}\left(s^{\prime}, a^{\prime} ; w\right)-\mathrm{Q}(s, a ; w)\right] \partial \mathrm{Q}(s, a ; \mathrm{w} \partial \mathrm{w})\right\}
$$


我们可以通过计算出的梯度，使用梯度下降算法更新我们的参数$w$。

使用深度神经网络逼近$Q$函数存在很多问题：

- 首先，在一个回合内采集到的各个时刻的数据是存在着相关性的。因此，如果我们使用了一个回合内的全部数据，那么我们计算出的梯度是有偏的。
- 其次，由于取出使Q函数最大的行为这个操作是离散的，即使Q函数变化很小，我们得到的行为也可能差别很大。这个问题会导致训练时我们的策略出现震荡。
- 最后，Q函数的动态范围有可能很大，并且我们很难预先知道Q函数的动态范围。因为当我们对一个环境没有足够了解时，很难计算出这个环境中可能得到的最大奖励。这个问题可能会使Q-Learning的工程中的梯度很大，导致训练不稳定。

那么，是如何解决的呢？

- 首先，Deep Q-Learning算法是使用了经验回放的算法。这个算法的基本思想是记住算法在这个环境中执行的历史信息。这个过程和人类的学习过程类似。人类在学习执行行为的策略时，不会只通过当前执行的策略的结果进行学习，还会利用之前的历史执行的策略的经验进行学习。因此，经验回放算法将算法在一个环境中所有的历史经验都存放起来。在学习的时候，可以从经验中采样出一定数量的跳转信息 $\left(s_{t}, a_{t}, r_{t+1}, s_{t+1}\right)$，也就是当下所处的环境信息，然后利用这些信息计算出梯度学习模型。因为不同的跳转信息是从不同的回合中采样出来的，所以它们之间不存在强相关性。这个采样过程还可以解决同一个回合中的各个时刻的数据的相关性问题。
- 而且，Deep Q-Learning算法使用了目标Q网络解决学习过程中的震荡问题。我们可以定义一个目标Q网络 $\mathrm{Q}\left(s^{\prime}, a^{\prime} ; w-\right)$ ，这个网络的结构和用来执行的Q网络的结构完全相同，唯一的不同就是使用的参数 $w−$不同。我们的目标函数可以通过目标Q网络计算。

    $$
    r+\gamma \max _{\mathrm{a'}}\left(s^{\prime}, a^{\prime} ; w-\right)
    $$

    目标Q网络的参数在很长时间内保持不变，每当在Q网络学习了一定的时间之后，可以用Q网络的参数$w$替换目标Q网络的参数$w−$，这样目标函数会在很长的时间里保持稳定，可以解决学习过程中的震荡问题。

- 最后，为了防止Q函数的值太大导致梯度不稳定，Deep Q-Learning算法对奖励设置了最大值和最小值（一般设置为$[-1,+1]$）。我们会把所有的奖励缩放到这个范围，这样算法计算出的梯度会更稳定。

Q-Learning算法的框图如图所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200209/o1hESLTOK9Pf.png?imageslim">
</p>


因为使用了深度神经网络来学习Q函数，Deep Q-Learning算法可以直接以图像作为输入学习复杂的策略，其中一个例子是学习Atari游戏。Atari游戏是计算机游戏的早期形式，一般图像比较粗糙，但是要玩好需要对图像进行理解，并且执行出复杂的策略，例如躲避、发射子弹、走迷宫等。

一些Atari游戏的例子如图6-3所示，我们注意到其中包含了一个简单的赛车游戏。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200209/pSkDfEwt7XFk.png?imageslim">
</p>


Deep Q-Learning算法在没有任何额外知识的情况下，完全以图像和获得的奖励进行输入，在大部分Atari游戏中都大大超过了人类的性能。这在没有深度学习或者增强学习时完全是不可能完成的任务。Atari游戏是第一个用Deep Q-Learning解决了其他算法都无法解决的问题，充分显示了将深度学习和增强学习结合的优越性和前景。
