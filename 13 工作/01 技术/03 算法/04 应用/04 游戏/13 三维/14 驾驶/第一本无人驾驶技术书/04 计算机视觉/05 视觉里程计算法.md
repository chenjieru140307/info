# 视觉里程计算法

<span style="color:red;">没有很理解计算过程。</span>

基于视觉的定位算法有两大分类：

- 一种是基于拓扑与地标的算法。基于拓扑与地标的算法把所有的地标组成一个拓扑图，然后当无人车监测到某个地标时，便可以大致推断出自己所在的位置。基于拓扑与地标的算法相对于基于几何的方法容易些，但是要求预先建立精准的拓扑图，比如将每个路口的标志物做成地标。
- 另一种是基于几何的视觉里程计算法。基于几何的视觉里程计算法计算比较复杂，但是并不需要预先建立精准的拓扑图，这种算法可以在定位的同时扩展地图。本节我们将着重介绍视觉里程计算法。

视觉里程计算法主要分为单目及双目两种。

- 纯单目视觉里程算法存在的主要问题是无法推算出观察到的物体大小，所以使用者必须假设或者推算出一个初步的大小，或者通过与其他的传感器结合（比如陀螺仪）进行准确的定位。
- 双目的视觉里程计算法通过左右图triangulation计算出特征点的深度，然后从深度信息中推算出物体的大小。

视觉里程计算法原理图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200208/wqzS24PaXGdO.png?imageslim">
</p>

双目视觉里程计算法的具体计算流程：


- （1）双目摄像机抓取左右两图。
- （2）双目图像经过triangulation产生当前帧的disparity map。
- （3）提取当前帧与之前帧的特征点，如果之前帧的特征点已经提取好了，那么我们可以直接使用之前帧的特征点。特征点提取可以使用Harris Corner Detector。
- （4）对比当前帧与之前帧的特征点，找出帧与帧之间的特征点对应关系。具体可以使用RANSAC算法。
- （5）根据帧与帧之间的特征点对应关系，推算出两帧之间车辆的运动。这个推算是最小化两帧之间的reprojection error实现的。
- （6）根据推算出的两帧之间车辆的运动，以及之前的车辆位置，计算出最新的车辆位置。

通过以上视觉里程计算法，无人车可以实时推算出自己的位置，进行自主导航，但是纯视觉定位计算的一个很大的问题是算法本身对光线相当敏感。

在不同的光线条件下，同样的场景不能被识别。特别在光线较弱时，图像会有很多噪点，极大地影响了特征点的质量。在反光的路面，这种算法也很容易失效。这也是影响视觉里程计算法在无人驾驶场景普及的一个主要原因。可能的解决方法是在光线条件不好的情况下，更加依赖根据车轮及雷达返回的信息进行定位，我们会在后面章节中详细讨论这部分内容。

