
# 无人驾驶的决策介绍


无人驾驶的决策是指给定感知模块解析出的环境信息如何控制汽车的行为达到驾驶的目标。例如，汽车加速、减速、左转、右转、换道、超车都是决策模块的输出。决策模块不仅需要考虑到汽车的安全性和舒适性，保证尽快到达目标地点，还需要在旁边的车辆恶意的情况下保证乘客的安全。

因此，决策模块一方面需要对行车的计划进行长期规划，另一方面需要对周围车辆和行人的行为进行预测。

而且，无人驾驶中的决策模块对安全性和可靠性有严格的要求。现有的无人驾驶的决策模块一般是根据规则构建的。虽然基于规则的构建可以应付大部分的驾驶情况，对于驾驶中可能出现的各种各样的突发情况，基于规则的决策系统不可能枚举到所有突发情况。我们需要一种自适应的系统来应对驾驶环境中出现的各种突发情况。

基于规则的决策系统大部分可以用有限状态机表示。例如，无人驾驶的高层行为可以分为向左换道、向右换道、跟随和紧急停车。决策系统根据目标可以决定执行高层行为。根据需要执行的高层行为，决策系统可以用相应的规则生成底层行为。基于规则的决策系统的主要缺点是缺乏灵活性。对于所有的突发情况，基于规则的决策系统都需要写一个决策。这种方式很难对所有的突发系统面面俱到。

## 1 无人驾驶模拟器


<span style="color:red;">这个应该单独拿出来。</span>

无人驾驶的决策过程中，模拟器起着非常重要的作用。决策模拟器负责对环境中常见的场景进行模拟，例如车道情况、路面情况、障碍物分布和行为、天气等。同时还可以将真实场景中采集到的数据进行回放。决策模拟器的接口和真车的接口保持一致，这样可以保证在真车上使用的决策算法可以直接在模拟器上运行。除了决策模拟器之外，无人驾驶的模拟器还包含了感知模拟器和控制模拟器，用来验证感知和控制模块。[7]这些模拟器不在本节的讨论氛围之内。

- 无人驾驶模拟器的第一个重要的功能是验证功能。在迭代决策算法的过程中，我们需要比较容易地衡量算法的性能。例如，我们需要确保新的决策算法能够在常见的场景中正确安全地运行。我们还需要对新的决策算法在常见场景的安全性、快捷性、舒适性进行打分。我们不可能每次更新算法时都在实际的场景中进行测试，这时有一个能可靠反映真实场景的无人驾驶模拟器是非常重要的。
- 模拟器的另一个重要功能是进行增强学习。通过在模拟器里模拟出各种突发情况，增强学习算法可以利用其在这些突发情况中获得的奖励学习如何应对这些突发情况。这样，我们只要能够模拟出足够的突发情况，我们的增强学习算法就可以学习到对应的突发情况的处理方法，而不用每种突发情况都单独写规则处理。而且，我们的模拟器也可以根据之前增强学习对于突发情况的处理结果，尽量产生出当前的增强学习算法无法解决的突发情况，从而增强学习的效率。<span style="color:red;">这部分是怎么实现的。</span>

综上所述，无人驾驶模拟器对决策模块的验证和学习都有着至关重要的作用，是无人驾驶领域的核心技术。如何创建出能够模拟出真实场景、覆盖大部分突发情况，并且和真实的汽车接口兼容的模拟器是无人驾驶研发的难点之一。

## 2 增强学习在无人驾驶中的应用和展望

增强学习在无人驾驶中有很高的前景。我们在TORCS模拟器中使用增强学习进行了探索性的工作。TORCS是一个赛车的模拟器。玩家在这个模拟器中的任务是超过其他的AI车，以最快的速度达到终点。虽然TORCS中的任务和真实的无人驾驶的任务还有很大的区别，但是由于其中算法的评估非常容易进行，TORCS现在常用于研究无人驾驶中的增强学习算法。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200209/Q4OP9duB91n5.png?imageslim">
</p>


TORCS模拟器的截图如图所示。增强学习算法一般可以以前方和后方看到的图像作为输入，也可以以环境的状态作为输入（例如自己的速度、离赛道边缘的距离和跟其他车的距离）。

我们这里使用了环境的状态作为输入，使用Deep Q-Learning作为学习算法学习。环境的奖励定义为在单位时刻车辆沿跑道的前进距离。另外，如果车出了跑道或者和其他车辆相撞，会得到额外的惩罚。环境的状态包括了车辆的速度、加速度、离跑道的左右边缘的距离、和跑道的切线的夹角、在各个方向上最近的车的距离等。车的行为包括向上换挡、向下换挡、加速、减速、向左打方向盘、向右打方向盘等。

与普通的Deep Q-Learning相比，我们做了以下改进。首先，我们使用了多步的TD算法进行更新。多步的TD算法能够比单步的算法每次学习时看到更多的执行步数，因此也能够更快地收敛。其次，我们使用了Actor-Critic的架构。Actor-Critic将算法的策略函数和值函数分别用两个网络表示，这样的表示有两个优点。

- （1）策略函数可以使用监督学习的方式进行初始化学习。
- （2）在环境比较复杂时，学习值函数非常困难。把策略函数和值函数分开学习可以降低策略函数学习的难度。

使用了改进后的Deep Q-Learning算法，我们学习到的策略在TORCS中可以实现沿跑道行走、换道、超车等行为。基本达到TORCS环境中的基本驾驶的需要。Google DeepMind直接使用图像作为输入，也获得了很好的效果，但是训练的过程要慢很多。

现有的增强学习算法在无人驾驶的模拟环境中获得了很有希望的结果，但是可以看到，如果需要增强学习真正能够在无人驾驶的场景下应用，增强学习算法还需要有很多改进。

- 第一个改进方向是增强学习的自适应能力。现有的增强学习算法在环境的性质发生改变时，需要试错很多次才能学习到正确的行为。而人在环境发生改变的情况下，只需要很少的试错就可以学习到正确的行为。如何只用非常少量的样本学习到正确的行为是增强学习能够实用的重要条件。<span style="color:red;">为啥只需要很少的样本呢？因为有别的一些信息进行补充。比如说别人的经历，一些文字说明，一些语音教导等等。</span>
- 第二个重要的改进方向是模型的可解释性。现在的增强学习中的策略函数和值函数都是由深度神经网络表示的。深度神经网络的可解释性比较差。由于可解释性差，在实际使用中出了问题很难找到问题的原因，也比较难排查。在无人驾驶这种人命关天的任务中，无法找到问题的原因是完全无法接受的。
- 第三个重要的改进方向是推理和想象的能力。很多时候，人在学习的过程中不需要有一定的推理和想象的能力。例如，在驾驶时，人们不用自己真正尝试，也知道危险的行为会带来毁灭性的后果，这是因为人类对这个世界有一个足够好的模型来推理和想象出相应行为可能会发生的后果。这种能力不仅对增强学习算法在存在危险行为环境中的表现非常重要，在安全的环境中也可以大大加快算法收敛的速度。

只有在这些方向做出了实质性的突破，增强学习才能真正使用到无人驾驶或者是机器人这种重要的任务场景中。希望更多的有志之士能够投身于增强学习的研究，为人工智能的发展贡献出自己的力量。

