
环境感知到策略控制


目前的自动驾驶系统由摄像机，激光雷达等传感器，控制器，GPS定位系统，数字地图，算法等多个部件构成，在这里我们重点介绍算法部分，尤其是机器学习技术在其中的应用情况。


## **百度阿波罗平台简介**

阿波罗

在 2017 年百度已经宣布阿波罗开源，目前有大量的厂商已经接该平台进行合作。我们可以通过阅读它的源代码和文档来了解其所采用的技术。

阿波罗的官网地址是：

[http://apollo.auto/](https://link.zhihu.com/?target=http%3A//apollo.auto/)

源代码，文档与数据下载地址为：

[https://github.com/apolloauto](https://link.zhihu.com/?target=https%3A//github.com/apolloauto)

在这里需要申明的是，[SIGAI](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/G9gW2ghTII57jGmXSIaf7w)的作者与百度以及阿波罗平台没有任何商业和其他关系，我们纯粹是站在技术和产品角度，以第三方的视角来分析他们的技术。

先看看阿波罗官方对目前状态的整体介绍：

阿波罗 2.5版本的目标是用低成本的传感器实现自动驾驶。它能让车辆保持在某一车道上，并与前面最近的车辆保持距离，这通过一个前视摄像头，以及前视雷达来实现。对摄像头图像的分析采用了深度神经网络，随着样本数据的累积，神经网络的预测将越来越准。官方说明，目前不支持在高度弯曲，没有车道线标志的道路上行驶。

首先来看它软件部分的整体结构（下图截取自阿波罗官网）：

![img](https://pic2.zhimg.com/80/v2-398e1736592aae34079bc827bb768685_hd.jpg)

在这里，我们重点关注的是感知模块，其他模块以后有机会的话会写文章分别介绍。



## **感知模块**

首先来看感知模块，它为我们提供了类似人类眼睛所提供的视觉功能，即理解我们所处的驾驶环境。首先来看阿波罗官方对感知模块的介绍（以下两段话引用了他们的原文）：

“Apollo 2.0感知模块包括障碍物检测识别和红绿灯检测识别两部分。障碍物检测识别模块通过输入激光雷达点云数据和毫米波雷达数据，输出基于两种传感器的障碍物融合结果，包括障碍物的位置、形状、类别、速度、朝向等信息。红绿灯检测识别模块通过输入两种焦距下的相机图像数据，输出红绿灯的位置、颜色状态等信息。上述两大感知功能，使无人车具备在简单城市道路自动驾驶的能力，希望能够对无人驾驶社区有帮助。如果对算法细节有兴趣，请查阅 github 上的技术文档。”

“通过安装在车身的各类传感器如激光雷达、摄像头和毫米波雷达等获取车辆周边的环境数据。利用多传感器融合技术，车端感知算法能够实时计算出环境中交通参与者的位置、类别和速度朝向等信息。背后支持这套自动驾驶感知系统的是多年积累的大数据和深度学习技术，海量的真实路测数据经过专业人员的标注变成机器能够理解的学习样本，大规模深度学习平台和 GPU 集群将离线学习大量数据所耗费的时间大幅缩短，训练好的最新模型通过在线更新的方式从云端更新到车载大脑。人工智能+数据驱动的解决方案使百度无人车感知系统能够持续不断的提升检测识别能力，为自动驾驶的决策规划控制模块提供准确、稳定、可靠的输入。”

从这里可以看到，他们采用了摄像机，激光雷达，毫米波雷达等多种传感器，用深度学习技术对这些传感器采集的数据进行分析，以确定车辆当前所处环境中的交通参与者，这里的参与者是指人，车等重要目标。

整个感知模块的结构如下图所（该图来自阿波罗在 github 上的公开文档）：

![img](https://pic1.zhimg.com/80/v2-ae4e419c4bed6d0c368f07643bce0994_hd.jpg)

从上图可以看出，核心的算法包括：

- 车道检测
- 目标检测
- 车道线检测
- 目标跟踪
- 轨迹管理
- 相机标定
- 预测算法
- 规划算法

在这里我们重点介绍前面 4 个算法。



**道路和车道线识别**

车道线属于静态目标，不会移动。准确的确定车道线，不仅对车辆的纵向控制有用，还对横向控制有用。车道线由一系列的线段集合来表示。首先，用卷积神经网络对摄像机采集的图像进行处理，预测出车道线的概率图，即每一点处是车道线的概率。然后，对这种图进行二值化，得到分割后的二值图像。接下来计算二值图像的联通分量，检测出所有的内轮廓，然后根据轮廓边缘点得到车道的标志点。从他们的描述文档看，核心的一步是用卷积神经网络预测出图像每一点是车道线的概率。



**障碍物检测识别**

下面来看官方对障碍物 检测模块的描述：

“障碍物模块包括基于激光雷达点云数据的障碍物检测识别、基于毫米波雷达数据的障碍物检测识别以及基于两种传感器的障碍物结果融合。基于激光雷达点云数据的障碍物检测识别，通过线下训练的卷积神经网络模型，学习点云特征并预测障碍物的相关属性（比如前景物体概率、相对于物体中心的偏移量、物体高度等），并根据这些属性进行障碍物分割。基于毫米波雷达数据的障碍物检测识别，主要用来对毫米波雷达原始数据进行处理而得到障碍物结果。该算法主要进行了 ID 扩展、噪点去除、检测结果构建以及 ROI 过滤。多传感器障碍物结果融合算法，用于将上述两种传感器的障碍物结果进行有效融合。该算法主要进行了单传感器结果和融合结果的管理、匹配以及基于卡尔曼滤波的障碍物速度融合。具体算法请参阅 github 技术文档。”

从这里可以看出，对障碍物的检测与车道线检测不同，这里采用的是基于激光雷达和毫米波雷达的数据。这是出于安全的考虑，如果采用摄像机，在恶劣天气如雨雪，以及极端光照条件下，图像将无法有效的分析，另外，激光雷达和毫米波雷达给出了物体准确的距离数据，这对安全的行驶至关重要，而单纯靠图像数据分析则很难做到。

接下来我们看他们对神经网络实现方案的描述：

“Deep network ingests an image and provides two detection outputs, lane lines and objects for Apollo 2.5. There is an ongoing debate on individual task and co-trained task for deep learning. Individual networks such as a lane detection network or an object detection network usually perform better than one co-trained multi-task network. However, with given limited resources, multiple individual networks will be costly and consume more time in processing. Therefore, for the economic design, co-train is inevitable with some compromise in performance. In Apollo 2.5, YOLO [1][2] was used as a base network of object and lane detection. The object has vehicle, truck, cyclist, and pedestrian categories and represented by a 2-D bounding box with orientation information. The lane lines are detected by segmentation using the same network with some modification.”

这里他们考虑了两种方案：车道线检测和障碍物检测使用同一个神经网络，以及各自使用一个神经网络。后者的准确率更高，但更耗费计算资源，而且处理时间可能会更长。最后选用的是第一种方案。具体的，采用了 YOLO[1][2]作为基础网络来进行目标和车道检测。目前区分的目标有汽车，自行车，行人。这些目标都用一个 2D 的矩形框表示，并且带有朝向信息。车道线用同一个网络的输出得到，使用了图像分割技术。在之前的[SIGAI](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/G9gW2ghTII57jGmXSIaf7w)公众号文章“[基于深度学习的目标检测算法综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/hu7fAhE76lHid9DN5I6Y9Q)”中我们已经简单介绍了 YOLO 和其他主要的算法，感兴趣的读者可以阅读，如有需要。实现时，使用了 Caffe。

阿波罗将目标分为两种类型，静态的和动态的，下面来看他们的描述：

“In a traffic scene, there are two kinds of objects: stationary objects and dynamic objects. Stationary objects include lane lines, traffic lights, and thousands of traffic signs written in different languages. Other than driving, there are multiple landmarks on the road mostly for visual localization including streetlamp, barrier, bridge on top of the road, or any skyline. For stationary object, we will detect only lane lines in Apollo 2.5.”

静态的目标包括车道线，交通灯，其他各种写有文字的交通标志。除此之外，路上还有一些标志可用于视觉定位，包括路灯，栅栏，天桥，地平线等。

下面来看对动态目标的描述：

“Among dynamic objects, we care passenger vehicles, trucks, cyclists, pedestrians, or any other object including animal or body parts on the road. We can also categorize object based on which lane the object is in. The most important object is CIPV (closest object in our path). Next important objects would be the one in neighbor lanes.”

动态目标目前关注的是车辆，自行车，行人，动物等。在这些目标中，最重要的是道路上离我们最近的物体，其次是相邻车道上的物体。

“Given a 2D box, with its 3D size and orientation in camera, this module searches the 3D position in a camera coordinate system and estimates an accurate 3D distance using either the width, the height, or the 2D area of that 2D box. The module works without accurate extrinsic camera parameters.”



**目标跟踪**

在检测出各个运动目标之后，接下来需要准确的跟踪这些目标，得到他们的运动参数和轨迹。目标跟踪是一个状态估计问题，这里的状态就是目标的位置，速度，加速度等参数。跟踪算法可以分为单目标跟踪和多目标跟踪两类，前者只跟踪单个目标，后者可以同时跟踪多个目标。

跟踪算法的数据来源是目标检测的输出结果，即在每一个时刻先检测出路上的移动目标，得到他们的位置，大小等信息，然后对不同时刻的这些数据进行分析，得到目标的状态和运动轨迹。

单目标跟踪算法的核心是估计出单个目标的位置，速度，加速度等状态信息，典型的算法有卡尔曼滤波，粒子滤波等。

和单个目标跟踪不同，多目标跟踪需要解决数据关联问题，即上一帧的每个目标和下一帧的哪个目标对应，还要解决新目标出现，老目标消失问题。多目标的跟踪的一般流程为每一时刻进行目标检测，然后进行数据关联，为已有目标找到当前时刻的新位置，在这里，目标可能会消失，也可能会有新目标出现，另外目标检测结果可能会存在虚警和漏检测。联合概率滤波，多假设跟踪，线性规划，全局数据关联，MCMC马尔可夫链蒙特卡洛算法先后被用于解决数据关联问题来完成多个目标的跟踪。

首先我们定义多目标跟踪的中的基本概念，目标是我们跟踪的对象，每个目标有自己的状态，如大小、位置、速度。观测是指目标检测算法在当前帧检测出的目标，同样的，它也有大小、位置、速度等状态值。在这里，我们要建立目标与观测之间的对应关系。下面是一个数据关联示意图：



![img](https://pic3.zhimg.com/80/v2-6d101951c6c80e5b315ee05ddb4298aa_hd.jpg)





在上图中，第一列圆形为跟踪的目标，即之前已经存在的目标；第二列圆为观测值，即当前帧检测出来的目标。在这里，第 1 个目标与第 2 个观察值匹配，第 3 个目标与第 1 个观测值匹配，第 4 个目标与第 3 个观测值匹配。第 2 个和第 5 个目标没有观测值与之匹配，这意味着它们在当前帧可能消失了，或者是当前帧没漏检，没有检测到这两个目标。类似的，第 4 个观测值没有目标与之匹配，这意味着它是新目标，或者虚警。



下面我们看对阿波罗官方对跟踪的描述：

“The object tracking module utilizes multiple cues such as 3D position, 2D image patches, 2D boxes, or deep learning ROI features. The tracking problem is formulated ase multiple hypothesis data association by combining the cues efficiently to provide the most correct association between tracks and detected object, thus obtaining correct ID association for each object.”

这里采用了多种线索来跟住目标，包括 3D 坐标，2D图像块，2D包围盒，以及通过深度学习得到的 ROI 特征。对于多目标跟踪，这里采用了多假设跟踪算法[3]，这种算法最早用于雷达数据的跟踪，如果对它感兴趣，可以阅读参考文献。



**红绿灯检测识别**

下面来看对红绿灯检测算法的描述：

“红绿灯模块根据自身的位置查找地图，可以获得前方红绿灯的坐标位置。通过标定参数，可以将红绿灯从世界坐标系投影到图像坐标系，从而完成相机的自适应选择切换。选定相机后，在投影区域外选取一个较大的感兴趣区域，在其中运行红绿灯检测来获得精确的红绿灯框位置，并根据此红绿灯框的位置进行红绿灯的颜色识别，得到红绿灯当前的状态。得到单帧的红绿灯状态后，通过时序的滤波矫正算法进一步确认红绿灯的最终状态。我们提出的基于 CNN 的红绿灯的检测和识别算法具有极高的召回率和准确率，可以支持白天和夜晚的红绿灯检测识别。具体算法请参阅 github 技术文档。”



## **模型数据**

下图是感知模块中的一些模型和参数：

![img](https://pic3.zhimg.com/80/v2-0111c563483e8ea2e02284e619b167e6_hd.jpg)

cnn_segmentation目录下是用于车道线分割的模型。traffic_light目录下有交通灯检测模型。yolo_camera_detector目录下有 YOLO 检测器的配置文件。感兴趣的读者可以进一步阅读。




# 相关

- [机器学习在自动驾驶中的应用-以百度阿波罗平台为例【上】](https://zhuanlan.zhihu.com/p/37472084)
