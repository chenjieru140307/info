
# 文字检测方法综述


文字检测是 OCR 技术重要的一环，它和通用物体检测最大的区别有两点：

- 一是正负样本选取的不同，比如一个 proposal 只覆盖了一长串文字中的几个字，可能 IOU 很小但是也应算作正样本。
- 二是文字检测对方向很敏感，物体检测中不管检测出的狗是正的还是斜的，后面都能识别对，而对于倾斜文字要检测出文字周围的 4 个点然后才能转正进行识别，如果只检测到两个点那文字只占了图片中一小部分，很难进行识别。


主流的文字检测方法大概有三种:

- 基于 SWT/MSER 等人工设计特征提取 character-level的 proposal，然后用 CNN 筛选 proposal，最后进行后处理得到文字框。这种方法之前比较流行，最近一年已经很少看到了，效果相对后面的方法较差
- 基于 RPN 进行检测，这种方法可以 end to end得到文字框，效果好，但由于原始的 RPN 对每个框是预测两个点的，所以对于倾斜文字目前公开的方法中没有很好的解决方案，不过是可以改进来预测倾斜框的四个点的，前几周 ICDAR 2015上新上传的方法貌似就是通过改进 RPN 解决的，不过论文还没有公布
- 基于 FCN 进行检测，这个方法缺点是 FCN 得到 heat map后还要进行复杂的后处理得到文字框，而通过 heat map难以分开多行文字，现有方法通过不同手段部分解决了这个问题，但复杂的步骤难以避免精度的损失，目前来看这类方法效果略差于基于 RPN 的方法

下面是我整理的一些论文以及笔记，最后整理了一个 PPT 在[这里](http://lufo.me/docs/text_detect.pdf)

SWT/MSER:

- Text-Attentional Convolutional Neural Network for Scene Text Detection

FCN:

- Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network
- Scene Text Detection via Holistic, Multi-Channel Prediction

RPN:

- DeepText: A Unified Framework for Text Proposal Generation and Text Detection in Natural Images
- TextBoxes: A Fast Text Detector with a Single Deep Neural Network
- Detecting Text in Natural Image with Connectionist Text Proposal Network

### Text-Attentional Convolutional Neural Network for Scene Text Detection

Contrast-Enhanced MSERs获得 char-level的 proposal，然后用 CNN 做一次过滤,CNN三个监督信息,segmentation,character-classfication和 text/non-text classfication,icdar 2013 82%

### DeepText: A Unified Framework for Text Proposal Generation and Text Detection in Natural Images

end-to-end,Inception结构，三个特点:

- ambiguous text category:为了解决文字检测中 iou 小的 proposal 也可能是正样本的问题，新增加 ambiguous text，即 iou 在 0.2-0.5之间的样本，变为三分类问题
- multi- level region-of-interest pooling:在 conv4 和 conv5 都进行 roi pooling，将得到的 feature concat，更好地 handle multi-scale的 text
- Iterative bounding box voting:将多个模型预测的 bbox 放在一起进行 nms，提高 recall

icdar2013 85%

### Detecting Text in Natural Image with Connectionist Text Proposal Network

- 新的正负样本选取方法，不考虑 x 方向，只考虑 y 方向的 overlap,anchor固定宽度，预测时只预测 y 方向
- 为了更好利用 x 方向的信息，图片经过 cnn 每一行过一个 lstm，去掉 lstm 效果差 8%
- 后处理简单，将 y 方向 overlap 超过 0.7,y方向距离小于阈值的 proposal 连在一起

icdar13上 88%，公布论文的方法里最好的

### TextBoxes: A Fast Text Detector with a Single Deep Neural Network

- end2end，类似 ssd
- 正负样本选择没有提到，应该和 rpn 一样
- 最后用对检测结果进行识别，过滤掉 score 低的

icdar13 86%，多个 size 的输入结合，只用 300x300 输入 81%，用时 0.09s(titan x)

### Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network

两个 cnn(Cascaded Convolutional Text Network,Fine Text Network)，结构类似，前面都是 vgg16，监督信息不同，第一个用于获取粗略的文字位置，输入为全图，第二个获取精准位置，输入为文字周围的图像，网络结构类似 vgg16，不同在于为了 handle 细长的文字,pool4和 pool5 之间有三个卷积层,kennel size分别为 3x3,3x7,7x3，将这三个卷积的输出加起来输入 pool5，然后 unsample 与 pool4 相加，获得 multi-scale信息同时也提高分辨率。第一个网络监督信息为 segmentation 的信息,gt bbox里面的为 1，外面为 0.第二个网络输出每行文字的中心行和文字位置，中心行的 gt 是 gt bbox中间为 1,0.25H和 0.75H为 0 的高斯分布，文字位置 gt 和第一个网络一样。先训练第一个网络，再 finetune 第二个.icdar 13 上结果为 86%

### Scene Text Detection via Holistic, Multi-Channel Prediction

先用 FCN 对每张图片预测 map of text region,map of character,map of linking orientation,map of text region和 map of character为 0-1之间的数，表示这个像素是问题的概率，为了避免多个字符的 map 重合，字符的 gt 缩小一半,map of linking orientation表示文字方向，在-180度到 180 度之间，归一化到 0-1，有三个 loss function，前两个为 softmax loss，最后一个为 sin(|预测角度-真实角度|)

得到这三个信息后，根据 text region的 heatmap 得到文字框，然后对文字框中每个字符根据 heatmap 得到他的中点和半径，使用 delaunay triangulation算法将这些点连起来,delaunay triangulation算法用线段连接这些点产生尽可能多的三角形，这样得到一个图，每个边的权值与两点的位置和方向有关，两点位置越近，两点连线的方向和两点连线上所有点预测的平均方向越近，两点权值越大

得到这个图之后，先用最大生成树算法得到这个图的生成树，然后确定 text line的个数 k 和每个 text line包含的字符，将这些点分为 k 类，计算每类的坐标点矩阵的特征值，取每类最大的特征值比次大的特征值之和最大的时候的 k，然后用最小生成树算法得到每类的点

icdar13 84.3% icdar15 64.7%





# 相关

- [文字检测方法综述](http://lufo.me/2017/02/text_detect/)
