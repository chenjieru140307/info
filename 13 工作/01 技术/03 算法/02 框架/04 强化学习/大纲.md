
# 大纲

作为一种新兴的深度学习技术，采用 DRL 面临着简单实现算法之外的诸多挑战，如训练数据集、环境、监测优化工具和精心设计的实验，以简化 DRL 技术的采用。考虑到机制与大多数传统的机器学习方法不同（DRL agent 尝试在给定环境中通过反复试验来完成任务），应用 DRL 更是困难。在这种情况下，环境和实验的稳健性在 DRL agent 开发的知识中起着最基本的作用。




## 可以补充进来的

- [DeepMind开源了强化学习库“松露”，团队自身也严重依赖它](https://blog.csdn.net/yH0VLDe8VG8ep9VGe/article/details/83155421)





强化学习（英语：Reinforcement learning，简称RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。

这个方法具有普适性，因此在其他许多领域都有研究，例如博弈论、控制论、运筹学、信息论、仿真优化、多主体系统学习、群体智能、统计学以及遗传算法。在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。在最优控制理论中也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在经济学和博弈论中，强化学习被用来解释在有限理性的条件下如何出现平衡。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。[[1\]](https://zhuanlan.zhihu.com/p/66834116#ref_1)

2018年许多大的公司都提出了自己的强化学习框架，下面就列出几个重要公司的强化学习开源框架：

**Dopamine framework强化学习框架**来自google，GitHub上的点赞数接近8K[[2\]](https://zhuanlan.zhihu.com/p/66834116#ref_2)，该研究框架用于快速原型化强化学习算法，是一个基于 Tensorflow 的框架。它旨在满足用户对一个小型的、容易修改的代码库的需求，用户可以在这个代码库中自由地实验各种想法。

该框架的设计原则是：

1.实验简单，让新用户可以轻松地运行基础实验；

2.开发灵活，让新用户更容易尝试研究想法；

3.紧凑可靠，实现一些经过实战检验的算法；

4.可重复性，保证结果的可重复性。

**强化学习模块TRFL**：该模块里面有用于在 TensorFlow 中编写强化学习代理的库，github上的点赞数有2500[[3\]](https://zhuanlan.zhihu.com/p/66834116#ref_3)。它代表了一系列关键算法组件，其内部使用了大量最成功的代理，如 DQN、 DDPG 和 Importance Weighted Actor Learner Architecture等。

TRFL库包括实现经典 RL 算法以及更多前沿技术的函数。 这里提供的损失函数和其他操作都是在纯 TensorFlow 中实现的。 它们不是完整的算法，而是构建全功能 RL 代理所需的特定于 RL 的数学计算的实现。

**Horizon平台：**来自Facebook 的开源应用强化学习平台，GitHub上的点赞数将近2K[[4\]](https://zhuanlan.zhihu.com/p/66834116#ref_4)。 Horizon 是一个端到端平台，旨在解决工业应用 RL 问题，其中数据集很大(数百万到数十亿个观测值) ，与模拟器相比，其反馈回路很慢，实验必须谨慎进行，因为它们不在模拟器中运行。 其他 RL 平台通常是为快速原型和实验而设计的，但**Horizon 是以生产用例为首要考虑的**。 该平台包含训练流程，包括数据预处理、特征转换、分布式训练、反事实策略评估和优化服务。 该平台还展示了一些真实的例子，在 Facebook 上，使用 Horizon 训练的模型的表现明显优于监督式学习系统。

TextWorld：来自微软的强化学习环境，GitHub上的点赞数差不多有700[[5\]](https://zhuanlan.zhihu.com/p/66834116#ref_5)多。该环境包含一个基于文本的游戏生成器和可扩展的沙盒学习环境，用于训练和测试强化学习代理。 TextWorld 将强化学习和自然语言结合起来，在 TextWorld 中，代理必须同时读取和生成自然语言，自然语言具有完全不同的、在许多情况下更为复杂的结构。







近日，[DeepMind](https://www.deepmind.com/) 开源了史上最全强化学习框架 [OpenSpiel](https://arxiv.org/abs/1908.09453)。

在这个框架中，实现了 28 款可用于研究强化学习的棋牌类游戏和 24 个强化学习中常用的算法，DeepMind 的成名作 AlphaGo 的基础算法也在其列。

此外，还包括用于分析学习动态和其他常见评估指标的工具。更关键的是，OpenSpiel 适用人群也非常广泛。它的核心 API 和游戏用 C++ 实现，并提供了 Python API，可用于更高级别的机器学习，优化和强化学习。这些语言也可以很容易地组合在一起。而且，代码的一个子集也已经使用 Swift for Tensorflow 直接移植到了 Swift 进行学习和推理，不需要跨语言操作。

目前，在 OpenSpiel 中实现的算法一共有 24 种，分别是：

> 极小化极大（Alpha-beta剪枝）搜索、蒙特卡洛树搜索、序列形式线性规划、虚拟遗憾最小化（CFR）、Exploitability、外部抽样蒙特卡洛 CFR、结果抽样蒙特卡洛CFR、Q-learning、价值迭代、优势动作评论算法(Advantage Actor Critic，A2C)、Deep Q-networks (DQN)、短期价值调整（EVA）、Deep CFR、Exploitability 下降(ED) 、（扩展形式）虚拟博弈（XFP）、神经虚拟自博弈(NFSP)、Neural Replicator Dynamics（NeuRD）、遗憾策略梯度（RPG, RMPG）、策略空间回应oracle（PSRO）、基于Q的所有行动策略梯度（QPG）、回归 CFR (RCFR)、PSROrN、α-Rank、复制/演化动力学。

Spiel 意指桌面游戏。因此，OpenSpiel 中的环境就是相关棋牌类游戏。一共有 28 款：

> 双陆棋、突围棋、定约桥牌、Coin Game、屏风式四子棋、协作推箱子、国际象棋、第一价格密封拍卖、围棋、Goofspiel（一种多玩家纸牌游戏）、三宝棋、六贯棋、Kuhn 扑克、Leduc 扑克、大话骰、Markov Soccer、配对硬币（3人游戏）、矩阵游戏、Oshi-Zumo、西非播棋、转盘五子棋、Phantom 三连棋、Pig 游戏、三连棋、Tiny Bridge、Y（一种棋类游戏）、Catch（仅支持Python）、Cliff-Walking在悬崖边走的醉汉（仅支持Python）。