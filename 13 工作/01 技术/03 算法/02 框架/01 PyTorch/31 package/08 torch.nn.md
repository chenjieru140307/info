
# TORCH.NN

## Parameters

- *CLASS*`torch.nn.``Parameter`[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/parameter.html#Parameter)

  A kind of Tensor that is to be considered a module parameter.Parameters are [`Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor) subclasses, that have a very special property when used with [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in [`parameters()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.parameters) iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as [`Parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter), these temporaries would get registered too.Parameters**data** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – parameter tensor.**requires_grad** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – if the parameter requires gradient. See [Excluding subgraphs from backward](https://pytorch.org/docs/stable/notes/autograd.html#excluding-subgraphs) for more details. Default: True

## Containers

### Module

- *CLASS*`torch.nn.``Module`[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module)

  Base class for all neural network modules.Your models should also subclass this class.Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:`import torch.nn as nn import torch.nn.functional as F  class Model(nn.Module):     def __init__(self):         super(Model, self).__init__()         self.conv1 = nn.Conv2d(1, 20, 5)         self.conv2 = nn.Conv2d(20, 20, 5)      def forward(self, x):        x = F.relu(self.conv1(x))        return F.relu(self.conv2(x)) `Submodules assigned in this way will be registered, and will have their parameters converted too when you call [`to()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.to), etc.`add_module`(*name*, *module*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.add_module)Adds a child module to the current module.The module can be accessed as an attribute using the given name.Parameters**name** (*string*) – name of the child module. The child module can be accessed from this module using the given name**module** ([*Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – child module to be added to the module.`apply`(*fn*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.apply)Applies `fn` recursively to every submodule (as returned by `.children()`) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init).Parameters**fn** ([`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) -> None) – function to be applied to each submoduleReturnsselfReturn type[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)Example:`>>> def init_weights(m): >>>     print(m) >>>     if type(m) == nn.Linear: >>>         m.weight.data.fill_(1.0) >>>         print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) ``buffers`(*recurse=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.buffers)Returns an iterator over module buffers.Parameters**recurse** ([*bool*](https://docs.python.org/3/library/functions.html#bool)) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.Yields*torch.Tensor* – module bufferExample:`>>> for buf in model.buffers(): >>>     print(type(buf.data), buf.size()) <class 'torch.FloatTensor'> (20L,) <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L) ``children`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.children)Returns an iterator over immediate children modules.Yields*Module* – a child module`cpu`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.cpu)Moves all model parameters and buffers to the CPU.ReturnsselfReturn type[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)`cuda`(*device=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.cuda)Moves all model parameters and buffers to the GPU.This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.Parameters**device** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – if specified, all parameters will be copied to that deviceReturnsselfReturn type[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)`double`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.double)Casts all floating point parameters and buffers to `double` datatype.ReturnsselfReturn type[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)`dump_patches` *= FALSE*This allows better BC support for [`load_state_dict()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.load_state_dict). In [`state_dict()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict), the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See `_load_from_state_dict` on how to use this information in loading.If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module’s _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change.`eval`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval)Sets the module in evaluation mode.This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. [`Dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout), `BatchNorm`, etc.`extra_repr`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.extra_repr)Set the extra representation of the moduleTo print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable.`float`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.float)Casts all floating point parameters and buffers to float datatype.ReturnsselfReturn type[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)`forward`(**input*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.forward)Defines the computation performed at every call.Should be overridden by all subclasses.NOTEAlthough the recipe for forward pass needs to be defined within this function, one should call the [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.`half`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.half)Casts all floating point parameters and buffers to `half` datatype.ReturnsselfReturn type[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)`load_state_dict`(*state_dict*, *strict=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.load_state_dict)Copies parameters and buffers from [`state_dict`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict) into this module and its descendants. If `strict` is `True`, then the keys of [`state_dict`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict) must exactly match the keys returned by this module’s [`state_dict()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict) function.Parameters**state_dict** ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict)) – a dict containing parameters and persistent buffers.**strict** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – whether to strictly enforce that the keys in [`state_dict`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict) match the keys returned by this module’s [`state_dict()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict)function. Default: `True`Returns**missing_keys** is a list of str containing the missing keys**unexpected_keys** is a list of str containing the unexpected keysReturn type`NamedTuple` with `missing_keys` and `unexpected_keys` fields`modules`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.modules)Returns an iterator over all modules in the network.Yields*Module* – a module in the networkNOTEDuplicate modules are returned only once. In the following example, `l` will be returned only once.Example:`>>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()):         print(idx, '->', m)  0 -> Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) ``named_buffers`(*prefix=''*, *recurse=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.named_buffers)Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.Parameters**prefix** ([*str*](https://docs.python.org/3/library/stdtypes.html#str)) – prefix to prepend to all buffer names.**recurse** ([*bool*](https://docs.python.org/3/library/functions.html#bool)) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.Yields*(string, torch.Tensor)* – Tuple containing the name and bufferExample:`>>> for name, buf in self.named_buffers(): >>>    if name in ['running_var']: >>>        print(buf.size()) ``named_children`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.named_children)Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.Yields*(string, Module)* – Tuple containing a name and child moduleExample:`>>> for name, module in model.named_children(): >>>     if name in ['conv4', 'conv5']: >>>         print(module) ``named_modules`(*memo=None*, *prefix=''*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.named_modules)Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.Yields*(string, Module)* – Tuple of name and moduleNOTEDuplicate modules are returned only once. In the following example, `l` will be returned only once.Example:`>>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()):         print(idx, '->', m)  0 -> ('', Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) ``named_parameters`(*prefix=''*, *recurse=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.named_parameters)Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.Parameters**prefix** ([*str*](https://docs.python.org/3/library/stdtypes.html#str)) – prefix to prepend to all parameter names.**recurse** ([*bool*](https://docs.python.org/3/library/functions.html#bool)) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.Yields*(string, Parameter)* – Tuple containing the name and parameterExample:`>>> for name, param in self.named_parameters(): >>>    if name in ['bias']: >>>        print(param.size()) ``parameters`(*recurse=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.parameters)Returns an iterator over module parameters.This is typically passed to an optimizer.Parameters**recurse** ([*bool*](https://docs.python.org/3/library/functions.html#bool)) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.Yields*Parameter* – module parameterExample:`>>> for param in model.parameters(): >>>     print(type(param.data), param.size()) <class 'torch.FloatTensor'> (20L,) <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L) ``register_backward_hook`(*hook*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_backward_hook)Registers a backward hook on the module.The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:`hook(module, grad_input, grad_output) -> Tensor or None `The `grad_input` and `grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of `grad_input` in subsequent computations.Returnsa handle that can be used to remove the added hook by calling `handle.remove()`Return type`torch.utils.hooks.RemovableHandle`WARNINGThe current implementation will not have the presented behavior for complex [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) that perform many operations. In some failure cases, `grad_input` and `grad_output` will only contain the gradients for a subset of the inputs and outputs. For such [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module), you should use [`torch.Tensor.register_hook()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.register_hook) directly on a specific input or output to get the required gradients.`register_buffer`(*name*, *tensor*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_buffer)Adds a persistent buffer to the module.This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm’s `running_mean` is not a parameter, but is part of the persistent state.Buffers can be accessed as attributes using given names.Parameters**name** (*string*) – name of the buffer. The buffer can be accessed from this module using the given name**tensor** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – buffer to be registered.Example:`>>> self.register_buffer('running_mean', torch.zeros(num_features)) ``register_forward_hook`(*hook*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_forward_hook)Registers a forward hook on the module.The hook will be called every time after [`forward()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward) has computed an output. It should have the following signature:`hook(module, input, output) -> None `The hook should not modify the input or output.Returnsa handle that can be used to remove the added hook by calling `handle.remove()`Return type`torch.utils.hooks.RemovableHandle``register_forward_pre_hook`(*hook*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook)Registers a forward pre-hook on the module.The hook will be called every time before [`forward()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward) is invoked. It should have the following signature:`hook(module, input) -> None `The hook should not modify the input.Returnsa handle that can be used to remove the added hook by calling `handle.remove()`Return type`torch.utils.hooks.RemovableHandle``register_parameter`(*name*, *param*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_parameter)Adds a parameter to the module.The parameter can be accessed as an attribute using given name.Parameters**name** (*string*) – name of the parameter. The parameter can be accessed from this module using the given name**param** ([*Parameter*](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter)) – parameter to be added to the module.`state_dict`(*destination=None*, *prefix=''*, *keep_vars=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.state_dict)Returns a dictionary containing a whole state of the module.Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.Returnsa dictionary containing a whole state of the moduleReturn type[dict](https://docs.python.org/3/library/stdtypes.html#dict)Example:`>>> module.state_dict().keys() ['bias', 'weight'] ``to`(**args*, **\*kwargs*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to)Moves and/or casts the parameters and buffers.This can be called as`to`(*device=None*, *dtype=None*, *non_blocking=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to)`to`(*dtype*, *non_blocking=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to)`to`(*tensor*, *non_blocking=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.to)Its signature is similar to [`torch.Tensor.to()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to), but only accepts floating point desired `dtype` s. In addition, this method will only cast the floating point parameters and buffers to `dtype` (if given). The integral parameters and buffers will be moved `device`, if that is given, but with dtypes unchanged. When `non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.See below for examples.NOTEThis method modifies the module in-place.Parameters**device** (`torch.device`) – the desired device of the parameters and buffers in this module**dtype** (`torch.dtype`) – the desired floating point type of the floating point parameters and buffers in this module**tensor** ([*torch.Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this moduleReturnsselfReturn type[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)Example:`>>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device("cuda:1") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device("cpu") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16) ``train`(*mode=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.train)Sets the module in training mode.This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. [`Dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout), `BatchNorm`, etc.ReturnsselfReturn type[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)`type`(*dst_type*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.type)Casts all parameters and buffers to `dst_type`.Parameters**dst_type** ([*type*](https://docs.python.org/3/library/functions.html#type) *or* *string*) – the desired typeReturnsselfReturn type[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)`zero_grad`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.zero_grad)Sets gradients of all model parameters to zero.

### Sequential

- *CLASS*`torch.nn.``Sequential`(**args*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#Sequential)

  A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in.To make it easier to understand, here is a small example:`# Example of using Sequential model = nn.Sequential(           nn.Conv2d(1,20,5),           nn.ReLU(),           nn.Conv2d(20,64,5),           nn.ReLU()         )  # Example of using Sequential with OrderedDict model = nn.Sequential(OrderedDict([           ('conv1', nn.Conv2d(1,20,5)),           ('relu1', nn.ReLU()),           ('conv2', nn.Conv2d(20,64,5)),           ('relu2', nn.ReLU())         ])) `

### ModuleList

- *CLASS*`torch.nn.``ModuleList`(*modules=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList)

  Holds submodules in a list.[`ModuleList`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleList) can be indexed like a regular python list, but modules it contains are properly registered, and will be visible by all [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) methods.Parameters**modules** (*iterable**,* *optional*) – an iterable of modules to addExample:`class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])      def forward(self, x):         # ModuleList can act as an iterable, or be indexed using ints         for i, l in enumerate(self.linears):             x = self.linears[i // 2](x) + l(x)         return x ``append`(*module*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList.append)Appends a given module to the end of the list.Parameters**module** ([*nn.Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – module to append`extend`(*modules*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList.extend)Appends modules from a python iterable to the end of the list.Parameters**modules** (*iterable*) – iterable of modules to append`insert`(*index*, *module*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleList.insert)Insert a given module before a given index in the list.Parameters**index** ([*int*](https://docs.python.org/3/library/functions.html#int)) – index to insert.**module** ([*nn.Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – module to insert

### ModuleDict

- *CLASS*`torch.nn.``ModuleDict`(*modules=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict)

  Holds submodules in a dictionary.[`ModuleDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict) can be indexed like a regular python dictionary, but modules it contains are properly registered, and will be visible by all [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) methods.[`ModuleDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict) is an **ordered** dictionary that respectsthe order of insertion, andin [`update()`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict.update), the order of the merged `OrderedDict` or another [`ModuleDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict) (the argument to [`update()`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict.update)).Note that [`update()`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict.update) with other unordered mapping types (e.g., python’s plain `dict`) does not preserve the order of the merged mapping.Parameters**modules** (*iterable**,* *optional*) – a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module)Example:`class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.choices = nn.ModuleDict({                 'conv': nn.Conv2d(10, 10, 3),                 'pool': nn.MaxPool2d(3)         })         self.activations = nn.ModuleDict([                 ['lrelu', nn.LeakyReLU()],                 ['prelu', nn.PReLU()]         ])      def forward(self, x, choice, act):         x = self.choices[choice](x)         x = self.activations[act](x)         return x ``clear`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.clear)Remove all items from the ModuleDict.`items`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.items)Return an iterable of the ModuleDict key/value pairs.`keys`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.keys)Return an iterable of the ModuleDict keys.`pop`(*key*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.pop)Remove key from the ModuleDict and return its module.Parameters**key** (*string*) – key to pop from the ModuleDict`update`(*modules*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.update)Update the [`ModuleDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict) with the key-value pairs from a mapping or an iterable, overwriting existing keys.NOTEIf `modules` is an `OrderedDict`, a [`ModuleDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict), or an iterable of key-value pairs, the order of new elements in it is preserved.Parameters**modules** (*iterable*) – a mapping (dictionary) from string to [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module), or an iterable of key-value pairs of type (string, [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module))`values`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ModuleDict.values)Return an iterable of the ModuleDict values.

### ParameterList

- *CLASS*`torch.nn.``ParameterList`(*parameters=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterList)

  Holds parameters in a list.[`ParameterList`](https://pytorch.org/docs/stable/nn.html#torch.nn.ParameterList) can be indexed like a regular python list, but parameters it contains are properly registered, and will be visible by all [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) methods.Parameters**parameters** (*iterable**,* *optional*) – an iterable of [`Parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter) to addExample:`class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])      def forward(self, x):         # ParameterList can act as an iterable, or be indexed using ints         for i, p in enumerate(self.params):             x = self.params[i // 2].mm(x) + p.mm(x)         return x ``append`(*parameter*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterList.append)Appends a given parameter at the end of the list.Parameters**parameter** ([*nn.Parameter*](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter)) – parameter to append`extend`(*parameters*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterList.extend)Appends parameters from a python iterable to the end of the list.Parameters**parameters** (*iterable*) – iterable of parameters to append

### ParameterDict

- *CLASS*`torch.nn.``ParameterDict`(*parameters=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict)

  Holds parameters in a dictionary.ParameterDict can be indexed like a regular python dictionary, but parameters it contains are properly registered, and will be visible by all Module methods.[`ParameterDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ParameterDict) is an **ordered** dictionary that respectsthe order of insertion, andin [`update()`](https://pytorch.org/docs/stable/nn.html#torch.nn.ParameterDict.update), the order of the merged `OrderedDict` or another [`ParameterDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ParameterDict) (the argument to [`update()`](https://pytorch.org/docs/stable/nn.html#torch.nn.ParameterDict.update)).Note that [`update()`](https://pytorch.org/docs/stable/nn.html#torch.nn.ParameterDict.update) with other unordered mapping types (e.g., python’s plain `dict`) does not preserve the order of the merged mapping.Parameters**parameters** (*iterable**,* *optional*) – a mapping (dictionary) of (string : [`Parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter)) or an iterable of key-value pairs of type (string, [`Parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter))Example:`class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.params = nn.ParameterDict({                 'left': nn.Parameter(torch.randn(5, 10)),                 'right': nn.Parameter(torch.randn(5, 10))         })      def forward(self, x, choice):         x = self.params[choice].mm(x)         return x ``clear`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.clear)Remove all items from the ParameterDict.`items`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.items)Return an iterable of the ParameterDict key/value pairs.`keys`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.keys)Return an iterable of the ParameterDict keys.`pop`(*key*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.pop)Remove key from the ParameterDict and return its parameter.Parameters**key** (*string*) – key to pop from the ParameterDict`update`(*parameters*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.update)Update the [`ParameterDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ParameterDict) with the key-value pairs from a mapping or an iterable, overwriting existing keys.NOTEIf `parameters` is an `OrderedDict`, a [`ParameterDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ParameterDict), or an iterable of key-value pairs, the order of new elements in it is preserved.Parameters**parameters** (*iterable*) – a mapping (dictionary) from string to [`Parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter), or an iterable of key-value pairs of type (string, [`Parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter))`values`()[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/container.html#ParameterDict.values)Return an iterable of the ParameterDict values.

## Convolution layers

### Conv1d

- *CLASS*`torch.nn.``Conv1d`(*in_channels*, *out_channels*, *kernel_size*, *stride=1*, *padding=0*, *dilation=1*, *groups=1*, *bias=True*, *padding_mode='zeros'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv1d)

  Applies a 1D convolution over an input signal composed of several input planes.In the simplest case, the output value of the layer with input size (N, C_{\text{in}}, L)(N,Cin,L) and output (N, C_{\text{out}}, L_{\text{out}})(N,Cout,Lout) can be precisely described as:\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)out(Ni,Coutj)=bias(Coutj)+k=0∑Cin−1weight(Coutj,k)⋆input(Ni,k)where \star⋆ is the valid [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation) operator, NN is a batch size, CC denotes a number of channels, LL is a length of signal sequence.`stride` controls the stride for the cross-correlation, a single number or a one-element tuple.`padding` controls the amount of implicit zero-paddings on both sides for `padding` number of points.`dilation` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.`groups` controls the connections between inputs and outputs. `in_channels` and `out_channels` must both be divisible by `groups`. For example,At groups=1, all inputs are convolved to all outputs.At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.At groups= `in_channels`, each input channel is convolved with its own set of filters, of size \left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor⌊in_channelsout_channels⌋.NOTEDepending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation), and not a full [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation). It is up to the user to add proper padding.NOTEWhen groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also termed in literature as depthwise convolution.In other words, for an input of size (N, C_{in}, L_{in})(N,Cin,Lin), a depthwise convolution with a depthwise multiplier K, can be constructed by arguments (C_\text{in}=C_{in}, C_\text{out}=C_{in} \times K, ..., \text{groups}=C_{in})(Cin=Cin,Cout=Cin×K,...,groups=Cin).NOTEIn some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting `torch.backends.cudnn.deterministic = True`. Please see the notes on [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) for background.Parameters**in_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels in the input image**out_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels produced by the convolution**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the convolving kernel**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Stride of the convolution. Default: 1**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Zero-padding added to both sides of the input. Default: 0**padding_mode** (*string**,* *optional*) – zeros**dilation** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Spacing between kernel elements. Default: 1**groups** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Number of blocked connections from input channels to output channels. Default: 1**bias** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, adds a learnable bias to the output. Default: `True`Shape:Input: (N, C_{in}, L_{in})(N,Cin,Lin)Output: (N, C_{out}, L_{out})(N,Cout,Lout) whereL_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloorLout=⌊strideLin+2×padding−dilation×(kernel_size−1)−1+1⌋Variables**~Conv1d.weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable weights of the module of shape (\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})(out_channels,groupsin_channels,kernel_size). The values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{C_\text{in} * \text{kernel\_size}}k=Cin∗kernel_size1**~Conv1d.bias** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable bias of the module of shape (out_channels). If `bias` is `True`, then the values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k)where k = \frac{1}{C_\text{in} * \text{kernel\_size}}k=Cin∗kernel_size1Examples:`>>> m = nn.Conv1d(16, 33, 3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) `

### Conv2d

- *CLASS*`torch.nn.``Conv2d`(*in_channels*, *out_channels*, *kernel_size*, *stride=1*, *padding=0*, *dilation=1*, *groups=1*, *bias=True*, *padding_mode='zeros'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv2d)

  Applies a 2D convolution over an input signal composed of several input planes.In the simplest case, the output value of the layer with input size (N, C_{\text{in}}, H, W)(N,Cin,H,W) and output (N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})(N,Cout,Hout,Wout) can be precisely described as:\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)out(Ni,Coutj)=bias(Coutj)+k=0∑Cin−1weight(Coutj,k)⋆input(Ni,k)where \star⋆ is the valid 2D [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation) operator, NN is a batch size, CC denotes a number of channels, HH is a height of input planes in pixels, and WW is width in pixels.`stride` controls the stride for the cross-correlation, a single number or a tuple.`padding` controls the amount of implicit zero-paddings on both sides for `padding` number of points for each dimension.`dilation` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.`groups` controls the connections between inputs and outputs. `in_channels` and `out_channels` must both be divisible by `groups`. For example,At groups=1, all inputs are convolved to all outputs.At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.At groups= `in_channels`, each input channel is convolved with its own set of filters, of size: \left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor⌊in_channelsout_channels⌋.The parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:a single `int` – in which case the same value is used for the height and width dimensiona `tuple` of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimensionNOTEDepending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation), and not a full [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation). It is up to the user to add proper padding.NOTEWhen groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also termed in literature as depthwise convolution.In other words, for an input of size (N, C_{in}, H_{in}, W_{in})(N,Cin,Hin,Win), a depthwise convolution with a depthwise multiplier K, can be constructed by arguments (in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})(in_channels=Cin,out_channels=Cin×K,...,groups=Cin).NOTEIn some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting `torch.backends.cudnn.deterministic = True`. Please see the notes on [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) for background.Parameters**in_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels in the input image**out_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels produced by the convolution**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the convolving kernel**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Stride of the convolution. Default: 1**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Zero-padding added to both sides of the input. Default: 0**padding_mode** (*string**,* *optional*) – zeros**dilation** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Spacing between kernel elements. Default: 1**groups** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Number of blocked connections from input channels to output channels. Default: 1**bias** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, adds a learnable bias to the output. Default: `True`Shape:Input: (N, C_{in}, H_{in}, W_{in})(N,Cin,Hin,Win)Output: (N, C_{out}, H_{out}, W_{out})(N,Cout,Hout,Wout) whereH_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloorHout=⌊stride[0]Hin+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1+1⌋W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloorWout=⌊stride[1]Win+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1+1⌋Variables**~Conv2d.weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable weights of the module of shape (\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},(out_channels,groupsin_channels, \text{kernel\_size[0]}, \text{kernel\_size[1]})kernel_size[0],kernel_size[1]). The values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}k=Cin∗∏i=01kernel_size[i]1**~Conv2d.bias** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable bias of the module of shape (out_channels). If `bias` is `True`, then the values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k)where k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}k=Cin∗∏i=01kernel_size[i]1Examples:`>>> # With square kernels and equal stride >>> m = nn.Conv2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> # non-square kernels and unequal stride and with padding and dilation >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) >>> input = torch.randn(20, 16, 50, 100) >>> output = m(input) `

### Conv3d

- *CLASS*`torch.nn.``Conv3d`(*in_channels*, *out_channels*, *kernel_size*, *stride=1*, *padding=0*, *dilation=1*, *groups=1*, *bias=True*, *padding_mode='zeros'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv3d)

  Applies a 3D convolution over an input signal composed of several input planes.In the simplest case, the output value of the layer with input size (N, C_{in}, D, H, W)(N,Cin,D,H,W) and output (N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout,Dout,Hout,Wout) can be precisely described as:out(N_i, C_{out_j}) = bias(C_{out_j}) + \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)out(Ni,Coutj)=bias(Coutj)+k=0∑Cin−1weight(Coutj,k)⋆input(Ni,k)where \star⋆ is the valid 3D [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation) operator`stride` controls the stride for the cross-correlation.`padding` controls the amount of implicit zero-paddings on both sides for `padding` number of points for each dimension.`dilation` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.`groups` controls the connections between inputs and outputs. `in_channels` and `out_channels` must both be divisible by `groups`. For example,At groups=1, all inputs are convolved to all outputs.At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.At groups= `in_channels`, each input channel is convolved with its own set of filters, of size \left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor⌊in_channelsout_channels⌋.The parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:a single `int` – in which case the same value is used for the depth, height and width dimensiona `tuple` of three ints – in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimensionNOTEDepending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation), and not a full [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation). It is up to the user to add proper padding.NOTEWhen groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also termed in literature as depthwise convolution.In other words, for an input of size (N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin,Din,Hin,Win), a depthwise convolution with a depthwise multiplier K, can be constructed by arguments (in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})(in_channels=Cin,out_channels=Cin×K,...,groups=Cin).NOTEIn some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting `torch.backends.cudnn.deterministic = True`. Please see the notes on [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) for background.Parameters**in_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels in the input image**out_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels produced by the convolution**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the convolving kernel**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Stride of the convolution. Default: 1**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Zero-padding added to all three sides of the input. Default: 0**padding_mode** (*string**,* *optional*) – zeros**dilation** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Spacing between kernel elements. Default: 1**groups** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Number of blocked connections from input channels to output channels. Default: 1**bias** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, adds a learnable bias to the output. Default: `True`Shape:Input: (N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin,Din,Hin,Win)Output: (N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout,Dout,Hout,Wout) whereD_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloorDout=⌊stride[0]Din+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1+1⌋H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloorHout=⌊stride[1]Hin+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1+1⌋W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloorWout=⌊stride[2]Win+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1+1⌋Variables**~Conv3d.weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable weights of the module of shape (\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},(out_channels,groupsin_channels, \text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]). The values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}k=Cin∗∏i=02kernel_size[i]1**~Conv3d.bias** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable bias of the module of shape (out_channels). If `bias` is `True`, then the values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k)where k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}k=Cin∗∏i=02kernel_size[i]1Examples:`>>> # With square kernels and equal stride >>> m = nn.Conv3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0)) >>> input = torch.randn(20, 16, 10, 50, 100) >>> output = m(input) `

### ConvTranspose1d

- *CLASS*`torch.nn.``ConvTranspose1d`(*in_channels*, *out_channels*, *kernel_size*, *stride=1*, *padding=0*, *output_padding=0*, *groups=1*, *bias=True*, *dilation=1*, *padding_mode='zeros'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose1d)

  Applies a 1D transposed convolution operator over an input image composed of several input planes.This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).`stride` controls the stride for the cross-correlation.`padding` controls the amount of implicit zero-paddings on both sides for `dilation *(kernel_size - 1) - padding` number of points. See note below for details.`output_padding` controls the additional size added to one side of the output shape. See note below for details.`dilation` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.`groups` controls the connections between inputs and outputs. `in_channels` and `out_channels` must both be divisible by `groups`. For example,At groups=1, all inputs are convolved to all outputs.At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.At groups= `in_channels`, each input channel is convolved with its own set of filters (of size \left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor⌊in_channelsout_channels⌋).NOTEDepending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation), and not a full [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation). It is up to the user to add proper padding.NOTEThe `padding` argument effectively adds `dilation * (kernel_size - 1) - padding`amount of zero padding to both sizes of the input. This is set so that when a [`Conv1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv1d) and a [`ConvTranspose1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose1d) are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when `stride > 1`, [`Conv1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv1d) maps multiple input shapes to the same output shape. `output_padding` is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that `output_padding`is only used to find output shape, but does not actually add zero-padding to output.NOTEIn some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting `torch.backends.cudnn.deterministic = True`. Please see the notes on [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) for background.Parameters**in_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels in the input image**out_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels produced by the convolution**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the convolving kernel**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Stride of the convolution. Default: 1**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – `dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of the input. Default: 0**output_padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Additional size added to one side of the output shape. Default: 0**groups** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Number of blocked connections from input channels to output channels. Default: 1**bias** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, adds a learnable bias to the output. Default: `True`**dilation** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Spacing between kernel elements. Default: 1Shape:Input: (N, C_{in}, L_{in})(N,Cin,Lin)Output: (N, C_{out}, L_{out})(N,Cout,Lout) whereL_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation} \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1Lout=(Lin−1)×stride−2×padding+dilation×(kernel_size−1)+output_padding+1Variables**~ConvTranspose1d.weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable weights of the module of shape (\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},(in_channels,groupsout_channels, \text{kernel\_size})kernel_size). The values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{C_\text{in} * \text{kernel\_size}}k=Cin∗kernel_size1**~ConvTranspose1d.bias** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable bias of the module of shape (out_channels). If `bias` is `True`, then the values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{C_\text{in} * \text{kernel\_size}}k=Cin∗kernel_size1

### ConvTranspose2d

- *CLASS*`torch.nn.``ConvTranspose2d`(*in_channels*, *out_channels*, *kernel_size*, *stride=1*, *padding=0*, *output_padding=0*, *groups=1*, *bias=True*, *dilation=1*, *padding_mode='zeros'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose2d)

  Applies a 2D transposed convolution operator over an input image composed of several input planes.This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).`stride` controls the stride for the cross-correlation.`padding` controls the amount of implicit zero-paddings on both sides for `dilation *(kernel_size - 1) - padding` number of points. See note below for details.`output_padding` controls the additional size added to one side of the output shape. See note below for details.`dilation` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.`groups` controls the connections between inputs and outputs. `in_channels` and `out_channels` must both be divisible by `groups`. For example,At groups=1, all inputs are convolved to all outputs.At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.At groups= `in_channels`, each input channel is convolved with its own set of filters (of size \left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor⌊in_channelsout_channels⌋).The parameters `kernel_size`, `stride`, `padding`, `output_padding` can either be:a single `int` – in which case the same value is used for the height and width dimensionsa `tuple` of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimensionNOTEDepending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation), and not a full [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation). It is up to the user to add proper padding.NOTEThe `padding` argument effectively adds `dilation * (kernel_size - 1) - padding`amount of zero padding to both sizes of the input. This is set so that when a [`Conv2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) and a [`ConvTranspose2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d) are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when `stride > 1`, [`Conv2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) maps multiple input shapes to the same output shape. `output_padding` is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that `output_padding`is only used to find output shape, but does not actually add zero-padding to output.NOTEIn some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting `torch.backends.cudnn.deterministic = True`. Please see the notes on [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) for background.Parameters**in_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels in the input image**out_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels produced by the convolution**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the convolving kernel**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Stride of the convolution. Default: 1**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – `dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Default: 0**output_padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Additional size added to one side of each dimension in the output shape. Default: 0**groups** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Number of blocked connections from input channels to output channels. Default: 1**bias** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, adds a learnable bias to the output. Default: `True`**dilation** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Spacing between kernel elements. Default: 1Shape:Input: (N, C_{in}, H_{in}, W_{in})(N,Cin,Hin,Win)Output: (N, C_{out}, H_{out}, W_{out})(N,Cout,Hout,Wout) whereH_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1Hout=(Hin−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1Wout=(Win−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1Variables**~ConvTranspose2d.weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable weights of the module of shape (\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},(in_channels,groupsout_channels, \text{kernel\_size[0]}, \text{kernel\_size[1]})kernel_size[0],kernel_size[1]). The values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}k=Cin∗∏i=01kernel_size[i]1**~ConvTranspose2d.bias** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable bias of the module of shape (out_channels) If `bias` is `True`, then the values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}k=Cin∗∏i=01kernel_size[i]1Examples:`>>> # With square kernels and equal stride >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) >>> input = torch.randn(20, 16, 50, 100) >>> output = m(input) >>> # exact output size can be also specified as an argument >>> input = torch.randn(1, 16, 12, 12) >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1) >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1) >>> h = downsample(input) >>> h.size() torch.Size([1, 16, 6, 6]) >>> output = upsample(h, output_size=input.size()) >>> output.size() torch.Size([1, 16, 12, 12]) `

### ConvTranspose3d

- *CLASS*`torch.nn.``ConvTranspose3d`(*in_channels*, *out_channels*, *kernel_size*, *stride=1*, *padding=0*, *output_padding=0*, *groups=1*, *bias=True*, *dilation=1*, *padding_mode='zeros'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose3d)

  Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes.This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).`stride` controls the stride for the cross-correlation.`padding` controls the amount of implicit zero-paddings on both sides for `dilation *(kernel_size - 1) - padding` number of points. See note below for details.`output_padding` controls the additional size added to one side of the output shape. See note below for details.`dilation` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.`groups` controls the connections between inputs and outputs. `in_channels` and `out_channels` must both be divisible by `groups`. For example,At groups=1, all inputs are convolved to all outputs.At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.At groups= `in_channels`, each input channel is convolved with its own set of filters (of size \left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor⌊in_channelsout_channels⌋).The parameters `kernel_size`, `stride`, `padding`, `output_padding` can either be:a single `int` – in which case the same value is used for the depth, height and width dimensionsa `tuple` of three ints – in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimensionNOTEDepending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation), and not a full [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation). It is up to the user to add proper padding.NOTEThe `padding` argument effectively adds `dilation * (kernel_size - 1) - padding`amount of zero padding to both sizes of the input. This is set so that when a [`Conv3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv3d) and a [`ConvTranspose3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose3d) are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when `stride > 1`, [`Conv3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv3d) maps multiple input shapes to the same output shape. `output_padding` is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that `output_padding`is only used to find output shape, but does not actually add zero-padding to output.NOTEIn some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting `torch.backends.cudnn.deterministic = True`. Please see the notes on [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) for background.Parameters**in_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels in the input image**out_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of channels produced by the convolution**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the convolving kernel**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Stride of the convolution. Default: 1**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – `dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Default: 0**output_padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Additional size added to one side of each dimension in the output shape. Default: 0**groups** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Number of blocked connections from input channels to output channels. Default: 1**bias** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, adds a learnable bias to the output. Default: `True`**dilation** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – Spacing between kernel elements. Default: 1Shape:Input: (N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin,Din,Hin,Win)Output: (N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout,Dout,Hout,Wout) whereD_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1Dout=(Din−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1Hout=(Hin−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1Wout=(Win−1)×stride[2]−2×padding[2]+dilation[2]×(kernel_size[2]−1)+output_padding[2]+1Variables**~ConvTranspose3d.weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable weights of the module of shape (\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},(in_channels,groupsout_channels, \text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]). The values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}k=Cin∗∏i=02kernel_size[i]1**~ConvTranspose3d.bias** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable bias of the module of shape (out_channels) If `bias` is `True`, then the values of these weights are sampled from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}k=Cin∗∏i=02kernel_size[i]1Examples:`>>> # With square kernels and equal stride >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2)) >>> input = torch.randn(20, 16, 10, 50, 100) >>> output = m(input) `

### Unfold

- *CLASS*`torch.nn.``Unfold`(*kernel_size*, *dilation=1*, *padding=0*, *stride=1*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/fold.html#Unfold)

  Extracts sliding local blocks from a batched input tensor.Consider an batched `input` tensor of shape (N, C, *)(N,C,∗), where NN is the batch dimension, CC is the channel dimension, and *∗ represent arbitrary spatial dimensions. This operation flattens each sliding `kernel_size`-sized block within the spatial dimensions of `input` into a column (i.e., last dimension) of a 3-D `output` tensor of shape (N, C \times \prod(\text{kernel\_size}), L)(N,C×∏(kernel_size),L), where C \times \prod(\text{kernel\_size})C×∏(kernel_size) is the total number of values within each block (a block has \prod(\text{kernel\_size})∏(kernel_size)spatial locations each containing a CC-channeled vector), and LL is the total number of such blocks:L = \prod_d \left\lfloor\frac{\text{spatial\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,L=d∏⌊stride[d]spatial_size[d]+2×padding[d]−dilation[d]×(kernel_size[d]−1)−1+1⌋,where \text{spatial\_size}spatial_size is formed by the spatial dimensions of `input` (*∗ above), and dd is over all spatial dimensions.Therefore, indexing `output` at the last dimension (column dimension) gives all values within a certain block.The `padding`, `stride` and `dilation` arguments specify how the sliding blocks are retrieved.`stride` controls the stride for the sliding blocks.`padding` controls the amount of implicit zero-paddings on both sides for `padding` number of points for each dimension before reshaping.`dilation` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.Parameters**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the sliding blocks**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – the stride of the sliding blocks in the input spatial dimensions. Default: 1**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – implicit zero padding to be added on both sides of input. Default: 0**dilation** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – a parameter that controls the stride of elements within the neighborhood. Default: 1If `kernel_size`, `dilation`, `padding` or `stride` is an int or a tuple of length 1, their values will be replicated across all spatial dimensions.For the case of two input spatial dimensions this operation is sometimes called `im2col`.NOTE[`Fold`](https://pytorch.org/docs/stable/nn.html#torch.nn.Fold) calculates each combined value in the resulting large tensor by summing all values from all containing blocks. [`Unfold`](https://pytorch.org/docs/stable/nn.html#torch.nn.Unfold) extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other.WARNINGCurrently, only 4-D input tensors (batched image-like tensors) are supported.Shape:Input: (N, C, *)(N,C,∗)Output: (N, C \times \prod(\text{kernel\_size}), L)(N,C×∏(kernel_size),L) as described aboveExamples:`>>> unfold = nn.Unfold(kernel_size=(2, 3)) >>> input = torch.randn(2, 5, 3, 4) >>> output = unfold(input) >>> # each patch contains 30 values (2x3=6 vectors, each of 5 channels) >>> # 4 blocks (2x3 kernels) in total in the 3x4 input >>> output.size() torch.Size([2, 30, 4])  >>> # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape) >>> inp = torch.randn(1, 3, 10, 12) >>> w = torch.randn(2, 3, 4, 5) >>> inp_unf = torch.nn.functional.unfold(inp, (4, 5)) >>> out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2) >>> out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1)) >>> # or equivalently (and avoiding a copy), >>> # out = out_unf.view(1, 2, 7, 8) >>> (torch.nn.functional.conv2d(inp, w) - out).abs().max() tensor(1.9073e-06) `

### Fold

- *CLASS*`torch.nn.``Fold`(*output_size*, *kernel_size*, *dilation=1*, *padding=0*, *stride=1*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/fold.html#Fold)

  Combines an array of sliding local blocks into a large containing tensor.Consider a batched `input` tensor containing sliding local blocks, e.g., patches of images, of shape (N, C \times \prod(\text{kernel\_size}), L)(N,C×∏(kernel_size),L), where NN is batch dimension, C \times \prod(\text{kernel\_size})C×∏(kernel_size) is the number of values within a block (a block has \prod(\text{kernel\_size})∏(kernel_size) spatial locations each containing a CC-channeled vector), and LL is the total number of blocks. (This is exactly the same specification as the output shape of [`Unfold`](https://pytorch.org/docs/stable/nn.html#torch.nn.Unfold).) This operation combines these local blocks into the large `output`tensor of shape (N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)(N,C,output_size[0],output_size[1],…) by summing the overlapping values. Similar to [`Unfold`](https://pytorch.org/docs/stable/nn.html#torch.nn.Unfold), the arguments must satisfyL = \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,L=d∏⌊stride[d]output_size[d]+2×padding[d]−dilation[d]×(kernel_size[d]−1)−1+1⌋,where dd is over all spatial dimensions.`output_size` describes the spatial shape of the large containing tensor of the sliding local blocks. It is useful to resolve the ambiguity when multiple input shapes map to same number of sliding blocks, e.g., with `stride > 0`.The `padding`, `stride` and `dilation` arguments specify how the sliding blocks are retrieved.`stride` controls the stride for the sliding blocks.`padding` controls the amount of implicit zero-paddings on both sides for `padding` number of points for each dimension before reshaping.`dilation` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.Parameters**output_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the shape of the spatial dimensions of the output (i.e., `output.sizes()[2:]`)**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the sliding blocks**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the stride of the sliding blocks in the input spatial dimensions. Default: 1**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – implicit zero padding to be added on both sides of input. Default: 0**dilation** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)*,* *optional*) – a parameter that controls the stride of elements within the neighborhood. Default: 1If `output_size`, `kernel_size`, `dilation`, `padding` or `stride` is an int or a tuple of length 1 then their values will be replicated across all spatial dimensions.For the case of two output spatial dimensions this operation is sometimes called `col2im`.NOTE[`Fold`](https://pytorch.org/docs/stable/nn.html#torch.nn.Fold) calculates each combined value in the resulting large tensor by summing all values from all containing blocks. [`Unfold`](https://pytorch.org/docs/stable/nn.html#torch.nn.Unfold) extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other.WARNINGCurrently, only 4-D output tensors (batched image-like tensors) are supported.Shape:Input: (N, C \times \prod(\text{kernel\_size}), L)(N,C×∏(kernel_size),L)Output: (N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)(N,C,output_size[0],output_size[1],…) as described aboveExamples:`>>> fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2)) >>> input = torch.randn(1, 3 * 2 * 2, 12) >>> output = fold(input) >>> output.size() torch.Size([1, 3, 4, 5]) `

## Pooling layers

### MaxPool1d

- *CLASS*`torch.nn.``MaxPool1d`(*kernel_size*, *stride=None*, *padding=0*, *dilation=1*, *return_indices=False*, *ceil_mode=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxPool1d)

  Applies a 1D max pooling over an input signal composed of several input planes.In the simplest case, the output value of the layer with input size (N, C, L)(N,C,L) and output (N, C, L_{out})(N,C,Lout) can be precisely described as:out(N_i, C_j, k) = \max_{m=0, \ldots, \text{kernel\_size} - 1} input(N_i, C_j, stride \times k + m)out(Ni,Cj,k)=m=0,…,kernel_size−1maxinput(Ni,Cj,stride×k+m)If `padding` is non-zero, then the input is implicitly zero-padded on both sides for `padding` number of points. `dilation` controls the spacing between the kernel points. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.Parameters**kernel_size** – the size of the window to take a max over**stride** – the stride of the window. Default value is `kernel_size`**padding** – implicit zero padding to be added on both sides**dilation** – a parameter that controls the stride of elements in the window**return_indices** – if `True`, will return the max indices along with the outputs. Useful for [`torch.nn.MaxUnpool1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxUnpool1d) later**ceil_mode** – when True, will use ceil instead of floor to compute the output shapeShape:Input: (N, C, L_{in})(N,C,Lin)Output: (N, C, L_{out})(N,C,Lout), whereL_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloorLout=⌊strideLin+2×padding−dilation×(kernel_size−1)−1+1⌋Examples:`>>> # pool of size=3, stride=2 >>> m = nn.MaxPool1d(3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) `

### MaxPool2d

- *CLASS*`torch.nn.``MaxPool2d`(*kernel_size*, *stride=None*, *padding=0*, *dilation=1*, *return_indices=False*, *ceil_mode=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxPool2d)

  Applies a 2D max pooling over an input signal composed of several input planes.In the simplest case, the output value of the layer with input size (N, C, H, W)(N,C,H,W), output (N, C, H_{out}, W_{out})(N,C,Hout,Wout) and `kernel_size` (kH, kW)(kH,kW) can be precisely described as:\begin{aligned} out(N_i, C_j, h, w) ={} &amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m, \text{stride[1]} \times w + n) \end{aligned}out(Ni,Cj,h,w)=m=0,…,kH−1maxn=0,…,kW−1maxinput(Ni,Cj,stride[0]×h+m,stride[1]×w+n)If `padding` is non-zero, then the input is implicitly zero-padded on both sides for `padding` number of points. `dilation` controls the spacing between the kernel points. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.The parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:a single `int` – in which case the same value is used for the height and width dimensiona `tuple` of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimensionParameters**kernel_size** – the size of the window to take a max over**stride** – the stride of the window. Default value is `kernel_size`**padding** – implicit zero padding to be added on both sides**dilation** – a parameter that controls the stride of elements in the window**return_indices** – if `True`, will return the max indices along with the outputs. Useful for [`torch.nn.MaxUnpool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxUnpool2d) later**ceil_mode** – when True, will use ceil instead of floor to compute the output shapeShape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout), whereH_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]} \times (\text{kernel\_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloorHout=⌊stride[0]Hin+2∗padding[0]−dilation[0]×(kernel_size[0]−1)−1+1⌋W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]} \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloorWout=⌊stride[1]Win+2∗padding[1]−dilation[1]×(kernel_size[1]−1)−1+1⌋Examples:`>>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool2d((3, 2), stride=(2, 1)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) `

### MaxPool3d

- *CLASS*`torch.nn.``MaxPool3d`(*kernel_size*, *stride=None*, *padding=0*, *dilation=1*, *return_indices=False*, *ceil_mode=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxPool3d)

  Applies a 3D max pooling over an input signal composed of several input planes.In the simplest case, the output value of the layer with input size (N, C, D, H, W)(N,C,D,H,W), output (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout,Hout,Wout) and `kernel_size` (kD, kH, kW)(kD,kH,kW) can be precisely described as:\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k, \text{stride[1]} \times h + m, \text{stride[2]} \times w + n) \end{aligned}out(Ni,Cj,d,h,w)=k=0,…,kD−1maxm=0,…,kH−1maxn=0,…,kW−1maxinput(Ni,Cj,stride[0]×d+k,stride[1]×h+m,stride[2]×w+n)If `padding` is non-zero, then the input is implicitly zero-padded on both sides for `padding` number of points. `dilation` controls the spacing between the kernel points. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what `dilation` does.The parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:a single `int` – in which case the same value is used for the depth, height and width dimensiona `tuple` of three ints – in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimensionParameters**kernel_size** – the size of the window to take a max over**stride** – the stride of the window. Default value is `kernel_size`**padding** – implicit zero padding to be added on all three sides**dilation** – a parameter that controls the stride of elements in the window**return_indices** – if `True`, will return the max indices along with the outputs. Useful for [`torch.nn.MaxUnpool3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxUnpool3d) later**ceil_mode** – when True, will use ceil instead of floor to compute the output shapeShape:Input: (N, C, D_{in}, H_{in}, W_{in})(N,C,Din,Hin,Win)Output: (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout,Hout,Wout), whereD_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloorDout=⌊stride[0]Din+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1+1⌋H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloorHout=⌊stride[1]Hin+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1+1⌋W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloorWout=⌊stride[2]Win+2×padding[2]−dilation[2]×(kernel_size[2]−1)−1+1⌋Examples:`>>> # pool of square window of size=3, stride=2 >>> m = nn.MaxPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = torch.randn(20, 16, 50,44, 31) >>> output = m(input) `

### MaxUnpool1d

- *CLASS*`torch.nn.``MaxUnpool1d`(*kernel_size*, *stride=None*, *padding=0*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxUnpool1d)

  Computes a partial inverse of [`MaxPool1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool1d).[`MaxPool1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool1d) is not fully invertible, since the non-maximal values are lost.[`MaxUnpool1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxUnpool1d) takes in as input the output of [`MaxPool1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool1d) including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.NOTE[`MaxPool1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool1d) can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument `output_size` in the forward call. See the Inputs and Example below.Parameters**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the max pooling window.**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Stride of the max pooling window. It is set to `kernel_size` by default.**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Padding that was added to the inputInputs:input: the input Tensor to invertindices: the indices given out by [`MaxPool1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool1d)output_size (optional): the targeted output sizeShape:Input: (N, C, H_{in})(N,C,Hin)Output: (N, C, H_{out})(N,C,Hout), whereH_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{kernel\_size}[0]Hout=(Hin−1)×stride[0]−2×padding[0]+kernel_size[0]or as given by `output_size` in the call operatorExample:`>>> pool = nn.MaxPool1d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool1d(2, stride=2) >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]]) >>> output, indices = pool(input) >>> unpool(output, indices) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])  >>> # Example showcasing the use of output_size >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]]) >>> output, indices = pool(input) >>> unpool(output, indices, output_size=input.size()) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])  >>> unpool(output, indices) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]]) `

### MaxUnpool2d

- *CLASS*`torch.nn.``MaxUnpool2d`(*kernel_size*, *stride=None*, *padding=0*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxUnpool2d)

  Computes a partial inverse of [`MaxPool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d).[`MaxPool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d) is not fully invertible, since the non-maximal values are lost.[`MaxUnpool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxUnpool2d) takes in as input the output of [`MaxPool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d) including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.NOTE[`MaxPool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d) can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument `output_size` in the forward call. See the Inputs and Example below.Parameters**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the max pooling window.**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Stride of the max pooling window. It is set to `kernel_size` by default.**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Padding that was added to the inputInputs:input: the input Tensor to invertindices: the indices given out by [`MaxPool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d)output_size (optional): the targeted output sizeShape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout), whereH_{out} = (H_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}Hout=(Hin−1)×stride[0]−2×padding[0]+kernel_size[0]W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}Wout=(Win−1)×stride[1]−2×padding[1]+kernel_size[1]or as given by `output_size` in the call operatorExample:`>>> pool = nn.MaxPool2d(2, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool2d(2, stride=2) >>> input = torch.tensor([[[[ 1.,  2,  3,  4],                             [ 5,  6,  7,  8],                             [ 9, 10, 11, 12],                             [13, 14, 15, 16]]]]) >>> output, indices = pool(input) >>> unpool(output, indices) tensor([[[[  0.,   0.,   0.,   0.],           [  0.,   6.,   0.,   8.],           [  0.,   0.,   0.,   0.],           [  0.,  14.,   0.,  16.]]]])  >>> # specify a different output size than input size >>> unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) tensor([[[[  0.,   0.,   0.,   0.,   0.],           [  6.,   0.,   8.,   0.,   0.],           [  0.,   0.,   0.,  14.,   0.],           [ 16.,   0.,   0.,   0.,   0.],           [  0.,   0.,   0.,   0.,   0.]]]]) `

### MaxUnpool3d

- *CLASS*`torch.nn.``MaxUnpool3d`(*kernel_size*, *stride=None*, *padding=0*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#MaxUnpool3d)

  Computes a partial inverse of [`MaxPool3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool3d).[`MaxPool3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool3d) is not fully invertible, since the non-maximal values are lost. [`MaxUnpool3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxUnpool3d) takes in as input the output of [`MaxPool3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool3d) including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.NOTE[`MaxPool3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool3d) can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument `output_size` in the forward call. See the Inputs section below.Parameters**kernel_size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Size of the max pooling window.**stride** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Stride of the max pooling window. It is set to `kernel_size` by default.**padding** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – Padding that was added to the inputInputs:input: the input Tensor to invertindices: the indices given out by [`MaxPool3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool3d)output_size (optional): the targeted output sizeShape:Input: (N, C, D_{in}, H_{in}, W_{in})(N,C,Din,Hin,Win)Output: (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout,Hout,Wout), whereD_{out} = (D_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}Dout=(Din−1)×stride[0]−2×padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}Hout=(Hin−1)×stride[1]−2×padding[1]+kernel_size[1]W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}Wout=(Win−1)×stride[2]−2×padding[2]+kernel_size[2]or as given by `output_size` in the call operatorExample:`>>> # pool of square window of size=3, stride=2 >>> pool = nn.MaxPool3d(3, stride=2, return_indices=True) >>> unpool = nn.MaxUnpool3d(3, stride=2) >>> output, indices = pool(torch.randn(20, 16, 51, 33, 15)) >>> unpooled_output = unpool(output, indices) >>> unpooled_output.size() torch.Size([20, 16, 51, 33, 15]) `

### AvgPool1d

- *CLASS*`torch.nn.``AvgPool1d`(*kernel_size*, *stride=None*, *padding=0*, *ceil_mode=False*, *count_include_pad=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AvgPool1d)

  Applies a 1D average pooling over an input signal composed of several input planes.In the simplest case, the output value of the layer with input size (N, C, L)(N,C,L), output (N, C, L_{out})(N,C,Lout)and `kernel_size` kk can be precisely described as:\text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1} \text{input}(N_i, C_j, \text{stride} \times l + m)out(Ni,Cj,l)=k1m=0∑k−1input(Ni,Cj,stride×l+m)If `padding` is non-zero, then the input is implicitly zero-padded on both sides for `padding` number of points.The parameters `kernel_size`, `stride`, `padding` can each be an `int` or a one-element tuple.Parameters**kernel_size** – the size of the window**stride** – the stride of the window. Default value is `kernel_size`**padding** – implicit zero padding to be added on both sides**ceil_mode** – when True, will use ceil instead of floor to compute the output shape**count_include_pad** – when True, will include the zero-padding in the averaging calculationShape:Input: (N, C, L_{in})(N,C,Lin)Output: (N, C, L_{out})(N,C,Lout), whereL_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloorLout=⌊strideLin+2×padding−kernel_size+1⌋Examples:`>>> # pool with window of size=3, stride=2 >>> m = nn.AvgPool1d(3, stride=2) >>> m(torch.tensor([[[1.,2,3,4,5,6,7]]])) tensor([[[ 2.,  4.,  6.]]]) `

### AvgPool2d

- *CLASS*`torch.nn.``AvgPool2d`(*kernel_size*, *stride=None*, *padding=0*, *ceil_mode=False*, *count_include_pad=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AvgPool2d)

  Applies a 2D average pooling over an input signal composed of several input planes.In the simplest case, the output value of the layer with input size (N, C, H, W)(N,C,H,W), output (N, C, H_{out}, W_{out})(N,C,Hout,Wout) and `kernel_size` (kH, kW)(kH,kW) can be precisely described as:out(N_i, C_j, h, w) = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)out(Ni,Cj,h,w)=kH∗kW1m=0∑kH−1n=0∑kW−1input(Ni,Cj,stride[0]×h+m,stride[1]×w+n)If `padding` is non-zero, then the input is implicitly zero-padded on both sides for `padding` number of points.The parameters `kernel_size`, `stride`, `padding` can either be:a single `int` – in which case the same value is used for the height and width dimensiona `tuple` of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimensionParameters**kernel_size** – the size of the window**stride** – the stride of the window. Default value is `kernel_size`**padding** – implicit zero padding to be added on both sides**ceil_mode** – when True, will use ceil instead of floor to compute the output shape**count_include_pad** – when True, will include the zero-padding in the averaging calculationShape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout), whereH_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloorHout=⌊stride[0]Hin+2×padding[0]−kernel_size[0]+1⌋W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloorWout=⌊stride[1]Win+2×padding[1]−kernel_size[1]+1⌋Examples:`>>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool2d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool2d((3, 2), stride=(2, 1)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) `

### AvgPool3d

- *CLASS*`torch.nn.``AvgPool3d`(*kernel_size*, *stride=None*, *padding=0*, *ceil_mode=False*, *count_include_pad=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AvgPool3d)

  Applies a 3D average pooling over an input signal composed of several input planes.In the simplest case, the output value of the layer with input size (N, C, D, H, W)(N,C,D,H,W), output (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout,Hout,Wout) and `kernel_size` (kD, kH, kW)(kD,kH,kW) can be precisely described as:\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp; \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\ &amp; \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k, \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)} {kD \times kH \times kW} \end{aligned}out(Ni,Cj,d,h,w)=k=0∑kD−1m=0∑kH−1n=0∑kW−1kD×kH×kWinput(Ni,Cj,stride[0]×d+k,stride[1]×h+m,stride[2]×w+n)If `padding` is non-zero, then the input is implicitly zero-padded on all three sides for `padding`number of points.The parameters `kernel_size`, `stride` can either be:a single `int` – in which case the same value is used for the depth, height and width dimensiona `tuple` of three ints – in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimensionParameters**kernel_size** – the size of the window**stride** – the stride of the window. Default value is `kernel_size`**padding** – implicit zero padding to be added on all three sides**ceil_mode** – when True, will use ceil instead of floor to compute the output shape**count_include_pad** – when True, will include the zero-padding in the averaging calculationShape:Input: (N, C, D_{in}, H_{in}, W_{in})(N,C,Din,Hin,Win)Output: (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout,Hout,Wout), whereD_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloorDout=⌊stride[0]Din+2×padding[0]−kernel_size[0]+1⌋H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloorHout=⌊stride[1]Hin+2×padding[1]−kernel_size[1]+1⌋W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloorWout=⌊stride[2]Win+2×padding[2]−kernel_size[2]+1⌋Examples:`>>> # pool of square window of size=3, stride=2 >>> m = nn.AvgPool3d(3, stride=2) >>> # pool of non-square window >>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2)) >>> input = torch.randn(20, 16, 50,44, 31) >>> output = m(input) `

### FractionalMaxPool2d

- *CLASS*`torch.nn.``FractionalMaxPool2d`(*kernel_size*, *output_size=None*, *output_ratio=None*, *return_indices=False*, *_random_samples=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#FractionalMaxPool2d)

  Applies a 2D fractional max pooling over an input signal composed of several input planes.Fractional MaxPooling is described in detail in the paper [Fractional MaxPooling](http://arxiv.org/abs/1412.6071) by Ben GrahamThe max-pooling operation is applied in kH \times kWkH×kW regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes.Parameters**kernel_size** – the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)**output_size** – the target output size of the image of the form oH x oW. Can be a tuple (oH, oW) or a single number oH for a square image oH x oH**output_ratio** – If one wants to have an output size as a ratio of the input size, this option can be given. This has to be a number or tuple in the range (0, 1)**return_indices** – if `True`, will return the indices along with the outputs. Useful to pass to `nn.MaxUnpool2d()`. Default: `False`Examples`>>> # pool of square window of size=3, and target output size 13x12 >>> m = nn.FractionalMaxPool2d(3, output_size=(13, 12)) >>> # pool of square window and target output size being half of input image size >>> m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) `

### LPPool1d

- *CLASS*`torch.nn.``LPPool1d`(*norm_type*, *kernel_size*, *stride=None*, *ceil_mode=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#LPPool1d)

  Applies a 1D power-average pooling over an input signal composed of several input planes.On each window, the function computed is:f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}f(X)=px∈X∑xpAt p = \infty∞, one gets Max PoolingAt p = 1, one gets Sum Pooling (which is proportional to Average Pooling)NOTEIf the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.Parameters**kernel_size** – a single int, the size of the window**stride** – a single int, the stride of the window. Default value is `kernel_size`**ceil_mode** – when True, will use ceil instead of floor to compute the output shapeShape:Input: (N, C, L_{in})(N,C,Lin)Output: (N, C, L_{out})(N,C,Lout), whereL_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloorLout=⌊strideLin+2×padding−kernel_size+1⌋Examples::`>>> # power-2 pool of window of length 3, with stride 2. >>> m = nn.LPPool1d(2, 3, stride=2) >>> input = torch.randn(20, 16, 50) >>> output = m(input) `

### LPPool2d

- *CLASS*`torch.nn.``LPPool2d`(*norm_type*, *kernel_size*, *stride=None*, *ceil_mode=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#LPPool2d)

  Applies a 2D power-average pooling over an input signal composed of several input planes.On each window, the function computed is:f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}f(X)=px∈X∑xpAt p = \infty∞, one gets Max PoolingAt p = 1, one gets Sum Pooling (which is proportional to average pooling)The parameters `kernel_size`, `stride` can either be:a single `int` – in which case the same value is used for the height and width dimensiona `tuple` of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimensionNOTEIf the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.Parameters**kernel_size** – the size of the window**stride** – the stride of the window. Default value is `kernel_size`**ceil_mode** – when True, will use ceil instead of floor to compute the output shapeShape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout), whereH_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloorHout=⌊stride[0]Hin+2×padding[0]−dilation[0]×(kernel_size[0]−1)−1+1⌋W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloorWout=⌊stride[1]Win+2×padding[1]−dilation[1]×(kernel_size[1]−1)−1+1⌋Examples:`>>> # power-2 pool of square window of size=3, stride=2 >>> m = nn.LPPool2d(2, 3, stride=2) >>> # pool of non-square window of power 1.2 >>> m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1)) >>> input = torch.randn(20, 16, 50, 32) >>> output = m(input) `

### AdaptiveMaxPool1d

- *CLASS*`torch.nn.``AdaptiveMaxPool1d`(*output_size*, *return_indices=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool1d)

  Applies a 1D adaptive max pooling over an input signal composed of several input planes.The output size is H, for any input size. The number of output features is equal to the number of input planes.Parameters**output_size** – the target output size H**return_indices** – if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool1d. Default: `False`Examples`>>> # target output size of 5 >>> m = nn.AdaptiveMaxPool1d(5) >>> input = torch.randn(1, 64, 8) >>> output = m(input) `

### AdaptiveMaxPool2d

- *CLASS*`torch.nn.``AdaptiveMaxPool2d`(*output_size*, *return_indices=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool2d)

  Applies a 2D adaptive max pooling over an input signal composed of several input planes.The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.Parameters**output_size** – the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a `int`, or `None`which means the size will be the same as that of the input.**return_indices** – if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: `False`Examples`>>> # target output size of 5x7 >>> m = nn.AdaptiveMaxPool2d((5,7)) >>> input = torch.randn(1, 64, 8, 9) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveMaxPool2d(7) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) >>> # target output size of 10x7 >>> m = nn.AdaptiveMaxPool2d((None, 7)) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) `

### AdaptiveMaxPool3d

- *CLASS*`torch.nn.``AdaptiveMaxPool3d`(*output_size*, *return_indices=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool3d)

  Applies a 3D adaptive max pooling over an input signal composed of several input planes.The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.Parameters**output_size** – the target output size of the image of the form D x H x W. Can be a tuple (D, H, W) or a single D for a cube D x D x D. D, H and W can be either a `int`, or `None` which means the size will be the same as that of the input.**return_indices** – if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: `False`Examples`>>> # target output size of 5x7x9 >>> m = nn.AdaptiveMaxPool3d((5,7,9)) >>> input = torch.randn(1, 64, 8, 9, 10) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveMaxPool3d(7) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) >>> # target output size of 7x9x8 >>> m = nn.AdaptiveMaxPool3d((7, None, None)) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) `

### AdaptiveAvgPool1d

- *CLASS*`torch.nn.``AdaptiveAvgPool1d`(*output_size*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool1d)

  Applies a 1D adaptive average pooling over an input signal composed of several input planes.The output size is H, for any input size. The number of output features is equal to the number of input planes.Parameters**output_size** – the target output size HExamples`>>> # target output size of 5 >>> m = nn.AdaptiveAvgPool1d(5) >>> input = torch.randn(1, 64, 8) >>> output = m(input) `

### AdaptiveAvgPool2d

- *CLASS*`torch.nn.``AdaptiveAvgPool2d`(*output_size*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool2d)

  Applies a 2D adaptive average pooling over an input signal composed of several input planes.The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.Parameters**output_size** – the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a `int`, or `None` which means the size will be the same as that of the input.Examples`>>> # target output size of 5x7 >>> m = nn.AdaptiveAvgPool2d((5,7)) >>> input = torch.randn(1, 64, 8, 9) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveAvgPool2d(7) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) >>> # target output size of 10x7 >>> m = nn.AdaptiveMaxPool2d((None, 7)) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) `

### AdaptiveAvgPool3d

- *CLASS*`torch.nn.``AdaptiveAvgPool3d`(*output_size*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool3d)

  Applies a 3D adaptive average pooling over an input signal composed of several input planes.The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.Parameters**output_size** – the target output size of the form D x H x W. Can be a tuple (D, H, W) or a single number D for a cube D x D x D. D, H and W can be either a `int`, or `None` which means the size will be the same as that of the input.Examples`>>> # target output size of 5x7x9 >>> m = nn.AdaptiveAvgPool3d((5,7,9)) >>> input = torch.randn(1, 64, 8, 9, 10) >>> output = m(input) >>> # target output size of 7x7x7 (cube) >>> m = nn.AdaptiveAvgPool3d(7) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) >>> # target output size of 7x9x8 >>> m = nn.AdaptiveMaxPool3d((7, None, None)) >>> input = torch.randn(1, 64, 10, 9, 8) >>> output = m(input) `

## Padding layers

### ReflectionPad1d

- *CLASS*`torch.nn.``ReflectionPad1d`(*padding*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReflectionPad1d)

  Pads the input tensor using the reflection of the input boundary.For N-dimensional padding, use [`torch.nn.functional.pad()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad).Parameters**padding** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (\text{padding\_left}padding_left, \text{padding\_right}padding_right)Shape:Input: (N, C, W_{in})(N,C,Win)Output: (N, C, W_{out})(N,C,Wout) whereW_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}Wout=Win+padding_left+padding_rightExamples:`>>> m = nn.ReflectionPad1d(2) >>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) >>> input tensor([[[0., 1., 2., 3.],          [4., 5., 6., 7.]]]) >>> m(input) tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],          [6., 5., 4., 5., 6., 7., 6., 5.]]]) >>> # using different paddings for different sides >>> m = nn.ReflectionPad1d((3, 1)) >>> m(input) tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],          [7., 6., 5., 4., 5., 6., 7., 6.]]]) `

### ReflectionPad2d

- *CLASS*`torch.nn.``ReflectionPad2d`(*padding*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReflectionPad2d)

  Pads the input tensor using the reflection of the input boundary.For N-dimensional padding, use [`torch.nn.functional.pad()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad).Parameters**padding** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (\text{padding\_left}padding_left, \text{padding\_right}padding_right, \text{padding\_top}padding_top, \text{padding\_bottom}padding_bottom)Shape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout) whereH_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}Hout=Hin+padding_top+padding_bottomW_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}Wout=Win+padding_left+padding_rightExamples:`>>> m = nn.ReflectionPad2d(2) >>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) >>> input tensor([[[[0., 1., 2.],           [3., 4., 5.],           [6., 7., 8.]]]]) >>> m(input) tensor([[[[8., 7., 6., 7., 8., 7., 6.],           [5., 4., 3., 4., 5., 4., 3.],           [2., 1., 0., 1., 2., 1., 0.],           [5., 4., 3., 4., 5., 4., 3.],           [8., 7., 6., 7., 8., 7., 6.],           [5., 4., 3., 4., 5., 4., 3.],           [2., 1., 0., 1., 2., 1., 0.]]]]) >>> # using different paddings for different sides >>> m = nn.ReflectionPad2d((1, 1, 2, 0)) >>> m(input) tensor([[[[7., 6., 7., 8., 7.],           [4., 3., 4., 5., 4.],           [1., 0., 1., 2., 1.],           [4., 3., 4., 5., 4.],           [7., 6., 7., 8., 7.]]]]) `

### ReplicationPad1d

- *CLASS*`torch.nn.``ReplicationPad1d`(*padding*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReplicationPad1d)

  Pads the input tensor using replication of the input boundary.For N-dimensional padding, use [`torch.nn.functional.pad()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad).Parameters**padding** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (\text{padding\_left}padding_left, \text{padding\_right}padding_right)Shape:Input: (N, C, W_{in})(N,C,Win)Output: (N, C, W_{out})(N,C,Wout) whereW_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}Wout=Win+padding_left+padding_rightExamples:`>>> m = nn.ReplicationPad1d(2) >>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) >>> input tensor([[[0., 1., 2., 3.],          [4., 5., 6., 7.]]]) >>> m(input) tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],          [4., 4., 4., 5., 6., 7., 7., 7.]]]) >>> # using different paddings for different sides >>> m = nn.ReplicationPad1d((3, 1)) >>> m(input) tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],          [4., 4., 4., 4., 5., 6., 7., 7.]]]) `

### ReplicationPad2d

- *CLASS*`torch.nn.``ReplicationPad2d`(*padding*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReplicationPad2d)

  Pads the input tensor using replication of the input boundary.For N-dimensional padding, use [`torch.nn.functional.pad()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad).Parameters**padding** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (\text{padding\_left}padding_left, \text{padding\_right}padding_right, \text{padding\_top}padding_top, \text{padding\_bottom}padding_bottom)Shape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout) whereH_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}Hout=Hin+padding_top+padding_bottomW_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}Wout=Win+padding_left+padding_rightExamples:`>>> m = nn.ReplicationPad2d(2) >>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) >>> input tensor([[[[0., 1., 2.],           [3., 4., 5.],           [6., 7., 8.]]]]) >>> m(input) tensor([[[[0., 0., 0., 1., 2., 2., 2.],           [0., 0., 0., 1., 2., 2., 2.],           [0., 0., 0., 1., 2., 2., 2.],           [3., 3., 3., 4., 5., 5., 5.],           [6., 6., 6., 7., 8., 8., 8.],           [6., 6., 6., 7., 8., 8., 8.],           [6., 6., 6., 7., 8., 8., 8.]]]]) >>> # using different paddings for different sides >>> m = nn.ReplicationPad2d((1, 1, 2, 0)) >>> m(input) tensor([[[[0., 0., 1., 2., 2.],           [0., 0., 1., 2., 2.],           [0., 0., 1., 2., 2.],           [3., 3., 4., 5., 5.],           [6., 6., 7., 8., 8.]]]]) `

### ReplicationPad3d

- *CLASS*`torch.nn.``ReplicationPad3d`(*padding*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ReplicationPad3d)

  Pads the input tensor using replication of the input boundary.For N-dimensional padding, use [`torch.nn.functional.pad()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad).Parameters**padding** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (\text{padding\_left}padding_left, \text{padding\_right}padding_right, \text{padding\_top}padding_top, \text{padding\_bottom}padding_bottom, \text{padding\_front}padding_front, \text{padding\_back}padding_back)Shape:Input: (N, C, D_{in}, H_{in}, W_{in})(N,C,Din,Hin,Win)Output: (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout,Hout,Wout) whereD_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}Dout=Din+padding_front+padding_backH_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}Hout=Hin+padding_top+padding_bottomW_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}Wout=Win+padding_left+padding_rightExamples:`>>> m = nn.ReplicationPad3d(3) >>> input = torch.randn(16, 3, 8, 320, 480) >>> output = m(input) >>> # using different paddings for different sides >>> m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1)) >>> output = m(input) `

### ZeroPad2d

- *CLASS*`torch.nn.``ZeroPad2d`(*padding*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ZeroPad2d)

  Pads the input tensor boundaries with zero.For N-dimensional padding, use [`torch.nn.functional.pad()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad).Parameters**padding** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (\text{padding\_left}padding_left, \text{padding\_right}padding_right, \text{padding\_top}padding_top, \text{padding\_bottom}padding_bottom)Shape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout) whereH_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}Hout=Hin+padding_top+padding_bottomW_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}Wout=Win+padding_left+padding_rightExamples:`>>> m = nn.ZeroPad2d(2) >>> input = torch.randn(1, 1, 3, 3) >>> input tensor([[[[-0.1678, -0.4418,  1.9466],           [ 0.9604, -0.4219, -0.5241],           [-0.9162, -0.5436, -0.6446]]]]) >>> m(input) tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],           [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]]) >>> # using different paddings for different sides >>> m = nn.ZeroPad2d((1, 1, 2, 0)) >>> m(input) tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],           [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],           [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]]) `

### ConstantPad1d

- *CLASS*`torch.nn.``ConstantPad1d`(*padding*, *value*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ConstantPad1d)

  Pads the input tensor boundaries with a constant value.For N-dimensional padding, use [`torch.nn.functional.pad()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad).Parameters**padding** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the padding. If is int, uses the same padding in both boundaries. If a 2-tuple, uses (\text{padding\_left}padding_left, \text{padding\_right}padding_right)Shape:Input: (N, C, W_{in})(N,C,Win)Output: (N, C, W_{out})(N,C,Wout) whereW_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}Wout=Win+padding_left+padding_rightExamples:`>>> m = nn.ConstantPad1d(2, 3.5) >>> input = torch.randn(1, 2, 4) >>> input tensor([[[-1.0491, -0.7152, -0.0749,  0.8530],          [-1.3287,  1.8966,  0.1466, -0.2771]]]) >>> m(input) tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,            3.5000],          [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,            3.5000]]]) >>> m = nn.ConstantPad1d(2, 3.5) >>> input = torch.randn(1, 2, 3) >>> input tensor([[[ 1.6616,  1.4523, -1.1255],          [-3.6372,  0.1182, -1.8652]]]) >>> m(input) tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],          [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]]) >>> # using different paddings for different sides >>> m = nn.ConstantPad1d((3, 1), 3.5) >>> m(input) tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],          [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]]) `

### ConstantPad2d

- *CLASS*`torch.nn.``ConstantPad2d`(*padding*, *value*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ConstantPad2d)

  Pads the input tensor boundaries with a constant value.For N-dimensional padding, use [`torch.nn.functional.pad()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad).Parameters**padding** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (\text{padding\_left}padding_left, \text{padding\_right}padding_right, \text{padding\_top}padding_top, \text{padding\_bottom}padding_bottom)Shape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout) whereH_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}Hout=Hin+padding_top+padding_bottomW_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}Wout=Win+padding_left+padding_rightExamples:`>>> m = nn.ConstantPad2d(2, 3.5) >>> input = torch.randn(1, 2, 2) >>> input tensor([[[ 1.6585,  0.4320],          [-0.8701, -0.4649]]]) >>> m(input) tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],          [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]]) >>> # using different paddings for different sides >>> m = nn.ConstantPad2d((3, 0, 2, 1), 3.5) >>> m(input) tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],          [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]]) `

### ConstantPad3d

- *CLASS*`torch.nn.``ConstantPad3d`(*padding*, *value*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html#ConstantPad3d)

  Pads the input tensor boundaries with a constant value.For N-dimensional padding, use [`torch.nn.functional.pad()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad).Parameters**padding** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple)) – the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (\text{padding\_left}padding_left, \text{padding\_right}padding_right, \text{padding\_top}padding_top, \text{padding\_bottom}padding_bottom, \text{padding\_front}padding_front, \text{padding\_back}padding_back)Shape:Input: (N, C, D_{in}, H_{in}, W_{in})(N,C,Din,Hin,Win)Output: (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout,Hout,Wout) whereD_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}Dout=Din+padding_front+padding_backH_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}Hout=Hin+padding_top+padding_bottomW_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}Wout=Win+padding_left+padding_rightExamples:`>>> m = nn.ConstantPad3d(3, 3.5) >>> input = torch.randn(16, 3, 10, 20, 30) >>> output = m(input) >>> # using different paddings for different sides >>> m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5) >>> output = m(input) `

## Non-linear activations (weighted sum, nonlinearity)

### ELU

- *CLASS*`torch.nn.``ELU`(*alpha=1.0*, *inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#ELU)

  Applies the element-wise function:\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))ELU(x)=max(0,x)+min(0,α∗(exp(x)−1))Parameters**alpha** – the \alphaα value for the ELU formulation. Default: 1.0**inplace** – can optionally do the operation in-place. Default: `False`Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/ELU.png](https://pytorch.org/docs/stable/_images/ELU.png)Examples:`>>> m = nn.ELU() >>> input = torch.randn(2) >>> output = m(input) `

### Hardshrink

- *CLASS*`torch.nn.``Hardshrink`(*lambd=0.5*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Hardshrink)

  Applies the hard shrinkage function element-wise:\text{HardShrink}(x) = \begin{cases} x, &amp; \text{ if } x &gt; \lambda \\ x, &amp; \text{ if } x &lt; -\lambda \\ 0, &amp; \text{ otherwise } \end{cases}HardShrink(x)=⎩⎪⎨⎪⎧x,x,0, if x>λ if x<−λ otherwise Parameters**lambd** – the \lambdaλ value for the Hardshrink formulation. Default: 0.5Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/Hardshrink.png](https://pytorch.org/docs/stable/_images/Hardshrink.png)Examples:`>>> m = nn.Hardshrink() >>> input = torch.randn(2) >>> output = m(input) `

### Hardtanh

- *CLASS*`torch.nn.``Hardtanh`(*min_val=-1.0*, *max_val=1.0*, *inplace=False*, *min_value=None*, *max_value=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Hardtanh)

  Applies the HardTanh function element-wiseHardTanh is defined as:\text{HardTanh}(x) = \begin{cases} 1 &amp; \text{ if } x &gt; 1 \\ -1 &amp; \text{ if } x &lt; -1 \\ x &amp; \text{ otherwise } \\ \end{cases}HardTanh(x)=⎩⎪⎨⎪⎧1−1x if x>1 if x<−1 otherwise The range of the linear region [-1, 1][−1,1] can be adjusted using `min_val` and `max_val`.Parameters**min_val** – minimum value of the linear region range. Default: -1**max_val** – maximum value of the linear region range. Default: 1**inplace** – can optionally do the operation in-place. Default: `False`Keyword arguments `min_value` and `max_value` have been deprecated in favor of `min_val` and `max_val`.Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/Hardtanh.png](https://pytorch.org/docs/stable/_images/Hardtanh.png)Examples:`>>> m = nn.Hardtanh(-2, 2) >>> input = torch.randn(2) >>> output = m(input) `

### LeakyReLU

- *CLASS*`torch.nn.``LeakyReLU`(*negative_slope=0.01*, *inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#LeakyReLU)

  Applies the element-wise function:\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)LeakyReLU(x)=max(0,x)+negative_slope∗min(0,x)or\text{LeakyRELU}(x) = \begin{cases} x, &amp; \text{ if } x \geq 0 \\ \text{negative\_slope} \times x, &amp; \text{ otherwise } \end{cases}LeakyRELU(x)={x,negative_slope×x, if x≥0 otherwise Parameters**negative_slope** – Controls the angle of the negative slope. Default: 1e-2**inplace** – can optionally do the operation in-place. Default: `False`Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/LeakyReLU.png](https://pytorch.org/docs/stable/_images/LeakyReLU.png)Examples:`>>> m = nn.LeakyReLU(0.1) >>> input = torch.randn(2) >>> output = m(input) `

### LogSigmoid

- *CLASS*`torch.nn.``LogSigmoid`[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#LogSigmoid)

  Applies the element-wise function:\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)LogSigmoid(x)=log(1+exp(−x)1)Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/LogSigmoid.png](https://pytorch.org/docs/stable/_images/LogSigmoid.png)Examples:`>>> m = nn.LogSigmoid() >>> input = torch.randn(2) >>> output = m(input) `

### MultiheadAttention

- *CLASS*`torch.nn.``MultiheadAttention`(*embed_dim*, *num_heads*, *dropout=0.0*, *bias=True*, *add_bias_kv=False*, *add_zero_attn=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention)

  Allows the model to jointly attend to information from different representation subspaces. See reference: Attention Is All You Need\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)MultiHead(Q,K,V)=Concat(head1,…,headh)WOwhereheadi=Attention(QWiQ,KWiK,VWiV)Parameters**embed_dim** – total dimension of the model**num_heads** – parallel attention layers, or headsExamples:`>>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads) >>> attn_output, attn_output_weights = multihead_attn(query, key, value) ``forward`(*query*, *key*, *value*, *key_padding_mask=None*, *incremental_state=None*, *need_weights=True*, *static_kv=False*, *attn_mask=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention.forward)Inputs of forward functionquery: [target length, batch size, embed dim] key: [sequence length, batch size, embed dim] value: [sequence length, batch size, embed dim] key_padding_mask: if True, mask padding based on batch size incremental_state: if provided, previous time steps are cashed need_weights: output attn_output_weights static_kv: key and value are staticOutputs of forward functionattn_output: [target length, batch size, embed dim] attn_output_weights: [batch size, target length, sequence length]

### PReLU

- *CLASS*`torch.nn.``PReLU`(*num_parameters=1*, *init=0.25*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#PReLU)

  Applies the element-wise function:\text{PReLU}(x) = \max(0,x) + a * \min(0,x)PReLU(x)=max(0,x)+a∗min(0,x)or\text{PReLU}(x) = \begin{cases} x, &amp; \text{ if } x \geq 0 \\ ax, &amp; \text{ otherwise } \end{cases}PReLU(x)={x,ax, if x≥0 otherwise Here aa is a learnable parameter. When called without arguments, nn.PReLU() uses a single parameter aa across all input channels. If called with nn.PReLU(nChannels), a separate aa is used for each input channel.NOTEweight decay should not be used when learning aa for good performance.NOTEChannel dim is the 2nd dim of input. When input has dims < 2, then there is no channel dim and the number of channels = 1.Parameters**num_parameters** ([*int*](https://docs.python.org/3/library/functions.html#int)) – number of aa to learn. Although it takes an int as input, there is only two values are legitimate: 1, or the number of channels at input. Default: 1**init** ([*float*](https://docs.python.org/3/library/functions.html#float)) – the initial value of aa. Default: 0.25Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the inputVariables**~PReLU.weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable weights of shape (`num_parameters`).![_images/PReLU.png](https://pytorch.org/docs/stable/_images/PReLU.png)Examples:`>>> m = nn.PReLU() >>> input = torch.randn(2) >>> output = m(input) `

### ReLU

- *CLASS*`torch.nn.``ReLU`(*inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#ReLU)

  Applies the rectified linear unit function element-wise:\text{ReLU}(x)= \max(0, x)ReLU(x)=max(0,x)Parameters**inplace** – can optionally do the operation in-place. Default: `False`Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/ReLU.png](https://pytorch.org/docs/stable/_images/ReLU.png)Examples:`  >>> m = nn.ReLU()   >>> input = torch.randn(2)   >>> output = m(input)   An implementation of CReLU - https://arxiv.org/abs/1603.05201    >>> m = nn.ReLU()   >>> input = torch.randn(2).unsqueeze(0)   >>> output = torch.cat((m(input),m(-input))) `

### ReLU6

- *CLASS*`torch.nn.``ReLU6`(*inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#ReLU6)

  Applies the element-wise function:\text{ReLU6}(x) = \min(\max(0,x), 6)ReLU6(x)=min(max(0,x),6)Parameters**inplace** – can optionally do the operation in-place. Default: `False`Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/ReLU6.png](http://images.iterate.site/blog/image/20200523/nbAEWoePDqwC.png?imageslim)Examples:`>>> m = nn.ReLU6() >>> input = torch.randn(2) >>> output = m(input) `

### RReLU

- *CLASS*`torch.nn.``RReLU`(*lower=0.125*, *upper=0.3333333333333333*, *inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#RReLU)

  Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:[Empirical Evaluation of Rectified Activations in Convolutional Network](https://arxiv.org/abs/1505.00853).The function is defined as:\text{RReLU}(x) = \begin{cases} x &amp; \text{if } x \geq 0 \\ ax &amp; \text{ otherwise } \end{cases}RReLU(x)={xaxif x≥0 otherwise where aa is randomly sampled from uniform distribution \mathcal{U}(\text{lower}, \text{upper})U(lower,upper).See: <https://arxiv.org/pdf/1505.00853.pdf>Parameters**lower** – lower bound of the uniform distribution. Default: \frac{1}{8}81**upper** – upper bound of the uniform distribution. Default: \frac{1}{3}31**inplace** – can optionally do the operation in-place. Default: `False`Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the inputExamples:`>>> m = nn.RReLU(0.1, 0.3) >>> input = torch.randn(2) >>> output = m(input) `

### SELU

- *CLASS*`torch.nn.``SELU`(*inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#SELU)

  Applied element-wise, as:\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))SELU(x)=scale∗(max(0,x)+min(0,α∗(exp(x)−1)))with \alpha = 1.6732632423543772848170429916717α=1.6732632423543772848170429916717 and \text{scale} = 1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.More details can be found in the paper [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515) .Parameters**inplace** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – can optionally do the operation in-place. Default: `False`Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/SELU.png](https://pytorch.org/docs/stable/_images/SELU.png)Examples:`>>> m = nn.SELU() >>> input = torch.randn(2) >>> output = m(input) `

### CELU

- *CLASS*`torch.nn.``CELU`(*alpha=1.0*, *inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#CELU)

  Applies the element-wise function:\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))CELU(x)=max(0,x)+min(0,α∗(exp(x/α)−1))More details can be found in the paper [Continuously Differentiable Exponential Linear Units](https://arxiv.org/abs/1704.07483) .Parameters**alpha** – the \alphaα value for the CELU formulation. Default: 1.0**inplace** – can optionally do the operation in-place. Default: `False`Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/CELU.png](https://pytorch.org/docs/stable/_images/CELU.png)Examples:`>>> m = nn.CELU() >>> input = torch.randn(2) >>> output = m(input) `

### Sigmoid

- *CLASS*`torch.nn.``Sigmoid`[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Sigmoid)

  Applies the element-wise function:\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}Sigmoid(x)=1+exp(−x)1Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/Sigmoid.png](https://pytorch.org/docs/stable/_images/Sigmoid.png)Examples:`>>> m = nn.Sigmoid() >>> input = torch.randn(2) >>> output = m(input) `

### Softplus

- *CLASS*`torch.nn.``Softplus`(*beta=1*, *threshold=20*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softplus)

  Applies the element-wise function:\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))Softplus(x)=β1∗log(1+exp(β∗x))SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.For numerical stability the implementation reverts to the linear function for inputs above a certain value.Parameters**beta** – the \betaβ value for the Softplus formulation. Default: 1**threshold** – values above this revert to a linear function. Default: 20Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/Softplus.png](https://pytorch.org/docs/stable/_images/Softplus.png)Examples:`>>> m = nn.Softplus() >>> input = torch.randn(2) >>> output = m(input) `

### Softshrink

- *CLASS*`torch.nn.``Softshrink`(*lambd=0.5*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softshrink)

  Applies the soft shrinkage function elementwise:\text{SoftShrinkage}(x) = \begin{cases} x - \lambda, &amp; \text{ if } x &gt; \lambda \\ x + \lambda, &amp; \text{ if } x &lt; -\lambda \\ 0, &amp; \text{ otherwise } \end{cases}SoftShrinkage(x)=⎩⎪⎨⎪⎧x−λ,x+λ,0, if x>λ if x<−λ otherwise Parameters**lambd** – the \lambdaλ value for the Softshrink formulation. Default: 0.5Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/Softshrink.png](https://pytorch.org/docs/stable/_images/Softshrink.png)Examples:`>>> m = nn.Softshrink() >>> input = torch.randn(2) >>> output = m(input) `

### Softsign

- *CLASS*`torch.nn.``Softsign`[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softsign)

  Applies the element-wise function:\text{SoftSign}(x) = \frac{x}{ 1 + |x|}SoftSign(x)=1+∣x∣xShape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/Softsign.png](https://pytorch.org/docs/stable/_images/Softsign.png)Examples:`>>> m = nn.Softsign() >>> input = torch.randn(2) >>> output = m(input) `

### Tanh

- *CLASS*`torch.nn.``Tanh`[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Tanh)

  Applies the element-wise function:\text{Tanh}(x) = \tanh(x) = \frac{e^x - e^{-x}} {e^x + e^{-x}}Tanh(x)=tanh(x)=ex+e−xex−e−xShape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/Tanh.png](https://pytorch.org/docs/stable/_images/Tanh.png)Examples:`>>> m = nn.Tanh() >>> input = torch.randn(2) >>> output = m(input) `

### Tanhshrink

- *CLASS*`torch.nn.``Tanhshrink`[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Tanhshrink)

  Applies the element-wise function:\text{Tanhshrink}(x) = x - \text{Tanh}(x)Tanhshrink(x)=x−Tanh(x)Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the input![_images/Tanhshrink.png](https://pytorch.org/docs/stable/_images/Tanhshrink.png)Examples:`>>> m = nn.Tanhshrink() >>> input = torch.randn(2) >>> output = m(input) `

### Threshold

- *CLASS*`torch.nn.``Threshold`(*threshold*, *value*, *inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Threshold)

  Thresholds each element of the input Tensor.Threshold is defined as:y = \begin{cases} x, &amp;\text{ if } x &gt; \text{threshold} \\ \text{value}, &amp;\text{ otherwise } \end{cases}y={x,value, if x>threshold otherwise Parameters**threshold** – The value to threshold at**value** – The value to replace with**inplace** – can optionally do the operation in-place. Default: `False`Shape:Input: (N, *)(N,∗) where * means, any number of additional dimensionsOutput: (N, *)(N,∗), same shape as the inputExamples:`>>> m = nn.Threshold(0.1, 20) >>> input = torch.randn(2) >>> output = m(input) `

## Non-linear activations (other)

### Softmin

- *CLASS*`torch.nn.``Softmin`(*dim=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softmin)

  Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1.Softmin is defined as:\text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}Softmin(xi)=∑jexp(−xj)exp(−xi)Shape:Input: (*)(∗) where * means, any number of additional dimensionsOutput: (*)(∗), same shape as the inputParameters**dim** ([*int*](https://docs.python.org/3/library/functions.html#int)) – A dimension along which Softmin will be computed (so every slice along dim will sum to 1).Returnsa Tensor of the same dimension and shape as the input, with values in the range [0, 1]Examples:`>>> m = nn.Softmin() >>> input = torch.randn(2, 3) >>> output = m(input) `

### Softmax

- *CLASS*`torch.nn.``Softmax`(*dim=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softmax)

  Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.Softmax is defined as:\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}Softmax(xi)=∑jexp(xj)exp(xi)Shape:Input: (*)(∗) where * means, any number of additional dimensionsOutput: (*)(∗), same shape as the inputReturnsa Tensor of the same dimension and shape as the input with values in the range [0, 1]Parameters**dim** ([*int*](https://docs.python.org/3/library/functions.html#int)) – A dimension along which Softmax will be computed (so every slice along dim will sum to 1).NOTEThis module doesn’t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use LogSoftmax instead (it’s faster and has better numerical properties).Examples:`>>> m = nn.Softmax() >>> input = torch.randn(2, 3) >>> output = m(input) `

### Softmax2d

- *CLASS*`torch.nn.``Softmax2d`[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#Softmax2d)

  Applies SoftMax over features to each spatial location.When given an image of `Channels x Height x Width`, it will apply Softmax to each location (Channels, h_i, w_j)(Channels,hi,wj)Shape:Input: (N, C, H, W)(N,C,H,W)Output: (N, C, H, W)(N,C,H,W) (same shape as input)Returnsa Tensor of the same dimension and shape as the input with values in the range [0, 1]Examples:`>>> m = nn.Softmax2d() >>> # you softmax over the 2nd dimension >>> input = torch.randn(2, 3, 12, 13) >>> output = m(input) `

### LogSoftmax

- *CLASS*`torch.nn.``LogSoftmax`(*dim=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#LogSoftmax)

  Applies the \log(\text{Softmax}(x))log(Softmax(x)) function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)LogSoftmax(xi)=log(∑jexp(xj)exp(xi))Shape:Input: (*)(∗) where * means, any number of additional dimensionsOutput: (*)(∗), same shape as the inputParameters**dim** ([*int*](https://docs.python.org/3/library/functions.html#int)) – A dimension along which LogSoftmax will be computed.Returnsa Tensor of the same dimension and shape as the input with values in the range [-inf, 0)Examples:`>>> m = nn.LogSoftmax() >>> input = torch.randn(2, 3) >>> output = m(input) `

### AdaptiveLogSoftmaxWithLoss

- *CLASS*`torch.nn.``AdaptiveLogSoftmaxWithLoss`(*in_features*, *n_classes*, *cutoffs*, *div_value=4.0*, *head_bias=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss)

  Efficient softmax approximation as described in [Efficient softmax approximation for GPUs](https://arxiv.org/abs/1609.04309) by Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou.Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the [Zipf’s law](https://en.wikipedia.org/wiki/Zipf%27s_law).Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute – that is, contain a small number of assigned labels.We highly recommend taking a look at the original paper for more details.`cutoffs` should be an ordered Sequence of integers sorted in the increasing order. It controls number of clusters and the partitioning of targets into clusters. For example setting `cutoffs =[10, 100, 1000]` means that first 10 targets will be assigned to the ‘head’ of the adaptive softmax, targets 11, 12, …, 100 will be assigned to the first cluster, and targets 101, 102, …, 1000 will be assigned to the second cluster, while targets 1001, 1002, …, n_classes - 1 will be assigned to the last, third cluster.`div_value` is used to compute the size of each additional cluster, which is given as \left\lfloor\frac{in\_features}{div\_value^{idx}}\right\rfloor⌊div_valueidxin_features⌋, where idxidx is the cluster index (with clusters for less frequent words having larger indices, and indices starting from 11).`head_bias` if set to True, adds a bias term to the ‘head’ of the adaptive softmax. See paper for details. Set to False in the official implementation.WARNINGLabels passed as inputs to this module should be sorted accoridng to their frequency. This means that the most frequent label should be represented by the index 0, and the least frequent label should be represented by the index n_classes - 1.NOTEThis module returns a `NamedTuple` with `output` and `loss` fields. See further documentation for details.NOTETo compute log-probabilities for all classes, the `log_prob` method can be used.Parameters**in_features** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of features in the input tensor**n_classes** ([*int*](https://docs.python.org/3/library/functions.html#int)) – Number of classes in the dataset**cutoffs** (*Sequence*) – Cutoffs used to assign targets to their buckets**div_value** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – value used as an exponent to compute sizes of the clusters. Default: 4.0**head_bias** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, adds a bias term to the ‘head’ of the adaptive softmax. Default: `False`Returns**output** is a Tensor of size `N` containing computed target log probabilities for each example**loss** is a Scalar representing the computed negative log likelihood lossReturn type`NamedTuple` with `output` and `loss` fieldsShape:input: (N, in\_features)(N,in_features)target: (N)(N) where each value satisfies 0 &lt;= target[i] &lt;= n\_classes0<=target[i]<=n_classesoutput1: (N)(N)output2: `Scalar``log_prob`(*input*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.log_prob)Computes log probabilities for all n\_classesn_classesParameters**input** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – a minibatch of examplesReturnslog-probabilities of for each class cc in range 0 &lt;= c &lt;= n\_classes0<=c<=n_classes, where n\_classesn_classes is a parameter passed to `AdaptiveLogSoftmaxWithLoss`constructor.Shape:Input: (N, in\_features)(N,in_features)Output: (N, n\_classes)(N,n_classes)`predict`(*input*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.predict)This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases.Parameters**input** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – a minibatch of examplesReturnsa class with the highest probability for each exampleReturn typeoutput ([Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor))Shape:Input: (N, in\_features)(N,in_features)Output: (N)(N)

## Normalization layers

### BatchNorm1d

- *CLASS*`torch.nn.``BatchNorm1d`(*num_features*, *eps=1e-05*, *momentum=0.1*, *affine=True*, *track_running_stats=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm1d)

  Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) .y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \betay=Var[x]+ϵx−E[x]∗γ+βThe mean and standard-deviation are calculated per-dimension over the mini-batches and \gammaγ and \betaβare learnable parameter vectors of size C (where C is the input size). By default, the elements of \gammaγare sampled from \mathcal{U}(0, 1)U(0,1) and the elements of \betaβ are set to 0.Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default `momentum` of 0.1.If `track_running_stats` is set to `False`, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.NOTEThis `momentum` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is \hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_tx^new=(1−momentum)×x^+momentum×xt, where \hat{x}x^ is the estimated statistic and x_txt is the new observed value.Because the Batch Normalization is done over the C dimension, computing statistics on (N, L) slices, it’s common terminology to call this Temporal Batch Normalization.Parameters**num_features** – CC from an expected input of size (N, C, L)(N,C,L) or LL from input of size (N, L)(N,L)**eps** – a value added to the denominator for numerical stability. Default: 1e-5**momentum** – the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1**affine** – a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`**track_running_stats** – a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`Shape:Input: (N, C)(N,C) or (N, C, L)(N,C,L)Output: (N, C)(N,C) or (N, C, L)(N,C,L) (same shape as input)Examples:`>>> # With Learnable Parameters >>> m = nn.BatchNorm1d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm1d(100, affine=False) >>> input = torch.randn(20, 100) >>> output = m(input) `

### BatchNorm2d

- *CLASS*`torch.nn.``BatchNorm2d`(*num_features*, *eps=1e-05*, *momentum=0.1*, *affine=True*, *track_running_stats=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d)

  Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) .y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \betay=Var[x]+ϵx−E[x]∗γ+βThe mean and standard-deviation are calculated per-dimension over the mini-batches and \gammaγ and \betaβare learnable parameter vectors of size C (where C is the input size). By default, the elements of \gammaγare sampled from \mathcal{U}(0, 1)U(0,1) and the elements of \betaβ are set to 0.Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default `momentum` of 0.1.If `track_running_stats` is set to `False`, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.NOTEThis `momentum` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is \hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_tx^new=(1−momentum)×x^+momentum×xt, where \hat{x}x^ is the estimated statistic and x_txt is the new observed value.Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W)slices, it’s common terminology to call this Spatial Batch Normalization.Parameters**num_features** – CC from an expected input of size (N, C, H, W)(N,C,H,W)**eps** – a value added to the denominator for numerical stability. Default: 1e-5**momentum** – the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1**affine** – a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`**track_running_stats** – a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`Shape:Input: (N, C, H, W)(N,C,H,W)Output: (N, C, H, W)(N,C,H,W) (same shape as input)Examples:`>>> # With Learnable Parameters >>> m = nn.BatchNorm2d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm2d(100, affine=False) >>> input = torch.randn(20, 100, 35, 45) >>> output = m(input) `

### BatchNorm3d

- *CLASS*`torch.nn.``BatchNorm3d`(*num_features*, *eps=1e-05*, *momentum=0.1*, *affine=True*, *track_running_stats=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm3d)

  Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) .y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \betay=Var[x]+ϵx−E[x]∗γ+βThe mean and standard-deviation are calculated per-dimension over the mini-batches and \gammaγ and \betaβare learnable parameter vectors of size C (where C is the input size). By default, the elements of \gammaγare sampled from \mathcal{U}(0, 1)U(0,1) and the elements of \betaβ are set to 0.Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default `momentum` of 0.1.If `track_running_stats` is set to `False`, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.NOTEThis `momentum` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is \hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_tx^new=(1−momentum)×x^+momentum×xt, where \hat{x}x^ is the estimated statistic and x_txt is the new observed value.Because the Batch Normalization is done over the C dimension, computing statistics on (N, D, H, W)slices, it’s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.Parameters**num_features** – CC from an expected input of size (N, C, D, H, W)(N,C,D,H,W)**eps** – a value added to the denominator for numerical stability. Default: 1e-5**momentum** – the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1**affine** – a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`**track_running_stats** – a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`Shape:Input: (N, C, D, H, W)(N,C,D,H,W)Output: (N, C, D, H, W)(N,C,D,H,W) (same shape as input)Examples:`>>> # With Learnable Parameters >>> m = nn.BatchNorm3d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm3d(100, affine=False) >>> input = torch.randn(20, 100, 35, 45, 10) >>> output = m(input) `

### GroupNorm

- *CLASS*`torch.nn.``GroupNorm`(*num_groups*, *num_channels*, *eps=1e-05*, *affine=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#GroupNorm)

  Applies Group Normalization over a mini-batch of inputs as described in the paper [Group Normalization](https://arxiv.org/abs/1803.08494) .y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \betay=Var[x]+ϵx−E[x]∗γ+βThe input channels are separated into `num_groups` groups, each containing `num_channels /num_groups` channels. The mean and standard-deviation are calculated separately over the each group. \gammaγ and \betaβ are learnable per-channel affine transform parameter vectors of size `num_channels`if `affine` is `True`.This layer uses statistics computed from input data in both training and evaluation modes.Parameters**num_groups** ([*int*](https://docs.python.org/3/library/functions.html#int)) – number of groups to separate the channels into**num_channels** ([*int*](https://docs.python.org/3/library/functions.html#int)) – number of channels expected in input**eps** – a value added to the denominator for numerical stability. Default: 1e-5**affine** – a boolean value that when set to `True`, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: `True`.Shape:Input: (N, C, *)(N,C,∗) where C=\text{num\_channels}C=num_channelsOutput: (N, C, *)(N,C,∗) (same shape as input)Examples:`>>> input = torch.randn(20, 6, 10, 10) >>> # Separate 6 channels into 3 groups >>> m = nn.GroupNorm(3, 6) >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm) >>> m = nn.GroupNorm(6, 6) >>> # Put all 6 channels into a single group (equivalent with LayerNorm) >>> m = nn.GroupNorm(1, 6) >>> # Activating the module >>> output = m(input) `

### SyncBatchNorm

- *CLASS*`torch.nn.``SyncBatchNorm`(*num_features*, *eps=1e-05*, *momentum=0.1*, *affine=True*, *track_running_stats=True*, *process_group=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm)

  Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) .y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \betay=Var[x]+ϵx−E[x]∗γ+βThe mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups. \gammaγ and \betaβ are learnable parameter vectors of size C (where C is the input size). By default, the elements of \gammaγ are sampled from \mathcal{U}(0, 1)U(0,1) and the elements of \betaβ are set to 0.Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default `momentum` of 0.1.If `track_running_stats` is set to `False`, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.NOTEThis `momentum` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is \hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_tx^new=(1−momentum)×x^+momemtum×xt, where \hat{x}x^ is the estimated statistic and x_txt is the new observed value.Because the Batch Normalization is done over the C dimension, computing statistics on (N, +) slices, it’s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.Currently SyncBatchNorm only supports DistributedDataParallel with single GPU per process. Use torch.nn.SyncBatchNorm.convert_sync_batchnorm() to convert BatchNorm layer to SyncBatchNorm before wrapping Network with DDP.Parameters**num_features** – CC from an expected input of size (N, C, +)(N,C,+)**eps** – a value added to the denominator for numerical stability. Default: 1e-5**momentum** – the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1**affine** – a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`**track_running_stats** – a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`**process_group** – synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole worldShape:Input: (N, C, +)(N,C,+)Output: (N, C, +)(N,C,+) (same shape as input)Examples:`>>> # With Learnable Parameters >>> m = nn.SyncBatchNorm(100) >>> # creating process group (optional) >>> # process_ids is a list of int identifying rank ids. >>> process_group = torch.distributed.new_group(process_ids) >>> # Without Learnable Parameters >>> m = nn.BatchNorm3d(100, affine=False, process_group=process_group) >>> input = torch.randn(20, 100, 35, 45, 10) >>> output = m(input)  >>> # network is nn.BatchNorm layer >>> sync_bn_network = torch.nn.utils.convert_sync_batchnorm(network, process_group) >>> # only single gpu per process is currently supported >>> ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel( >>>                         sync_bn_network, >>>                         device_ids=[args.local_rank], >>>                         output_device=args.local_rank) `*CLASSMETHOD* `convert_sync_batchnorm`(*module*, *process_group=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm)Helper function to convert torch.nn.BatchNormND layer in the model totorch.nn.SyncBatchNorm layer.Parameters**module** ([*nn.Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – containing module**process_group** (*optional*) – process group to scope synchronization,default is the whole worldReturnsThe original module with the converted torch.nn.SyncBatchNorm layerExample:`>>> # Network with nn.BatchNorm layer >>> module = torch.nn.Sequential( >>>            torch.nn.Linear(20, 100), >>>            torch.nn.BatchNorm1d(100) >>>          ).cuda() >>> # creating process group (optional) >>> # process_ids is a list of int identifying rank ids. >>> process_group = torch.distributed.new_group(process_ids) >>> sync_bn_module = convert_sync_batchnorm(module, process_group) `

### InstanceNorm1d

- *CLASS*`torch.nn.``InstanceNorm1d`(*num_features*, *eps=1e-05*, *momentum=0.1*, *affine=False*, *track_running_stats=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#InstanceNorm1d)

  Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022) .y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \betay=Var[x]+ϵx−E[x]∗γ+βThe mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \gammaγ and \betaβ are learnable parameter vectors of size C (where C is the input size) if `affine` is `True`.By default, this layer uses instance statistics computed from input data in both training and evaluation modes.If `track_running_stats` is set to `True`, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default `momentum` of 0.1.NOTEThis `momentum` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is \hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_tx^new=(1−momentum)×x^+momemtum×xt, where \hat{x}x^ is the estimated statistic and x_txt is the new observed value.NOTE[`InstanceNorm1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm1d) and [`LayerNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) are very similar, but have some subtle differences. [`InstanceNorm1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm1d) is applied on each channel of channeled data like multidimensional time series, but [`LayerNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) is usually applied on entire sample and often in NLP tasks. Additionaly, [`LayerNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) applies elementwise affine transform, while [`InstanceNorm1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm1d) usually don’t apply affine transform.Parameters**num_features** – CC from an expected input of size (N, C, L)(N,C,L) or LL from input of size (N, L)(N,L)**eps** – a value added to the denominator for numerical stability. Default: 1e-5**momentum** – the value used for the running_mean and running_var computation. Default: 0.1**affine** – a boolean value that when set to `True`, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: `False`.**track_running_stats** – a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `False`Shape:Input: (N, C, L)(N,C,L)Output: (N, C, L)(N,C,L) (same shape as input)Examples:`>>> # Without Learnable Parameters >>> m = nn.InstanceNorm1d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm1d(100, affine=True) >>> input = torch.randn(20, 100, 40) >>> output = m(input) `

### InstanceNorm2d

- *CLASS*`torch.nn.``InstanceNorm2d`(*num_features*, *eps=1e-05*, *momentum=0.1*, *affine=False*, *track_running_stats=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#InstanceNorm2d)

  Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022) .y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \betay=Var[x]+ϵx−E[x]∗γ+βThe mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \gammaγ and \betaβ are learnable parameter vectors of size C (where C is the input size) if `affine` is `True`.By default, this layer uses instance statistics computed from input data in both training and evaluation modes.If `track_running_stats` is set to `True`, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default `momentum` of 0.1.NOTEThis `momentum` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is \hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_tx^new=(1−momentum)×x^+momemtum×xt, where \hat{x}x^ is the estimated statistic and x_txt is the new observed value.NOTE[`InstanceNorm2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm2d) and [`LayerNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) are very similar, but have some subtle differences. [`InstanceNorm2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm2d) is applied on each channel of channeled data like RGB images, but[`LayerNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) is usually applied on entire sample and often in NLP tasks. Additionaly, [`LayerNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm)applies elementwise affine transform, while [`InstanceNorm2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm2d) usually don’t apply affine transform.Parameters**num_features** – CC from an expected input of size (N, C, H, W)(N,C,H,W)**eps** – a value added to the denominator for numerical stability. Default: 1e-5**momentum** – the value used for the running_mean and running_var computation. Default: 0.1**affine** – a boolean value that when set to `True`, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: `False`.**track_running_stats** – a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `False`Shape:Input: (N, C, H, W)(N,C,H,W)Output: (N, C, H, W)(N,C,H,W) (same shape as input)Examples:`>>> # Without Learnable Parameters >>> m = nn.InstanceNorm2d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm2d(100, affine=True) >>> input = torch.randn(20, 100, 35, 45) >>> output = m(input) `

### InstanceNorm3d

- *CLASS*`torch.nn.``InstanceNorm3d`(*num_features*, *eps=1e-05*, *momentum=0.1*, *affine=False*, *track_running_stats=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/instancenorm.html#InstanceNorm3d)

  Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022) .y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \betay=Var[x]+ϵx−E[x]∗γ+βThe mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \gammaγ and \betaβ are learnable parameter vectors of size C (where C is the input size) if `affine` is `True`.By default, this layer uses instance statistics computed from input data in both training and evaluation modes.If `track_running_stats` is set to `True`, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default `momentum` of 0.1.NOTEThis `momentum` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is \hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_tx^new=(1−momentum)×x^+momemtum×xt, where \hat{x}x^ is the estimated statistic and x_txt is the new observed value.NOTE[`InstanceNorm3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm3d) and [`LayerNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) are very similar, but have some subtle differences. [`InstanceNorm3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm3d) is applied on each channel of channeled data like 3D models with RGB color, but [`LayerNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) is usually applied on entire sample and often in NLP tasks. Additionaly, [`LayerNorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) applies elementwise affine transform, while [`InstanceNorm3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm3d) usually don’t apply affine transform.Parameters**num_features** – CC from an expected input of size (N, C, D, H, W)(N,C,D,H,W)**eps** – a value added to the denominator for numerical stability. Default: 1e-5**momentum** – the value used for the running_mean and running_var computation. Default: 0.1**affine** – a boolean value that when set to `True`, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: `False`.**track_running_stats** – a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `False`Shape:Input: (N, C, D, H, W)(N,C,D,H,W)Output: (N, C, D, H, W)(N,C,D,H,W) (same shape as input)Examples:`>>> # Without Learnable Parameters >>> m = nn.InstanceNorm3d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm3d(100, affine=True) >>> input = torch.randn(20, 100, 35, 45, 10) >>> output = m(input) `

### LayerNorm

- *CLASS*`torch.nn.``LayerNorm`(*normalized_shape*, *eps=1e-05*, *elementwise_affine=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#LayerNorm)

  Applies Layer Normalization over a mini-batch of inputs as described in the paper [Layer Normalization](https://arxiv.org/abs/1607.06450) .y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \betay=Var[x]+ϵx−E[x]∗γ+βThe mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape`. \gammaγ and \betaβ are learnable affine transform parameters of `normalized_shape` if `elementwise_affine` is `True`.NOTEUnlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the `affine` option, Layer Normalization applies per-element scale and bias with `elementwise_affine`.This layer uses statistics computed from input data in both training and evaluation modes.Parameters**normalized_shape** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*list*](https://docs.python.org/3/library/stdtypes.html#list) *or* *torch.Size*) –input shape from an expected input of size[* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1] \times \ldots \times \text{normalized\_shape}[-1]][∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.**eps** – a value added to the denominator for numerical stability. Default: 1e-5**elementwise_affine** – a boolean value that when set to `True`, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: `True`.Shape:Input: (N, *)(N,∗)Output: (N, *)(N,∗) (same shape as input)Examples:`>>> input = torch.randn(20, 5, 10, 10) >>> # With Learnable Parameters >>> m = nn.LayerNorm(input.size()[1:]) >>> # Without Learnable Parameters >>> m = nn.LayerNorm(input.size()[1:], elementwise_affine=False) >>> # Normalize over last two dimensions >>> m = nn.LayerNorm([10, 10]) >>> # Normalize over last dimension of size 10 >>> m = nn.LayerNorm(10) >>> # Activating the module >>> output = m(input) `

### LocalResponseNorm

- *CLASS*`torch.nn.``LocalResponseNorm`(*size*, *alpha=0.0001*, *beta=0.75*, *k=1.0*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/normalization.html#LocalResponseNorm)

  Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.b_{c} = a_{c}\left(k + \frac{\alpha}{n} \sum_{c&#x27;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&#x27;}^2\right)^{-\beta}bc=ac⎝⎛k+nαc′=max(0,c−n/2)∑min(N−1,c+n/2)ac′2⎠⎞−βParameters**size** – amount of neighbouring channels used for normalization**alpha** – multiplicative factor. Default: 0.0001**beta** – exponent. Default: 0.75**k** – additive factor. Default: 1Shape:Input: (N, C, *)(N,C,∗)Output: (N, C, *)(N,C,∗) (same shape as input)Examples:`>>> lrn = nn.LocalResponseNorm(2) >>> signal_2d = torch.randn(32, 5, 24, 24) >>> signal_4d = torch.randn(16, 5, 7, 7, 7, 7) >>> output_2d = lrn(signal_2d) >>> output_4d = lrn(signal_4d) `

## Recurrent layers

### RNN

- *CLASS*`torch.nn.``RNN`(**args*, **\*kwargs*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNN)

  Applies a multi-layer Elman RNN with tanhtanh or ReLUReLU non-linearity to an input sequence.For each element in the input sequence, each layer computes the following function:h_t = \text{tanh}(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})ht=tanh(Wihxt+bih+Whhh(t−1)+bhh)where h_tht is the hidden state at time t, x_txt is the input at time t, and h_{(t-1)}h(t−1) is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0. If `nonlinearity` is `'relu'`, then ReLUis used instead of tanh.Parameters**input_size** – The number of expected features in the input x**hidden_size** – The number of features in the hidden state h**num_layers** – Number of recurrent layers. E.g., setting `num_layers=2` would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1**nonlinearity** – The non-linearity to use. Can be either `'tanh'` or `'relu'`. Default: `'tanh'`**bias** – If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`**batch_first** – If `True`, then the input and output tensors are provided as (batch, seq, feature). Default: `False`**dropout** – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to `dropout`. Default: 0**bidirectional** – If `True`, becomes a bidirectional RNN. Default: `False`Inputs: input, h_0**input** of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See [`torch.nn.utils.rnn.pack_padded_sequence()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence) or [`torch.nn.utils.rnn.pack_sequence()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_sequence) for details.**h_0** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.Outputs: output, h_n**output** of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a [`torch.nn.utils.rnn.PackedSequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) has been given as the input, the output will also be a packed sequence.For the unpacked case, the directions can be separated using `output.view(seq_len,batch, num_directions, hidden_size)`, with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.**h_n** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.Like *output*, the layers can be separated using `h_n.view(num_layers,num_directions, batch, hidden_size)`.Shape:Input1: (L, N, H_{in})(L,N,Hin) tensor containing input features where H_{in}=\text{input\_size}Hin=input_sizeand L represents a sequence length.Input2: (S, N, H_{out})(S,N,Hout) tensor containing the initial hidden state for each element in the batch. H_{out}=\text{hidden\_size}Hout=hidden_size Defaults to zero if not provided. where S=\text{num\_layers} * \text{num\_directions}S=num_layers∗num_directions If the RNN is bidirectional, num_directions should be 2, else it should be 1.Output1: (L, N, H_{all})(L,N,Hall) where H_all=\text{num\_directions} * \text{hidden\_size}Hall=num_directions∗hidden_sizeOutput2: (S, N, H_{out})(S,N,Hout) tensor containing the next hidden state for each element in the batchVariables**~RNN.weight_ih_l[k]** – the learnable input-hidden weights of the k-th layer, of shape (hidden_size, input_size) for k = 0. Otherwise, the shape is (hidden_size, num_directions * hidden_size)**~RNN.weight_hh_l[k]** – the learnable hidden-hidden weights of the k-th layer, of shape (hidden_size, hidden_size)**~RNN.bias_ih_l[k]** – the learnable input-hidden bias of the k-th layer, of shape (hidden_size)**~RNN.bias_hh_l[k]** – the learnable hidden-hidden bias of the k-th layer, of shape (hidden_size)NOTEAll the weights and biases are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{\text{hidden\_size}}k=hidden_size1NOTEIf the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype `torch.float16` 4) V100 GPU is used, 5) input data is not in `PackedSequence` format persistent algorithm can be selected to improve performance.Examples:`>>> rnn = nn.RNN(10, 20, 2) >>> input = torch.randn(5, 3, 10) >>> h0 = torch.randn(2, 3, 20) >>> output, hn = rnn(input, h0) `

### LSTM

- *CLASS*`torch.nn.``LSTM`(**args*, **\*kwargs*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM)

  Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.For each element in the input sequence, each layer computes the following function:\begin{array}{ll} \\ i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\ f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\ g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\ o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\ c_t = f_t * c_{(t-1)} + i_t * g_t \\ h_t = o_t * \tanh(c_t) \\ \end{array}it=σ(Wiixt+bii+Whih(t−1)+bhi)ft=σ(Wifxt+bif+Whfh(t−1)+bhf)gt=tanh(Wigxt+big+Whgh(t−1)+bhg)ot=σ(Wioxt+bio+Whoh(t−1)+bho)ct=ft∗c(t−1)+it∗gtht=ot∗tanh(ct)where h_tht is the hidden state at time t, c_tct is the cell state at time t, x_txt is the input at time t, h_{(t-1)}h(t−1) is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and i_tit, f_tft, g_tgt, o_tot are the input, forget, cell, and output gates, respectively. \sigmaσ is the sigmoid function, and *∗ is the Hadamard product.In a multilayer LSTM, the input x^{(l)}_txt(l) of the ll -th layer (l &gt;= 2l>=2) is the hidden state h^{(l-1)}_tht(l−1) of the previous layer multiplied by dropout \delta^{(l-1)}_tδt(l−1) where each \delta^{(l-1)}_tδt(l−1) is a Bernoulli random variable which is 00 with probability `dropout`.Parameters**input_size** – The number of expected features in the input x**hidden_size** – The number of features in the hidden state h**num_layers** – Number of recurrent layers. E.g., setting `num_layers=2` would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1**bias** – If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`**batch_first** – If `True`, then the input and output tensors are provided as (batch, seq, feature). Default: `False`**dropout** – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`. Default: 0**bidirectional** – If `True`, becomes a bidirectional LSTM. Default: `False`Inputs: input, (h_0, c_0)**input** of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See [`torch.nn.utils.rnn.pack_padded_sequence()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence) or[`torch.nn.utils.rnn.pack_sequence()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_sequence) for details.**h_0** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1.**c_0** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch.If (h_0, c_0) is not provided, both **h_0** and **c_0** default to zero.Outputs: output, (h_n, c_n)**output** of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a [`torch.nn.utils.rnn.PackedSequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) has been given as the input, the output will also be a packed sequence.For the unpacked case, the directions can be separated using `output.view(seq_len,batch, num_directions, hidden_size)`, with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.**h_n** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.Like *output*, the layers can be separated using `h_n.view(num_layers,num_directions, batch, hidden_size)` and similarly for *c_n*.**c_n** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t = seq_len.Variables**~LSTM.weight_ih_l[k]** – the learnable input-hidden weights of the \text{k}^{th}kth layer(W_ii|W_if|W_ig|W_io), of shape (4*hidden_size, input_size) for k = 0. Otherwise, the shape is (4*hidden_size, num_directions * hidden_size)**~LSTM.weight_hh_l[k]** – the learnable hidden-hidden weights of the \text{k}^{th}kth layer(W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size, hidden_size)**~LSTM.bias_ih_l[k]** – the learnable input-hidden bias of the \text{k}^{th}kth layer(b_ii|b_if|b_ig|b_io), of shape (4*hidden_size)**~LSTM.bias_hh_l[k]** – the learnable hidden-hidden bias of the \text{k}^{th}kth layer(b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)NOTEAll the weights and biases are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{\text{hidden\_size}}k=hidden_size1NOTEIf the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype `torch.float16` 4) V100 GPU is used, 5) input data is not in `PackedSequence` format persistent algorithm can be selected to improve performance.Examples:`>>> rnn = nn.LSTM(10, 20, 2) >>> input = torch.randn(5, 3, 10) >>> h0 = torch.randn(2, 3, 20) >>> c0 = torch.randn(2, 3, 20) >>> output, (hn, cn) = rnn(input, (h0, c0)) `

### GRU

- *CLASS*`torch.nn.``GRU`(**args*, **\*kwargs*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#GRU)

  Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.For each element in the input sequence, each layer computes the following function:\begin{array}{ll} r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\ z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\ n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \end{array}rt=σ(Wirxt+bir+Whrh(t−1)+bhr)zt=σ(Wizxt+biz+Whzh(t−1)+bhz)nt=tanh(Winxt+bin+rt∗(Whnh(t−1)+bhn))ht=(1−zt)∗nt+zt∗h(t−1)where h_tht is the hidden state at time t, x_txt is the input at time t, h_{(t-1)}h(t−1) is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and r_trt, z_tzt, n_tnt are the reset, update, and new gates, respectively. \sigmaσ is the sigmoid function, and *∗ is the Hadamard product.In a multilayer GRU, the input x^{(l)}_txt(l) of the ll -th layer (l &gt;= 2l>=2) is the hidden state h^{(l-1)}_tht(l−1) of the previous layer multiplied by dropout \delta^{(l-1)}_tδt(l−1) where each \delta^{(l-1)}_tδt(l−1) is a Bernoulli random variable which is 00 with probability `dropout`.Parameters**input_size** – The number of expected features in the input x**hidden_size** – The number of features in the hidden state h**num_layers** – Number of recurrent layers. E.g., setting `num_layers=2` would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1**bias** – If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`**batch_first** – If `True`, then the input and output tensors are provided as (batch, seq, feature). Default: `False`**dropout** – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to `dropout`. Default: 0**bidirectional** – If `True`, becomes a bidirectional GRU. Default: `False`Inputs: input, h_0**input** of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See [`torch.nn.utils.rnn.pack_padded_sequence()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence) for details.**h_0** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.Outputs: output, h_n**output** of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a [`torch.nn.utils.rnn.PackedSequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using `output.view(seq_len, batch, num_directions, hidden_size)`, with forward and backward being direction 0 and 1 respectively.Similarly, the directions can be separated in the packed case.**h_n** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_lenLike *output*, the layers can be separated using `h_n.view(num_layers,num_directions, batch, hidden_size)`.Shape:Input1: (L, N, H_{in})(L,N,Hin) tensor containing input features where H_{in}=\text{input\_size}Hin=input_sizeand L represents a sequence length.Input2: (S, N, H_{out})(S,N,Hout) tensor containing the initial hidden state for each element in the batch. H_{out}=\text{hidden\_size}Hout=hidden_size Defaults to zero if not provided. where S=\text{num\_layers} * \text{num\_directions}S=num_layers∗num_directions If the RNN is bidirectional, num_directions should be 2, else it should be 1.Output1: (L, N, H_{all})(L,N,Hall) where H_all=\text{num\_directions} * \text{hidden\_size}Hall=num_directions∗hidden_sizeOutput2: (S, N, H_{out})(S,N,Hout) tensor containing the next hidden state for each element in the batchVariables**~GRU.weight_ih_l[k]** – the learnable input-hidden weights of the \text{k}^{th}kth layer (W_ir|W_iz|W_in), of shape (3*hidden_size, input_size) for k = 0. Otherwise, the shape is (3*hidden_size, num_directions * hidden_size)**~GRU.weight_hh_l[k]** – the learnable hidden-hidden weights of the \text{k}^{th}kth layer (W_hr|W_hz|W_hn), of shape (3*hidden_size, hidden_size)**~GRU.bias_ih_l[k]** – the learnable input-hidden bias of the \text{k}^{th}kth layer (b_ir|b_iz|b_in), of shape (3*hidden_size)**~GRU.bias_hh_l[k]** – the learnable hidden-hidden bias of the \text{k}^{th}kth layer (b_hr|b_hz|b_hn), of shape (3*hidden_size)NOTEAll the weights and biases are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{\text{hidden\_size}}k=hidden_size1NOTEIf the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype `torch.float16` 4) V100 GPU is used, 5) input data is not in `PackedSequence` format persistent algorithm can be selected to improve performance.Examples:`>>> rnn = nn.GRU(10, 20, 2) >>> input = torch.randn(5, 3, 10) >>> h0 = torch.randn(2, 3, 20) >>> output, hn = rnn(input, h0) `

### RNNCell

- *CLASS*`torch.nn.``RNNCell`(*input_size*, *hidden_size*, *bias=True*, *nonlinearity='tanh'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNNCell)

  An Elman RNN cell with tanh or ReLU non-linearity.h&#x27; = \tanh(W_{ih} x + b_{ih} + W_{hh} h + b_{hh})h′=tanh(Wihx+bih+Whhh+bhh)If `nonlinearity` is ‘relu’, then ReLU is used in place of tanh.Parameters**input_size** – The number of expected features in the input x**hidden_size** – The number of features in the hidden state h**bias** – If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`**nonlinearity** – The non-linearity to use. Can be either `'tanh'` or `'relu'`. Default: `'tanh'`Inputs: input, hidden**input** of shape (batch, input_size): tensor containing input features**hidden** of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.Outputs: h’**h’** of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batchShape:Input1: (N, H_{in})(N,Hin) tensor containing input features where H_{in}Hin = input_sizeInput2: (N, H_{out})(N,Hout) tensor containing the initial hidden state for each element in the batch where H_{out}Hout = hidden_size Defaults to zero if not provided.Output: (N, H_{out})(N,Hout) tensor containing the next hidden state for each element in the batchVariables**~RNNCell.weight_ih** – the learnable input-hidden weights, of shape (hidden_size, input_size)**~RNNCell.weight_hh** – the learnable hidden-hidden weights, of shape (hidden_size, hidden_size)**~RNNCell.bias_ih** – the learnable input-hidden bias, of shape (hidden_size)**~RNNCell.bias_hh** – the learnable hidden-hidden bias, of shape (hidden_size)NOTEAll the weights and biases are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{\text{hidden\_size}}k=hidden_size1Examples:`>>> rnn = nn.RNNCell(10, 20) >>> input = torch.randn(6, 3, 10) >>> hx = torch.randn(3, 20) >>> output = [] >>> for i in range(6):         hx = rnn(input[i], hx)         output.append(hx) `

### LSTMCell

- *CLASS*`torch.nn.``LSTMCell`(*input_size*, *hidden_size*, *bias=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTMCell)

  A long short-term memory (LSTM) cell.\begin{array}{ll} i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\ f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\ g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\ o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\ c&#x27; = f * c + i * g \\ h&#x27; = o * \tanh(c&#x27;) \\ \end{array}i=σ(Wiix+bii+Whih+bhi)f=σ(Wifx+bif+Whfh+bhf)g=tanh(Wigx+big+Whgh+bhg)o=σ(Wiox+bio+Whoh+bho)c′=f∗c+i∗gh′=o∗tanh(c′)where \sigmaσ is the sigmoid function, and *∗ is the Hadamard product.Parameters**input_size** – The number of expected features in the input x**hidden_size** – The number of features in the hidden state h**bias** – If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`Inputs: input, (h_0, c_0)**input** of shape (batch, input_size): tensor containing input features**h_0** of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch.**c_0** of shape (batch, hidden_size): tensor containing the initial cell state for each element in the batch.If (h_0, c_0) is not provided, both **h_0** and **c_0** default to zero.Outputs: (h_1, c_1)**h_1** of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch**c_1** of shape (batch, hidden_size): tensor containing the next cell state for each element in the batchVariables**~LSTMCell.weight_ih** – the learnable input-hidden weights, of shape (4*hidden_size, input_size)**~LSTMCell.weight_hh** – the learnable hidden-hidden weights, of shape(4*hidden_size, hidden_size)**~LSTMCell.bias_ih** – the learnable input-hidden bias, of shape (4*hidden_size)**~LSTMCell.bias_hh** – the learnable hidden-hidden bias, of shape (4*hidden_size)NOTEAll the weights and biases are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{\text{hidden\_size}}k=hidden_size1Examples:`>>> rnn = nn.LSTMCell(10, 20) >>> input = torch.randn(6, 3, 10) >>> hx = torch.randn(3, 20) >>> cx = torch.randn(3, 20) >>> output = [] >>> for i in range(6):         hx, cx = rnn(input[i], (hx, cx))         output.append(hx) `

### GRUCell

- *CLASS*`torch.nn.``GRUCell`(*input_size*, *hidden_size*, *bias=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#GRUCell)

  A gated recurrent unit (GRU) cell\begin{array}{ll} r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\ z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\ n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\ h&#x27; = (1 - z) * n + z * h \end{array}r=σ(Wirx+bir+Whrh+bhr)z=σ(Wizx+biz+Whzh+bhz)n=tanh(Winx+bin+r∗(Whnh+bhn))h′=(1−z)∗n+z∗hwhere \sigmaσ is the sigmoid function, and *∗ is the Hadamard product.Parameters**input_size** – The number of expected features in the input x**hidden_size** – The number of features in the hidden state h**bias** – If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`Inputs: input, hidden**input** of shape (batch, input_size): tensor containing input features**hidden** of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.Outputs: h’**h’** of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batchShape:Input1: (N, H_{in})(N,Hin) tensor containing input features where H_{in}Hin = input_sizeInput2: (N, H_{out})(N,Hout) tensor containing the initial hidden state for each element in the batch where H_{out}Hout = hidden_size Defaults to zero if not provided.Output: (N, H_{out})(N,Hout) tensor containing the next hidden state for each element in the batchVariables**~GRUCell.weight_ih** – the learnable input-hidden weights, of shape (3*hidden_size, input_size)**~GRUCell.weight_hh** – the learnable hidden-hidden weights, of shape (3*hidden_size, hidden_size)**~GRUCell.bias_ih** – the learnable input-hidden bias, of shape (3*hidden_size)**~GRUCell.bias_hh** – the learnable hidden-hidden bias, of shape (3*hidden_size)NOTEAll the weights and biases are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{\text{hidden\_size}}k=hidden_size1Examples:`>>> rnn = nn.GRUCell(10, 20) >>> input = torch.randn(6, 3, 10) >>> hx = torch.randn(3, 20) >>> output = [] >>> for i in range(6):         hx = rnn(input[i], hx)         output.append(hx) `

## Linear layers

### Identity

- *CLASS*`torch.nn.``Identity`(**args*, **\*kwargs*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Identity)

  A placeholder identity operator that is argument-insensitive.Parameters**args** – any argument (unused)**kwargs** – any keyword argument (unused)Examples:`>>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False) >>> input = torch.randn(128, 20) >>> output = m(input) >>> print(output.size()) torch.Size([128, 20]) `

### Linear

- *CLASS*`torch.nn.``Linear`(*in_features*, *out_features*, *bias=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear)

  Applies a linear transformation to the incoming data: y = xA^T + by=xAT+bParameters**in_features** – size of each input sample**out_features** – size of each output sample**bias** – If set to `False`, the layer will not learn an additive bias. Default: `True`Shape:Input: (N, *, H_{in})(N,∗,Hin) where *∗ means any number of additional dimensions and H_{in} = \text{in\_features}Hin=in_featuresOutput: (N, *, H_{out})(N,∗,Hout) where all but the last dimension are the same shape as the input and H_{out} = \text{out\_features}Hout=out_features.Variables**~Linear.weight** – the learnable weights of the module of shape (\text{out\_features}, \text{in\_features})(out_features,in_features). The values are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k), where k = \frac{1}{\text{in\_features}}k=in_features1**~Linear.bias** – the learnable bias of the module of shape (\text{out\_features})(out_features). If `bias` is `True`, the values are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k) where k = \frac{1}{\text{in\_features}}k=in_features1Examples:`>>> m = nn.Linear(20, 30) >>> input = torch.randn(128, 20) >>> output = m(input) >>> print(output.size()) torch.Size([128, 30]) `

### Bilinear

- *CLASS*`torch.nn.``Bilinear`(*in1_features*, *in2_features*, *out_features*, *bias=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Bilinear)

  Applies a bilinear transformation to the incoming data: y = x_1 A x_2 + by=x1Ax2+bParameters**in1_features** – size of each first input sample**in2_features** – size of each second input sample**out_features** – size of each output sample**bias** – If set to False, the layer will not learn an additive bias. Default: `True`Shape:Input1: (N, *, H_{in1})(N,∗,Hin1) where H_{in1}=\text{in1\_features}Hin1=in1_features and *∗ means any number of additional dimensions. All but the last dimension of the inputs should be the same.Input2: (N, *, H_{in2})(N,∗,Hin2) where H_{in2}=\text{in2\_features}Hin2=in2_features.Output: (N, *, H_{out})(N,∗,Hout) where H_{out}=\text{out\_features}Hout=out_features and all but the last dimension are the same shape as the input.Variables**~Bilinear.weight** – the learnable weights of the module of shape (\text{out\_features}, \text{in1\_features}, \text{in2\_features})(out_features,in1_features,in2_features). The values are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k), where k = \frac{1}{\text{in1\_features}}k=in1_features1**~Bilinear.bias** – the learnable bias of the module of shape (\text{out\_features})(out_features). If `bias`is `True`, the values are initialized from \mathcal{U}(-\sqrt{k}, \sqrt{k})U(−k,k), where k = \frac{1}{\text{in1\_features}}k=in1_features1Examples:`>>> m = nn.Bilinear(20, 30, 40) >>> input1 = torch.randn(128, 20) >>> input2 = torch.randn(128, 30) >>> output = m(input1, input2) >>> print(output.size()) torch.Size([128, 40]) `

## Dropout layers

### Dropout

- *CLASS*`torch.nn.``Dropout`(*p=0.5*, *inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#Dropout)

  During training, randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580) .Furthermore, the outputs are scaled by a factor of \frac{1}{1-p}1−p1 during training. This means that during evaluation the module simply computes an identity function.Parameters**p** – probability of an element to be zeroed. Default: 0.5**inplace** – If set to `True`, will do this operation in-place. Default: `False`Shape:Input: (*)(∗). Input can be of any shapeOutput: (*)(∗). Output is of the same shape as inputExamples:`>>> m = nn.Dropout(p=0.2) >>> input = torch.randn(20, 16) >>> output = m(input) `

### Dropout2d

- *CLASS*`torch.nn.``Dropout2d`(*p=0.5*, *inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#Dropout2d)

  Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 2D tensor \text{input}[i, j]input[i,j]). Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution.Usually the input comes from `nn.Conv2d` modules.As described in the paper [Efficient Object Localization Using Convolutional Networks](http://arxiv.org/abs/1411.4280) , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.In this case, `nn.Dropout2d()` will help promote independence between feature maps and should be used instead.Parameters**p** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – probability of an element to be zero-ed.**inplace** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If set to `True`, will do this operation in-placeShape:Input: (N, C, H, W)(N,C,H,W)Output: (N, C, H, W)(N,C,H,W) (same shape as input)Examples:`>>> m = nn.Dropout2d(p=0.2) >>> input = torch.randn(20, 16, 32, 32) >>> output = m(input) `

### Dropout3d

- *CLASS*`torch.nn.``Dropout3d`(*p=0.5*, *inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#Dropout3d)

  Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 3D tensor \text{input}[i, j]input[i,j]). Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution.Usually the input comes from `nn.Conv3d` modules.As described in the paper [Efficient Object Localization Using Convolutional Networks](http://arxiv.org/abs/1411.4280) , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.In this case, `nn.Dropout3d()` will help promote independence between feature maps and should be used instead.Parameters**p** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – probability of an element to be zeroed.**inplace** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If set to `True`, will do this operation in-placeShape:Input: (N, C, D, H, W)(N,C,D,H,W)Output: (N, C, D, H, W)(N,C,D,H,W) (same shape as input)Examples:`>>> m = nn.Dropout3d(p=0.2) >>> input = torch.randn(20, 16, 4, 32, 32) >>> output = m(input) `

### AlphaDropout

- *CLASS*`torch.nn.``AlphaDropout`(*p=0.5*, *inplace=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html#AlphaDropout)

  Applies Alpha Dropout over the input.Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.During training, it randomly masks some of the elements of the input tensor with probability *p* using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.During evaluation the module simply computes an identity function.More details can be found in the paper [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515) .Parameters**p** ([*float*](https://docs.python.org/3/library/functions.html#float)) – probability of an element to be dropped. Default: 0.5**inplace** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If set to `True`, will do this operation in-placeShape:Input: (*)(∗). Input can be of any shapeOutput: (*)(∗). Output is of the same shape as inputExamples:`>>> m = nn.AlphaDropout(p=0.2) >>> input = torch.randn(20, 16) >>> output = m(input) `

## Sparse layers

### Embedding

- *CLASS*`torch.nn.``Embedding`(*num_embeddings*, *embedding_dim*, *padding_idx=None*, *max_norm=None*, *norm_type=2.0*, *scale_grad_by_freq=False*, *sparse=False*, *_weight=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding)

  A simple lookup table that stores embeddings of a fixed dictionary and size.This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.Parameters**num_embeddings** ([*int*](https://docs.python.org/3/library/functions.html#int)) – size of the dictionary of embeddings**embedding_dim** ([*int*](https://docs.python.org/3/library/functions.html#int)) – the size of each embedding vector**padding_idx** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – If given, pads the output with the embedding vector at `padding_idx` (initialized to zeros) whenever it encounters the index.**max_norm** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – If given, each embedding vector with norm larger than `max_norm` is renormalized to have norm `max_norm`.**norm_type** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – The p of the p-norm to compute for the `max_norm`option. Default `2`.**scale_grad_by_freq** (*boolean**,* *optional*) – If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default `False`.**sparse** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – If `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.Variables**~Embedding.weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from \mathcal{N}(0, 1)N(0,1)Shape:Input: (*)(∗), LongTensor of arbitrary shape containing the indices to extractOutput: (*, H)(∗,H), where * is the input shape and H=\text{embedding\_dim}H=embedding_dimNOTEKeep in mind that only a limited number of optimizers support sparse gradients: currently it’s `optim.SGD` (CUDA and CPU), `optim.SparseAdam` (CUDA and CPU) and `optim.Adagrad`(CPU)NOTEWith `padding_idx` set, the embedding vector at `padding_idx` is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from [`Embedding`](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) is always zero.Examples:`>>> # an Embedding module containing 10 tensors of size 3 >>> embedding = nn.Embedding(10, 3) >>> # a batch of 2 samples of 4 indices each >>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) >>> embedding(input) tensor([[[-0.0251, -1.6902,  0.7172],          [-0.6431,  0.0748,  0.6969],          [ 1.4970,  1.3448, -0.9685],          [-0.3677, -2.7265, -0.1685]],          [[ 1.4970,  1.3448, -0.9685],          [ 0.4362, -0.4004,  0.9400],          [-0.6431,  0.0748,  0.6969],          [ 0.9124, -2.3616,  1.1151]]])   >>> # example with padding_idx >>> embedding = nn.Embedding(10, 3, padding_idx=0) >>> input = torch.LongTensor([[0,2,0,5]]) >>> embedding(input) tensor([[[ 0.0000,  0.0000,  0.0000],          [ 0.1535, -2.0309,  0.9315],          [ 0.0000,  0.0000,  0.0000],          [-0.1655,  0.9897,  0.0635]]]) `*CLASSMETHOD* `from_pretrained`(*embeddings*, *freeze=True*, *padding_idx=None*, *max_norm=None*, *norm_type=2.0*, *scale_grad_by_freq=False*, *sparse=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding.from_pretrained)Creates Embedding instance from given 2-dimensional FloatTensor.Parameters**embeddings** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as `num_embeddings`, second as `embedding_dim`.**freeze** (*boolean**,* *optional*) – If `True`, the tensor does not get updated in the learning process. Equivalent to `embedding.weight.requires_grad = False`. Default: `True`**padding_idx** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – See module initialization documentation.**max_norm** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – See module initialization documentation.**norm_type** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – See module initialization documentation. Default `2`.**scale_grad_by_freq** (*boolean**,* *optional*) – See module initialization documentation. Default `False`.**sparse** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – See module initialization documentation.Examples:`>>> # FloatTensor containing pretrained weights >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) >>> embedding = nn.Embedding.from_pretrained(weight) >>> # Get embeddings for index 1 >>> input = torch.LongTensor([1]) >>> embedding(input) tensor([[ 4.0000,  5.1000,  6.3000]]) `

### EmbeddingBag

- *CLASS*`torch.nn.``EmbeddingBag`(*num_embeddings*, *embedding_dim*, *max_norm=None*, *norm_type=2.0*, *scale_grad_by_freq=False*, *mode='mean'*, *sparse=False*, *_weight=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#EmbeddingBag)

  Computes sums or means of ‘bags’ of embeddings, without instantiating the intermediate embeddings.For bags of constant length and no `per_sample_weights`, this classwith `mode="sum"` is equivalent to [`Embedding`](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) followed by `torch.sum(dim=0)`,with `mode="mean"` is equivalent to [`Embedding`](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) followed by `torch.mean(dim=0)`,with `mode="max"` is equivalent to [`Embedding`](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) followed by `torch.max(dim=0)`.However, [`EmbeddingBag`](https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag) is much more time and memory efficient than using a chain of these operations.EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by `mode`. If `per_sample_weights`` is passed, the only supported `mode` is `"sum"`, which computes a weighted sum according to `per_sample_weights`.Parameters**num_embeddings** ([*int*](https://docs.python.org/3/library/functions.html#int)) – size of the dictionary of embeddings**embedding_dim** ([*int*](https://docs.python.org/3/library/functions.html#int)) – the size of each embedding vector**max_norm** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – If given, each embedding vector with norm larger than `max_norm` is renormalized to have norm `max_norm`.**norm_type** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – The p of the p-norm to compute for the `max_norm`option. Default `2`.**scale_grad_by_freq** (*boolean**,* *optional*) – if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default `False`. Note: this option is not supported when `mode="max"`.**mode** (*string**,* *optional*) – `"sum"`, `"mean"` or `"max"`. Specifies the way to reduce the bag. `"sum"` computes the weighted sum, taking `per_sample_weights` into consideration. `"mean"` computes the average of the values in the bag, `"max"`computes the max value over each bag. Default: `"mean"`**sparse** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – if `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when `mode="max"`.Variables**~EmbeddingBag.weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from \mathcal{N}(0, 1)N(0,1).Inputs: `input` (LongTensor), `offsets` (LongTensor, optional), and`per_index_weights` (Tensor, optional)If `input` is 2D of shape (B, N),it will be treated as `B` bags (sequences) each of fixed length `N`, and this will return `B`values aggregated in a way depending on the `mode`. `offsets` is ignored and required to be `None` in this case.If `input` is 1D of shape (N),it will be treated as a concatenation of multiple bags (sequences). `offsets` is required to be a 1D tensor containing the starting index positions of each bag in `input`. Therefore, for `offsets` of shape (B), `input` will be viewed as having `B` bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.per_sample_weights (Tensor, optional): a tensor of float / double weights, or Noneto indicate all weights should be taken to be `1`. If specified, `per_sample_weights`must have exactly the same shape as input and is treated as having the same`offsets`, if those are not `None`. Only supported for `mode='sum'`.Output shape: (B, embedding_dim)Examples:`>>> # an Embedding module containing 10 tensors of size 3 >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum') >>> # a batch of 2 samples of 4 indices each >>> input = torch.LongTensor([1,2,4,5,4,3,2,9]) >>> offsets = torch.LongTensor([0,4]) >>> embedding_sum(input, offsets) tensor([[-0.8861, -5.4350, -0.0523],         [ 1.1306, -2.5798, -1.0044]]) `*CLASSMETHOD* `from_pretrained`(*embeddings*, *freeze=True*, *max_norm=None*, *norm_type=2.0*, *scale_grad_by_freq=False*, *mode='mean'*, *sparse=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#EmbeddingBag.from_pretrained)Creates EmbeddingBag instance from given 2-dimensional FloatTensor.Parameters**embeddings** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as ‘num_embeddings’, second as ‘embedding_dim’.**freeze** (*boolean**,* *optional*) – If `True`, the tensor does not get updated in the learning process. Equivalent to `embeddingbag.weight.requires_grad =False`. Default: `True`**max_norm** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – See module initialization documentation. Default: `None`**norm_type** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – See module initialization documentation. Default `2`.**scale_grad_by_freq** (*boolean**,* *optional*) – See module initialization documentation. Default `False`.**mode** (*string**,* *optional*) – See module initialization documentation. Default: `"mean"`**sparse** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – See module initialization documentation. Default: `False`.Examples:`>>> # FloatTensor containing pretrained weights >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) >>> embeddingbag = nn.EmbeddingBag.from_pretrained(weight) >>> # Get embeddings for index 1 >>> input = torch.LongTensor([[1, 0]]) >>> embeddingbag(input) tensor([[ 2.5000,  3.7000,  4.6500]]) `

## Distance functions

### CosineSimilarity

- *CLASS*`torch.nn.``CosineSimilarity`(*dim=1*, *eps=1e-08*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/distance.html#CosineSimilarity)

  Returns cosine similarity between x_1x1 and x_2x2, computed along dim.\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}.similarity=max(∥x1∥2⋅∥x2∥2,ϵ)x1⋅x2.Parameters**dim** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Dimension where cosine similarity is computed. Default: 1**eps** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – Small value to avoid division by zero. Default: 1e-8Shape:Input1: (\ast_1, D, \ast_2)(∗1,D,∗2) where D is at position dimInput2: (\ast_1, D, \ast_2)(∗1,D,∗2), same shape as the Input1Output: (\ast_1, \ast_2)(∗1,∗2)Examples::`>>> input1 = torch.randn(100, 128) >>> input2 = torch.randn(100, 128) >>> cos = nn.CosineSimilarity(dim=1, eps=1e-6) >>> output = cos(input1, input2) `

### PairwiseDistance

- *CLASS*`torch.nn.``PairwiseDistance`(*p=2.0*, *eps=1e-06*, *keepdim=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/distance.html#PairwiseDistance)

  Computes the batchwise pairwise distance between vectors v_1v1, v_2v2 using the p-norm:\Vert x \Vert _p = \left( \sum_{i=1}^n \vert x_i \vert ^ p \right) ^ {1/p}.∥x∥p=(i=1∑n∣xi∣p)1/p.Parameters**p** (*real*) – the norm degree. Default: 2**eps** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – Small value to avoid division by zero. Default: 1e-6**keepdim** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Determines whether or not to keep the vector dimension. Default: FalseShape:Input1: (N, D)(N,D) where D = vector dimensionInput2: (N, D)(N,D), same shape as the Input1Output: (N)(N). If `keepdim` is `True`, then (N, 1)(N,1).Examples::`>>> pdist = nn.PairwiseDistance(p=2) >>> input1 = torch.randn(100, 128) >>> input2 = torch.randn(100, 128) >>> output = pdist(input1, input2) `

## Loss functions

### L1Loss

- *CLASS*`torch.nn.``L1Loss`(*size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#L1Loss)

  Creates a criterion that measures the mean absolute error (MAE) between each element in the input xx and target yy.The unreduced (i.e. with `reduction` set to `'none'`) loss can be described as:\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right|,ℓ(x,y)=L={l1,…,lN}⊤,ln=∣xn−yn∣,where NN is the batch size. If `reduction` is not `'none'` (default `'mean'`), then:\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#x27;mean&#x27;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&#x27;sum&#x27;.} \end{cases}ℓ(x,y)={mean(L),sum(L),if reduction=’mean’;if reduction=’sum’.xx and yy are tensors of arbitrary shapes with a total of nn elements each.The sum operation still operates over all the elements, and divides by nn.The division by nn can be avoided if one sets `reduction = 'sum'`.Parameters**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (N, *)(N,∗) where *∗ means, any number of additional dimensionsTarget: (N, *)(N,∗), same shape as the inputOutput: scalar. If `reduction` is `'none'`, then (N, *)(N,∗), same shape as the inputExamples:`>>> loss = nn.L1Loss() >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.randn(3, 5) >>> output = loss(input, target) >>> output.backward() `

### MSELoss

- *CLASS*`torch.nn.``MSELoss`(*size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MSELoss)

  Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xx and target yy.The unreduced (i.e. with `reduction` set to `'none'`) loss can be described as:\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left( x_n - y_n \right)^2,ℓ(x,y)=L={l1,…,lN}⊤,ln=(xn−yn)2,where NN is the batch size. If `reduction` is not `'none'` (default `'mean'`), then:\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#x27;mean&#x27;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&#x27;sum&#x27;.} \end{cases}ℓ(x,y)={mean(L),sum(L),if reduction=’mean’;if reduction=’sum’.xx and yy are tensors of arbitrary shapes with a total of nn elements each.The sum operation still operates over all the elements, and divides by nn.The division by nn can be avoided if one sets `reduction = 'sum'`.Parameters**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (N, *)(N,∗) where *∗ means, any number of additional dimensionsTarget: (N, *)(N,∗), same shape as the inputExamples:`>>> loss = nn.MSELoss() >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.randn(3, 5) >>> output = loss(input, target) >>> output.backward() `

### CrossEntropyLoss

- *CLASS*`torch.nn.``CrossEntropyLoss`(*weight=None*, *size_average=None*, *ignore_index=-100*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss)

  This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.It is useful when training a classification problem with C classes. If provided, the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.The input is expected to contain raw, unnormalized scores for each class.input has to be a Tensor of size either (minibatch, C)(minibatch,C) or (minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1,d2,...,dK)with K \geq 1K≥1 for the K-dimensional case (described later).This criterion expects a class index in the range [0, C-1][0,C−1] as the target for each value of a 1D tensor of size minibatch; if ignore_index is specified, this criterion also accepts this class index (this index may not necessarily be in the class range).The loss can be described as:\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) = -x[class] + \log\left(\sum_j \exp(x[j])\right)loss(x,class)=−log(∑jexp(x[j])exp(x[class]))=−x[class]+log(j∑exp(x[j]))or in the case of the `weight` argument being specified:\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)loss(x,class)=weight[class](−x[class]+log(j∑exp(x[j])))The losses are averaged across observations for each minibatch.Can also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1,d2,...,dK) with K \geq 1K≥1, where KK is the number of dimensions, and a target of appropriate shape (see below).Parameters**weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*,* *optional*) – a manual rescaling weight given to each class. If given, has to be a Tensor of size C**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**ignore_index** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets.**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K)(N,C,d1,d2,...,dK) with K \geq 1K≥1 in the case of K-dimensional loss.Target: (N)(N) where each value is 0 \leq \text{targets}[i] \leq C-10≤targets[i]≤C−1, or (N, d_1, d_2, ..., d_K)(N,d1,d2,...,dK) with K \geq 1K≥1 in the case of K-dimensional loss.Output: scalar. If `reduction` is `'none'`, then the same size as the target: (N)(N), or (N, d_1, d_2, ..., d_K)(N,d1,d2,...,dK) with K \geq 1K≥1 in the case of K-dimensional loss.Examples:`>>> loss = nn.CrossEntropyLoss() >>> input = torch.randn(3, 5, requires_grad=True) >>> target = torch.empty(3, dtype=torch.long).random_(5) >>> output = loss(input, target) >>> output.backward() `

### CTCLoss

- *CLASS*`torch.nn.``CTCLoss`(*blank=0*, *reduction='mean'*, *zero_infinity=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CTCLoss)

  The Connectionist Temporal Classification loss.Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be “many-to-one”, which limits the length of the target sequence such that it must be \leq≤ the input length.**Args:****blank** (int, optional): blank label. Default 00. reduction (string, optional): Specifies the reduction to apply to the output:`'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: `'mean'`**zero_infinity** (bool, optional):Whether to zero infinite losses and the associated gradients. Default: `False`Infinite losses mainly occur when the inputs are too short to be aligned to the targets.**Inputs:****log_probs**: Tensor of size (T, N, C)(T,N,C)T = \text{input length}T=input lengthN = \text{batch size}N=batch sizeC = \text{number of classes (including blank)}C=number of classes (including blank)The logarithmized probabilities of the outputs (e.g. obtained with [`torch.nn.functional.log_softmax()`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.log_softmax)).**targets**: Tensor of size (N, S)(N,S) or \((\text{sum(target_lengths)})\)N = \text{batch size}N=batch sizeS = \text{max target length, if shape is } (N, S)S=max target length, if shape is (N,S).Target sequences. Each element in the target sequence is a class index. Target index cannot be blank (default=0).In the (N, S)(N,S) form, targets are padded to the length of the longest sequence, and stacked.In the \((\text{sum(target_lengths)})\) form, the targets are assumed to be un-padded and concatenated within 1 dimension.**input_lengths**: Tuple or tensor of size (N)(N).Lengths of the inputs (must each be \leq T≤T). Lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths.**target_lengths**: Tuple or tensor of size (N)(N).Lengths of the targets. Lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths.If target shape is (N,S)(N,S), target_lengths are effectively the stop index s_nsn for each target sequence, such that `target_n = targets[n,0:s_n]` for each target in a batch. Lengths must each be \leq S≤SIf the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor.Example:`>>> T = 50      # Input sequence length >>> C = 20      # Number of classes (excluding blank) >>> N = 16      # Batch size >>> S = 30      # Target sequence length of longest target in batch >>> S_min = 10  # Minimum target length, for demonstration purposes >>> >>> # Initialize random batch of input vectors, for *size = (T,N,C) >>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_() >>> >>> # Initialize random batch of targets (0 = blank, 1:C+1 = classes) >>> target = torch.randint(low=1, high=C+1, size=(N, S), dtype=torch.long) >>> >>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long) >>> target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long) >>> ctc_loss = nn.CTCLoss() >>> loss = ctc_loss(input, target, input_lengths, target_lengths) >>> loss.backward() `Reference:A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: <https://www.cs.toronto.edu/~graves/icml_2006.pdf>NOTEIn order to use CuDNN, the following must be satisfied: `targets` must be in concatenated format, all `input_lengths` must be T. blank=0blank=0, `target_lengths` \leq 256≤256, the integer arguments must be of dtype `torch.int32`.The regular implementation uses the (more common in PyTorch) torch.long dtype.NOTEIn some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting `torch.backends.cudnn.deterministic = True`. Please see the notes on [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) for background.

### NLLLoss

- *CLASS*`torch.nn.``NLLLoss`(*weight=None*, *size_average=None*, *ignore_index=-100*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#NLLLoss)

  The negative log likelihood loss. It is useful to train a classification problem with C classes.If provided, the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.The input given through a forward call is expected to contain log-probabilities of each class. inputhas to be a Tensor of size either (minibatch, C)(minibatch,C) or (minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1,d2,...,dK) with K \geq 1K≥1 for the K-dimensional case (described later).Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.The target that this loss expects should be a class index in the range [0, C-1][0,C−1] where C = number of classes; if ignore_index is specified, this loss also accepts this class index (this index may not necessarily be in the class range).The unreduced (i.e. with `reduction` set to `'none'`) loss can be described as:\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},ℓ(x,y)=L={l1,…,lN}⊤,ln=−wynxn,yn,wc=weight[c]⋅1{c̸=ignore_index},where NN is the batch size. If `reduction` is not `'none'` (default `'mean'`), then\ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp; \text{if reduction} = \text{&#x27;mean&#x27;;}\\ \sum_{n=1}^N l_n, &amp; \text{if reduction} = \text{&#x27;sum&#x27;.} \end{cases}ℓ(x,y)={∑n=1N∑n=1Nwyn1ln,∑n=1Nln,if reduction=’mean’;if reduction=’sum’.Can also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1,d2,...,dK) with K \geq 1K≥1, where KK is the number of dimensions, and a target of appropriate shape (see below). In the case of images, it computes NLL loss per-pixel.Parameters**weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*,* *optional*) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**ignore_index** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets.**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (N, C)(N,C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K)(N,C,d1,d2,...,dK) with K \geq 1K≥1 in the case of K-dimensional loss.Target: (N)(N) where each value is 0 \leq \text{targets}[i] \leq C-10≤targets[i]≤C−1, or (N, d_1, d_2, ..., d_K)(N,d1,d2,...,dK) with K \geq 1K≥1 in the case of K-dimensional loss.Output: scalar. If `reduction` is `'none'`, then the same size as the target: (N)(N), or (N, d_1, d_2, ..., d_K)(N,d1,d2,...,dK) with K \geq 1K≥1 in the case of K-dimensional loss.Examples:`>>> m = nn.LogSoftmax(dim=1) >>> loss = nn.NLLLoss() >>> # input is of size N x C = 3 x 5 >>> input = torch.randn(3, 5, requires_grad=True) >>> # each element in target has to have 0 <= value < C >>> target = torch.tensor([1, 0, 4]) >>> output = loss(m(input), target) >>> output.backward() >>> >>> >>> # 2D loss example (used, for example, with image inputs) >>> N, C = 5, 4 >>> loss = nn.NLLLoss() >>> # input is of size N x C x height x width >>> data = torch.randn(N, 16, 10, 10) >>> conv = nn.Conv2d(16, C, (3, 3)) >>> m = nn.LogSoftmax(dim=1) >>> # each element in target has to have 0 <= value < C >>> target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C) >>> output = loss(m(conv(data)), target) >>> output.backward() `

### PoissonNLLLoss

- *CLASS*`torch.nn.``PoissonNLLLoss`(*log_input=True*, *full=False*, *size_average=None*, *eps=1e-08*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#PoissonNLLLoss)

  Negative log likelihood loss with Poisson distribution of target.The loss can be described as:\text{target} \sim \mathrm{Poisson}(\text{input}) \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input}) + \log(\text{target!})target∼Poisson(input)loss(input,target)=input−target∗log(input)+log(target!)The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.Parameters**log_input** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – if `True` the loss is computed as \exp(\text{input}) - \text{target}*\text{input}exp(input)−target∗input, if `False` the loss is \text{input} - \text{target}*\log(\text{input}+\text{eps})input−target∗log(input+eps).**full** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) –whether to compute full loss, i. e. to add the Stirling approximation term\text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).target∗log(target)−target+0.5∗log(2πtarget).**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**eps** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – Small value to avoid evaluation of \log(0)log(0) when `log_input =False`. Default: 1e-8**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Examples:`>>> loss = nn.PoissonNLLLoss() >>> log_input = torch.randn(5, 2, requires_grad=True) >>> target = torch.randn(5, 2) >>> output = loss(log_input, target) >>> output.backward() `Shape:Input: (N, *)(N,∗) where *∗ means, any number of additional dimensionsTarget: (N, *)(N,∗), same shape as the inputOutput: scalar by default. If `reduction` is `'none'`, then (N, *)(N,∗), the same shape as the input

### KLDivLoss

- *CLASS*`torch.nn.``KLDivLoss`(*size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#KLDivLoss)

  The [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback-Leibler_divergence) LossKL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions.As with [`NLLLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss), the input given is expected to contain *log-probabilities* and is not restricted to a 2D Tensor. The targets are given as *probabilities* (i.e. without taking the logarithm).This criterion expects a target Tensor of the same size as the input Tensor.The unreduced (i.e. with `reduction` set to `'none'`) loss can be described as:l(x,y) = L = \{ l_1,\dots,l_N \}, \quad l_n = y_n \cdot \left( \log y_n - x_n \right)l(x,y)=L={l1,…,lN},ln=yn⋅(logyn−xn)where the index NN spans all dimensions of `input` and LL has the same shape as `input`. If `reduction` is not `'none'` (default `'mean'`), then:\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#x27;mean&#x27;;} \\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&#x27;sum&#x27;.} \end{cases}ℓ(x,y)={mean(L),sum(L),if reduction=’mean’;if reduction=’sum’.In default `reduction` mode `'mean'`, the losses are averaged for each minibatch over observations**as well as** over dimensions. `'batchmean'` mode gives the correct KL divergence where losses are averaged over batch dimension only. `'mean'` mode’s behavior will be changed to the same as`'batchmean'` in the next major release.Parameters**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed.`'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`NOTE`size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.NOTE`reduction` = `'mean'` doesn’t return the true kl divergence value, please use `reduction` = `'batchmean'` which aligns with KL math definition. In the next major release, `'mean'` will be changed to be the same as `'batchmean'`.Shape:Input: (N, *)(N,∗) where *∗ means, any number of additional dimensionsTarget: (N, *)(N,∗), same shape as the inputOutput: scalar by default. If :attr:`reduction` is `'none'`, then (N, *)(N,∗), the same shape as the input

### BCELoss

- *CLASS*`torch.nn.``BCELoss`(*weight=None*, *size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#BCELoss)

  Creates a criterion that measures the Binary Cross Entropy between the target and the output:The unreduced (i.e. with `reduction` set to `'none'`) loss can be described as:\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],ℓ(x,y)=L={l1,…,lN}⊤,ln=−wn[yn⋅logxn+(1−yn)⋅log(1−xn)],where NN is the batch size. If `reduction` is not `'none'` (default `'mean'`), then\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#x27;mean&#x27;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&#x27;sum&#x27;.} \end{cases}ℓ(x,y)={mean(L),sum(L),if reduction=’mean’;if reduction=’sum’.This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets yy should be numbers between 0 and 1.Parameters**weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*,* *optional*) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch.**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (N, *)(N,∗) where *∗ means, any number of additional dimensionsTarget: (N, *)(N,∗), same shape as the inputOutput: scalar. If `reduction` is `'none'`, then (N, *)(N,∗), same shape as input.Examples:`>>> m = nn.Sigmoid() >>> loss = nn.BCELoss() >>> input = torch.randn(3, requires_grad=True) >>> target = torch.empty(3).random_(2) >>> output = loss(m(input), target) >>> output.backward() `

### BCEWithLogitsLoss

- *CLASS*`torch.nn.``BCEWithLogitsLoss`(*weight=None*, *size_average=None*, *reduce=None*, *reduction='mean'*, *pos_weight=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#BCEWithLogitsLoss)

  This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.The unreduced (i.e. with `reduction` set to `'none'`) loss can be described as:\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n) + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],ℓ(x,y)=L={l1,…,lN}⊤,ln=−wn[yn⋅logσ(xn)+(1−yn)⋅log(1−σ(xn))],where NN is the batch size. If `reduction` is not `'none'` (default `'mean'`), then\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#x27;mean&#x27;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&#x27;sum&#x27;.} \end{cases}ℓ(x,y)={mean(L),sum(L),if reduction=’mean’;if reduction=’sum’.This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1.It’s possible to trade off recall and precision by adding weights to positive examples. In the case of multi-label classification the loss can be described as:\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c}) + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],ℓc(x,y)=Lc={l1,c,…,lN,c}⊤,ln,c=−wn,c[pcyn,c⋅logσ(xn,c)+(1−yn,c)⋅log(1−σ(xn,c))],where cc is the class number (c &gt; 1c>1 for multi-label binary classification, c = 1c=1 for single-label binary classification), nn is the number of the sample in the batch and p_cpc is the weight of the positive answer for the class cc.p_c &gt; 1pc>1 increases the recall, p_c &lt; 1pc<1 increases the precision.For example, if a dataset contains 100 positive and 300 negative examples of a single class, then pos_weight for the class should be equal to \frac{300}{100}=3100300=3. The loss would act as if the dataset contains 3\times 100=3003×100=300 positive examples.Examples:`>>> target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10 >>> output = torch.full([10, 64], 0.999)  # A prediction (logit) >>> pos_weight = torch.ones([64])  # All weights are equal to 1 >>> criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight) >>> criterion(output, target)  # -log(sigmoid(0.999)) tensor(0.3135) `Parameters**weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*,* *optional*) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch.**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`**pos_weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*,* *optional*) – a weight of positive examples. Must be a vector with length equal to the number of classes.Shape:Input: (N, *)(N,∗) where *∗ means, any number of additional dimensionsTarget: (N, *)(N,∗), same shape as the inputOutput: scalar. If `reduction` is `'none'`, then (N, *)(N,∗), same shape as input.Examples:`>>> loss = nn.BCEWithLogitsLoss() >>> input = torch.randn(3, requires_grad=True) >>> target = torch.empty(3).random_(2) >>> output = loss(input, target) >>> output.backward() `

### MarginRankingLoss

- *CLASS*`torch.nn.``MarginRankingLoss`(*margin=0.0*, *size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MarginRankingLoss)

  Creates a criterion that measures the loss given inputs x1x1, x2x2, two 1D mini-batch Tensors, and a label 1D mini-batch tensor yy (containing 1 or -1).If y = 1y=1 then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y = -1y=−1.The loss function for each sample in the mini-batch is:\text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})loss(x,y)=max(0,−y∗(x1−x2)+margin)Parameters**margin** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – Has a default value of 00.**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (N, D)(N,D) where N is the batch size and D is the size of a sample.Target: (N)(N)Output: scalar. If `reduction` is `'none'`, then (N)(N).

### HingeEmbeddingLoss

- *CLASS*`torch.nn.``HingeEmbeddingLoss`(*margin=1.0*, *size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#HingeEmbeddingLoss)

  Measures the loss given an input tensor xx and a labels tensor yy (containing 1 or -1). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as xx, and is typically used for learning nonlinear embeddings or semi-supervised learning.The loss function for nn-th sample in the mini-batch isl_n = \begin{cases} x_n, &amp; \text{if}\; y_n = 1,\\ \max \{0, \Delta - x_n\}, &amp; \text{if}\; y_n = -1, \end{cases}ln={xn,max{0,Δ−xn},ifyn=1,ifyn=−1,and the total loss functions is\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#x27;mean&#x27;;}\\ \operatorname{sum}(L), &amp; \text{if reduction} = \text{&#x27;sum&#x27;.} \end{cases}ℓ(x,y)={mean(L),sum(L),if reduction=’mean’;if reduction=’sum’.where L = \{l_1,\dots,l_N\}^\topL={l1,…,lN}⊤.Parameters**margin** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – Has a default value of 1.**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (*)(∗) where *∗ means, any number of dimensions. The sum operation operates over all the elements.Target: (*)(∗), same shape as the inputOutput: scalar. If `reduction` is `'none'`, then same shape as the input

### MultiLabelMarginLoss

- *CLASS*`torch.nn.``MultiLabelMarginLoss`(*size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MultiLabelMarginLoss)

  Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xx (a 2D mini-batch Tensor) and output yy (which is a 2D Tensor of target class indices). For each sample in the mini-batch:\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}loss(x,y)=ij∑x.size(0)max(0,1−(x[y[j]]−x[i]))where x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}x∈{0,⋯,x.size(0)−1}, y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}y∈{0,⋯,y.size(0)−1}, 0 \leq y[j] \leq \text{x.size}(0)-10≤y[j]≤x.size(0)−1, and i \neq y[j]i̸=y[j] for all ii and jj.yy and xx must have the same size.The criterion only considers a contiguous block of non-negative targets that starts at the front.This allows for different samples to have variable amounts of target classes.Parameters**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (C)(C) or (N, C)(N,C) where N is the batch size and C is the number of classes.Target: (C)(C) or (N, C)(N,C), label targets padded by -1 ensuring same shape as the input.Output: scalar. If `reduction` is `'none'`, then (N)(N).Examples:`>>> loss = nn.MultiLabelMarginLoss() >>> x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]]) >>> # for target y, only consider labels 3 and 0, not after label -1 >>> y = torch.LongTensor([[3, 0, -1, 1]]) >>> loss(x, y) >>> # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4))) tensor(0.8500) `

### SmoothL1Loss

- *CLASS*`torch.nn.``SmoothL1Loss`(*size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#SmoothL1Loss)

  Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. It is less sensitive to outliers than the MSELoss and in some cases prevents exploding gradients (e.g. see Fast R-CNN paper by Ross Girshick). Also known as the Huber loss:\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}loss(x,y)=n1i∑ziwhere z_{i}zi is given by:z_{i} = \begin{cases} 0.5 (x_i - y_i)^2, &amp; \text{if } |x_i - y_i| &lt; 1 \\ |x_i - y_i| - 0.5, &amp; \text{otherwise } \end{cases}zi={0.5(xi−yi)2,∣xi−yi∣−0.5,if ∣xi−yi∣<1otherwise xx and yy arbitrary shapes with a total of nn elements each the sum operation still operates over all the elements, and divides by nn.The division by nn can be avoided if sets `reduction = 'sum'`.Parameters**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (N, *)(N,∗) where *∗ means, any number of additional dimensionsTarget: (N, *)(N,∗), same shape as the inputOutput: scalar. If `reduction` is `'none'`, then (N, *)(N,∗), same shape as the input

### SoftMarginLoss

- *CLASS*`torch.nn.``SoftMarginLoss`(*size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#SoftMarginLoss)

  Creates a criterion that optimizes a two-class classification logistic loss between input tensor xx and target tensor yy (containing 1 or -1).\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}loss(x,y)=i∑x.nelement()log(1+exp(−y[i]∗x[i]))Parameters**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (*)(∗) where *∗ means, any number of additional dimensionsTarget: (*)(∗), same shape as the inputOutput: scalar. If `reduction` is `'none'`, then same shape as the input

### MultiLabelSoftMarginLoss

- *CLASS*`torch.nn.``MultiLabelSoftMarginLoss`(*weight=None*, *size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MultiLabelSoftMarginLoss)

  Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xx and target yy of size (N, C)(N,C). For each sample in the minibatch:loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1}) + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)loss(x,y)=−C1∗i∑y[i]∗log((1+exp(−x[i]))−1)+(1−y[i])∗log((1+exp(−x[i]))exp(−x[i]))where i \in \left\{0, \; \cdots , \; \text{x.nElement}() - 1\right\}i∈{0,⋯,x.nElement()−1}, y[i] \in \left\{0, \; 1\right\}y[i]∈{0,1}.Parameters**weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*,* *optional*) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (N, C)(N,C) where N is the batch size and C is the number of classes.Target: (N, C)(N,C), label targets padded by -1 ensuring same shape as the input.Output: scalar. If `reduction` is `'none'`, then (N)(N).

### CosineEmbeddingLoss

- *CLASS*`torch.nn.``CosineEmbeddingLoss`(*margin=0.0*, *size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CosineEmbeddingLoss)

  Creates a criterion that measures the loss given input tensors x_1x1, x_2x2 and a Tensor label yy with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.The loss function for each sample is:\text{loss}(x, y) = \begin{cases} 1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1 \end{cases}loss(x,y)={1−cos(x1,x2),max(0,cos(x1,x2)−margin),if y=1if y=−1Parameters**margin** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – Should be a number from -1−1 to 11, 00 to 0.50.5 is suggested. If `margin` is missing, the default value is 00.**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`

### MultiMarginLoss

- *CLASS*`torch.nn.``MultiMarginLoss`(*p=1*, *margin=1.0*, *weight=None*, *size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#MultiMarginLoss)

  Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xx (a 2D mini-batch Tensor) and output yy (which is a 1D tensor of target class indices, 0 \leq y \leq \text{x.size}(1)-10≤y≤x.size(1)−1):For each mini-batch sample, the loss in terms of the 1D input xx and scalar output yy is:\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}loss(x,y)=x.size(0)∑imax(0,margin−x[y]+x[i]))pwhere x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}x∈{0,⋯,x.size(0)−1} and i \neq yi̸=y.Optionally, you can give non-equal weighting on the classes by passing a 1D `weight` tensor into the constructor.The loss function then becomes:\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}loss(x,y)=x.size(0)∑imax(0,w[y]∗(margin−x[y]+x[i]))p)Parameters**p** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – Has a default value of 11. 11 and 22 are the only supported values.**margin** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – Has a default value of 11.**weight** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*,* *optional*) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`

### TripletMarginLoss

- *CLASS*`torch.nn.``TripletMarginLoss`(*margin=1.0*, *p=2.0*, *eps=1e-06*, *swap=False*, *size_average=None*, *reduce=None*, *reduction='mean'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#TripletMarginLoss)

  Creates a criterion that measures the triplet loss given an input tensors x1x1, x2x2, x3x3 and a margin with a value greater than 00. This is used for measuring a relative similarity between samples. A triplet is composed by a, p and n (i.e., anchor, positive examples and negative examples respectively). The shapes of all input tensors should be (N, D)(N,D).The distance swap is described in detail in the paper [Learning shallow convolutional feature descriptors with triplet losses](http://www.bmva.org/bmvc/2016/papers/paper119/index.html) by V. Balntas, E. Riba et al.The loss function for each sample in the mini-batch is:L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}L(a,p,n)=max{d(ai,pi)−d(ai,ni)+margin,0}whered(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_pd(xi,yi)=∥xi−yi∥pParameters**margin** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – Default: 11.**p** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – The norm degree for pairwise distance. Default: 22.**swap** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. Default: `False`.**size_average** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`**reduce** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`**reduction** (*string**,* *optional*) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`Shape:Input: (N, D)(N,D) where DD is the vector dimension.Output: scalar. If `reduction` is `'none'`, then (N)(N).`>>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) >>> input1 = torch.randn(100, 128, requires_grad=True) >>> input2 = torch.randn(100, 128, requires_grad=True) >>> input3 = torch.randn(100, 128, requires_grad=True) >>> output = triplet_loss(input1, input2, input3) >>> output.backward() `

## Vision layers

### PixelShuffle

- *CLASS*`torch.nn.``PixelShuffle`(*upscale_factor*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/pixelshuffle.html#PixelShuffle)

  Rearranges elements in a tensor of shape (*, C \times r^2, H, W)(∗,C×r2,H,W) to a tensor of shape (*, C, H \times r, W \times r)(∗,C,H×r,W×r).This is useful for implementing efficient sub-pixel convolution with a stride of 1/r1/r.Look at the paper: [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158) by Shi et. al (2016) for more details.Parameters**upscale_factor** ([*int*](https://docs.python.org/3/library/functions.html#int)) – factor to increase spatial resolution byShape:Input: (N, L, H_{in}, W_{in})(N,L,Hin,Win) where L=C \times \text{upscale\_factor}^2L=C×upscale_factor2Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout) where H_{out} = H_{in} \times \text{upscale\_factor}Hout=Hin×upscale_factor and W_{out} = W_{in} \times \text{upscale\_factor}Wout=Win×upscale_factorExamples:`>>> pixel_shuffle = nn.PixelShuffle(3) >>> input = torch.randn(1, 9, 4, 4) >>> output = pixel_shuffle(input) >>> print(output.size()) torch.Size([1, 1, 12, 12]) `

### Upsample

- *CLASS*`torch.nn.``Upsample`(*size=None*, *scale_factor=None*, *mode='nearest'*, *align_corners=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html#Upsample)

  Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.The input data is assumed to be of the form minibatch x channels x [optional depth] x [optional height] x width. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.One can either give a `scale_factor` or the target output `size` to calculate the output size. (You cannot give both, as it is ambiguous)Parameters**size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* *Tuple**[*[*int*](https://docs.python.org/3/library/functions.html#int)*] or* *Tuple**[*[*int*](https://docs.python.org/3/library/functions.html#int)*,* [*int*](https://docs.python.org/3/library/functions.html#int)*] or* *Tuple**[*[*int*](https://docs.python.org/3/library/functions.html#int)*,* [*int*](https://docs.python.org/3/library/functions.html#int)*,* [*int*](https://docs.python.org/3/library/functions.html#int)*]**,* *optional*) – output spatial sizes**scale_factor** ([*float*](https://docs.python.org/3/library/functions.html#float) *or* *Tuple**[*[*float*](https://docs.python.org/3/library/functions.html#float)*] or* *Tuple**[*[*float*](https://docs.python.org/3/library/functions.html#float)*,* [*float*](https://docs.python.org/3/library/functions.html#float)*] or* *Tuple**[*[*float*](https://docs.python.org/3/library/functions.html#float)*,* [*float*](https://docs.python.org/3/library/functions.html#float)*,* [*float*](https://docs.python.org/3/library/functions.html#float)*]**,**optional*) – multiplier for spatial size. Has to match input size if it is a tuple.**mode** ([*str*](https://docs.python.org/3/library/stdtypes.html#str)*,* *optional*) – the upsampling algorithm: one of `'nearest'`, `'linear'`, `'bilinear'`, `'bicubic'` and `'trilinear'`. Default: `'nearest'`**align_corners** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – if `True`, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when `mode` is `'linear'`, `'bilinear'`, or `'trilinear'`. Default: `False`Shape:Input: (N, C, W_{in})(N,C,Win), (N, C, H_{in}, W_{in})(N,C,Hin,Win) or (N, C, D_{in}, H_{in}, W_{in})(N,C,Din,Hin,Win)Output: (N, C, W_{out})(N,C,Wout), (N, C, H_{out}, W_{out})(N,C,Hout,Wout) or (N, C, D_{out}, H_{out}, W_{out})(N,C,Dout,Hout,Wout), whereD_{out} = \left\lfloor D_{in} \times \text{scale\_factor} \right\rfloorDout=⌊Din×scale_factor⌋H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloorHout=⌊Hin×scale_factor⌋W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloorWout=⌊Win×scale_factor⌋WARNINGWith `align_corners = True`, the linearly interpolating modes (linear, bilinear, bicubic, and trilinear) don’t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is `align_corners = False`. See below for concrete examples on how this affects the outputs.NOTEIf you want downsampling/general resizing, you should use `interpolate()`.Examples:`>>> input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) >>> input tensor([[[[ 1.,  2.],           [ 3.,  4.]]]])  >>> m = nn.Upsample(scale_factor=2, mode='nearest') >>> m(input) tensor([[[[ 1.,  1.,  2.,  2.],           [ 1.,  1.,  2.,  2.],           [ 3.,  3.,  4.,  4.],           [ 3.,  3.,  4.,  4.]]]])  >>> m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False >>> m(input) tensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],           [ 1.5000,  1.7500,  2.2500,  2.5000],           [ 2.5000,  2.7500,  3.2500,  3.5000],           [ 3.0000,  3.2500,  3.7500,  4.0000]]]])  >>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) >>> m(input) tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],           [ 1.6667,  2.0000,  2.3333,  2.6667],           [ 2.3333,  2.6667,  3.0000,  3.3333],           [ 3.0000,  3.3333,  3.6667,  4.0000]]]])  >>> # Try scaling the same data in a larger tensor >>> >>> input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3) >>> input_3x3[:, :, :2, :2].copy_(input) tensor([[[[ 1.,  2.],           [ 3.,  4.]]]]) >>> input_3x3 tensor([[[[ 1.,  2.,  0.],           [ 3.,  4.,  0.],           [ 0.,  0.,  0.]]]])  >>> m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False >>> # Notice that values in top left corner are the same with the small input (except at boundary) >>> m(input_3x3) tensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],           [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],           [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],           [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],           [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])  >>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) >>> # Notice that values in top left corner are now changed >>> m(input_3x3) tensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],           [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],           [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],           [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],           [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]]) `

### UpsamplingNearest2d

- *CLASS*`torch.nn.``UpsamplingNearest2d`(*size=None*, *scale_factor=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d)

  Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.To specify the scale, it takes either the `size` or the `scale_factor` as it’s constructor argument.When `size` is given, it is the output size of the image (h, w).Parameters**size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* *Tuple**[*[*int*](https://docs.python.org/3/library/functions.html#int)*,* [*int*](https://docs.python.org/3/library/functions.html#int)*]**,* *optional*) – output spatial sizes**scale_factor** ([*float*](https://docs.python.org/3/library/functions.html#float) *or* *Tuple**[*[*float*](https://docs.python.org/3/library/functions.html#float)*,* [*float*](https://docs.python.org/3/library/functions.html#float)*]**,* *optional*) – multiplier for spatial size.WARNINGThis class is deprecated in favor of `interpolate()`.Shape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout) whereH_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloorHout=⌊Hin×scale_factor⌋W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloorWout=⌊Win×scale_factor⌋Examples:`>>> input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) >>> input tensor([[[[ 1.,  2.],           [ 3.,  4.]]]])  >>> m = nn.UpsamplingNearest2d(scale_factor=2) >>> m(input) tensor([[[[ 1.,  1.,  2.,  2.],           [ 1.,  1.,  2.,  2.],           [ 3.,  3.,  4.,  4.],           [ 3.,  3.,  4.,  4.]]]]) `

### UpsamplingBilinear2d

- *CLASS*`torch.nn.``UpsamplingBilinear2d`(*size=None*, *scale_factor=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d)

  Applies a 2D bilinear upsampling to an input signal composed of several input channels.To specify the scale, it takes either the `size` or the `scale_factor` as it’s constructor argument.When `size` is given, it is the output size of the image (h, w).Parameters**size** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* *Tuple**[*[*int*](https://docs.python.org/3/library/functions.html#int)*,* [*int*](https://docs.python.org/3/library/functions.html#int)*]**,* *optional*) – output spatial sizes**scale_factor** ([*float*](https://docs.python.org/3/library/functions.html#float) *or* *Tuple**[*[*float*](https://docs.python.org/3/library/functions.html#float)*,* [*float*](https://docs.python.org/3/library/functions.html#float)*]**,* *optional*) – multiplier for spatial size.WARNINGThis class is deprecated in favor of `interpolate()`. It is equivalent to `nn.functional.interpolate(..., mode='bilinear', align_corners=True)`.Shape:Input: (N, C, H_{in}, W_{in})(N,C,Hin,Win)Output: (N, C, H_{out}, W_{out})(N,C,Hout,Wout) whereH_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloorHout=⌊Hin×scale_factor⌋W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloorWout=⌊Win×scale_factor⌋Examples:`>>> input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) >>> input tensor([[[[ 1.,  2.],           [ 3.,  4.]]]])  >>> m = nn.UpsamplingBilinear2d(scale_factor=2) >>> m(input) tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],           [ 1.6667,  2.0000,  2.3333,  2.6667],           [ 2.3333,  2.6667,  3.0000,  3.3333],           [ 3.0000,  3.3333,  3.6667,  4.0000]]]]) `

## DataParallel layers (multi-GPU, distributed)

### DataParallel

- *CLASS*`torch.nn.``DataParallel`(*module*, *device_ids=None*, *output_device=None*, *dim=0*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/parallel/data_parallel.html#DataParallel)

  Implements data parallelism at the module level.This container parallelizes the application of the given `module` by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.The batch size should be larger than the number of GPUs used.See also: [Use nn.DataParallel instead of multiprocessing](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-dataparallel-instead)Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be **scattered** on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model’s forward pass.The parallelized `module` must have its parameters and buffers on `device_ids[0]` before running this [`DataParallel`](https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel) module.WARNINGIn each forward, `module` is **replicated** on each device, so any updates to the running module in `forward` will be lost. For example, if `module` has a counter attribute that is incremented in each `forward`, it will always stay at the initial value because the update is done on the replicas which are destroyed after `forward`. However, [`DataParallel`](https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel) guarantees that the replica on`device[0]` will have its parameters and buffers sharing storage with the base parallelized `module`. So **in-place** updates to the parameters or buffers on `device[0]` will be recorded. E.g., [`BatchNorm2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d) and [`spectral_norm()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.spectral_norm) rely on this behavior to update the buffers.WARNINGForward and backward hooks defined on `module` and its submodules will be invoked `len(device_ids)` times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via[`register_forward_pre_hook()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_forward_pre_hook) be executed before all `len(device_ids)` [`forward()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward) calls, but that each such hook be executed before the corresponding [`forward()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward) call of that device.WARNINGWhen `module` returns a scalar (i.e., 0-dimensional tensor) in `forward()`, this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.NOTEThere is a subtlety in using the `pack sequence -> recurrent network -> unpack sequence`pattern in a [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) wrapped in [`DataParallel`](https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel). See [My recurrent network doesn’t work with data parallelism](https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism) section in FAQ for details.Parameters**module** ([*Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – module to be parallelized**device_ids** (*list of python:int* *or* [*torch.device*](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device)) – CUDA devices (default: all devices)**output_device** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*torch.device*](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device)) – device location of output (default: device_ids[0])Variables**~DataParallel.module** ([*Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – the module to be parallelizedExample:`>>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2]) >>> output = net(input_var)  # input_var can be on any device, including CPU `

### DistributedDataParallel

- *CLASS*`torch.nn.parallel.``DistributedDataParallel`(*module*, *device_ids=None*, *output_device=None*, *dim=0*, *broadcast_buffers=True*, *process_group=None*, *bucket_cap_mb=25*, *find_unused_parameters=False*, *check_reduction=False*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel)

  Implements distributed data parallelism that is based on `torch.distributed` package at the module level.This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.The batch size should be larger than the number of GPUs used locally.See also: [Basics](https://pytorch.org/docs/stable/distributed.html#distributed-basics) and [Use nn.DataParallel instead of multiprocessing](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-dataparallel-instead). The same constraints on input as in [`torch.nn.DataParallel`](https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel) apply.Creation of this class requires that `torch.distributed` to be already initialized, by calling [`torch.distributed.init_process_group()`](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group).`DistributedDataParallel` can be used in the following two ways:Single-Process Multi-GPUIn this case, a single process will be spawned on each host/node and each process will operate on all the GPUs of the node where it’s running. To use `DistributedDataParallel` in this way, you can simply construct the model as the following:`>>> torch.distributed.init_process_group(backend="nccl") >>> model = DistributedDataParallel(model) # device_ids will include all GPU devices by default `Multi-Process Single-GPUThis is the highly recommended way to use `DistributedDataParallel`, with multiple processes, each of which operates on a single GPU. This is currently the fastest approach to do data parallel training using PyTorch and applies to both single-node(multi-GPU) and multi-node data parallel training. It is proven to be significantly faster than [`torch.nn.DataParallel`](https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel) for single-node multi-GPU data parallel training.Here is how to use it: on each host with N GPUs, you should spawn up N processes, while ensuring that each process individually works on a single GPU from 0 to N-1. Therefore, it is your job to ensure that your training script operates on a single given GPU by calling:`>>> torch.cuda.set_device(i) `where i is from 0 to N-1. In each process, you should refer the following to construct this module:`>>> torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') >>> model = DistributedDataParallel(model, device_ids=[i], output_device=i) `In order to spawn up multiple processes per node, you can use either `torch.distributed.launch`or `torch.multiprocessing.spawn`NOTE`nccl` backend is currently the fastest and highly recommended backend to be used with Multi-Process Single-GPU distributed training and this applies to both single-node and multi-node distributed trainingNOTEThis module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of fp16 and fp32, the gradient reduction on these mixed types of parameters will just work fine. Also note that `nccl`backend is currently the fastest and highly recommended backend for fp16/fp32 mixed-precision training.NOTEIf you use `torch.save` on one process to checkpoint the module, and `torch.load` on some other processes to recover it, make sure that `map_location` is configured properly for every process. Without `map_location`, `torch.load` would recover the module to devices where the module was saved from.WARNINGThis module works only with the `gloo` and `nccl` backends.WARNINGConstructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different processes might be executing different code.WARNINGThis module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.WARNINGThis module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient all-reduction following the reverse order of the registered parameters of the model. In other words, it is users’ responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.WARNINGThis module assumes all buffers and gradients are dense.WARNINGThis module doesn’t work with [`torch.autograd.grad()`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) (i.e. it will only work if gradients are to be accumulated in `.grad` attributes of parameters).WARNINGIf you plan on using this module with a `nccl` backend or a `gloo` backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to `forkserver` (python 3 only) or `spawn`. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don’t change this setting.WARNINGForward and backward hooks defined on `module` and its submodules won’t be invoked anymore, unless the hooks are initialized in the `forward()` method.WARNINGYou should never try to change your model’s parameters after wrapping up your model with DistributedDataParallel. In other words, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model’s parameters after the DistributedDataParallel construction, this is not supported and unexpected behaviors can happen, since some parameters’ gradient reduction functions might not get called.NOTEParameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.Parameters**module** ([*Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – module to be parallelized**device_ids** (*list of python:int* *or* [*torch.device*](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device)) – CUDA devices. This should only be provided when the input module resides on a single CUDA device. For single-device modules, the `i``th :attr:`module` replica is placed on ``device_ids[i]`. For multi-device modules and CPU modules, device_ids must be None or an empty list, and input data for the forward pass must be placed on the correct device. (default: all devices for single-device modules)**output_device** ([*int*](https://docs.python.org/3/library/functions.html#int) *or* [*torch.device*](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device)) – device location of output for single-device CUDA modules. For multi-device modules and CPU modules, it must be None, and the module itself dictates the output location. (default: device_ids[0] for single-device modules)**broadcast_buffers** ([*bool*](https://docs.python.org/3/library/functions.html#bool)) – flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function. (default: `True`)**process_group** – the process group to be used for distributed data all-reduction. If `None`, the default process group, which is created by ``torch.distributed.init_process_group``, will be used. (default: `None`)**bucket_cap_mb** – DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. `bucket_cap_mb` controls the bucket size in MegaBytes (MB) (default: 25)**find_unused_parameters** ([*bool*](https://docs.python.org/3/library/functions.html#bool)) – Traverse the autograd graph of all tensors contained in the return value of the wrapped module’s `forward` function. Parameters that don’t receive gradients as part of this graph are preemptively marked as being ready to be reduced. (default: `False`)**check_reduction** – when setting to `True`, it enables DistributedDataParallel to automatically check if the previous iteration’s backward reductions were successfully issued at the beginning of every iteration’s forward function. You normally don’t need this option enabled unless you are observing weird behaviors such as different ranks are getting different gradients, which should not happen if DistributedDataParallel is correctly used. (default: `False`)Variables**~DistributedDataParallel.module** ([*Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – the module to be parallelizedExample:`>>> torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') >>> net = torch.nn.DistributedDataParallel(model, pg) `

### DistributedDataParallelCPU

- *CLASS*`torch.nn.parallel.``DistributedDataParallelCPU`(*module*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed_cpu.html#DistributedDataParallelCPU)

  Implements distributed data parallelism for CPU at the module level.This module supports the `mpi` and `gloo` backends.This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.This module could be used in conjunction with the DistributedSampler, (see [`DistributedSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler)) which will load a subset of the original dataset for each node with the same batch size. So strong scaling should be configured like this:n = 1, batch size = 12n = 2, batch size = 64n = 4, batch size = 32n = 8, batch size = 16Creation of this class requires the distributed package to be already initialized in the process group mode (see [`torch.distributed.init_process_group()`](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)).WARNINGConstructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different node might be executing different code.WARNINGThis module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later.WARNINGThis module assumes all gradients are dense.WARNINGThis module doesn’t work with [`torch.autograd.grad()`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) (i.e. it will only work if gradients are to be accumulated in `.grad` attributes of parameters).WARNINGForward and backward hooks defined on `module` and its submodules won’t be invoked anymore, unless the hooks are initialized in the `forward()` method.NOTEParameters are broadcast between nodes in the __init__() function. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all nodes in the same way.Parameters**module** – module to be parallelizedExample:`>>> torch.distributed.init_process_group(world_size=4, init_method='...') >>> net = torch.nn.DistributedDataParallelCPU(model) `

## Utilities

### clip_grad_norm_

- `torch.nn.utils.``clip_grad_norm_`(*parameters*, *max_norm*, *norm_type=2*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_)

  Clips gradient norm of an iterable of parameters.The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.Parameters**parameters** (*Iterable**[*[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*] or* [*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – an iterable of Tensors or a single Tensor that will have gradients normalized**max_norm** ([*float*](https://docs.python.org/3/library/functions.html#float) *or* [*int*](https://docs.python.org/3/library/functions.html#int)) – max norm of the gradients**norm_type** ([*float*](https://docs.python.org/3/library/functions.html#float) *or* [*int*](https://docs.python.org/3/library/functions.html#int)) – type of the used p-norm. Can be `'inf'` for infinity norm.ReturnsTotal norm of the parameters (viewed as a single vector).

### clip_grad_value_

- `torch.nn.utils.``clip_grad_value_`(*parameters*, *clip_value*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html#clip_grad_value_)

  Clips gradient of an iterable of parameters at specified value.Gradients are modified in-place.Parameters**parameters** (*Iterable**[*[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*] or* [*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – an iterable of Tensors or a single Tensor that will have gradients normalized**clip_value** ([*float*](https://docs.python.org/3/library/functions.html#float) *or* [*int*](https://docs.python.org/3/library/functions.html#int)) – maximum allowed value of the gradients. The gradients are clipped in the range \left[\text{-clip\_value}, \text{clip\_value}\right][-clip_value,clip_value]

### parameters_to_vector

- `torch.nn.utils.``parameters_to_vector`(*parameters*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/convert_parameters.html#parameters_to_vector)

  Convert parameters to one vectorParameters**parameters** (*Iterable**[*[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*]*) – an iterator of Tensors that are the parameters of a model.ReturnsThe parameters represented by a single vector

### vector_to_parameters

- `torch.nn.utils.``vector_to_parameters`(*vec*, *parameters*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/convert_parameters.html#vector_to_parameters)

  Convert one vector to the parametersParameters**vec** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – a single vector represents the parameters of a model.**parameters** (*Iterable**[*[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*]*) – an iterator of Tensors that are the parameters of a model.

### weight_norm

- `torch.nn.utils.``weight_norm`(*module*, *name='weight'*, *dim=0*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html#weight_norm)

  Applies weight normalization to a parameter in the given module.\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}w=g∥v∥vWeight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by `name` (e.g. `'weight'`) with two parameters: one specifying the magnitude (e.g. `'weight_g'`) and one specifying the direction (e.g. `'weight_v'`). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every `forward()` call.By default, with `dim=0`, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use `dim=None`.See <https://arxiv.org/abs/1602.07868>Parameters**module** ([*Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – containing module**name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str)*,* *optional*) – name of weight parameter**dim** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – dimension over which to compute the normReturnsThe original module with the weight norm hookExample:`>>> m = weight_norm(nn.Linear(20, 40), name='weight') >>> m Linear(in_features=20, out_features=40, bias=True) >>> m.weight_g.size() torch.Size([40, 1]) >>> m.weight_v.size() torch.Size([40, 20]) `

### remove_weight_norm

- `torch.nn.utils.``remove_weight_norm`(*module*, *name='weight'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html#remove_weight_norm)

  Removes the weight normalization reparameterization from a module.Parameters**module** ([*Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – containing module**name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str)*,* *optional*) – name of weight parameterExample`>>> m = weight_norm(nn.Linear(20, 40)) >>> remove_weight_norm(m) `

### spectral_norm

- `torch.nn.utils.``spectral_norm`(*module*, *name='weight'*, *n_power_iterations=1*, *eps=1e-12*, *dim=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/spectral_norm.html#spectral_norm)

  Applies spectral normalization to a parameter in the given module.\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})}, \sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}WSN=σ(W)W,σ(W)=h:h̸=0max∥h∥2∥Wh∥2Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm \sigmaσ of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every `forward()` call.See [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957) .Parameters**module** ([*nn.Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – containing module**name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str)*,* *optional*) – name of weight parameter**n_power_iterations** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – number of power iterations to calculate spectral norm**eps** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – epsilon for numerical stability in calculating norms**dim** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`ReturnsThe original module with the spectral norm hookExample:`>>> m = spectral_norm(nn.Linear(20, 40)) >>> m Linear(in_features=20, out_features=40, bias=True) >>> m.weight_u.size() torch.Size([40]) `

### remove_spectral_norm

- `torch.nn.utils.``remove_spectral_norm`(*module*, *name='weight'*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/spectral_norm.html#remove_spectral_norm)

  Removes the spectral normalization reparameterization from a module.Parameters**module** ([*Module*](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)) – containing module**name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str)*,* *optional*) – name of weight parameterExample`>>> m = spectral_norm(nn.Linear(40, 10)) >>> remove_spectral_norm(m) `

### PackedSequence

- `torch.nn.utils.rnn.``PackedSequence`(*data*, *batch_sizes=None*, *sorted_indices=None*, *unsorted_indices=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#PackedSequence)

  Holds the data and list of `batch_sizes` of a packed sequence.All RNN modules accept packed sequences as inputs.NOTEInstances of this class should never be created manually. They are meant to be instantiated by functions like [`pack_padded_sequence()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence).Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to [`pack_padded_sequence()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence). For instance, given data `abc`and `x` the [`PackedSequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) would contain data `axbc` with `batch_sizes=[2,1,1]`.Variables**~PackedSequence.data** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – Tensor containing packed sequence**~PackedSequence.batch_sizes** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – Tensor of integers holding information about the batch size at each sequence step**~PackedSequence.sorted_indices** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*,* *optional*) – Tensor of integers holding how this [`PackedSequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) is constructed from sequences.**~PackedSequence.unsorted_indices** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*,* *optional*) – Tensor of integers holding how this to recover the original sequences with correct order.NOTE`data` can be on arbitrary device and of arbitrary dtype. `sorted_indices` and `unsorted_indices` must be `torch.int64` tensors on the same device as `data`.However, `batch_sizes` should always be a CPU `torch.int64` tensor.This invariant is maintained throughout [`PackedSequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) class, and all functions that construct a :class:PackedSequence in PyTorch (i.e., they only pass in tensors conforming to this constraint).

### pack_padded_sequence

- `torch.nn.utils.rnn.``pack_padded_sequence`(*input*, *lengths*, *batch_first=False*, *enforce_sorted=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#pack_padded_sequence)

  Packs a Tensor containing padded sequences of variable length.`input` can be of size `T x B x *` where T is the length of the longest sequence (equal to `lengths[0]`), `B` is the batch size, and `*` is any number of dimensions (including 0). If `batch_first`is `True`, `B x T x *` `input` is expected.For unsorted sequences, use enforce_sorted = False. If `enforce_sorted` is `True`, the sequences should be sorted by length in a decreasing order, i.e. `input[:,0]` should be the longest sequence, and `input[:,B-1]` the shortest one. enforce_sorted = True is only necessary for ONNX export.NOTEThis function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a [`PackedSequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) object by accessing its `.data` attribute.Parameters**input** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – padded batch of variable length sequences.**lengths** ([*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – list of sequences lengths of each batch element.**batch_first** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – if `True`, the input is expected in `B x T x *` format.**enforce_sorted** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – if `True`, the input is expected to contain sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`.Returnsa [`PackedSequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) object

### pad_packed_sequence

- `torch.nn.utils.rnn.``pad_packed_sequence`(*sequence*, *batch_first=False*, *padding_value=0.0*, *total_length=None*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#pad_packed_sequence)

  Pads a packed batch of variable length sequences.It is an inverse operation to [`pack_padded_sequence()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence).The returned Tensor’s data will be of size `T x B x *`, where T is the length of the longest sequence and B is the batch size. If `batch_first` is True, the data will be transposed into `B x T x *` format.Batch elements will be ordered decreasingly by their length.NOTE`total_length` is useful to implement the `pack sequence -> recurrent network -> unpacksequence` pattern in a [`Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) wrapped in [`DataParallel`](https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel). See [this FAQ section](https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism) for details.Parameters**sequence** (*PackedSequence*) – batch to pad**batch_first** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – if `True`, the output will be in `B x T x *` format.**padding_value** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – values for padded elements.**total_length** ([*int*](https://docs.python.org/3/library/functions.html#int)*,* *optional*) – if not `None`, the output will be padded to have length `total_length`. This method will throw [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError) if `total_length` is less than the max sequence length in `sequence`.ReturnsTuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch.

### pad_sequence

- `torch.nn.utils.rnn.``pad_sequence`(*sequences*, *batch_first=False*, *padding_value=0*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#pad_sequence)

  Pad a list of variable length Tensors with `padding_value``pad_sequence` stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size `L x *` and if batch_first is False, and `T x B x *`otherwise.B is batch size. It is equal to the number of elements in `sequences`. T is length of the longest sequence. L is length of the sequence. * is any number of trailing dimensions, including none.Example`>>> from torch.nn.utils.rnn import pad_sequence >>> a = torch.ones(25, 300) >>> b = torch.ones(22, 300) >>> c = torch.ones(15, 300) >>> pad_sequence([a, b, c]).size() torch.Size([25, 3, 300]) `NOTEThis function returns a Tensor of size `T x B x *` or `B x T x *` where T is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.Parameters**sequences** ([*list*](https://docs.python.org/3/library/stdtypes.html#list)*[*[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*]*) – list of variable length sequences.**batch_first** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – output will be in `B x T x *` if True, or in `T x B x *`otherwise**padding_value** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – value for padded elements. Default: 0.ReturnsTensor of size `T x B x *` if `batch_first` is `False`. Tensor of size `B x T x *` otherwise

### pack_sequence

- `torch.nn.utils.rnn.``pack_sequence`(*sequences*, *enforce_sorted=True*)[[SOURCE\]](https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html#pack_sequence)

  Packs a list of variable length Tensors`sequences` should be a list of Tensors of size `L x *`, where L is the length of a sequence and * is any number of trailing dimensions, including zero.For unsorted sequences, use enforce_sorted = False. If `enforce_sorted` is `True`, the sequences should be sorted in the order of decreasing length. `enforce_sorted = True` is only necessary for ONNX export.Example`>>> from torch.nn.utils.rnn import pack_sequence >>> a = torch.tensor([1,2,3]) >>> b = torch.tensor([4,5]) >>> c = torch.tensor([6]) >>> pack_sequence([a, b, c]) PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1])) `Parameters**sequences** ([*list*](https://docs.python.org/3/library/stdtypes.html#list)*[*[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)*]*) – A list of sequences of decreasing length.**enforce_sorted** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – if `True`, checks that the input contains sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`.Returnsa [`PackedSequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) object
