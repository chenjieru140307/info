







## 线性回归 （Linear Regreesion）


举例:

```py
import torch
from torch.nn import Linear, Module, MSELoss
from torch.optim import SGD
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

# 线性函数 y=5x+7y
x = np.linspace(0, 20, 500)
y = 5 * x + 7
plt.plot(x, y)

# 生成训练数据
x = np.random.rand(256)
noise = np.random.randn(256) / 4
y = x * 5 + 7 + noise
df = pd.DataFrame()
df['x'] = x
df['y'] = y
sns.lmplot(x='x', y='y', data=df)

# 使用PyTorch建立一个线性的模型来对其进行拟合
model = Linear(1, 1)
criterion = MSELoss()
optim = SGD(model.parameters(), lr=0.01)
epochs = 3000

x_train = x.reshape(-1, 1).astype('float32')
y_train = y.reshape(-1, 1).astype('float32')

for i in range(epochs):
    # 整理输入和输出的数据，这里输入和输出一定要是torch的Tensor类型
    inputs = torch.from_numpy(x_train)
    labels = torch.from_numpy(y_train)
    # 使用模型进行预测
    outputs = model(inputs)
    # 梯度置0，否则会累加
    optim.zero_grad()
    # 计算损失
    loss = criterion(outputs, labels)
    # 反向传播
    loss.backward()
    # 使用优化器默认方法优化
    optim.step()
    if (i % 100 == 0):
        # 每 100次打印一下损失函数，看看效果
        print('epoch {}, loss {:1.4f}'.format(i, loss.data.item()))

[w, b] = model.parameters()
print(w.item(), b.item())
print(5 - w.data.item(), 7 - b.data.item())

predicted = model.forward(torch.from_numpy(x_train)).data.numpy()
plt.plot(x_train, y_train, 'go', label='data', alpha=0.3)
plt.plot(x_train, predicted, label='predicted', alpha=1)
plt.legend()
plt.show()
```

输出:

```txt
epoch 0, loss 83.2337
epoch 100, loss 0.8360
epoch 200, loss 0.2598
...略...
epoch 2900, loss 0.0636
4.8304595947265625 7.075088024139404
0.1695404052734375 -0.0750880241394043
```

图像：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200522/8PrbTgMoI2Op.png?imageslim">
</p>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200522/tibdAXMMhQiw.png?imageslim">
</p>



说明：

- 准备训练数据: `x_train`, `y_train` 的形状是 (256, 1)， 代表 `mini-batch` 大小为256， `feature`为1. `astype('float32')` 是为了下一步可以直接转换为 `torch.float`.
- 用 `model.parameters()` 提取模型参数。 w， b 是我们所需要训练的模型参数 我们期望的数据 w=5，b=7 可以做一下对比



## 损失函数(Loss Function)

损失函数

- 损失函数是用来估量模型的预测值(我们例子中的output)与真实值（例子中的y_train）的不一致程度，
- 是一个非负实值函数
- 损失函数越小，模型的鲁棒性就越好。 我们训练模型的过程，就是通过不断的迭代计算，使用梯度下降的优化算法，使得损失函数越来越小。损失函数越小就表示算法达到意义上的最优。

注意：

- PyTorch是使用mini-batch来进行计算的，所以损失函数的计算出来的结果已经对mini-batch取了平均

常见损失函数：

- nn.L1Loss:
  - 输入x和目标y之间差的绝对值，要求 x 和 y 的维度要一样（可以是向量或者矩阵），得到的 loss 维度也是对应一样的
  - $loss(x,y)=\frac{1}{n} \sigma |x_i −y_i|$
- nn.NLLLoss:
  - 用于多分类的负对数似然损失函数
  - $loss(x, class) = -x[class]$
  - NLLLoss中如果传递了weights参数，会对损失进行加权，公式就变成了
  - $loss(x, class) = -weights[class] * x[class]$
- nn.MSELoss:
  - 均方损失函数 ，输入x和目标y之间均方差
  - $loss(x,y)=\frac{1}{n} \sigma (x_i −y_i)2$
- nn.CrossEntropyLoss:
  - 多分类用的交叉熵损失函数，LogSoftMax和NLLLoss集成到一个类中，会调用nn.NLLLoss函数，我们可以理解为 CrossEntropyLoss()=log_softmax() + NLLLoss()
  - $\begin{aligned} loss(x, class) &= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))}\ &= -x[class] + log(\sum_j exp(x[j])) \end{aligned}$
  - 因为使用了NLLLoss，所以也可以传入weight参数，这时loss的计算公式变为：
  - $loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))$
  - 所以一般多分类的情况会使用这个损失函数
- nn.BCELoss:
  - 计算 x 与 y 之间的二进制交叉熵。
  - $loss(o,t)=-\frac{1}{n}\sum_i(t[i]* log(o[i])+(1-t[i])* log(1-o[i]))$ 
  - 与NLLLoss类似，也可以添加权重参数： 
  - $loss(o,t)=-\frac{1}{n}\sum_iweights[i]* (t[i]* log(o[i])+(1-t[i])* log(1-o[i]))$
  - 用的时候需要在该层前面加上 Sigmoid 函数。








## 梯度下降

梯度下降落入局部最优解的问题：


- 这个问题在以前的机器学习中可能会遇到，因为机器学习中的特征比较少，所以导致很可能陷入到一个局部最优解中出不来，但是到了深度学习，动辄百万甚至上亿的特征，出现这种情况的概率几乎为0，所以我们可以不用考虑这个问题。









## torch.optim



`torch.optim`是一个实现了各种优化算法的库。大部分常用优化算法都有实现，我们直接调用即可。

- `torch.optim.SGD`
  - 随机梯度下降算法，带有动量（momentum）的算法作为一个可选参数可以进行设置，样例如下：
  - `optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)`
    - lr参数为学习率，对于SGD来说一般选择0.1 0.01.0.001，如何设置会在后面实战的章节中
    - 如果设置了momentum，就是带有动量的SGD，可以不设置
- `torch.optim.RMSprop`
  - 除了以上的带有动量Momentum梯度下降法外，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法，利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，使其梯度下降的速度变得更快
  - `optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)`
- `torch.optim.Adam`
  - Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法
  - `optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)`
    - 这里的lr，betas，还有eps都是用默认值即可，所以Adam是一个使用起来最简单的优化方法











## 正则化

利用正则化来解决High variance 的问题，正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度，这里我们简单的介绍一下正则化的概念



- L1正则化：
  - 损失函数基础上加上权重参数的绝对值
  - $L=E_{in}+\lambda{\sum_j} \left|w_j\right|$
- L2正则化
  - 损失函数基础上加上权重参数的平方和
  - $ L=E_{in}+\lambda{\sum_j} w^2_j$






