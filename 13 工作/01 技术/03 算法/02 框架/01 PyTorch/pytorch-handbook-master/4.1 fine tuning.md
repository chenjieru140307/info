

# Fine tuning 模型微调

针对于某个任务，自己的训练数据不多，可以找到一个同类的别人训练好的模型，把别人现成的训练好了的模型拿过来，换成自己的数据，调整一下参数，再训练一遍，这就是微调（fine-tune）。

PyTorch里面提供的经典的网络模型都是官方通过Imagenet的数据集与训练好的数据，如果我们的数据训练数据不够，这些数据是可以作为基础模型来使用的。

## 为什么要微调

1. 对于数据集本身很小（几千张图片）的情况，从头开始训练具有几千万参数的大型神经网络是不现实的，因为越大的模型对数据量的要求越大，过拟合无法避免。这时候如果还想用上大型神经网络的超强特征提取能力，只能靠微调已经训练好的模型。
2. 可以降低训练成本：如果使用导出特征向量的方法进行迁移学习，后期的训练成本非常低，用 CPU 都完全无压力，没有深度学习机器也可以做。
3. 前人花很大精力训练出来的模型在大概率上会比你自己从零开始搭的模型要强悍，没有必要重复造轮子。


## 迁移学习 Transfer Learning

总是有人把 迁移学习和神经网络的训练联系起来，这两个概念刚开始是无关的。
迁移学习是机器学习的分支，现在之所以 迁移学习和神经网络联系如此紧密，现在图像识别这块发展的太快效果也太好了，所以几乎所有的迁移学习都是图像识别方向的，所以大家看到的迁移学习基本上都是以神经网络相关的计算机视觉为主，本文中也会以这方面来举例子

迁移学习初衷是节省人工标注样本的时间，让模型可以通过一个已有的标记数据的领域向未标记数据领域进行迁移从而训练出适用于该领域的模型，直接对目标域从头开始学习成本太高，我们故而转向运用已有的相关知识来辅助尽快地学习新知识

## 如何微调

对于不同的领域微调的方法也不一样，比如语音识别领域一般微调前几层，图片识别问题微调后面几层，这个原因我这里也只能讲个大概，具体还要大神来解释：

- 对于图片来说，我们CNN的前几层学习到的都是低级的特征，比如，点、线、面，这些低级的特征对于任何图片来说都是可以抽象出来的，所以我们将他作为通用数据，只微调这些低级特征组合起来的高级特征即可，例如，这些点、线、面，组成的是园还是椭圆，还是正方形，这些代表的含义是我们需要后面训练出来的。
- 对于语音来说，每个单词表达的意思都是一样的，只不过发音或者是单词的拼写不一样，比如 苹果，apple，apfel（德语），都表示的是同一个东西，只不过发音和单词不一样，但是他具体代表的含义是一样的，就是高级特征是相同的，所以我们只要微调低级的特征就可以了。







其实这里有两种做法：

- 使用最后一个fc layer之前的fc layer获得的特征，学习个线性分类器(比如SVM) 
- 重新训练最后一个fc layer，Fine-tuning the ConvNet。固定前几层的参数，只对最后几层进行fine-tuning,
   - 比较极端的，比如先计算出预训练模型的卷积层对所有训练和测试数据的特征向量，然后抛开预训练模型，只训练自己定制的简配版全连接网络。这个方式的一个好处就是节省计算资源，每次迭代都不会再去跑全部的数据，而只是跑一下简配的全连接



注意：

1. 新数据集和原始数据集合类似，那么直接可以微调一个最后的FC层或者重新指定一个新的分类器
2. 新数据集比较小和原始数据集合差异性比较大，那么可以使用从模型的中部开始训练，只对最后几层进行fine-tuning
3. 新数据集比较小和原始数据集合差异性比较大，如果上面方法还是不行的化那么最好是重新训练，只将预训练的模型作为一个新模型初始化的数据
4. 新数据集的大小一定要与原始数据集相同，比如CNN中输入的图片大小一定要相同，才不会报错
5. 如果数据集大小不同的话，可以在最后的fc层之前添加卷积或者pool层，使得最后的输出与fc层一致，但这样会导致准确度大幅下降，所以不建议这样做
6. 对于不同的层可以设置不同的学习率，一般情况下建议，对于使用的原始数据做初始化的层设置的学习率要小于（一般可设置小于10倍）初始化的学习率，这样保证对于已经初始化的数据不会扭曲的过快，而使用初始化学习率的新层可以快速的收敛。





# 微调实例


- 使用官方训练好的resnet50来参加kaggle上面的 [dog breed](https://www.kaggle.com/c/dog-breed-identification) 狗的种类识别来。
- [百度网盘下载](https://pan.baidu.com/s/1H4qbuOi34yeG6-MNyIzWXw) 提取码：ytao
- [resnet50 下载地址](https://pan.baidu.com/s/1s1QHBcjs1Xog4TFL2QI1sg) 提取码：kvxq

首先我们需要下载官方的数据解压，只要保持数据的目录结构即可，这里指定一下目录的位置，并且看下内容


```py
import torch, os, torchvision
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, models, transforms
from PIL import Image
from sklearn.model_selection import StratifiedShuffleSplit

#
DATA_ROOT = 'data'
all_labels_df = pd.read_csv(os.path.join(DATA_ROOT, 'labels.csv'))
all_labels_df.head()

# 定义了两个字典，分别以名字和id作为对应，方便后面处理
breeds = all_labels_df.breed.unique()
breed2idx = dict((breed, idx) for idx, breed in enumerate(breeds))
idx2breed = dict((idx, breed) for idx, breed in enumerate(breeds))
len(breeds)

# 将 index 添加到 dataframe 中
all_labels_df['label_idx'] = [breed2idx[b] for b in all_labels_df.breed]
print(all_labels_df.head())


# 定义数据集
class DogDataset(Dataset):
    def __init__(self, labels_df, img_path, transform=None):
        self.labels_df = labels_df
        self.img_path = img_path
        self.transform = transform

    def __len__(self):
        return self.labels_df.shape[0]

    def __getitem__(self, idx):
        image_name = os.path.join(self.img_path, self.labels_df.id[idx]) + '.jpg'
        img = Image.open(image_name)
        label = self.labels_df.label_idx[idx]

        if self.transform:
            img = self.transform(img)
        return img, label


# 一些超参数
IMG_SIZE = 224  # resnet50的输入是224的所以需要将图片统一大小
BATCH_SIZE = 256  # 这个批次大小需要占用4.6-5g的显存，如果不够的化可以改下批次，如果内存超过10G可以改为512
IMG_MEAN = [0.485, 0.456, 0.406]
IMG_STD = [0.229, 0.224, 0.225]
CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if CUDA else "cpu")
print(DEVICE)

# 图片转换规则
train_transforms = transforms.Compose([
    transforms.Resize(IMG_SIZE),
    transforms.RandomResizedCrop(IMG_SIZE),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(30),
    transforms.ToTensor(),
    transforms.Normalize(IMG_MEAN, IMG_STD)
])

val_transforms = transforms.Compose([
    transforms.Resize(IMG_SIZE),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(IMG_MEAN, IMG_STD)
])

# 划分训练集测试集
dataset_names = ['train', 'valid']
stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)
train_split_idx, val_split_idx = next(iter(stratified_split.split(all_labels_df.id, all_labels_df.breed)))
train_df = all_labels_df.iloc[train_split_idx].reset_index()
val_df = all_labels_df.iloc[val_split_idx].reset_index()
print(len(train_df))
print(len(val_df))

# 载入数据
image_transforms = {'train': train_transforms, 'valid': val_transforms}
train_dataset = DogDataset(train_df, os.path.join(DATA_ROOT, 'train'), transform=image_transforms['train'])
val_dataset = DogDataset(val_df, os.path.join(DATA_ROOT, 'train'), transform=image_transforms['valid'])
image_dataset = {'train': train_dataset, 'valid': val_dataset}
image_dataloader = {x: DataLoader(image_dataset[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=0) for x in
                    dataset_names}
dataset_sizes = {x: len(image_dataset[x]) for x in dataset_names}

# 预训练模型处理
model_ft = models.resnet50(pretrained=True)  # 这里自动下载官方的预训练模型，并且将所有的参数层进行冻结
for param in model_ft.parameters():
    param.requires_grad = False
# 这里打印下全连接层的信息
print(model_ft.fc)
num_fc_ftr = model_ft.fc.in_features  # 获取到fc层的输入
model_ft.fc = nn.Linear(num_fc_ftr, len(breeds))  # 定义一个新的FC层
model_ft = model_ft.to(DEVICE)  # 放到设备中
print(model_ft)  # 最后再打印一下新的模型

# 设置训练参数
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam([
    {'params': model_ft.fc.parameters()}
], lr=0.001)  # 指定 新加的fc层的学习率


# 定义训练函数
def train(model,device, train_loader, epoch):
    model.train()
    for batch_idx, data in enumerate(train_loader):
        x,y= data
        x=x.to(device)
        y=y.to(device)
        optimizer.zero_grad()
        y_hat= model(x)
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
    print ('Train Epoch: {}\t Loss: {:.6f}'.format(epoch,loss.item()))

# 定义测试函数
def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for i,data in enumerate(test_loader):
            x,y= data
            x=x.to(device)
            y=y.to(device)
            optimizer.zero_grad()
            y_hat = model(x)
            test_loss += criterion(y_hat, y).item() # 将 batch loss 累加
            pred = y_hat.max(1, keepdim=True)[1] # get the index of the max log-probability
            correct += pred.eq(y.view_as(pred)).sum().item()
    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(val_dataset),
        100. * correct / len(val_dataset)))

# 训练
for epoch in range(1, 10):
    train(model=model_ft,device=DEVICE, train_loader=image_dataloader["train"],epoch=epoch)
    test(model=model_ft, device=DEVICE, test_loader=image_dataloader["valid"])





# in_list= [] #这里存放所有的输出
# def hook(module, input, output):
#     #input是一个tuple代表顺序代表每一个输入项，我们这里只有一项，所以直接获取
#     #需要全部的参数信息可以使用这个打印
#     #for val in input:
#     #   print("input val:",val)
#     for i in range(input[0].size(0)):
#         in_list.append(input[0][i].cpu().numpy())
#
# # 将 hook 注册到 avgpool 后面
# model_ft.avgpool.register_forward_hook(hook)
# # 跑一遍，就在 in_list 里得到 features 了
# with torch.no_grad():
#     for batch_idx, data in enumerate(image_dataloader["train"]):
#         x,y= data
#         x=x.to(DEVICE)
#         y=y.to(DEVICE)
#         y_hat = model_ft(x)
# # 保存每个图片的特征，这样，下次直接加载特征进行全连接训练。
# features=np.array(in_list)
# np.save("features",features)
```

输出：

```txt
                                 id             breed  label_idx
0  000bec180eb18c7604dcecc8fe0dba07       boston_bull          0
1  001513dfcb2ffafc82cccf4d8bbaba97             dingo          1
2  001cdf01b096e06d78e9e5112d419397          pekinese          2
3  00214f311d5d2247d5dfe4fe24b2303d          bluetick          3
4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever          4
cuda
9199
1023
Linear(in_features=2048, out_features=1000, bias=True)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    ...略...
  )
  (layer2): Sequential(
    ...略...
  )
  (layer3): Sequential(
    ...略...
  )
  (layer4): Sequential(
    ...略...
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=120, bias=True)
)
Train Epoch: 1	 Loss: 2.669307

Test set: Average loss: 0.0078, Accuracy: 663/1023 (65%)

...略...

Train Epoch: 9	 Loss: 1.154993

Test set: Average loss: 0.0025, Accuracy: 834/1023 (82%)
```



说明：


- 配置网络，由于ImageNet是识别1000个物体，我们的狗的分类一共只有120，所以需要对模型的最后一层全连接层进行微调，将输出从1000改为120
- 我们看到只训练了9次就达到了80%的准确率，效果还是可以的。




- 但是每次训练都需要将一张图片在全部网络中进行计算，而且计算的结果每次都是一样的，这样浪费了很多计算的资源。
- 下面我们就将这些不进行反向传播或者说不更新网络权重参数层的计算结果保存下来，这样我们以后使用的时候就可以直接将这些结果输入到FC层或者以这些结果构建新的网络层，省去了计算的时间，并且这样如果只训练全连接层，CPU就可以完成了。
- 可以使用 PyTorch 的 hook 来将固定层的向量导出

- 在相应的层注册hook函数，保证函数能够正常工作，我们这里直接 hook 全连接层前面的pool层，`model_ft.avgpool.register_forward_hook(hook)` 获取pool层的输入数据，这样会获得更多的特征

- `with torch.no_grad()` 因为不需要反向传播，所以直接可以使用no_grad嵌套
- `np.save("features",features)` 这样再训练时我们只需将这个数组读出来，然后可以直接使用这个数组再输入到linear或者我们前面讲到的sigmod层就可以了。

我们在这里在pool层前获取了更多的特征，可以将这些特征使用更高级的分类器，例如SVM，树型的分类器进行分类。


疑问：

- `np.save("features",features)` 得到的 features 要怎么使用进行训练？是什么格式的？