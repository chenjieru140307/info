
# 反向传播


## 相关概念


概念：

- `.requires_grad`：为 `True`，那么将会追踪所有对于该张量的操作。
- `.backward()`。当完成计算后通过调用 `.backward()`，自动计算所有的梯度，这个张量的所有梯度将会自动积累到 `.grad` 属性。（怎么自动累计的？）
- `.detach()`：可以调用 `.detach()` 来阻止张量跟踪历史记录，`.detach()` 将其与计算历史记录分离，并禁止跟踪它将来的计算记录。
- `with torch.no_grad()：`：为了防止跟踪历史记录（和使用内存），可以将代码块包装在`with torch.no_grad()：`中。
- 这个在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练参数，但是我们不需要梯度计算。


- `Tensor` 和 `Function` 互相连接并生成一个非循环图，它表示和存储了完整的计算历史。
  - `.grad_fn`。每个张量都有一个 `.grad_fn` 属性，这个属性引用了一个创建了 `Tensor` 的 `Function`（如果这个张量是用户手动创建的，那么自然这个张量的 `grad_fn` 是 `None`）。<span style="color:red;">具体一般是什么 `Function` 创建了这个 `Tensor` 呢？嗯，关于这个想知道更多。</span>
  - `.backward()`。如果需要计算导数，你可以在 `Tensor` 上调用 `.backward()`。
  - 如果 `Tensor` 是一个标量（即它包含一个元素数据）则不需要为 `backward()` 指定任何参数，但是如果它有更多的元素，你需要指定一个 `gradient` 参数来匹配张量的形状。<span style="color:red;">怎么指定？</span>




## 相关概念

- `requires_grad=True`
- `grad_fn`
- `requires_grad_`
- `with T.no_grad()`

举例：


```py
import torch as T

x = T.ones(2, 2, requires_grad=True)
y = T.zeros(2, 2)
print(x)
print(y)
print()

print(x + 2)
print(y + 2)
print(x + y)
print(x * x * 3)
print(x.mean())
print(x.sum())
print()

y.requires_grad_(True)
print(y + 2)
print()

print(x.requires_grad)
print((x ** 2).requires_grad)
with T.no_grad():
    print((x ** 2).requires_grad)
```

输出：

```
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
tensor([[0., 0.],
        [0., 0.]])

tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
tensor([[2., 2.],
        [2., 2.]])
tensor([[1., 1.],
        [1., 1.]], grad_fn=<AddBackward0>)
tensor([[3., 3.],
        [3., 3.]], grad_fn=<MulBackward0>)
tensor(1., grad_fn=<MeanBackward0>)
tensor(4., grad_fn=<SumBackward0>)

tensor([[2., 2.],
        [2., 2.]], grad_fn=<AddBackward0>)

True
True
False
```

可见：

- 设定 `requires_grad=True` 后，进行相关计算时，会自动进行 autograd 计算。
- 设定 `requires_grad=True` 后，相关的被创建出来的时候，创建它的那一步操作就会被存在 `grad_fn` 属性里。没有指定的话，默认是 `False`。
- 可以使用 `.requires_grad_()` 来修改 `requires_grad` 属性
- 如果 `.requires_grad=True` 但是你又不希望进行 autograd 的计算，可以使用使用 `with torch.no_grad()` 包裹来禁止 autograd 计算。


## 反向传播 backward


```python
import torch as T

x = T.ones(2, 2, requires_grad=True)
y = x + 2
z = y * y * 3
out = z.mean()

print(x)
print(y)
print(z)
print(out)
print()

out.backward(retain_graph=True)
print(out.grad)
print(z.grad)
print(y.grad)
print(x.grad)
print()

gradients = T.tensor([[0.1, 1.0], [1.0, 1.0]], dtype=T.float)
z.backward(gradients, retain_graph=True)
print(x.grad)
out.backward(retain_graph=True)
print(x.grad)

x.grad = None
out.backward()
print(x.grad)
```

输出：

```
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward0>)
tensor(27., grad_fn=<MeanBackward0>)

None
None
None
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])

tensor([[ 6.3000, 22.5000],
        [22.5000, 22.5000]])
tensor([[10.8000, 27.0000],
        [27.0000, 27.0000]])
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
```

计算过程:

- 由：
  - $y_i = x_i+2=3$
  - $z_i = 3(y_i)^2=27$
  - $o = \frac{1}{4}\sum_i z_i=27$
- 张量 $\vec{y}=f(\vec{x})$ 的导数为：
  - $\begin{aligned}J=\left(\begin{array}{ccc}  \frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}}\\  \vdots & \ddots & \vdots\\  \frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}  \end{array}\right)\end{aligned}$
  - 即 Jacobian 矩阵。
- 由于 `out` 是一个纯量（scalar），`out.backward()` 等于 `out.backward(torch.tensor(1))`。
- 所以：
  - $\frac{\partial o}{\partial x_i} =\frac{1}{4}\frac{\partial z_i}{\partial x_i}=\frac{6}{4}\frac{y_i\partial y_i}{\partial x_i}=\frac{6\times 3}{4}=4.5$
- 然后，我们使用了 `z.backward` 对 `x` 求导：
  - $\frac{\partial z_1}{\partial x_1} =0.1\times 6\frac{y_i\partial y_i}{\partial x_i}+4.5=0.1\times 6\times 3 +4.5=1.8+4.5=6.3$
- 然后，我们使用了 `out.backward()` 再次求导：
  - $\frac{\partial o_1}{\partial x_1} =\frac{1}{4}\frac{\partial z_i}{\partial x_i}+6.3=\frac{6}{4}\frac{y_i\partial y_i}{\partial x_i}+6.3=\frac{6\times 3}{4}+6.3=4.5+6.3=10.8$


说明:

- 可以使用 `x.grad=None` 来清掉上次计算的偏导数。
- `retain_graph=True` 可以基于上次偏导进行叠加。（还没有很清楚，看下文档）


疑问：

- 为什么是 `out.backward(torch.tensor(1))` ？


