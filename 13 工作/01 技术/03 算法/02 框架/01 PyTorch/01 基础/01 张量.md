
# 张量


## Tensor 常见的类型

| Data type                | CPU tensor     | GPU tensor          |
| ------------------------ | -------------- | ------------------- |
| 32-bit floating point    | T.FloatTensor  | T.cuda.FloatTensor  |
| 64-bit floating point    | T.DoubleTensor | T.cuda.DoubleTensor |
| 16-bit floating point    | T.HalfTensor   | T.cuda.HalfTensor   |
| 8-bit integer (unsigned) | T.ByteTensor   | T.cuda.ByteTensor   |
| 8-bit integer (signed)   | T.CharTensor   | T.cuda.CharTensor   |
| 16-bit integer (signed)  | T.ShortTensor  | T.cuda.ShortTensor  |
| 32-bit integer (signed)  | T.IntTensor    | T.cuda.IntTensor    |
| 64-bit integer (signed)  | T.LongTensor   | T.cuda.LongTensor   |


疑问：

- 如果需要切换 CPU 与 GPU 的 Tensor 要怎么操作？要改代码吗？


## Tensor 属性


```py
import torch as T

t = T.Tensor([[1, 2, 3], [2, 2, 5]])
print(t)
print(t.size())  # 返回 size 类型
print(T.numel(t))  # 返回总元素个数
```


输出：

```
tensor([[1., 2., 3.],
        [2., 2., 5.]])
torch.Size([2, 3])
6
```

说明：

- `T.numel(t)` 返回总元素个数
- size 方法与Numpy的shape属性返回的相同，张量也支持shape属性
- `torch.Size` 返回值是 tuple类型, 所以它支持tuple类型的所有操作.


## 创建 Tensor

举例：

```py
import torch as T

print(T.Tensor(2, 3)) # 尽量不用这个，而是使用有初始化的
print()

print(T.arange(1, 9, 2))
print(T.linspace(1, 9, 4))
print()

print(T.ones(2, 3))
print(T.zeros(2, 3))
print(T.eye(2, 3))
print(T.eye(2))
print()


t = T.Tensor(2, 3)
print(T.Tensor(t.size()))
print(T.ones(t.size()))
print(T.ones_like(t))
print(T.zeros(t.size()))
print(T.zeros_like(t))
print(T.empty(t.size()))
print(T.empty_like(t))
print(T.full(t.size(), 1))
print(T.full_like(t, 1))
print()

print(T.rand(2, 3))  # 均匀分布
print(T.randn(2, 3))  # 标准正态分布
print(T.normal(T.rand(2, 3), 0.0001))
print(T.randn_like(T.rand(2, 3), dtype=T.float))  # 指定 dtype
print()
```

输出：

```
tensor([[1.9432e-19, 1.7439e+28, 4.7418e+30],
        [1.1431e+27, 1.8491e+31, 1.0766e+21]])

tensor([1, 3, 5, 7])
tensor([1.0000, 3.6667, 6.3333, 9.0000])

tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 0., 0.],
        [0., 1., 0.]])
tensor([[1., 0.],
        [0., 1.]])

tensor([[0.0000e+00, 0.0000e+00, 2.8026e-45],
        [0.0000e+00, 1.4013e-45, 0.0000e+00]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[0.0000e+00, 0.0000e+00, 8.4078e-45],
        [0.0000e+00, 1.4013e-45, 0.0000e+00]])
tensor([[0.0000e+00, 0.0000e+00, 8.4078e-45],
        [0.0000e+00, 1.4013e-45, 0.0000e+00]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])

tensor([[0.9657, 0.1302, 0.8465],
        [0.9192, 0.2186, 0.9254]])
tensor([[ 0.4162,  0.6456, -0.0313],
        [-0.2587, -0.0761, -0.6782]])
tensor([[0.6984, 0.5296, 0.6761],
        [0.9156, 0.8922, 0.7979]])
tensor([[-0.2517, -0.0713, -0.0572],
        [-0.2955,  1.9038,  2.0103]])
```

说明：

- `T.normal(T.rand(2, 3), 0.0001)` 是在 `T.rand(2, 3)` 的基础上进行以标准差为 0.0001 的正态采样。

疑问：

- 为什么 `T.Tensor(2, 3)` 这个在 pycharm 中提示是有问题的。

**从 numpy 创建：**

```py
import torch as T
import numpy as np

x = np.ones(5)
y1 = T.Tensor(x)
y2 = T.from_numpy(x)
print(x)
print(y1)
print(y2)
np.add(x, 1, out=x)
print(x)
print(y1)
print(y2)
```

输出：

```
[1. 1. 1. 1. 1.]
tensor([1., 1., 1., 1., 1.])
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
[2. 2. 2. 2. 2.]
tensor([1., 1., 1., 1., 1.])
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
```

注意：

- `T.Tensor(x)` 与 `T.from_numpy(x)` 两种方式转化的 Tensor 是不同的，其中 `from_numpy` 得到的 y 实际上是对 x 的一个引用，所以，当 x 修改了，y 也会修改。
- **CharTensor 类型不支持到 NumPy 的转换。**

使用说明：

- 使用 `T.from_numpy(x)` 来转换时，Tensor 和 np.array 共享内存，**所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。所以有些 PyTorch 没有但 numpy 有的操作可以先转化为 numpy 进行操作再转化回来。代价可以忽略不计。**




## Tensor 拷贝

```py
import torch as T

x = T.ones(2, 3)
y = T.rand(2, 3)
print(y)
y.copy_(x)
print(y)
```

输出:

```txt
tensor([[0.5567, 0.5371, 0.5022],
        [0.6897, 0.5890, 0.7535]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
```




## Tensor 比较操作

张量比较

```py
import torch as T

print(T.eq(T.Tensor([[1, 2], [3, 4]]), T.Tensor([[1, 1], [4, 4]])))
print(T.ge(T.Tensor([[1, 2], [3, 4]]), T.Tensor([[1, 1], [4, 4]])))
print(T.gt(T.Tensor([[1, 2], [3, 4]]), T.Tensor([[1, 1], [4, 4]])))
print()

print(T.equal(T.Tensor([1, 2]), T.Tensor([1, 2])))
```

输出：

```
tensor([[ True, False],
        [False,  True]])
tensor([[ True,  True],
        [False,  True]])
tensor([[False,  True],
        [False, False]])

True
```

说明：

- `torch.eq（input,other,out=None）`，比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量。
  - 可以配合 `T.masked_select(t, mask)` 来对符合条件的进行选择。
- `torch.equal（tensor1, tensor2）`如果两个张量有相同的形状和元素值，则返回 True，否则 False。


补充:

- 想知道对于这些返回的结果的一些统计要怎么做？

## Tensor 维度转换

举例：

```py
import torch as T

t = T.Tensor([[1, 2, 3], [2, 2, 5]])
print(t)
print(t.view(3, 2))  # 维度重整
print(t.view(-1, 1))  
print()

print(t.t_)

t = T.Tensor([[1, 2, 3], [2, 2, 5]])
print(t.unsqueeze(0))  # 在 di 个维度处升维、 变为 1*2*3
print(t.squeeze(0))  # 若 di 维是 1，压缩，否则不变。若无参数，压缩所有“1”维

x = T.zeros(2, 1, 1)
print(x)
print(T.squeeze(x))  # 2*1*1 -> 2 注意:由于不是 2*1 ，所以是一行的。
print(T.squeeze(x, 0))  # 0 为 2，因此不变
print(T.squeeze(x, 1))  # 1 为 1，因此压缩掉，变为 2*1
```


输出：

```txt
tensor([[1., 2., 3.],
        [2., 2., 5.]])
tensor([[1., 2.],
        [3., 2.],
        [2., 5.]])
tensor([[1.],
        [2.],
        [3.],
        [2.],
        [2.],
        [5.]])

tensor([[[1., 2., 3.],
         [2., 2., 5.]]])
tensor([[1., 2., 3.],
        [2., 2., 5.]])
tensor([[[0.]],
        [[0.]]])
tensor([0., 0.])
tensor([[[0.]],
        [[0.]]])
tensor([[0.],
        [0.]])
```

说明：

- `torch.view`: 可以改变张量的维度和大小。torch.view 与Numpy的reshape类似



## Tensor 拆分与拼接

举例：

```py
import torch as T

t = T.Tensor([[1, 2, 3], [2, 2, 5]])
print(t)
print(T.cat((t, t), 1))  # 拼接 tensor，按 axis 进行
print(T.chunk(t, 3, 0))  # 在 di 维上将 t 分成 i 份，最后一份的维度不定（若不能整除）
```

输出：

```
tensor([[1., 2., 3.],
        [2., 2., 5.]])
tensor([[1., 2., 3., 1., 2., 3.],
        [2., 2., 5., 2., 2., 5.]])
(tensor([[1., 2., 3.]]), tensor([[2., 2., 5.]]))
```

说明：

- `T.cat((t,t,...),di)` 拼接 tensor，按 axis 进行
- `T.chunk(t,i,di)` ：在 di 维上将 t 分成 i 份，最后一份的维度不定（若不能整除）



## 切片与选择

举例：

```py
import torch as T
import numpy as np

t = T.Tensor([[1, 0, 0, 2, 3, 0], [4, 5, 0, 0, 2, 6]])
indices = T.LongTensor(range(1, 5, 2))
mask = T.BoolTensor(np.eye(2, 6))
print(t)
print(indices)
print(mask)
print()

print(t[:, 1])
print(T.index_select(t, 1, indices))  # 在第 di 维上将 t 的 indices 抽取出来组成新 Tensor。
print(T.masked_select(t, mask))  # 按照 0-1 Tensor mask的格式筛选 t，返回一维 Tensor
print()

print(T.nonzero(t))  # 输出非零元素的 index
print()

x = T.randn(1)
print(x)
print(x.item())
print()
```

输出：

```
tensor([[1., 0., 0., 2., 3., 0.],
        [4., 5., 0., 0., 2., 6.]])
tensor([1, 3])
tensor([[ True, False, False, False, False, False],
        [False,  True, False, False, False, False]])

tensor([0., 5.])
tensor([[0., 2.],
        [5., 0.]])
tensor([1., 5.])

tensor([[0, 0],
        [0, 3],
        [0, 4],
        [1, 0],
        [1, 1],
        [1, 4],
        [1, 5]])

tensor([1.1650])
1.1650481224060059
```

说明：

- `T.index_select(t, di, indices)` ：在第 di 维上将 t 的 indices 抽取出来组成新 Tensor。
- `T.masked_select(t, mask)`：按照 0-1Tensor mask的格式筛选 t，返回一维 Tensor
- `T.nonzero(t)`：输出非零元素的 index。（是不是有一些类似这个的函数？）
- `x.item()` 只有当 x 是一个元素时才可以使用。

疑问：

- 为什么 `t[:, 1]` 得到的是一个行张量？
