# 简单神经网络

## 神经网络

网络：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190627/yJpTM4tD80SK.png?imageslim">
</p>



我们使用 `torch.nn` 包来构建神经网络。


- `torch.nn` 包依赖 `autograd` 包来定义模型并求导。
- `nn.Module` 包含各个层和一个 `forward(input)` 方法，该方法返回 `output`。<span style="color:red;">嗯，是返回一个 `output`。</span>



## 网络搭建

举例：


```py
import torch as T
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim



class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)  # 输入 1 channel 输出 6 channel 卷积核 5*5
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 6 * 5, 120)  # 全连接 y = Wx + b
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (1, 2))  # 最大池化 2*2
        print(x.size())  # (32-4)/1=28 (32-4)/2=14
        x = F.max_pool2d(F.relu(self.conv2(x)), (4, 2))  # 最大池化 4*2
        print(x.size())  # (28-4)/4=6 (14-4)/2=5
        x = x.view(-1, self.num_flat_features(x))  # 转成对应的列数
        print(x.size())  # 16*6*5=480
        x = F.relu(self.fc1(x))
        print(x.size())  #
        x = F.relu(self.fc2(x))
        print(x.size())
        x = self.fc3(x)
        print(x.size())
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # 除了 batch 之外的所有维度
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)

# 网络信息
params = list(net.parameters())
print(len(params))
for param in params:
    print(param.size())
print()

# 前向计算
input = T.randn(2, 1, 32, 32)  # batch 2， channel 1， 行数 32， 列数 32
out = net(input)
print(out)
print()

# 计算损失
target = T.randn(2 * 10)  # 生成随机数作为标准输出
target = target.view(2, -1)
print(target)
criterion = nn.MSELoss()
loss = criterion(out, target)
print(loss)
print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU
print()

# 反向传播
net.zero_grad()  # 清除梯度
print('grad before backward')
print(net.conv1.weight.size())
print(net.conv1.weight.grad)
print(net.conv1.bias.size())
print(net.conv1.bias.grad)
loss.backward()
print('grad after backward')
print(net.conv1.weight.size())
print(net.conv1.weight.grad.size())
print(net.conv1.bias.size())
print(net.conv1.bias.grad.size())
print(net.conv1.bias)
print(net.conv1.bias.grad)
print()

# 使用梯度对网络权重进行更新
learning_rate = 0.1
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
print('conv1.bias.grad after backward and update parameter:')
print(net.conv1.bias)
print(net.conv1.bias.grad)
print()

# 使用自带的优化器 替代 手动权重更新
optimizer = optim.SGD(net.parameters(), lr=0.1)
# optimizer.zero_grad()  # 训练前清空网络梯度
optimizer.step()  # 使用优化器更新网络权重
print('conv1.bias.grad after sgd')
print(net.conv1.bias)
print(net.conv1.bias.grad)
```


输出：

```txt
Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=480, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
10
torch.Size([6, 1, 5, 5])
torch.Size([6])
torch.Size([16, 6, 5, 5])
torch.Size([16])
torch.Size([120, 480])
torch.Size([120])
torch.Size([84, 120])
torch.Size([84])
torch.Size([10, 84])
torch.Size([10])

torch.Size([2, 6, 28, 14])
torch.Size([2, 16, 6, 5])
torch.Size([2, 480])
torch.Size([2, 120])
torch.Size([2, 84])
torch.Size([2, 10])
tensor([[ 0.0222, -0.1411, -0.0784, -0.0624,  0.0588, -0.0709, -0.0811, -0.1470,
         -0.0116, -0.1270],
        [ 0.0095, -0.1627, -0.0704, -0.0388,  0.1087, -0.0656, -0.0625, -0.1592,
         -0.0316, -0.1437]], grad_fn=<AddmmBackward>)

tensor([[ 0.1267, -0.1959,  1.1751,  2.0560,  0.1988, -1.5837,  0.5208, -0.6628,
          1.0882, -0.3389],
        [ 0.0934, -0.5098,  1.9559,  0.7255,  1.0689,  0.1811,  0.3313,  1.1540,
         -1.2373,  0.9242]])
tensor(1.0269, grad_fn=<MseLossBackward>)
<MseLossBackward object at 0x0000021F82C93F88>
<AddmmBackward object at 0x0000021F86786108>
<AccumulateGrad object at 0x0000021F82C93F88>

grad before backward
torch.Size([6, 1, 5, 5])
None
torch.Size([6])
None
grad after backward
torch.Size([6, 1, 5, 5])
torch.Size([6, 1, 5, 5])
torch.Size([6])
torch.Size([6])
Parameter containing:
tensor([-0.1236, -0.1716,  0.0377, -0.0702,  0.0925,  0.1356],
       requires_grad=True)
tensor([0.0102, 0.0076, 0.0060, 0.0119, 0.0033, 0.0005])

conv1.bias.grad after backward and update parameter:
Parameter containing:
tensor([-0.1246, -0.1724,  0.0370, -0.0714,  0.0922,  0.1356],
       requires_grad=True)
tensor([0.0102, 0.0076, 0.0060, 0.0119, 0.0033, 0.0005])

conv1.bias.grad after sgd
Parameter containing:
tensor([-0.1256, -0.1731,  0.0364, -0.0726,  0.0919,  0.1355],
       requires_grad=True)
tensor([0.0102, 0.0076, 0.0060, 0.0119, 0.0033, 0.0005])
```


说明：


网络搭建部分：

- `def __init__(self)` 部分：
  - `self.conv1 = nn.Conv2d(1, 6, 5)` 接收的输入是 1个 channel 的输入，输出 6 个 channel，使用的是 5*5 的卷积核。
  - `self.conv2 = nn.Conv2d(6, 16, 5)` 接收的是 6 个 channel 的输入，输出的是 16 个 channel ，使用过的是 5*5 的卷积核。
  - `self.fc1 = nn.Linear(16 * 5 * 5, 120)` 接收的是 $16*5*5=400$ 维的向量，输出的是 120 维的向量。**同时，说明了上一层的每个 channel 的输出是 $5*5$ 的，在 forward 中我们可以确认下。**。
  - `self.fc2 = nn.Linear(120, 84)` 接收的是 120 维的向量，输出的是 84 维的向量。
  - `self.fc3 = nn.Linear(84, 10)` 接收的是 84 维的向量，输出的是 10 维的向量。
- 输出 `param.size()` 部分
  - `net.parameters()` 返回可被学习的参数（权重）列表和值。
  - `torch.Size([6, 1, 5, 5])` 对应的第一个卷积层，输入是 1 个 channel，输出是 6 个 channel，也即 6 个卷积核，$5*5$ 是每个卷积核的尺寸为 $5*5$。
  - `torch.Size([6])` 对应的是 6 个卷积核每个的 bias 值。
  - `torch.Size([16, 6, 5, 5])` 与 `torch.Size([16])` 同上。
  - `torch.Size([120, 400])` 是表示 400 个输入与 120 个输出的权重个数：400*120
  - `torch.Size([120])` 表示 120 个输出的 bias。
  - `torch.Size([84, 120])` 与 `torch.Size([84])` 与 `torch.Size([10, 84])` 与 `torch.Size([10])` 同上。
  - 因此，由于参数按 w、b 这样存储，因此，有5 层网络，却有 $5*2$ 层的参数。



前向计算部分：

- 说明，forward 部分：
  - `x = F.max_pool2d(F.relu(self.conv1(x)), (1, 2))` 将 `self.conv1(x)` 输出的内容进行非线性激活后进行最大池化处理。池化使用的是 1*2 的滑窗，即水平方向一格，竖直方向 2 格。<span style="color:red;">可见，池化是接在非线性激活后面的。</span>
  - `x = F.max_pool2d(F.relu(self.conv2(x)), (4, 2))` 将上面最大池化的输入进行卷积后，做非线性激活，再做滑窗。
  - `x = x.view(-1, self.num_flat_features(x))` 这里，将上面池化的输出，先用 `self.num_flat_features(x)` 统计出有多少个输出，然后用 `x.view` 来改变维度，$-1$ 说明第一个维度的尺寸由 `self.num_flat_features(x)` 的数字计算得到。
  - `x = F.relu(self.fc1(x))` 对拉直的输入进行全连接，然后进行非线性激活。
  - `x = F.relu(self.fc2(x))` 再次进行全连接和非线性激活。
  - `x = self.fc3(x)` 进行全连接，将结果直接输出。
- 说明，`num_flat_features` 函数：
  - `size = x.size()[1:]` 获得输入的 size，卷积层的输出的 size 一般内容为：`(batch size,channel,columns,rows)`，这里取 `(channel,columns,rows)` 部分。<span style="color:blue;"> 具体可以看下面的对于forward 计算过程中的输出尺寸的说明。</span>
  - `for s in size: num_features *= s` 计算出 $channel*rows*height$ 即卷积层输出的元素的个数。
- 说明，输入的图像的尺寸：
  - `input = torch.randn(2, 1, 32, 32)` 输入的是 batch size 为 2 的图片，即 两张图片。1 为每个图片的 channel 个数为 1，$32*32$ 为 $横向*竖向$ 尺寸。
- 说明，forward 计算过程中的输出尺寸：
  - `torch.Size([2, 6, 28, 14])` 是经过 `x = F.max_pool2d(F.relu(self.conv1(x)), (1, 2))` 输出的尺寸，2 为输入的 batch size，一个 batch 有多少个图片输入。6 为经过卷积后输出的 channel 的个数为 6。28 为经过卷积核扫描后得到的单个 channel 的横向尺寸再进行 1 step 的池化后的结果，$(32-(5-1))/1=28$。14 为经过卷积核扫描后得到的单个 channel 的纵向尺寸再进行 2 step 的池化后的结果，$(32-(5-1))/2=14$。
  - `torch.Size([2, 16, 6, 5])` 是经过 `x = F.max_pool2d(F.relu(self.conv2(x)), (4, 2))` 输出的尺寸。首先 x 现在的输入尺寸是上面的 `torch.Size([2, 6, 28, 14])`，2 仍然是 batch size ，16 是卷积层输出的 16 个 channel，6 是 $(28-(5-1))/4=6$ ，5 是 $(14-(5-1))/2=5$。
- 注意：
  - `nn.Conv2d` 接受一个 4 维的张量， 每一维分别是 `sSamples * nChannels * Height * Width`（`样本数*通道数*高*宽`）。
  - 如果把单个样本 `(channels,height,width)` 传入网络，只需使用 `input.unsqueeze(0)` 来添加第一个维度。<span style="color:red;">嗯。好的，这个之前有用过。现在更加明确了。</span>
  - 在模型中必须要定义 `forward` 函数，`backward` 函数（用来计算梯度）会被 `autograd` 自动创建。
  - 可以在 `forward` 函数中使用任何针对 Tensor 的操作。<span style="color:red;">到底有哪些操作，进行补充下。</span>

计算损失部分：

- loss:
  - `criterion = nn.MSELoss()`是一个比较简单的损失函数，它计算输出和目标间的**均方误差**。
  - 通过 `loss = criterion(output, target)` 计算出 output 与 target 之间的误差。
  - 我们可以通过 `loss.grad_fn.next_functions[0][0]` 来回溯之前的 `grad_fn`。
- 各种不同的损失函数：
  - `nn` 包中有很多不同的[损失函数](https://pytorch.org/docs/nn.html#loss-functions)。


反向传播部分：

- 说明：
  - 在 backward 之前卷积层的权重和偏置的梯度都是 None。
- 注意：
  - 在调用 `loss.backward()` 获得反向传播的误差之前，需要调用 `net.zero_grad()` 将网络内所有参数的梯度缓存清零。**否则梯度将被累加到已存在的梯度。**


手动权重更新部分：

- 权重更新
  - 使用 `for f in net.parameters():` 来遍历网络的所有参数。
  - `f.data.sub_(f.grad.data * learning_rate)` 中，`f.data.sub_(x)` 是在 网络权重或偏置值得原地减去 x。`f.grad.data` 即为这个参数对应的梯度值。因此，此式即为：`weight = weight - learning_rate * gradient`
- 说明，`net.conv1.bias` 的值的变化：
  - 看 `net.conv1.bias` 的第二个值，`-0.1716-(0.0076*0.1)=-0.17236=-0.1724`，完成了偏置值的更新。





使用 sgd 进行权重更新部分：

- sgd:
  - 可以使用使用自带的优化器 替代 手动权重更新
  - 说明，`net.conv1.bias` 的值的变化：
  - 看 `net.conv1.bias` 的第二个值，`-0.1724-(0.0076*0.1)=-0.1731`，完成了偏置值的更新。
- 注意：
  - 每个 batch 的训练之前，需要用 `optimizer.zero_grad()` 把梯度缓存清空。不然，梯度会累加起来，使得根据这个梯度更新出的 权重或者偏置是错的。

补充：

- `torch.optim` 包
  - SGD
  - Nesterov-SGD
  - Adam
  - RMSPROP
  - 等
- `nn`包，包含了各种用来构成深度神经网络构建块的模块和损失函数，完整的文档请查看[here](https://pytorch.org/docs/nn)。<span style="color:red;">这个还是要总结进来的。</span>



## 问题与解决

**问题1：**

注意：

- 我们把

```py
x = F.max_pool2d(F.relu(self.conv1(x)), (1, 2))
x = F.max_pool2d(F.relu(self.conv2(x)), (4, 2))
```

改为：

```py
x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))
```

看起来是不是很合理？

实际上是有问题的：

我们计算下修改后的x 的尺寸：

- 经过 `x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))` 后，输出尺寸为 (2,6,14,14) ，其中 $(32-(5-1))/2=14$
- 经过 `x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))` 后，输出尺寸为 (2,16,5,5)，其中 $(14-(5-1))/2=5$

可见，(2,16,5,5) 与修改前得到的 (2,16,6,5) 有了区别。

原因是什么呢？

- $(32-(5-1))/1=28$ $(28-(5-1))/4=6$
- $(32-(5-1))/2=14$ $(14-(5-1))/2=5$

可见，不同处就在于 卷积核在扫描的时候，会扣除一定的格子，扣除之后再进行除法，结果就不同了。
