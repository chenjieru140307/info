
# 可以补充进来的

- 简直了，书上写的各种错。。这是啥书。。


# 实现前馈神经网络


前面已经简单介绍了前馈神经网络的原理，接下来我们用案例来实现吧。

```py
import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
import torch.utils.data as Data
import matplotlib.pyplot as plt

# 超参数
input_size = 784 # 输入层个数
hidden_size = 500 # 隐藏层
num_classes = 10 # 分类种类
num_epochs = 5 # 训练遍数
batch_size = 100 # 每批训练个数
learning_rate = 0.001 # 学习率

# MNIST Dataset
train_dataset = dsets.MNIST(root='../data',
                            train=True,
                            transform=transforms.ToTensor(),
                            download=True)
test_dataset = dsets.MNIST(root='../data',
                           train=False,
                           transform=transforms.ToTensor())

# Data Loader (Input Pipeline)
# 注意，batch_size 在 loader 这个地方已经起作用了。
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)
test_y = test_dataset.test_labels


# 神经网络模型 （单隐层）
class Net(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size) # full connect 1
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes) # full connect 2

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

net = Net(input_size, hidden_size, num_classes)

# 损失函数和优化器
criterion = nn.CrossEntropyLoss() # 交叉熵损失
optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)

# 训练模型
# batch_size 在 loader 就已经起作用了
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # 这个 images 和 labesl 里面有 batch_size 个图片及对应的标签
        # 将 torch tensor 转化为 Variable
        images = Variable(images.view(-1, 28 * 28))
        labels = Variable(labels)

        # Forward + Backward + Optimize
        optimizer.zero_grad()  # zero the gradient buffer
        outputs = net(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()


        if (i + 1) % 100 == 0:
            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'
                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.item()))

# 进行预测 计算出准确率。
correct = 0
total = 0
for images, labels in test_loader:
    # 这个 images 和 labesl 里面有 batch_size 个图片及对应的标签
    images = Variable(images.view(-1, 28 * 28))
    outputs = net(images)
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted == labels).sum()

print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))

# 画出前 3 个预测的图形
for i in range(1, 4):
    plt.imshow(train_dataset.train_data[i].numpy(), cmap='gray')
    plt.title('%i' % train_dataset.train_labels[i])
plt.show()

# 将模型保存下来
torch.save(net.state_dict(), 'model.pkl')
```


输出：

```
Epoch [1/5], Step [100/600], Loss: 0.4157
Epoch [1/5], Step [200/600], Loss: 0.1697
略..
Epoch [5/5], Step [300/600], Loss: 0.0534
Epoch [5/5], Step [400/600], Loss: 0.0305
Epoch [5/5], Step [500/600], Loss: 0.0177
Epoch [5/5], Step [600/600], Loss: 0.0340
Accuracy of the network on the 10000 test images: 97 %
```

参数说明如下：

要使用 torch.optim，您必须构造一个 Optimizer 对象。这个对象能保存当前的参数状态，并且基于计算梯度更新参数。要构造一个 Optimizer，你必须给它一个包含参数（必须都是 Variable 对象）并进行优化。然后，可以指定 Optimizer 的参数选项，比如学习率、权重衰减等。Optimizer 也支持为每个参数单独设置选项。我们不需要直接传入 Variable 的 Iterable，而是利用 Dict 进行传入数据。每一个 Dict 都分别定义了一组参数，实现一一对应。其他的参数与 Optimizer 所接收的其他参数的关键字相匹配，实现参数优化。

一些优化算法例如 Conjugate Gradient 和 LBFGS 需要重复多次计算函数，因此你需要传入一个闭包来允许它们重新计算你的模型。<span style="color:red;">这个是什么意思？ Conjugate Gradient 和 LBFGS 是什么？怎么传入一个闭包来允许重新计算你的模型？</span>这个闭包会清空梯度，计算损失，然后返回。常用的优化函数有 SGD 全名 Stochastic Gradient Descent，即随机梯度下降。NesterovMomentum 优化函数等。<span style="color:red;">NesterovMomentum 优化函数是什么意思？</span>


输出的结果为 97%，说明前馈神经网络对手写图片的分类具有良好的分类效果。经过 5 个批次 600 次训练，准确率达到 97%。


加载保存的模型，并在测试集上取前 20 张图片进行验证：


```py
import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
import torch.utils.data as Data
import matplotlib.pyplot as plt

# 超参数
input_size = 784 # 输入层个数
hidden_size = 500 # 隐藏层
num_classes = 10 # 分类种类
num_epochs = 5 # 训练遍数
batch_size = 100 # 每批训练个数
learning_rate = 0.001 # 学习率

# MNIST Dataset
train_dataset = dsets.MNIST(root='../data',
                            train=True,
                            transform=transforms.ToTensor(),
                            download=True)
test_dataset = dsets.MNIST(root='../data',
                           train=False,
                           transform=transforms.ToTensor())

# Data Loader (Input Pipeline)
# 注意，batch_size 在 loader 这个地方已经起作用了。
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)
test_y = test_dataset.test_labels
test_data=test_dataset.test_data

# 神经网络模型 （单隐层）
class Net(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size) # full connect 1
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes) # full connect 2

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

net = Net(input_size, hidden_size, num_classes)

# 加载模型权重
net.load_state_dict(torch.load('model.pkl'))


# 对前 20 张图片进行预测。对比结果。
img_datas =test_data[:20]
images = Variable((img_datas.float().view(-1, 28 * 28))) # 如果不转化为 float 则 net 计算有问题。
test_output = net(images)
_, predicted = torch.max(test_output.data, 1)
print('prediction number', predicted)
pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze() # TODO 这个地方不是很熟练
print('prediction number', pred_y)
print('real number', test_y[:20].numpy())
```


输出：

```
prediction number tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4])
prediction number [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]
real number [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]
```


<span style="color:red;">感觉还是有些准的。</span>

我们从前馈神经网络模型的案例中已经大概了解 PyTorch 形成神经网络的流程。

下面我们来具体介绍一下，形成完整的神经网络模型所需的 PyTorch 包的组成。



# 相关

- 《深度学习框架 Pytorch 快速开发与实战》
