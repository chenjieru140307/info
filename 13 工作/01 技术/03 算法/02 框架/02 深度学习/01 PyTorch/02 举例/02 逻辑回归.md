
# 逻辑回归

Logistic Regression（逻辑回归）是当前业界比较常用的机器学习方法，用于估计某种事物的可能性。比如某用户购买某商品的可能性，某病人患有某种疾病的可能性，以及某广告被用户点击的可能性等。

## 二分类问题

二分类问题是指预测的 $y$ 值只有两个取值（0或 1），二分类问题可以扩展到多分类问题。例如：我们要做一个垃圾邮件过滤系统， $x(i)$ 是邮件的特征，预测的 $y$ 值就是邮件的类别，是垃圾邮件还是正常邮件。


对于类别我们通常称为正类（Positive Class）和负类（Negative Class），垃圾邮件的例子中，正类就是正常邮件，负类就是垃圾邮件。

## Logistic函数

LR分类器（Logistic Regression Classifier），在分类情形下，经过学习之后的 LR 分类器其实就是一组权值 $w_0,w_1,w_2,\ldots,w_m$。当测试样本集中的测试数据来到时，这一组权值按照与测试数据线性加和的方式，求出一个 $z$ 值。

${z}={w}_{0}+{w}_{1} \times {x}_{1}+{w}_{2} \times {x}_{2}+\ldots+{w}_{{m}} \times {x}_{{m}}$。（其中 $x_0,x_1,x_2,\ldots,x_m$ 是样本数据的各个特征，维度为 $m$），如果我们忽略二分类问题中 $y$ 的取值是一个离散的取值（$0$ 或 $1$），我们继续使用线性回归来预测 $y$ 的取值。这样做会导致 $y$ 的取值并不为 $0$ 或 $1$。逻辑回归使用一个函数来归一化 $y$ 值，使 $y$ 的取值在区间 $(0,1)$ 内，这个函数称为 Logistic 函数（Logistic Function），也称为 Sigmoid 函数（Sigmoid Function）。

之后按照 Sigmoid 函数的形式求出 $g(z)$ 。

$$
g(z)=\frac{1}{1+e^{-z}}
$$

由于 Sigmoid 函数的定义域是 $(-\inf,\inf)$，而值域为 $(0,1)$ 。因此最基本的 LR 分类器适合对两类目标进行分类。

Logistic 函数当 $z$ 趋近于无穷大时， $g(z)$ 趋近于 $1$ ；当 $z$ 趋近于无穷小时，$g(z)$ 趋近于 $0$。Logistic 函数的图形如图 4.3 所示。



```py
import torch
import torch.nn as nn
from torch.autograd import Variable

import torchvision.datasets as dsets
import torchvision.transforms as transforms

# 超参数
input_size = 784
num_classes = 10
num_epochs = 10
batch_size = 50
learning_rate = 0.001

# TODO 想看下这里面是怎么实现稳定的下载的。
# MNIST Dataset (Images and Labels)
train_dataset = dsets.MNIST(root='./data',
                            train=True,
                            transform=transforms.ToTensor(),
                            download=True)
test_dataset = dsets.MNIST(root='./data',
                           train=False,
                           transform=transforms.ToTensor())

# Dataset Loader (Input Pipline)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)


# Logistic 回归模型
class LogisticRegression(nn.Module):
    def __init__(self, input_size, num_classes):
        super(LogisticRegression, self).__init__()
        self.linear = nn.Linear(input_size, num_classes)

    def forward(self, x):
        out = self.linear(x)
        return out


model = LogisticRegression(input_size, num_classes)

# 设定损失函数和优化器
criterion = nn.CrossEntropyLoss()  # 交叉熵损失
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

# 训练模型
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        images = Variable(images.view(-1, 28 * 28))  # 这个 images.view 是什么用法？相当于拉平吗？
        labels = Variable(labels)

        # 训练
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        if (i + 1) % 100 == 0:
            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f'
                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.item()))

# TODO 为什么没有 model.eval()？ linear_regression 的例子里有的。
# 测试模型
correct = 0
total = 0
for images, labels in test_loader:
    images = Variable(images.view(-1, 28 * 28))
    outputs = model(images)
    _, predicted = torch.max(outputs.data, 1)  # torch.max 嗯，好的。
    total += labels.size(0)  # 这个地方难道不是肯定是 1 吗？
    correct += (predicted == labels).sum()

print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))

# 保存模型
torch.save(model.state_dict(), 'model.pkl')
```

输出：

```
Epoch: [1/10], Step: [100/1200], Loss: 2.1895
Epoch: [1/10], Step: [200/1200], Loss: 2.1632
Epoch: [1/10], Step: [300/1200], Loss: 2.0083
Epoch: [1/10], Step: [400/1200], Loss: 1.9481
Epoch: [1/10], Step: [500/1200], Loss: 1.8151
略...
Epoch: [10/10], Step: [900/1200], Loss: 0.5133
Epoch: [10/10], Step: [1000/1200], Loss: 0.6117
Epoch: [10/10], Step: [1100/1200], Loss: 0.5545
Epoch: [10/10], Step: [1200/1200], Loss: 0.6839
Accuracy of the model on the 10000 test images: 87 %
```

简单的说明：

torchvision.datasets 里面有很多数据类型，里面有官网处理好的数据，比如我们要使用的 MNIST 数据集，可以通过 `torchvision.datasets.MNIST()` 来得到，还有一个常使用的是 `torchvision.datasets.ImageFolder()`，这个可以让我们按文件夹来取图片。

`torchvision.transforms` 里面的操作是对导入的图片做处理，比如可以随机取（50,50）这样的窗框大小，或者随机翻转，或者去中间的（50,50）的窗框大小部分等，但是里面必须要用的是 `transforms.ToTensor()`，这可以将 PIL 的图片类型转换成 Tensor。<span style="color:red;">如果是 opencv 的图片呢？如果是 bytes 格式的图片呢？其他的方式要怎么写？</span>

其中，`dsets.MNIST` 函数参数如下：

- `root`：数据集，存在于根目录 processed/training.pt 和 processed/test.pt中。
- `train`:True = 训练集，False = 测试集
- `download`：如果为 True，请从 Internet 下载数据集并将其放在根目录中。如果数据集已经下载，则不会再次下载。
- `transform`：接收 PIL 映像并返回转换版本的函数/变换。
- `target_transform`：一个接收目标并转换它的函数/变换。

其中，`DataLoader` 参数如下：

- `dataset`：加载数据的数据集。
- `batch_size`：加载批训练的数据个数。
- `shuffle`：如果为 True，在每个 Epoch 重新排列数据。
- `sampler`：从数据集中提取样本。
- `batch_sampler`：一次返回一批索引。<span style="color:red;">这个是什么？</span>
- `num_workers`：用于数据加载的子进程数。0 表示数据将在主进程中加载。<span style="color:red;">多子进程有用吗？效果怎么样？</span>
- `collate_fn`：合并样本列表以形成小批量。<span style="color:red;">这个是什么意思？一般什么时候用？</span>
- `pin_memory`：如果为 True，数据加载器在返回前将张量复制到 CUDA 固定内存中。<span style="color:red;">为什么要将张量复制到 CUDA 固定内存中？是还需要使用吗？</span>
- `drop_last`：如果数据集大小不能被 `batch_size` 整除，设置为 True 可删除最后一个不完整的批处理。如果设为 False 并且数据集的大小不能被 batch_size 整除，则最后一个 batch 将更小。<span style="color:red;">嗯嗯，是的，那么这个对训练有影响吗？影响大吗？</span>

`DataLoader` 默认 Load 进去的图片类型是 `PIL.Image.open` 的类型。<span style="color:red;">确认下这个地方。</span>


不是很清楚的地方：

- `transforms.ToTensor()` 是什么？起到什么作用？看书上说是吧 PIL 图片转换，转换成什么？补充下。
- processed/training.pt 和 processed/test.pt 格式的数据是什么？为什么要保存成 pt 格式？有什么好处吗？
- 想更多的知道 `DataLoader` 这个地方，是相当于 `generator` 吗？





# 相关

- 《深度学习框架 Pytorch 快速开发与实战》
