# 强化学习

强化学习:

- 如图：

<p align="center">
  <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/GrvGir0bwYpV.png?imageslim">
</p>

分类:

- 有监督学习：
  - Labeled 数据， 直接反馈，预测未知label数据
- 无监督学习 :
  - Unlabeled 数据，无反馈，寻找数据隐藏结构。
- 强化学习特点 :
  - 无监督数据， 只有奖励信号
  - 奖励信号不一定实时，大部分情况奖励信号滞后
  - 研究的非 i.i.d 数据， 时间序列
  - 当前的行为影响后续数据分布。



应用：

- 棋类游戏：alpha go
- Atari游戏
- 机器人控制
- 自动驾驶直升机 (http://heli.stanford.edu/)
- 自动驾驶
- 自动交易

如何建立强化学习数学模型：

- Model of uncertainty
  - Environment, actions, Agent knowledge
- Focus on decision making
- Maximize long-term reward

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/Wn6b0TKlOrTY.png?imageslim">
</p>


- 说明：
  - Reward奖励 $R _{\mathrm{t}}$
    - 标量函数
    - 奖励假设: 所有问题解决的目标都可以被描述成最大化累计奖励
  - History 历史 $H_{\mathbf{t}}$
    - 历史是观测、行为、奖励的序列: $\quad \mathrm{H}_{\mathrm{t}}=\mathrm{O}_{1}, \mathrm{R}_{1}, \mathrm{a}_{1}, \ldots, \mathrm{O}_{\mathrm{t}-1}, \mathrm{R}_{\mathrm{t}-1}, \mathrm{a}_{\mathrm{t}-1}, \mathrm{O}_{t}, \mathrm{R}_{t}, \mathrm{a}_{\mathrm{t}}$
  - State 状态  $S_{\mathrm{t}}$
    - 状态是所有决定将来的已有的信息，是关于历史的函数 $s_{\mathrm{t}}=f\left(\mathrm{H}_{\mathrm{t}}\right)$
  - Action 动作  $a_{\mathrm{t}}$


理解：

- 本质上时一个  Sequential Decision Making 序列决策
  - 目标 : 选择一定的行为系列 以最大化未来的总体奖励
  - 这些行为可能是一个长期的序列
  - 奖励可能而且通常是延迟的
  - 有时候宁愿牺牲即时（短期）的奖励以获取更多的长期奖励

举例：

- 如图：

<p align="center">
    <img width="40%" height="70%" src="http://images.iterate.site/blog/image/20200627/6RReykfTsqzK.png?imageslim">
</p>

- 说明:
  - Objective: 平衡 Cart-Pole
  - State: 角度，角速度，位置，速度
  - Action: 作用在 cart 上 F 的大小
  - Reward: +1 如果 pole 垂直

- 如图：Robot Locomotion


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/46ixtIW2U0WM.png?imageslim">
</p>

- 说明:
  - Objective: robot直立行走
  - State: 关节的角度与位置
  - Action: 作用在关节上的力矩
  - Reward: +1如果水平向前移动

- 如图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/6dA9Dc4OxAy6.png?imageslim">
</p>

- 说明:
  - Objective: 游戏得到高分
  - State: 游戏每时刻的输入画面 （可能不是一帧画面，是几帧画面）
  - Action: 游戏控制键：上下左右
  - Reward: 实时的游戏得分

- 如图：
<p align="center">
    <img width="40%" height="70%" src="http://images.iterate.site/blog/image/20200627/tSNsceNiTsR8.png?imageslim">
</p>

- 说明:
  - Objective: 战胜对手， 贏得棋局
  - State: 每颗棋子的位置
  - Action: 下一个棋子的位置
  - Reward: +1最终贏棋，0输棋。这个 reward 是非常延迟的，只有最后赢了才有 reward。



Markov Property:马尔科夫性

- A state $S_{t}$ is Markov iff $: P\left(S_{t+1} \mid S_{t}, S_{t-1}, \ldots, S_{1}\right)=P\left(S_{t+1} \mid S_{t}\right)$
  - The state captures all relevant information from the history.
  - Once the state is known, the history may be thrown away.
  - Transition Matrix: $P_{s s^{\prime}}=P\left(S_{t+1}=s^{\prime} \mid S_{t}=s\right)$ (finite state).


How to choose state to obtain a Markov Process?

- Example: Helicopter (位置，速度，角速度)
- State 选择不正确 或者选少了，: memory effect !!!
  - 如果，在牛顿系统中，只选择了 position 作为 state ，那么下一时刻的 position 无法推出，因为没有 v ，没有 a。



Markov Process 马尔科夫过程：

- 随机过程S_1，S_1...满足马尔科夫性。
- 即：
  - A Markov Process (or Markov Chain) is a tuple $\langle\mathcal{S}, \mathcal{P}\rangle$
    - $\mathcal{S}$ is a (finite) set of states
    - $\mathcal{P}$ is a state transition probability matrix, $\mathcal{P}_{s s^{\prime}}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s\right]$


举例：

- 如图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/Nj4qSFv24cJi.png?imageslim">
</p>

- 其中：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/yrdNP2G37w3i.png?imageslim">
</p>

- 轨迹例子:
  - C1 C2 C3 Pass Sleep
  - C1 FB FB C1 C2 Sleep
  - C1 C2 C3 Pub C2 C3 Pass Sleep
  - C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep


Markov Decision Process (MDP)马尔科夫价策过程：

- A Markov Decision Process is a tuple $\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma\rangle$
  - $\mathcal{S}$ is a finite set of states
  - $A$ is a finite set of actions
  - $\mathcal{P}$ is a state transition probability matrix. $\mathcal{P}_{\mathrm{ss}^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right]$
    - 注：这个是环境隐含的，因为，环境达到什么 $S_{t+1}$ 是环境自己的事情。
  - $\mathcal{R}$ is a reward function, $\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]$
  - $\gamma$ is a discount factor $\gamma \in[0,1]$
- 理解：
  - 环境信息就是：$P_{s s}^{a}$ 和 $R^{a}_{s}$
  - MDP 是绝大多数的 RL 问题的数学表示
  - 马尔科夫性

典型过程：

- $t=0$ 初始状态 $s_{0} \sim P\left(s_{0}\right)$
- while Not done:
  - Al根据 $s_t$ 选择action $a_t$
  - 环境给出 reward $r_{t} \sim R\left(. \mid s_{t}, a_{t}\right)$
  - 环境给 出下一时状态 $s_{t+1} \sim P\left(. \mid s_{t}, a_{t}\right)$
  - Al 接收 $r_{\mathrm{t}}$ 和 $\mathrm{s}_{\mathrm{t}+1}$

强化学习数学模型：


- RL Objective: 找出最仇策略 $\pi^{*}$ 使得累计回报最大
    $$
    \pi^{*}=\arg \max _{\pi} \mathbb{E}\left[\sum_{t \geq 0} \gamma^{t} r_{t} \mid \pi\right] \quad \text { with } s_{0} \sim p\left(s_{0}\right), a_{t} \sim \pi\left(\cdot \mid s_{t}\right), s_{t+1} \sim p\left(\cdot \mid s_{t}, a_{t}\right)
    $$
    - 即，maxmize 这个 累计回报的 expection，given 我们的不同的 policy
    - 其中：
      - Policy 策略 $\pi$，agent 根据 state 选择哪种 action。这个也就是我们要求的。注，与前面的 $\mathcal{P}_{\mathrm{ss}^{\prime}}^{a}$ 是不同的。
        - Deterministic Policy $\pi: S \rightarrow A$
          - 即固定的 action
        - stochastic Policy $\pi(\mathrm{a} \mid \mathrm{s})=\mathrm{P}\left[\mathrm{A}_{\mathrm{t}}=\mathrm{a} \mid \mathrm{S}_{\mathrm{t}}=\mathrm{s}\right]$
          - 根据 state 有一定的概率选择某种 action
      - 累计回报 $G_t$:（why discount?）
          $$
          G_{t}=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}
          $$
          - 即agent 的目标是累计回报。
          - 因为不确定以后的 回报 是什么，所以，加一个 discont $\gamma$。
            - 而且，从数学上，加了 一个 discont ，这样 G 不是 无穷大的数。

为了求解上面的 函数，需要定义两个 function：

- State-Value Function 价值函数。即评价我当前的 state 是多好或者多坏。
  - 评价基于策略 $\pi$ 下的 state 的好坏 (基于长期价值)
  - 表示从状态 $s$开始， 遵循策略 $\pi$ 能得到的收获的期望值
  - 计算
    $$
    \mathrm{V}_{\pi}(\mathrm{s})=\mathrm{E}_{\pi}\left[\mathrm{G}_{\mathrm{t}} \mid \mathrm{S}_{\mathrm{t}}=\mathrm{s}\right]
    $$
    - 说明：
      - 即：在 策略 $\pi$ 下，这个 state 的 value 是，基于当前 state 的累计回报的 expection。
  - 将 $G_t$ 拆开。
    $$
    v_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=\mathrm{s}\right]
    $$
    - 说明：
      - 即：写成了一个 期望套一个期望的形式。
      - 即：当前的 $v_{\pi}(s)$ 可以由下个时刻的 $v_{\pi}\left(S_{t+1}\right)$ 和当前的 $R_{t+1}$ 来得到。
- State-action Value Function 动作价值函数。也就是 q-function。
  - 评价共于策略 $\pi$ ， 在火态 $s$ 下执行动作 $a$ 的好坏
  - 表示在状态$s$下先执行动作$a$，然后遵循策略 $\pi$ 能得到的收获的期望值
  - 计算：
    $$\mathrm{q}_{\pi}(\mathrm{s}, \mathrm{a})=\mathrm{E}_{\pi}\left[\mathrm{G}_{\mathrm{t}} \mid \mathrm{S}_{\mathrm{t}}=\mathrm{s}, \mathrm{A}_{\mathrm{t}}=\mathrm{a}\right]
    $$
  - 将 $G_t$ 拆开。
    $$
    q_{\pi}(s, a)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) \mid S_{t}=s, A_{t}=a\right]
    $$


上面两个 function 之间的关系：

- 当我们想计算 $v_{\pi}(s)$ 时：
  - 如图
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/TtJmCo8E3aPc.png?imageslim">
    </p>
  - 即：
    $$v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s) q_{\pi}(s, a)$$
  - 说明：
    - 我们可以对在 $s$ 情况下的 所有的 $a$ 的情况的 $q_{\pi}(s, a)$ 求和。
- 当我们想计算 $q_{\pi}(s, a)$ 时：
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/c6g7hcfImudf.png?imageslim">
    </p>
  - 即：
    $$q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)$$
  - 说明：
    - 当我们对 环境的 $s$ ，给了一个 action 时，我们的环境会立即给我们一个 reward，也就是 $\mathcal{R}_{s}^{a}$，并且 环境根据 $\mathcal{P}_{s s^{\prime}}^{a}$ 有一定概率走到 $s^{\prime}$，那么，如果在 s' 的 value 是 $v_{\pi}\left(s^{\prime}\right)$，那么，就可以把 $q_{\pi}(s, a)$ 写成他们加和的形式。并且，加上一个 $\gamma$ 系数。

那么，上面的两个 fuction 自己和自己是什么关系呢？Bellman Expectation Equation

- 对于 $v_{\pi}(s)$：
  - 如图:

  <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/jdxDMXRQ0kz8.png?imageslim">
  </p>

  - 即，相当于把 $q_{\pi}(s, a)$ 带入上面的 $v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s) q_{\pi}(s, a)$：

      $$
      \begin{aligned}
      v_{\pi}(s)=&\sum_{a \in \mathcal{A}} \pi(a \mid s)\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right)
      \\=&\sum_{a \in \mathcal{A}} \pi(a \mid s)\mathcal{R}_{s}^{a}+\sum_{a \in \mathcal{A}} \pi(a \mid s)\left(\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right)
      \\=&\sum_{a \in \mathcal{A}} \pi(a \mid s)\mathcal{R}_{s}^{a}+  \gamma   \sum_{s^{\prime} \in \mathcal{S}}\sum_{a \in \mathcal{A}} \pi(a \mid s)\mathcal{P}_{s s^{\prime}}^{a}v_{\pi}\left(s^{\prime}\right)
      \\=&\mathcal{R}_{s}^{\pi}+ \gamma  \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s, s^{\prime}}^{\pi} v_{\pi}\left(s^{\prime}\right)
      \end{aligned}
      $$
  - 说明:
    - 第三行到第四行：令：
      - $\mathcal{P}_{s, s^{\prime}}^{\pi}=\sum_{a \in \mathcal{A}} \pi(a \mid s) \mathcal{P}_{s s^{\prime}}^{a}$
      - $\mathcal{R}_{s}^{\pi}=\sum_{a \in \mathcal{A}} \pi(a \mid s) \mathcal{R}_{s}^{a}$
  - 将上式写成矩阵形式：（为什么写成矩阵是这样的？）
    $$v_{\pi}=\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} v_{\pi}$$
  - 求解得：
    $$v_{\pi}=\left(I-\gamma \mathcal{P}^{\pi}\right)^{-1} \mathcal{R}^{\pi}$$
  - 即：
    - 已知我的策略，和环境的 reward function，以及环境的 $\mathcal{P}_{\mathrm{ss}^{\prime}}^{a}$，那么我在这个情况下的每个 状态的好坏，是可以算出来的。
      - 举例：相当于，我和对方的下棋的策略固定后，对于任何一种棋盘，我都可以知道当前棋盘的好坏。也不需要去跑一个 similation 或者不需要去下棋，直接就可以求出 棋盘的好坏。
        - 注意，这是下棋的策略是固定的情况下，才能估计棋盘的好坏。
- 对于 $q_{\pi}(s, a)$：
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/v8GCuxyKCSgd.png?imageslim">
    </p>
  - 即，相当于把 $v_{\pi}(s)$ 带入上面的 $q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)$：
    $$q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \sum_{a^{\prime} \in \mathcal{A}} \pi\left(a^{\prime} \mid s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)$$


上面的讨论是基于一个 固定的 policy 的情况下的。

那么，实际上我们是想找到一个 optimal 的 policy 的。

- Optimal State-Value function
  - 如图
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/WC4UlL10eYlE.png?imageslim">
    </p>
  - 即：
    $$
    V _{*}(s)=\max _{\pi} V_{\pi}(s)
    $$
  - 理解：
    - 即，如果固定了 policy ，那么我们可以求出一个 state 的 v，那么，那个 policy 最好呢？我们要让这个 v 最大的 policy。
  - 与之前类的，有：
    $$v_{*}(s)=\max _{a} q_{*}(s, a)$$
  - 疑问：
    - 为什么这个地方是 $q_{*}$ 呢？满足 $V \cdot(s)$ 的 $\pi$ 并不一定满足 $q_*$ 吧？（在后面有一个 MDP 的 Optimal Policy 定理，里面有说明了原因。）
- Optimal Action-Value function
  - 如图
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/0S0YXo9FgJVB.png?imageslim">
    </p>
  - 即：
    $$
    q _{*}(s, a)=\max _{\pi} q_{\pi}(s, a)
    $$
  - 理解：
    - 即，如果固定了 policy ，那么我们可以求出基于一个 state 和 action 的 q，那么，那个 policy 最好呢？我们要让这个 q 最大的 policy。.
  - 与之前类似的，有：
    $$q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)$$


同样的，有 Bellman Optimality Equation

- $v_{*}(s)$
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/U7mcbHBTm2E9.png?imageslim">
    </p>
  - 即：
    $$v_{*}(s)=\max _{a} \mathcal{R}_{s}^{2}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_*\left(s^{\prime}\right)$$
- $q_{*}(s, a)$
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200627/HU5jhkcpKLDl.png?imageslim">
    </p>
  - 即：
    $$q_{*}(s, a)=\mathcal{R}_{s}^{2}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \max _{a^{\prime}} q_*\left(s^{\prime}, a^{\prime}\right)$$
  - 说明：
    - 这个是整个强化学习的核心，Q-learning 就是求解这个。
- 说明：
  - 这两个是求解，最优策略下，v 和 q 的好坏。

那么，什么是最优策略，怎么衡量策略的好坏：

- 最优的策略
  - $\pi \geq \pi^{\prime}$ if $v_{\pi}(s) \geq v_{\pi^{\prime}}(s), \forall s$
    - 即，这个策略上，所有的 state 的估值都要比别的策略的要好。
- 对于 MDP（Markov Decision Process） ，有一个非常重要的定理：Optimal Policy 定理:
  - 对于任意的 马尔科夫决策过程：
    - There exists an optimal policy $\pi_{*}$ that is better than or equal to all other policies, $\pi_{*} \geq \pi, \forall \pi$
      - 所以，是肯定会有的。
    - All optimal policies achieve the optimal value function. $v_{\pi_*}(s)=v_{*}(s)$
    - All optimal policies achieve the optimal action-value function. $q_{\pi_{*}}(s, a)=q_{*}(s, a)$
  - 对于上面这个定理，是已经被证明了的。这个就是 RL 的解的存在性的证明。是比较核心的。
- 那么怎么得到这个 policy 呢？
    $$\pi_{*}(a \mid s)=\left\{\begin{array}{cc}1 & \text { if } a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} q_{*}(s, a) \\ 0 & \text { otherwise }\end{array}\right.$$
  - 即：
    - 如果我们已经找到了这个 $q_{*}(s, a)$，那么，我们就得到了一个表，每次遇到某个 state，我们就在表中查找令 $q_{*}(s, a)$ 最大的 action，这些 action 就是我们的 policy。


强化学习的算法 大致分类：

- MDP Problems
  - MDP Planning: 已知模型，找到最优策略
    - 就是，已经知道 $\mathcal{R}$ 和 $\mathcal{P}_{\mathrm{ss}^{\prime}}^{a}$ 了。
  - MDP Learning: 未知模型， 理解环境，尝试找到最优策略

- RL Algorithms
  - Value Based
    - 就是，把 $q_{*}(s, a)$ 找到，这样就直接查到我的 policy。
  - Policy Based
    - 由于 $\pi_{*}(a \mid s)$ 也是一个概率函数，我们直接学习这个概率函数，吧这个概率函数找到。
  - Actor-Critic
    - 把两个结合起来，actor 就是 policy ，执行，critic 就是评价，就是 value based。

- RL Algorithms
  - State and Action space is discrete and small
  - State and Action space is large: function approximation, Deep RL
    - 比如，平衡车 的 action 是连续的力

