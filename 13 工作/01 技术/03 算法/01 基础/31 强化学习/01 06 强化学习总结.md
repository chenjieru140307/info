# 强化学习总结

注意：

- 业界使用的更放心的是 policy gradient 梯度策略，而不是 DQN deep Q network。
  - 因为 DQN ，如果样本不够多，网络可能是不收敛的。

（对于下面的这些，还要仔细思量）

一些要点：

- 强化学习的难点：
  - credit assignment problem
  - the exploration-exploitation dilemma
- 怎么定义强化学习？
  - 马尔可夫决策过程（如果不是马尔可夫决策过程怎么处理？）
- 怎么把“眼光”放长远？
  - discounted future reward（疑问，为什么 discount 的系数是相同的？）
- 怎么预估“未来收益"？
  - table-basedQ-learning 算法
- 状态空间太大怎么办？
  - 使用深度神经网络
- 如何实际应用 
  - “重演”策略

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/yanagrg8fKlm.png?imageslim">
</p>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/RYzvxCp5C4gD.png?imageslim">
</p>

说名：

- At each step $t$ the agent:
  - Executes action $A_{t}$
  - Receives observation $O_{t}$
  - Receives scalar reward $R_{t}$
- The environment:
  - Receives action $A_{t}$
  - Emits observation $O_{t+1}$
  - Emits scalar reward $R_{t+1}$（疑问，为什么 reward 是 scalar 的？）
- $t$ increments at env. step


举例：

- 如图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/JBHm2INFFlxA.png?imageslim">
</p>

- 说明：
  - Atari Breakout 游戏
    - 三种动作：向左，向右以及开火（把球射出）
    - 状态：所处的位置，异幕上方状况等
    - 奖励：得分增减
- 问题：
  - 信用分配问题 (credit assignment problem) 
    - 击中砖块并且得分和前一时刻如何移动横杆没有直接关系
    - 前面某一时刻的横杆移动有作用
  - 探索-利用困境 （exploration-exploitation dilemma）
    - 比如，你已经有一个移动策略可以得到 50 分了。
    - 满足于得分还是去探索更多的可能性？（可能更好或者更坏）
- 马尔可夫决策过程 MDP
  - 操作者或智能体 ( agent) 
  - 状态（state）（比如横杆的位置， 球的位置， 球的方向，当前 环境中的砖块等等）
  - 动作（actions）（比如向左或何右移动横杆）
  - 奖励（reward）（比如分数的增加）
  - 策略（policy）： 在state下采取行动的原则

强化学习的难点：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/LzfPzh4gDqzV.png?imageslim">
</p>

马尔可夫决策过程：

- 马尔可夫决策过程：
    $$s_{0}, a_{0}, r_{1}, s_{1}, a_{1}, r_{2}, s_{2}, \dots, s_{n-1}, a_{n-1}, r_{n}, s_{n}$$
- 一个马尔科夫决策过程，它对应的奖励总和
    $$
    R=r_{1}+r_{2}+r_{3}+\ldots+r_{n}
    $$
- $t$ 时刻的未来奖励
    $$
    R_{t}=r_{t}+r_{t+1}+r_{t+2}+\ldots+r_{n}
    $$
- 无法确定在两次采取相同动作，agent能够获得相同的奖励，未来有不确定性，计算 “打折的未来类励”：（为什么这个折扣是相同的？）
    $$
    R_{t}=r_{t}+\gamma r_{t+1}+\gamma^{2} r_{t+2} \ldots+\gamma^{n-t} r_{n}
    $$
- 说明：
  - 只要是时序的，就需要考虑折扣的问题。
  - $\gamma$ 是一个 0 到 1 的值，使得我让更少地考虑哪 些更长远未来的奖劲
- 此时，$\mathrm{R}_{\mathrm{t}}$ 可以用 $\mathrm{R}_{\mathrm{t}+1}$ 来表示，写成递推式
    $$
    R_{t}=r_{t}+\gamma\left(r_{t+1}+\gamma\left(r_{t+2}+\ldots\right)\right)=r_{t}+\gamma R_{t+1}
    $$

对于 Q-learning：

- $Q(s, a)$ 函数，用来表示智能体在 $s$ 状态下采用  $a$ 动作并在之后采取最优动作条件下的 打折的 未来奖励
    $$
    Q\left(s_{t}, a_{t}\right)=\max R_{t+1}
    $$
- 说明：
  - 注意，是在当前采用了 $a$ 动作，然后的动作假设都使用的是最优动作。
  - 其实，$Q(s, a)$ 是指，在 $s$ 和 $a$ 之后，能拿到的打折奖励的上限。
- 所以，如果在 $s$ 的时候，有 $a1,a2,a3$ 等等选择，那么，我在 $s$ 时候的最优选择，就是一组 $(s, a)$，它们 对应的上限 $\max R_{t+1}$ 是最大的。也就是说，这个 $a$ 就是我们在 $s$ 情况下的最优选择，这也就是我们的选择策略：
    $$
    \pi(s)=\arg \max _{a} Q(s, a)
    $$
- 我们可以采用与打折的未来奖励相同 的方式定义这一状态下 的Q函数
    $$
    Q(s, a)=r+\gamma \max _{a \prime} Q\left(s^{\prime}, a^{\prime}\right)
    $$
- 说明：
  - 这个就是 贝尔曼公式。
  - 即：当前 $s,a$ 情况下的 $Q(s,a)$ 等于 ，我执行了 $a$ 之后拿到的 reward ，和 我后续都采用最优策略 $a'$ 的时候，执行 $a'$ 得到的打折奖励，也就是 $Q(s',a')$ 的和。
- Q-learning 的核心思想：
  - 根据贝尔曼公式，我们就可以达代地近似 Q 函数。 
- 最简单的情况下，我们可以采用一种填表的方式学习 Q-函数。这个表包含状态空间大小的行，以及动作个数大小的列。
  - 流程：
    - 01 initialize Q[ num_states, num actions ] arbitrarily 
    - 02 observe initial state $s$
    - 03 repeat 
    - 04 $\quad$ select and carry out an action $a$ 
    - 05 $\quad$ observe reward $r$ and new state $s^{\prime}$
    - 06 $\quad$ $Q[s, a]=Q[s, a]+\alpha\left(r+\gamma \max _{a^{\prime}} Q\left[s^{\prime}, a^{\prime}\right]-Q[s, a]\right)$
    - 07 $\quad$ $s=s^{\prime}$
    - 08 until terminated
  - 说明：
    - 其中 $\alpha$ 是在更新 $Q[s, a]$ 时，调节旧 $Q[s,a]$ 与新 $Q[s, a]$ 比例的学习速率
      - 如果 $\alpha=1$，$\mathrm{Q}[\mathrm{s}, \mathrm{a}]$ 就被消掉，这时更新方式就完全与贝尔曼公式相同

疑问：

- 为什么这样做，会收敛？

举例：

- 目的：
  - 利用无监督训练来学习未知环境的 agent，找到最佳策略，从任一房间走到 5 号房间。
- 环境：
  - 一幢建筑里有 5 个房问，房问之间通过门相连。
  - 我们将这五个房问 按照从 0 至 4 进行编号，且建筑的外围可认为是一个大的房间，编号为 5.
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/Ug6FNw94HgmF.png?imageslim">
    </p>
  - 将房间作为图的节点，两个房间若有门相连，则相应节点间对应一条边，如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/5ry1zupzVP5B.png?imageslim">
    </p>

- 算法：
  - 算法流程：
    - 01 给定参数 $\gamma$ 和 reward 矩阵 $R$
    - 02 令 $Q:=0$
    - 03 For each episode:
    - 04 $\quad$ 随机选择一个初始的状态 s.
    - 04 $\quad$ 若未达到目标状态，则执行以下几步
    - 06 $\quad$ $\quad$ 在当前状态 s 的所有可能行为中选取一个行为 a.
    - 07 $\quad$ $\quad$ 利用选定的行为 a，得到下一个状态 $\widetilde{s}$
    - 08 $\quad$ $\quad$ 按照 $Q(s, a)=R(s, a)+\gamma \cdot \max _{\tilde{a}}\{Q(\tilde{s}, \tilde{a})\}$ 计算 $Q(s, a)$
    - 09 $\quad$ $\quad$ 令 $s:=\widetilde{s}$
  - 说明：
    - Agent 利用上述算法从经验中进行学习。每一个 episode 相当于一个 training session.
    - 在一个 training session 中，agent 探索外界环境，并接收外界环境的 reward，直到达到目标 状态。
    - 训练的目的是要强化 agent 的“大脑”(用 Q 表示)。训练得越多，则 Q 被优化得更好。 当矩阵 Q 被训练强化后，agent 便很容易找到达到目标状态的最快路径了。 
    - 公式 $Q(s, a)=R(s, a)+\gamma \cdot \max _{\tilde{a}}\{Q(\tilde{s}, \tilde{a})\}$ 中的 $\gamma$ 满足 $0 \leq \gamma<1$ 。
      - $\gamma$ 趋向于 0 表示 agent 主要考虑 immediate reward,
      - $\gamma$ 趋向于 1 表示 agent 将同时考虑 future rewards。
  - 利用训练好的矩阵 Q，我们可以很容易地找出一条从任意状态 $s_{0}$ 出发达到目标状态的行为路径。流程：
    - 令当前状态 $s:=s_{0}$
    - 确定 $a,$ 它满足 $Q(s, a)=\max _{\tilde{a}}\{Q(s, \widetilde{a})\}$
    - 令当前状态 $s:=\widetilde{s}(\widetilde{s} \text { 表示 } a$ 对应的下一个状态).
    - 重复执行步 2 和步 3 直到 s 成为目标状态。
- 过程：
  - 设定 reward：
    - 我们首先将 agent 置于建筑中的任意一个房间，然后从那个房间开始，让其走到建筑外，那是我们的目标房间 (即编号为 5 的房间)。
    - 为了将编号为 5 的房间设置为目标，我们为每一扇门 (即相应的边) 关联一个 reward 值: 直接连接到目标房间的门的 reward 值为 100，其他门的 reward 值为 0. 因为每一扇门都有两个方向（如由 0 号房问可以去 4 号房间，而由 4 号房间也可以返同 0 号房间)，因此每一个房间上指定两个箭头，且每个箭头上带有一个 reward 值。
    - 如图：
      <p align="center">
          <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/HzmQCMOwGagA.png?imageslim">
      </p>
    - 注意：
      - 编号为 5 的房问有一个指向自己的箭头，其 reward 值为 100，其他直接指向目标房间的边的 reward 值也为 100. 
      - Q-learning 的目标是达到 reward 值最大的 state，因此，当 agent 到达目标房间后将永远停留在那里。
    - 我们将每一房间 (包括 5 号房间) 称为一个“状态”，将 agent 从一个房问走到另外一个房间称为一个“行为”。
    - 可以画出下图：
      <p align="center">
          <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/s3qAAaYUe7LQ.png?imageslim">
      </p>
    - 说明：
      - 图中，一个“状态”对应一个节点，而一种“行为”对应一个箭头.
    - 我们以状态为行，行为为列，构建一个关于 reward 值的矩阵 $R$，其中， -1 表示空值 (相应节点之间没有边相连)
    - reward 矩阵：
      <p align="center">
          <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20200708/nq91U21FoXe9.png?imageslim">
      </p>
  - Q 矩阵迭代：
    - 参数设定：
      - 取学习参数 $\gamma=0.8$ 
    - episode 1：
      - 随机取初始房间为 $1$
      - 构建一个 Q 矩阵，由于刚开始时 agent 对外界环境一无所知，因此矩阵 Q 应初始化为零矩阵。
          <p align="center">
              <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20200708/elHXzgXYcECO.png?imageslim">
          </p>
      - 此时，观察矩阵 $R$ 的第二行 (对应房间 1 或状态1)，它包含两个非负值，即当前状态 1 的下一步行为有两种可能: 转至状态 3 或转至状态 5。
      - 随机地，我们选取转至状态 5.
      - 想象一下，当我们的 agent 位于状态 5 以后，会发生什么事情呢？观察矩阵 $R$ 的第 6 行 (对应状态 5)，它对应三个可能的行为：转至状态 1, 4 或 5。
      - 则，根据 $Q(s, a)=R(s, a)+\gamma \cdot \max _{\tilde{a}}\{Q(\tilde{s}, \tilde{a})\}$ ，我们有：
          $$\begin{aligned}
          Q(1,5) &=R(1,5)+0.8 * \max \{Q(5,1), Q(5,4), Q(5,5)\} \\
          &=100+0.8 * \max \{0,0,0\} \\
          &=100
          \end{aligned}$$
      - 现在状态 5 变成了当前状态。
      - 因为状态 5 即为目标状态，故一次 episode 便完成了。
      - 至此，agent 的“大脑”中的 Q 矩阵刷新为：
          <p align="center">
              <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20200708/KaGvfWrhkhgM.png?imageslim">
          </p>
    - episode 2：
      - 随机选取一个初始状态，这次我们选取状态 3
      - 观察矩阵 $R$ 的第四行 (对应状态 3)，它对应三个可能的行为：转至状态 1，2 或 4。
      - 随机地，我们选取转至状态 1。
      - 因此观察矩阵 $R$ 的第二行 (对应状态 1)，它对应两个可能的行为： 转至状态 3 或 5。根据 $Q(s, a)=R(s, a)+\gamma \cdot \max _{\tilde{a}}\{Q(\tilde{s}, \tilde{a})\}$，我们有
        $$\begin{aligned}
        Q(3,1) &=R(3,1)+0.8 * \max \{Q(1,3), Q(1,5)\} \\
        &=0+0.8 * \max \{0,100\} \\
        &=80
        \end{aligned}$$
      - 注意：
        - 上式中的 $Q(1,5)$ 用到了图 9 中的刷新值。
      - 此时，矩阵 $Q$ 变为
        <p align="center">
            <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20200708/V8KiN2K8bl3B.png?imageslim">
        </p>
      - 现在，状态 1 变成了当前状态。
      - 因为状态 1 还不是目标状态，因此我们需要继续往前探索。
      - 状态 1 对应三个可能的行为：转至状态 3 或 5。
      - 假定我们幸运地选择了状态 5.
        <p align="center">
            <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/q58X4K88huRt.png?imageslim">
        </p>
      - 此时，同前面的分析一样，状态 5 有三个可能的行为：转至状态 1，4 或 5。根据 $Q(s, a)=R(s, a)+\gamma \cdot \max _{\tilde{a}}\{Q(\tilde{s}, \tilde{a})\}$，我们有：
        $$\begin{aligned}
        Q(1,5) &=R(1,5)+0.8 * \max \{Q(5,1), Q(5,4), Q(5,5)\} \\
        &=100+0.8 * \max \{0,0,0\} \\
        &=100
        \end{aligned}$$
      - 注意，经过上一步刷新，矩阵 Q 并没有发生变化
      - 因为状态 5 即为目标状态，故这一次 episode 便完成了。
      - 至此，agent 的“大脑”中的 Q 矩阵刷新为：
        <p align="center">
            <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20200708/Ys6WWxUwd7Ab.png?imageslim">
        </p>
    - 若我们继续执行更多的 episode，
      - 矩阵 Q 将最终收敛为：
        <p align="center">
            <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20200708/EjTUVgeh8mXs.png?imageslim">
        </p>
      - 对其进行规范化，每个非零元素都除以矩阵 Q 的最大元素 (这里为 500)，可得：（这里省略了百分号）（一定要进行规范化吗？）
        <p align="center">
            <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20200708/X5hBEDMuJM96.png?imageslim">
        </p>
      - 此时，如图：
        <p align="center">
            <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/Y3CzROmfDXiq.png?imageslim">
        </p>
  - 使用 Q 矩阵寻找最优路径：
    - 从 2 为初始状态，利用 Q，可得：
      - 从状态 2，最大 Q 元素值指向状态 3
      - 从状态 3，最大 Q 元素值指向状态 1 或 4 (这里假设我们随机地选择了 1)
      - 从状态 1，最大 Q 元素值指向状态 5
    - 因此最佳路径的序列为 2-3-1-5。


Deep Q Network：

- 缘由：
  - 当我们使用连续的 4 个游戏画面作为状态时，此时，若屏幕大小 84*84， 每个像素点有 256 个灰度值， 则，共有 $256^{(84 * 84 * 4)}$ 约 $10^{67970}$ 种可能的秋态，此时，如果使用 Q-table，则 Q-table 有 $10^{67970}$ 行，且非常稀疏（有很多状态遇不到！ ! ）
  - 因此，可以用一个神经网络对 Q-函数进行建模，来模拟 Q-table 的作用：
    - 可以根据一个状态，来输出所有 a 对应的得分。

举例：

- DeepMind论文中使用的Q网络：
- 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200708/pslhki5Jym6M.png?imageslim">
    </p>
- 说明：
  - 没有池化层！ !
    - 池化层会带来位置不变性，会使得网络对于物体位置不敏感，从而无法有效的使别游戏中球的位置。
- 由于，我们想得到的是一个 Q value，也就是一个值，所以，这是一个回归问题，它的 loss function 如下：
    $$L=\frac{1}{2}[\underbrace{r+m a x_{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)}_{\text {target }}-\underbrace{Q(s, a)}_{\text {prediction }}]^{2}$$
- 说明：
  - 标准答案是 $r+m a x_{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)$，网络输出的是 $Q(s, a)$。
    - 这个标准答案并不是一定能拿到的。
- 训练过程中的技巧：
  - 最重要的技巧是经验回放 (experience replay)
    - 在玩游戏的过程中，所有经历的都被记录起来。
    - 训练神经网络时，我们从这些记录的中随机选取一些 mini-batch 作为训练数据训练，而不是按照时序地选取一些连续的。
      - 因为：按时序选取，训练实例之间相似性较大，网络很容易收敛到局部最小值。
- Exploration-Exploitation
  - Q-learning 算法尝试解决信用分配问题
    - 通过Q-learning，奖励被回馈到关键的决策时刻，使用了 $\gamma$ 来 discont 未来的奖励。
  - 依旧存在 探索-利用困境
    - 在游戏开始阶段，Q-table 或Q-network是随机初始化的。它给其的 $Q$ 值最高的动作是完全随机的，智能体表现出的是随机的“探索”。
    - 当 Q-函数收敘时，随机“探索”的情况减少。所以，Q-learning中包含“探索”的成分。但是这种探索是“贪心”的，它只会探索当前模型认为的最好的策略。
  - 一种简单修正技巧：（有没有别的解决方法？）
    - $\epsilon$ -贪心探索
      - 以 $\epsilon$ 的概率选取随机的动作做为下一步动作，$1-\epsilon$ 的概率选取分数最高的动作
      - DeepMind 的系统中，$\epsilon$ 随着时间从 1 减少到0.1。这意味着开始时，系统完全随机地探索状态空间，最后以固定的概率探索。
- DeepMind 还使用了一系列其他的技巧，比如：目标网络、误差截断、回馈截断。
- 新的论文进展包括 Double Q-learning, Prioritized Experience Replay, Dueling Network Architecture, extension to continuous action space



Policy Gradient：

- 我们首选PG算法是因为它是端到端(End to end)的：
  - 有一个显式的策略，和一个能够直接优化期望回报（expected reward）的规范的方法。
- 好的，作为例子，我们将会从零开始，在像素级别，使用一个深度神经网络，并用PG来学习去玩儿ATARI的游戏（Pong！）。所有这些事情只需要130行的Python就能搞定，并且只用到了numpy库。让我们试试吧！


Model-Free RL：

- PG policy gradient，
  - 不关心具体的 Q，只是期望 reward 最大化。
  - 有点像判别模型
    - 判别模型，关心 p(y|x) 如 SVM
- DQN
  - 关心的是最好的 Q
  - 有点像生成模型
    - 生成模型，关心 p(x,y) 如 Naive Bayes
- Actor-Critic
  - 有点像 PG 于 Q learning 的结合，使用 $Q^*(s,a)$ 做一些事情，让 PG 的方差不要那么大。







PG 策略梯度：


- 策略网络（Policy Network）
  - “策略网络”这个概念。它能作为我们的“玩家”（或者“代理”，即agent）。这个网络能够以当前的游戏状态为输入，并且能够决定我们应该采取什么动作（向上或者向下）。
  - 为了计算方便，我们使用了一个两层神经网络，接受原生的图像像素作为输入（一共100,800个数（210*160*3）），然后产生一个数字来表示“向上移动挡板”的概率。
    - 通常情况下我们会用“stochastic”策略，就是说我们只产生一个向上移动的概率值。每一次迭代我们都会从这个概率分布去采样（就好像投掷一枚不均匀的硬币）来决定到底是向上还是向下。
    - 注意：这个产生出来的值只是一个向上的概率，而不是说当这个值大于某个阈值就向上，小于阈值就向下。真正使用的时候就是以这个概率值去抛硬币。也就是说几遍这个概率告诉你百分之90要向上，你也是有可能采样获得“向下”这个决策的。
    - 这么做的原因我们会在讲解训练算法部分仔细说明。



训练过程：

- 首先我们初始化策略网络的 W1 ， W2 然后进行100次Pong的游戏（我们称这些策略叫做rollouts”）。
- 假设每一次游戏由200帧组成，所以我们一共要进行20,000次“向上”或者“向下”的决定，而且对于每个决定，我们都能够知道梯度，这个梯度则告诉了我们想要在未来做出更好的决策应该做出怎样的调整。现在只剩下去对我们做的每个决策标记上是“好”还是“坏”了。
- 例如我们赢了12场游戏，输了88场游戏。
  - 我们将会用在赢的游戏过程中做出的200*12=2400个决策做出“正”的更新（对采样得到的动作赋值+1的梯度，然后进行BP算法，参数更新将会鼓励我们所采取的这些动作）。
  - 然后我们会将其他在输掉的游戏中所做的00*88=17600个决策标记为“负”的更新（不鼓励采取这些动作）。
- 然后网络将会逐渐偏向做出有用的动作，而减少做出无用的动作。
- 然后我们再用我们轻微修正过的新的策略网络进行100场游戏，然后再去轻微地更新策略网络。

即：

策略梯度：让动作执行一会儿，看看什么动作将会产生高回报，那么就提高这部分动作在当前情况下出现的概率。


一个疑问：

- 如果你仔细想想这个过程，你会发现一些有趣的事情。比如说如果我们在第50帧时做了一个“正确”的动作（比如将球正确地反击回去），但是在第150帧的时候却丢了球该怎么办？如果此时每一个动作都被标记成“不好”的动作（因为我们丢了球），那么会不会在第50帧的动作也会被压制？是的，它会被抵制。然而其实你可以考虑一下当游戏进行了成千次甚至上百万次时，虽然你第一次正确的反击本应让你偏向赢得游戏，但平均来说，正确更新的数量还是要大于错误更新的数量的，所以最终你的策略将会学习到做出正确的动作。（这个地方没看懂？）





在有监督学习任务中：

- 在最简单的有监督学习任务中，目标函数是最大化 $\sum_{i} \log p\left(y_{i} \mid x_{i}\right)$ ，其中 $x_{i}$ 和 $y_{i}$ 是训练样本（比如说是图片和它们的标签）。策略梯度与有监督学习除了这两点以外是完全一样的：1)我们没有正确的标签 $y_{i}，所以我们用网络看到 $x_{i}$ 时采样的动作作为“假标签”来代替。2）我们基于最终的结果（译注：赢或输）来调整基于多个样本相乘的损失函数（译注：因为最终的损失函数会由多个样本相乘组成，参见极大似然估计MLE），就是说，我们要对有效的动作提高它的log概率，对无效的动作降低它的log概率。综上所述，我们的损失函数现在应该是这样：$\sum_{i} A_{i} \log p\left(y_{i} \mid x_{i}\right)$ ，其中 $y_{i}$ 是我们采样得到的动作， $A_{i}$ 是一个被称为“advantage”的数。比如在Pong例子中，当我们经过整个episode最终赢得了胜利 $A_{i}$ 就是+1，反之如果我们输了就是-1.这能保证我们动作的log概率在好结果中会被最大化，坏结果中会被最小化。所以说，增强学习完全就是有监督学习，只不过区别在于数据集是在不断变化的（每次都会有不同的episode），然后与"advantage"相乘，然后我们每次采样得到的数据集只会做一次（或者少量的）更新。 （译注：与有监督学习相比，RL重点在于在整个episode结束前，是不能确定每个标签的。所以说，产生一个episode->更新策略网络->再基于最新的策略网络产生新的episode->循环进行。）
- 对于 $A_{i}$，更加通用的“advantage”函数
  - 我之前说过要多讲一讲“回报”。目前为止，我们去评价一个动作的好坏都是依据最终会不会赢得游戏。一个更加通用的增强学习的设计是我们将会在每一个时间步都获得一个回报 $r_{t}$。一个常用的选择就是“discounted reward”。所以上图的最终回报（eventual reward）就会变成 $R_{t}=\sum_{k=0}^{\infty} \gamma^{k} r_{t+k}$。其中， $\gamma$ 是一个 $0 \sim 1$ 之间的数，称作是discount factor(取值比如说0.99)。这个表达式说明，我们对于一个采样动作的鼓励强度，是后续所有回报的加权求和决定的，但是后面回报的重要性是以指数级衰减的。在实践中，标准化这些回报也非常重要。比如说我们要计算所有100场Pong游戏中的20,000个动作的回报 $R_{t}$。一个比较好的做法就是在把这些数据送到BP算法之前，对这些回报做一下标准化的操作（比如通过减均值除标准差的方法）。这样我们总能大约鼓励一半的动作、压制一半的动作。从数学上来说，这也可以被认为是一种控制策略梯度的方差的一个技巧。






推导策略梯度：

- 下面我会大概说一说策略梯度是怎样从数学中得到的。
- 策略梯度是一个更加广义的概念——“score function gradient estimator”的特殊形式。
- 通常的情况下我们需要计算形如 $E_{x \sim p(x \mid \theta)}[f(x)]$ 的表达式，比如说计算一个标量值函数(scalar valued score function)  $f(x)$ 服从以 $\theta$ 为参数的概率分布 $p(x ; \theta)$ 的期望。提示： $f(x)$ 可以作为我们的回报函数（或者更通用地说，可以作为advantage函数），而 $p(x)$ 是策略网络，也就是给定任意图象 $I$ 后对动作 $a$ 的分布函数 $p(a \mid I)$ 。那么，我们感兴趣的是我们应该如何通过参数 $\theta$ 来修正这个分布，使得它能对样本有着更高的得分，而这个得分是由 $f$ 决定的（即，我们应该如何修改网络的参数才能让动作样本得到更高的分数）。我们有下面的公式：

$$\begin{aligned}
\nabla_{\theta} E_{x}[f(x)] &=\nabla_{\theta} \sum_{x} p(x) f(x) \\
&=\sum_{x} \nabla_{\theta} p(x) f(x) \\
&=\sum_{x} p(x) \frac{\nabla_{\theta} p(x)}{p(x)} f(x) \\
&=\sum_{x} p(x) \nabla_{\theta} \log p(x) f(x) \\
&=E_{x}\left[f(x) \nabla_{\theta} \log p(x)\right]
\end{aligned}$$

- 译注：经过上面的公式，我们想要计算 $\nabla_{\theta} E[f(x)]$，不再需要真的去计算带有函数 $f(x)$ 的梯度了。只需要 1) 按照某个概率分布采样， 2) 直接计算 $E_{x}\left[f(x) \nabla_{\theta} \log p(x)\right]$ 来代替即可。这么做的好处是我们可以定义一个非常复杂的打分函数 $f(x)$，甚至这个函数可以不可微，因为我们在后续的求导中，完全没有这个函数的参与，只要概率分布可微就行了。所以，在实际应用中， $f(x)$ 常常是一个复杂的函数，而 $p(x)$ 则可以是一个由神经网络组成的策略网络等模型，因为神经网络一般是可微的


说人话就是：我们有某个概率分布 $p(x ; \theta)$（为了简便所以写成 $p(x)$），并且可以从中进行采样（比如说从一个高斯分布）。对于每一个样本，我们可以对其用评分函数(score function) $f$ 打分，即输入样本到函数中，得到一个标量值作为分数。这个等式告诉了我们应该如何调整分布（由参数 $\theta$ 控制）才能得到用 $f$ 评价的更高的分数。特别地，它像是在说：抽一些样本 $x$，然后评价一下它们的分数 $f(x)$，然后对每个 $x$ 再计算一下第二项 $\nabla_{\theta} \log p(x ; \theta)$。第二项的内容是啥？它是一个向量：也就是能够知道我们在参数空间应该如何调整才能使 出现的概率尽可能大。换句话说，如果我们有在 $\nabla_{\theta} \log p(x ; \theta)$ 的方向有连续不断的 $\theta$，我们将会看到产生 $x$ 的概率将会稍稍增加。在回顾一下公式，它还告诉我们我们要把这个方向与那个标量打分函数(scalar-valued score) $f(x)$ 相乘。这会使得具有更高得分的样本对概率密度的“拖拽”力度要比分数低的样本大一些。所以如果我们基于多个从 $p$ 中采样的样本去更新概率密度函数，我们将会在想得分更高的样本方向多偏离一些，这能够让得分更高的样本更有可能被采样出来。

<p align="center">
  <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200710/hpn5DE2cluYQ.png?imageslim">
</p>

- 说明：
  - 一张关于分数梯度函数的可视化图。
  - 左：一个高斯分布，在其中采样了几个样本（蓝色点）。在每一个蓝色点上，我们也画出了对应高斯均值参数的 log 概率的梯度（译注：即 $\nabla_{\theta} \log p(x ; \theta)$）。箭头指向了想要使分布更容易采样到样本的调整方向的均值。
  - 中：一些打分函数对样本给出了 -1 分，而在一些小区域中给出了 +1 分（注意此处不必须是一个可微的标量值函数）。现在箭头被染色了，因为在更新中包含了乘法，我们会将所有绿色和红色的箭头分别取平均。
  - 右：经过参数更新之后，绿色的箭头和相反的红色箭头（译注：与 -1 相乘表示取向量的反方向）会将分布向左下角轻轻推动。正如我们所希望的一样，现在再从这个分布中取值，将会更容易得到更高的分数。



希望我讲明白了这与增强学习之间的连接。我们的策略网络就是让我么能够采样出一系列比其他动作更好的动作（通过advantage函数判断好坏）。这些数学公式的片段告诉我们了一种能够修正策略参数的方法就是先让动作执行一会儿，然后计算采样的动作的梯度，乘上它们的得分，然后将他们加起来，这就是上面做的事情。


Pong：

- 使用了OpenAI Gym的ATARI2600 Pong游戏。
- 训练了一个包括了200个隐层单元的两层策略梯度网络，并且用RMSProp算法在batch大小为10的episodes上训练（每一个episode都包含了几场游戏，因为比赛最高成绩就是21分）。
- Episode的数量共计8,000左右，算法大约玩儿了200,000场Pone的游戏（非常多！对不？），大约做了800次梯度的更新。

学习到的权重
- 经过预处理，我们输入的都是80x80大小的图片（当前帧减去上一帧。译注：保留一部分运动信息）。
- 我们现在取 W1 的每一行，把他们展开成80x80的大小然后画出来（译注：此处可能跟之前提到的210x160x3的结构不太一样。这里的输入图片单张只存为80x80=6400，然后传输到200个神经元中，所以矩阵 W1 的维度是6400x200）。下图表示了40个（共计200个）神经元的可视化结果。白色的像素表示正值，黑色的像素表示负值。我们会发现有几个神经元被调整出了特定的球的运动路径，并被编码成了“黑白相间”的线段。由于球只能在一点处被击中，所以这些神经元能处理多个任务并且
会在球沿着线的多个位置“激发”（译注：此处的激发应该不是指击球。而是说当球沿着这些路径的时候，这些神经元将会活跃起来，处于激发的状态）。“黑白相间”很有趣，因为在球运动的过程中，神经元像sin函数一样波动是由于使用了ReLU作为激活函数，它会在轨迹的不同位置上离散式地激活（译注：ReLU在x大于0的时候是线性，小于0时就是0，所以不是随时激活）。图片看起来有一些噪声，我觉得或许用上L2正则效果会好一些（译注：加上L2正则项能一定程度上避免过拟合，即避免了图像“记住”一些特例，更容易分辨一些“普遍的特征”。这些特例可能就会产生图像上的噪声）。

<p align="center">
  <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200710/U5uV4bcndgw8.png?imageslim">
</p>

策略梯度玩游戏与人类的区别：

- 在实践种，我们通常会用某种方式来交流沟通这个任务，但是对于标准的增强学习来说，你只能假设一个回报函数然后从与环境的交互中进行探索。而当人类参与到Pong中时，如果没有明确的关于回报函数的知识（尤其是回报函数是静态但是随机的）时，人类在学习的过程中将会面临很多困难。但是策略梯度就则不关心这些，并且可能比人类做的还优秀。相似地，如果我们把各帧的像素随机地置换，对于人类来说很有可能玩儿不好，但是对于策略梯度来说甚至可能都感觉不出什么区别（如果它们用的都是全连接网络的话）
- 人类是带有大量先验知识的，比如基本的物理学（球会反弹而不会瞬间移动，也不太可能会突然停住，因为它带有一些速度）。和一些基本的心理学（AI对手“渴望”去赢，那就有可能用某种显而易见的策略去移动球等等）。而且你心中有“控制”挡板的这个概念，并且会反映在向上向下的按键上。与此相反，我们的算法从零开始但很快就让人印象深刻（因为它确实有效）和沮丧（因为我们并不知道它什么时候就失效了）
- 策略梯度是一种暴力搜索的方法，正确的动作最终会被发现并且固化到策略中。人类会构建一个强大又抽象的模型，然后用它做规划。在Pong中，我们能推理出对手比较慢，所以“将球高速反击”将会是一个比较好的策略，这可能会导致对手无法在足够的时间内碰到球。然而，我们也会最终将这策略以肌肉记忆的形式固化在身体里。例如，当我们想要学习一种新的需要移动的任务（比如说学习驾驶一辆手动变速的车）你在刚开始会需要经常思考，但是最终会变得不需要思考，自动就能进行了
- 策略梯度必须直接输入正回报，并且需要经常体验到正汇报，以便于它能慢慢地推动策略参数偏向能重复产生高回报动作的状态。而人类能在不需要直接体验的回报（译注：知道最终结果前）就能知道那些动作更容易获得高回报。我不用将我的汽车撞毁几百次才能缓慢地学会如何避免碰撞。

举例：

<p align="center">
  <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200710/rKim339ox6I1.png?imageslim">
</p>

- 说明：
  - 蒙特祖玛的复仇（ Montezuma's Revenge ）：一个对我们的增强学习算法来说非常复杂的游戏。玩家必须跳跃、攀爬、拿到钥匙然后打开门。计算机随机采样了几十亿次的移动，但是 99% 都是会掉下去摔死或者被怪物杀死。换句话说，它非常难以进入到奖励状态（译注：绝大部分的采样最终都是负回报，几乎没有正回报，所以没法训练）。

<p align="center">
  <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200710/L2YcaJLIHIX5.png?imageslim">
</p>

- 说明：
  - 另一个非常难的游戏 Frostbite ，人类能理解运动，一些东西可以接触，另外一些东西不能接触，游戏的目标是一块一块搭建一个冰屋。 Building Machines That Learn and Think Like People 是一个不错的关于这个游戏的分析和关于人与机器之间区别的讨论。



所以：

- 强项：
  - 一些，经常有回报信号，并且需要精准操控、快速反应的游戏，策略梯度能轻易打败人类。特别是那些不需要做长期规划的游戏很合适。这些短期的游戏的动作与回报之间的关系能够被算法轻易地“发掘”到，然后悄悄地被策略驱使着完善自己。
    - 这在我们开发的Pong的智能体中已经出现：它能学习到一种策略，等待球靠近然后在边缘将它接到，因为这样能以很高的速度将球传回去。我们的智能体多次用到了这个策略。深度Q学习在很多ATARI游戏上都用这样的方式打败了人类，比如说Pinball, Breakout等等。
- 弱项：
  - 距离构建一个抽象、并且具有丰富的游戏表示来帮助我们进行规划并且能快速学习的系统还差得远。
    - 比如，也许，一台计算机将会意识到一个像素数组是一把钥匙、一扇门，并且能意识到要捡起钥匙去开门。
    - 我们已经了解到这些算法使用了包里搜索的方法，也就是刚开始随机地游走，然后一定会跌跌撞撞地得到至少一次回报，然后再理想情况下，这个过程要重复很多次，直到策略分布通过调整它的参数能够做出合理的动作。我们也了解到这些问题对人类来说也是非常困难的。这就好像快速构建一个抽象模型——我们仅仅在研究中触及冰山一角（尽管很多然都在尝试）。这些抽象模型非常难（如果不是不可能的话）以明确地被评注，这也就是为什么我们最近在（无监督）生成模型和程序归纳(Program Induction)领域表现出极大兴趣的原因。

策略梯度的其它使用：

- 神经网络中的不可导运算
  - 我想提一个跟策略梯度玩儿游戏无关的应用：它能够让我们去设计并训练一个由不可导单元组成的神经网络。这个概念第一次由Williams于1992年介绍，并且最近在《Recurrent Models of Visual Attention》中用“hard attention”这个名字重新发展起来。这个模型用一系列低解析度的“foveal glances”来处理图像。特别是在每一次迭代中，RNN都会收到图像的一个小片段，然后随机采样一个下一次会看到的位置。比如说，RNN有可能正在看(5, 30)这个位置，然后看到了图像的一个小片段，然后决定去看(24, 50)这个未知，以此类推。这个想法有个问题就是这将会产生一个“下一次看哪里”的分布，然后从这个分布采样一个位置。但不幸的是，这个操作是一个不可导的操作，因为我们不知道如果采样得到其他的位置将会有什么变化。具体一些地说，考虑一个神经网络，接受一些输入产生一些输出：
    <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200710/rojgjxlPxmVb.png?imageslim">
    </p>
  - 我们会发现大部分的箭头（蓝色的）像往常一样都是可导的，但是一些变换形式有可能包含一些不可导的采样操作（红色的）。我们能正确地对蓝色箭头用BP算法，但是红色箭头所表示的依赖关系我们是没法用BP算法的。
  - 而策略梯度则则解决了这个问题！我们可以将网络的一部分看作是嵌入在一个大网络中的随机策略。因此，在训练过程中我们将会产生几个样本（就是下图中的分支），然后我们会去鼓励产生那些最终能带来正回报的样本（比如说需要在整个过程结束后才能评价损失函数值的时候）。换句话说，我们将会像平时一样去用BP算法来训练蓝色箭头表示的参数，而对于红色箭头中的参数，我们现在会用策略梯度的方法来向前传递，然后鼓励能采样出使最终损失降低的样本。这个想法有一个很好的形式化说明：[《Gradient Estimation Using Stochastic Computation Graphs》](https://arxiv.org/abs/1506.05254)
    <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200710/kNRkd9hlrlzt.png?imageslim">
    </p>
- 可训练的存储I/O：
  - 你可能会在很多其他的论文中见到这个想法。比如说，神经图灵机(Neural Turing Machine)有一个“磁带存储器”，并且能从上面读取数据，向上面写入数据。执行写操作一般会执行形如 `m[i] = x` 的操作，其中的`i` 和 `x` 是通过一个RNN控制器网络(RNN controller network)来产生的。然而，这个操作是一个不可导操作，因为并没有一个信号告诉我们如果我们向一个不同的位置 `j != i` 写入，损失函数会有什么变化。因此，神经图灵机必须去做“soft”读和写操作。它要预测产生一个分布 `a` (每一个元素都是0~1之间，而且和为 1，然后在我们要写入的索引附近的位置应该有很多峰值。译注：表示这个位置更容易被采样得到)，然后执行 `for all i: m[i] = a[i]\*x` 。虽然现在可导了，但是我们不得不付出很大的计算代价，因为必须访问每一个内存单元去写入。想象一下，当每一次赋值操作都会访问整个内存！ （译注：这一段是说，神经图灵机不能像真的计算机一样，直接就能向某个内存单元写入。因为这样的操作是不可导的。解决方法就是通过产生一个分布，然后从这个分布中采样得到每一个“每一个内存单元内是否写入数据”的样本，然后通过遍历每一个内存单元与这个样本共同控制是否写入内容。这样虽然让这个问题变得可导、能用BP算法运算，但是每一次操作都会伴随着对所有内存单元的访问。）
  - 然而，我们能用策略梯度（在理论上）来解决这个问题，而RL-NTM也就是这么做的。我们现在还是预测一个attention分布 `a` ，但是现在我们不再做soft写入，而是采样得到一个写入的位置： `i=sample(a);m[i]=x` 。在训练过程中，我们将会对一个小批 `i` 进行这个操作，而最终要看哪一个分支工作的最好。而且在测试阶段，我们只需要对一个位置进行写入/读取就可以了，这回打来巨大的计算力的解放。然而，就像这篇论文中指出的一样，这个策略非常难以发挥作用，因为采样总会导致意外的错误。现在，大家一致认为策略网络只有在只包含了几个离散的选择时才有效，当需要从巨大的搜索空间中搜索时，这个方法就变得毫无希望了。
  - 然而，我们拥有策略梯度算法和大量的数据和计算能力，我们现在能想的长远一些，比如说我们能够设计一个神经网络，去学习如何与一个巨大的、不可导的模块进行交互，例如LaTeX的编译器（比如说你可以做一个字符级的RNN然后产生LaTeX代码然后编译），或者一个SLAM(Simultaneous Localization and Mapping)系统、一个LQR(LinearQuadratic Regulator)求解器，或者其他的什么东西。再比如说，一个超级智能的东西能够学习如何用TCP/IP协议与互联网进行交互（这显然是一个没法求导的问题）去了解如何接管这个世界的致命信息。





对于策略梯度的一些扩展：

- 关于复杂机器人的应用。算法并不能简单地推广到需要大量探索的场景中。比如说，一个机器人应用中有可能包含了一个（或几个）机器与现实世界中的实时交流。我们上面提到的算法在这个场景中可能没法使用。一个试图解决这个问题的相关工作是[deterministic policy gradients](http://jmlr.org/proceedings/papers/v32/silver14.pdf)。这个方法不是通过从随机策略中采样然后鼓励这个采样能获得更高的分数，而是通过确定的策略直接从第二个网络（称为critic）中获得梯度信息并对打分函数建模。这个方法能够在高维动作场景中取得更好的效果，而动作采样的方法收敛性不强。但是目前从经验上看来想要让它有效果还是需要很多繁琐的工作需要做的。另一个相关的方法是规模化机器人（scale up robotics），就像谷歌的[机械臂农场](http://googleresearch.blogspot.com/2016/03/deep-learning-for-robots-learning-from.html)，或许还包括[Tesla的Model S和自动驾驶技术](http://qz.com/694520/tesla-has-780-million-miles-of-driving-data-and-adds-another-million-every-10-hours/)
- 还有另一种方法能使得搜索过程更加容易，那就是加上额外的监督信息。在一些特定的情况，机器能获取到人类专家的指导。比如说[AlphaGo](https://deepmind.com/alpha-go)就首度利用了监督学习的方法去预测人类围棋专家的落子，然后去再通过策略梯度针对赢得游戏的“真实的”目标函数微调之后，模仿人类的落子策略。在一些情况下只需要不多的专家指导（比如[远程操作机器人](https://www.youtube.com/watch?v=kZlg0QvKkQQ)），话有一些其他技术通过[apprenticeship learning](http://103.95.217.7/ai.stanford.edu/~pabbeel//thesis/thesis.pdf)利用这些数据。最后如果没有任何人类提供的监督数据，仍然有些代价很高的优化算法可以用。比如说[trajectory optimization](http://people.eecs.berkeley.edu/~igor.mordatch/policy/index.html)就是一个广为人知的动力模型（比如说物理引擎中的 ），或者其他可以通过学习局部近似的动态模型（常见于非常靠谱的框架[Guided Policy Search](http://arxiv.org/abs/1504.00702)）





应用于实践：


在上面我提到过，我想做一些在RNN那篇博文中做过的事情。我在那篇文章中展示了RNN的魔力，并且展示了一个自动序列生成问题。但事实上，想要让这些模型正确工作起来是需要很多小技巧的，它需要小心而又专业地调参，并且在很多时候，用这么复杂的模型就像大炮轰蚊子。可能一个更简单的方法在90%的情况下都是有效的。同样，对于策略梯度来说，它也不是全自动就能用的：你需要大量的样本，然后永远训练下去，当它不好使的时候也很难去debug。你应该在决定使用火箭筒之前先试试玩具枪。在增强学习的例子中，有一个很厉害的基准方法你应该先试试，那就是交叉熵方法（[Cross-entropy Method, CEM](https://en.wikipedia.org/wiki/Cross-entropy_method)）。它是一个由进化算法启发而成的简单的随机向上的“猜测、验证”方法。如果你一定要在你的问题中尝试使用策略梯度，那就一定要多花些经历在很多论文tricks的章节。从最简单的开始，使用一种PG的变种[TRPO](https://arxiv.org/abs/1502.05477)。它在[实践中](https://arxiv.org/abs/1604.06778)几乎总是比最普通的策略梯度算法有效。它的核心思想是避免因参数更新导致策略变化的太频繁，强制增加了一个子一组数据上得到的新老两个分布KL距离的约束（而不是结合上所有的梯度，关于这个想法最简单的例子就是线搜索(line search)然后一直检查KL距离）。