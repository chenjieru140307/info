# 词的表达

1. 给定包含$N$篇文档的语料库$\mathbb D =\{\mathcal D_1,\mathcal D_2,\cdots,\mathcal D_N \}$，所有的单词来自于包含$V$个词汇的词汇表$\mathbb V = \{\text{word}_1,\text{word}_2,\cdots,\text{word}_V\}$，其中$V$表示词汇表的大小 。

   每篇文档$\mathcal D_i$包含单词序列$(\text{word}_{w_1^i},\text{word}_{w_2^i},\cdots,\text{word}_{w_{n_i}^i} )$，其中$w_j^i \in \{1,2,\cdots,V\}$表示第$i$篇文档的第$j$个单词在词汇表中的编号，$n_i$表示第$i$篇文档包含$n_i$个单词。

   词的表达任务要解决的问题是：如何表示每个词汇$\text{word}_v$。

2. 最简单的表示方式是`one-hot` 编码：对于词汇表中第$v$个单词$\text{word}_v$，将其表示为$\text{word}_v \rightarrow (0,0,\cdots,0,1,0,\cdots,0)^T$，即第$v$位取值为`1`，剩余位取值为`0`。

   这种表示方式有两个主要缺点：

   - 无法表达单词之间的关系：对于任意一对单词$(\text{word}_i,\text{word}_j)$，其向量距离均为$\sqrt 2$。
   - 向量维度过高：对于中文词汇表，其大小可能达到数十万，因此`one-hot` 向量的维度也在数十万维。这对于存储、计算都消耗过大。

3. `BOW:Bag of Words`：词在文档中不考虑先后顺序，这称作词袋模型。

## 一、向量空间模型 VSM

1. 向量空间模型主要用于文档的表达。
2. 向量空间模型假设单词和单词之间是相互独立的，每个单词代表一个独立的语义单元。实际上该假设很难满足：
   - 文档中的单词和单词之间存在一定关联性，单词和其前面几个单词、后面几个单词可能存在语义上的相关性，而向量空间模型忽略了这种上下文的作用。
   - 文档中存在很多的一词多义和多词同义的现象，每个单词并不代表一个独立的语义单元。

### 1.1 文档-单词 矩阵

1. 给定语料库$\mathbb D$和词汇表$\mathbb V$，定义`文档-单词` 矩阵为：

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-2">\begin{array}{c|ccccc}
   ```

   & \text{word}_1& \text{word}_2& \text{word}_3& \cdots & \text{word}_V\ \hline\ \mathcal D_1&0&0&1&\cdots &0\ \mathcal D_2&1&0&1&\cdots &0\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \ \mathcal D_N&0&1&1&\cdots &0\ \end{array}

令矩阵为$\mathbf D$，则：$D(i,j)=1$表示文档$\mathcal D_i$中含有单词$\text{word}_j$；$D(i,j)=0$表示文档$\mathcal D_i$中不含单词$\text{word}_j$。

于是文档$\mathcal D_i$表示为：$\mathcal D_i \rightarrow (0,1,0,1,\cdots,0)^T$，其中文档$\mathcal D_i$中包含的单词对应的位置取值为1，其它位置取值为 0 。

事实上，文档的上述表达并未考虑单词的顺序，也未考虑单词出现的次数。一种改进策略是考虑单词出现的次数，从而赋予`文档-单词` 矩阵以不同的权重：



```
	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-3">\mathbf D=\begin{bmatrix}
```

w_{1,1}&w_{1,2}&w_{1,3}&\cdots&w_{1,V}\ w_{2,1}&w_{2,2}&w_{2,3}&\cdots&w_{2,V}\ \vdots &\vdots&\vdots&\ddots&\vdots\ w_{N,1}&w_{N,2}&w_{N,3}&\cdots&w_{N,V} \end{bmatrix}

其中$w_{i,j}$表示单词$\text{word}*j$在文档$\mathcal D_i$中的权重。*

- *如果单词$\text{word}\*j$在文档$\mathcal D_i$中未出现，则权重$w\*{i,j}=0$*
- *如果单词$\text{word}\*j$在文档$\mathcal D_i$中出现，则权重$w\*{i,j}\ne 0$*

*权重$w*{i,j}$有两种常用的选取方法：

- 单词权重等于单词出现的频率`TF`：$w_{i,j} = TF(\mathcal D_i,\text{word}_j)$。
  - 函数$TF(\mathcal D_i,\text{word}_j)$返回单词$\text{word}*j$在文档$\mathcal D_i$中出现的频数。*
  - *其缺点是：一些高频词（如：`我们`，`是`，`大家`）以较大的权重出现在每个文档中，这意味着对每篇文档这些高频词是比较重要的。事实上对于绝大多数 `NLP` 任务，将这些词过滤掉不会有任何影响。*
- *单词权重等于单词的`TF-IDF`：$w*{i,j} = TF(\mathcal D_i,\text{word}_j) \times IDF(\text{word}_j)$。
  - 函数$IDF(\text{word}_j)$是单词的逆文档频率：$IDF(\text{word}_j) = \log \frac{N}{DF(\text{word}_j)}$。其中：$N$为语料库的文档数量，$DF(\text{word}_j)$为出现单词$\text{word}_j$的文档的数量，$\frac {DF(\text{word}_j)}{N}$为单词$\text{word}_j$出现在一篇文档中的概率。
  - `TF-IDF` 对于高频词进行降权。如果单词$\text{word}_j$出现在大多数文档中，则$\frac {DF(\text{word}_j)}{N}$较大，因此$IDF(\text{word}_j)$会较小。

`TF-IDF` 不仅考虑了单词的局部特征，也考虑了单词的全局特征。

- 词频$TF(\mathcal D_i,\text{word}_j)$描述了单词$\text{word}*j$在文档$\mathcal D_i$中的局部统计特征。*
- *逆文档频率$IDF(\text{word}\*j)$描述了单词$\text{word}\*j$在语料库$\mathbb D$中的全局统计特征。

### 1.2 相似度

1. 给定 `文档-单词` 矩阵，则很容易得到文档的向量表达：$\mathcal D_i \rightarrow (w\*{i,1},w\*{i,2},\cdots,w*{i,V})^T$。

   给定文档$\mathcal D_i,\mathcal D_j$，则文档的相似度为：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$similar(\mathcal D_i,\mathcal D_j) = \cos(\mathbf{\vec w}_i,\mathbf{\vec w}_j) = \frac{\mathbf{\vec w}_i\cdot \mathbf{\vec w}_j}{||\mathbf{\vec w}_i||\cdot ||\mathbf{\vec w}_j||}$</div></div><p><span>其中 </span>$\mathbf{\vec w}_i=  (w_{i,1},w_{i,2},\cdots,w_{i,V})^T,\quad \mathbf{\vec w}_j=  (w_{j,1},w_{j,2},\cdots,w_{j,V})^T$<span>。</span></p><blockquote><p><span>也可以使用其它方式的相似度，如 </span>$L_2$<span> 距离相似度。</span></p></blockquote></li></ol><h2><a name="二、lsa" class="md-header-anchor"></a><span>二、LSA</span></h2><ol start='' ><li><span>潜在语义分析</span><code>latent semantic analysis:LSA</code><span> 的基本假设是：如果两个词多次出现在同一篇文档中，则这两个词具有语义上的相似性。</span></li></ol><h3><a name="2.1-原理" class="md-header-anchor"></a><span>2.1 原理</span></h3><ol start='' ><li><p><span>给定</span><code>文档-单词</code><span> 矩阵 </span>$\mathbf D$<span> </span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n85" cid="n85" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-5">\mathbf D=\begin{bmatrix}
   ```

   w_{1,1}&w_{1,2}&w_{1,3}&\cdots&w_{1,V}\ w_{2,1}&w_{2,2}&w_{2,3}&\cdots&w_{2,V}\ \vdots &\vdots&\vdots&\ddots&\vdots\ w_{N,1}&w_{N,2}&w_{N,3}&\cdots&w_{N,V} \end{bmatrix}

   其中$w_{i,j}$表示单词$\text{word}_j$在文档$\mathcal D_i$中的权重，可以为：单词$\text{word}_j$在文档$\mathcal D_i$中是否出现的`0/1` 值、单词$\text{word}*j$在文档$\mathcal D_i$中出现的频次、或者单词$\text{word}\*j$在文档$\mathcal D_i$中的`TF-IDF` 值。

2. 定义$\mathbf{\vec v}\*j=(w\*{1,j},w\*{2,j},\cdots,w*{N,j})^T$， 它为矩阵$\mathbf D$的第$j$列，代表单词$\text{word}_j$的`单词-文档向量`，描述了该单词和所有文档的关系。

   - 向量内积$\mathbf{\vec v}_p\cdot \mathbf{\vec v}_q$描述了单词$\text{word}*p$和单词$\text{word}\*q$在文档集合中的相似性。
   - 矩阵乘积$\mathbf D^T\mathbf D \in \mathbb R^{V\times V}$包含了所有词向量内积的结果。

3. 定义$\mathbf{\vec d}\*i= (d\*{i,1},d\*{i,2},\cdots,d*{i,V})^T$，它为矩阵$\mathbf D$的第$i$行，代表文档$\mathcal D_i$的`文档-单词`向量，描述了该文档和所有单词的关系。

   - 向量内积$\mathbf{\vec d}_s\cdot \mathbf{\vec d}_t$描述了文档$\mathcal D_s$和文档$\mathcal D_t$在文档集合中的相似性。
   - 矩阵乘积$\mathbf D\mathbf D^T \in \mathbb R^{N\times N}$包含了所有文档向量内积的结果。

4. 对矩阵$\mathbf D$进行`SVD` 分解。假设矩阵$\mathbf D$可以分解为：$\mathbf D = \mathbf P \mathbf \Sigma \mathbf Q^T$。其中：

   -$\mathbf P \in \mathbb R^{N\times N},\mathbf Q \in \mathbb R^{V\times V}$为单位正交矩阵。

   -$\mathbf \Sigma \in \mathbb R^{N\times V}$为广义对角矩阵。

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-6">\mathbf\Sigma = \begin{bmatrix}
     ```

     \sigma_1&0&\cdots&0&0&\cdots&0\ 0&\sigma_2&\cdots&0&0&\cdots&0\ \vdots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\ 0&0&\cdots&\sigma_r&0&\cdots&0\ 0&0&\cdots&0&0&\cdots&0\ \vdots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\ 0&0&0&\cdots&0&\cdots&0 \end{bmatrix}

     其中$\sigma_1\ge \sigma_2\ge\cdots\ge\sigma_r\gt 0$称作奇异值。

5. `SVD` 分解的物理意义为：将文档按照主题分解。所有的文档一共有$r$个主题，每个主题的强度 （主题强度就是主题在数据集中出现的次数）分别为：$\sigma_1,\sigma_2,\cdots,\sigma_r$。

   - 第$i$篇文档$\mathcal D_i$由这$r$个主题组成，文档的主题概率分布（称作`文档-主题向量`）为：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec p}^{(i)} = ( P(i,1),  P(i,2),\cdots, P(i,r))^T$</div></div></li><li><p><span>第 </span>$t$<span>  个主题由 </span>$V$<span> 个单词组成，主题的单词概率分布（称作</span><code>主题-单词向量</code><span> ）为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n118" cid="n118" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec q}^{(t)} =   (Q(t,1),  Q(t,2),\cdots,  Q(t,V))^T$</div></div></li><li><p><span>第 </span>$j$<span> 个单词由 </span>$r$<span> 个主题组成，单词的主题概率分布（称作 </span><code>单词-主题</code><span> 向量）为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n121" cid="n121" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec v}^{(j)} = (Q(1,j),Q(2,j),\cdots,Q(r,j))^T$</div></div></li><li><p><span>根据 </span>$\mathbf D = \mathbf P \mathbf \Sigma \mathbf Q^T$<span> 有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n124" cid="n124" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-10">\mathbf D = \mathbf P 
     ```

     \begin{bmatrix} 1&0&\cdots&0\ 0&1&\cdots&0\ \vdots&\vdots&\ddots&\vdots\ 0&0&\cdots& 1\ 0&0&\cdots&0\ \vdots&\vdots&\ddots&\vdots\ 0&0&\cdots&0 \end{bmatrix}*{N\times r } \begin{bmatrix} \sigma_1&0&\cdots&0\ 0&\sigma_2&\cdots&0\ \vdots&\vdots&\ddots&\vdots\ 0&0&\cdots& \sigma_r \end{bmatrix} \begin{bmatrix} 1&0&\cdots&0&0&\cdots&0\ 0&1&\cdots&0&0&\cdots&0\ \vdots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\ 0&0&\cdots&1&0&\cdots&0 \end{bmatrix}*{r\times V} \mathbf Q^T

     则该分解的物理意义为：`文档-单词` 矩阵 = `文档-主题` 矩阵 x `主题强度` x `主题-单词` 矩阵。

### 2.2 应用

1. 得到了文档的主题分布、单词的主题分布之后，可以获取文档的相似度和单词的相似度。

   - 文档$\mathcal D_i$和文档$\mathcal D_j$的相似度：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$sim(\mathcal D_i,\mathcal D_j) = \frac{\mathbf{\vec p}^{(i)} \cdot \mathbf{\vec p}^{(j)} }{||\mathbf{\vec p}^{(i)} ||\times||\mathbf{\vec p}^{(j)}|| }$</div></div></li><li><p><span>单词 </span>$\text{word}_i$<span> 和单词 </span>$\text{word}_j$<span> 的相似度：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n136" cid="n136" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$sim(\text{word}_i,\text{word}_j)= \frac{\mathbf{\vec v}^{(i)}\cdot \mathbf{\vec v}^{(j)}}{||\mathbf{\vec v}^{(i)}||\times ||\mathbf{\vec v}^{(j)}||}$</div></div></li></ul></li><li><p><span>虽然获得了文档的单词分布，但是并不能获得主题的相似度。因为 </span>$\mathbf Q$<span> 是正交矩阵，因此有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n139" cid="n139" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$(Q(i,1),  Q(i,2),\cdots,  Q(i,V))^T \cdot (Q(j,1),  Q(j,2),\cdots,  Q(j,V))^T =0,\quad i \ne j$</div></div><p><span>则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n141" cid="n141" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-14">sim(\text{topic}_i,\text{topic}_j) = \frac{\mathbf{\vec q}^{(i)} \cdot \mathbf{\vec q}^{(j)} }{||\mathbf{\vec q}^{(i)} ||\times ||\mathbf{\vec q}^{(j)} ||}\\
     ```

     = \frac { (Q(i,1), Q(i,2),\cdots, Q(i,V))^T \cdot (Q(j,1), Q(j,2),\cdots, Q(j,V))^T}{||\mathbf{\vec q}^{(i)} ||\times ||\mathbf{\vec q}^{(j)} ||} = 0,\quad i\ne j

     因此，任意两个主题之间的相似度为 0 。

   - `文档-主题向量`由$\mathbf P$决定。根据：$\mathbf D = \mathbf P \mathbf\Sigma \mathbf Q^T \rightarrow \mathbf P = \mathbf D\mathbf Q\mathbf \Sigma ^{-1} \rightarrow \mathbf P^T = \mathbf \Sigma ^{-1} \mathbf Q^T \mathbf D^T$，而`文档-主题向量`为$\mathbf P$的行向量，也就是$\mathbf P^T$的列向量。`文档-单词向量`为$\mathbf D$的行向量，也就是$\mathbf D^T$的列向量。

     因此对于一篇新的文档$\mathcal D_s$，假设其`文档-单词向量`为$\mathbf{\vec w}_s$，则其`文档-主题向量`为：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec p}^{(s)} = \mathbf\Sigma^{-1}\mathbf Q ^T \mathbf{\vec w}_s$</div></div></li><li><p><code>LSA</code><span> 可以应用在以下几个方面：</span></p><ul><li><span>通过对文档的</span><code>文档-主题向量</code><span> 进行比较，从而进行基于主题的文档聚类或者分类。</span></li><li><span>通过对单词的</span><code>单词-主题向量</code><span>进行比较，从而用于同义词、多义词进行检测。</span></li><li><span>通过将</span><code>query</code><span> 映射到主题空间，进而进行信息检索。</span></li></ul></li></ol><h3><a name="2.3-性质" class="md-header-anchor"></a><span>2.3 性质</span></h3><ol start='' ><li><p><code>LSA</code><span> 的本质是将矩阵 </span>$\mathbf D$<span> 通过  </span><code>SVD</code><span> 进行降维，降维主要是由于以下原因：</span></p><ul><li><span>原始的</span><code>文档-单词</code><span> 矩阵  </span>$\mathbf D$<span> 太大计算机无法处理，通过降维得到原始矩阵的一个近似。</span></li><li><span>原始的</span><code>文档-单词</code><span> 矩阵  </span>$\mathbf D$<span> 含有噪音，通过降维去掉原始矩阵的噪音。</span></li><li><span>原始的</span><code>文档-单词</code><span> 矩阵  </span>$\mathbf D$<span> 过于稀疏，通过降维得到一个稠密的矩阵。</span></li></ul></li><li><p><code>LSA</code><span> 的降维可以解决一部分同义词的问题，也可以解决一部分二义性的问题。</span></p><ul><li><span>经过降维，意义相同的同义词的维度会因降维被合并。</span></li><li><span>经过降维，拥有多个意义的词，其不同的含义会叠加到对应的同义词所在的维度上。</span></li></ul></li><li><p><code>LSA</code><span> 的缺点：</span></p><ul><li><p><span>产生的主题维度可能存在某些主题可解释性差。即：它们并不代表一个人类理解上的主题。</span></p></li><li><p><span>由于</span><code>Bag of words:BOW</code><span> 模型的局限性，它无法捕捉单词的前后顺序关系。</span></p><p><span>一个解决方案是：采用二元词组或者多元词组。</span></p></li><li><p><code>LSA</code><span>  假设单词和文档形成联合高斯分布。实际观察发现，它们是一个联合泊松分布。这种情况下，用</span><code>pLSA</code><span> 模型效果会更好。</span></p></li></ul></li></ol><h2><a name="三、word2vec" class="md-header-anchor"></a><span>三、Word2Vec</span></h2><h3><a name="3.1-cbow-模型" class="md-header-anchor"></a><span>3.1 CBOW 模型</span></h3><ol start='' ><li><code>CBOW</code><span> 模型（</span><code>continuous bag-of-word</code><span>）：根据上下文来预测下一个单词。</span></li></ol><h4><a name="3.1.1-一个单词上下文" class="md-header-anchor"></a><span>3.1.1 一个单词上下文</span></h4><ol start='' ><li><p><span>在一个单词上下文的</span><code>CBOW</code><span> 模型中：输入是前一个单词，输出是后一个单词，输入为输出的上下文。</span></p><p><span>由于只有一个单词作为输入，因此称作一个单词的上下文。</span></p></li><li><p><span>一个单词上下文的</span><code>CBOW</code><span>  模型如下：</span></p><p><img src="../imgs/word_representation/word2vec_cbow1.png" width="400px" /></p><p><span>其中：</span></p><ul><li><p>$N$<span> 为隐层的大小，即隐向量 </span>$\mathbf{\vec h} = (h_1,h_2,\cdots,h_N)^T\in \mathbb R^N$<span> 。</span></p></li><li><p><span>网络输入 </span>$\mathbf{\vec x}=(x_1,x_2,\cdots,x_V)^T \in \mathbb R^V$<span> ，它是输入单词（即上下文单词）的 </span><code>one-hote</code><span> 编码，其中只有一位为 1，其他位都为 0 。</span></p></li><li><p><span>网络输出 </span>$\mathbf{\vec y}=(y_1,y_2,\cdots,y_V)^T  \in \mathbb R^V$<span>，它是输出单词为词汇表各单词的概率。</span></p></li><li><p><span>相邻层之间为全连接：</span></p><ul><li><span>输入层和隐层之间的权重矩阵为 </span>$\mathbf W \in \mathbb R^{V\times N}$<span> </span></li><li><span>隐层和输出层之间的权重矩阵为 </span>$\mathbf W^\prime\in \mathbb R^{N\times V}$</li></ul></li></ul></li><li><p><span>假设没有激活函数，没有偏置项。给定输入 </span>$\mathbf{\vec x} \in \mathbb R^V$<span>，则其对应的隐向量 </span>$\mathbf{\vec h}\in \mathbb R^N$<span> 为：  </span>$\mathbf{\vec h}=\mathbf W^T\mathbf{\vec x}$<span> 。</span></p><p><span>令：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n215" cid="n215" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-16">\mathbf W=\begin{bmatrix}
     ```

     \mathbf{\vec w}_1^T\ \mathbf{\vec w}_2^T\ \vdots\ \mathbf{\vec w}_V^T\ \end{bmatrix}

     由于$\mathbf{\vec x}$是个`one-hot`编码，假设它为词表$\mathbb V$中第$j$个单词$\text{word}_j$，即：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$x_1=0,x_2=0,\cdots,x_{j-1}=0,x_j=1,x_{j+1}=0,\cdots,x_V=0$</div></div><p><span>则有：</span>$\mathbf{\vec h}=\mathbf{\vec w}_j$<span>。</span></p><p><span>即：</span>$\mathbf W$<span> 的第 </span>$j$<span> 行 </span>$\mathbf{\vec w}_j ^T$<span> 就是词表 </span>$\mathbb V$<span> 中第 </span>$j$<span> 个单词  </span>$\text{word}_j$<span> 的表达，称作单词 </span>$\text{word}_j$<span> 的输入向量。</span></p></li><li><p><span>给定隐向量 </span>$\mathbf {\vec h}$<span>，其对应的输出向量 </span>$\mathbf{\vec u} \in \mathbb R^V $<span>为：  </span>$\mathbf{\vec u}=\mathbf W^{\prime T}\mathbf{\vec h}$<span> 。令：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n222" cid="n222" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf W^\prime=[\mathbf{\vec w}_1^{\prime },\mathbf{\vec w}_2^{\prime },\cdots,\mathbf{\vec w}_V^{\prime }]$</div></div><p><span>则有：</span>$u_j=\mathbf{\vec w}^{\prime}_j\cdot \mathbf{\vec h}$<span> 。 </span></p><ul><li>$u_j$<span> 表示词表  </span>$\mathbb V$<span> 中，第 </span>$j$<span> 个单词  </span>$\text{word}_j$<span> 的得分。</span></li><li>$\mathbf{\vec w}_j^{\prime }$<span> 为矩阵 </span>$\mathbf W^\prime$<span> 的第 </span>$j$<span> 列，称作单词  </span>$ \text{word}_j$<span> 的输出向量。</span></li></ul></li><li><p>$\mathbf{\vec u}  $<span> 之后接入一层 </span><code>softmax</code><span> 层，则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n231" cid="n231" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$y_j=p(\text{word}_j\mid \mathbf{\vec x})=\frac{\exp(u_j)}{\sum_{j^\prime=1}^V \exp(u_{j^\prime})},\quad j=1,2,\cdots,V$</div></div><p><span>即 </span>$y_j$<span> 表示词汇表 </span>$\mathbb V$<span> 中第 </span>$j$<span> 个单词 </span>$\text{word}_j$<span> 为真实输出单词的概率。</span></p><p><span>假设输入单词为  </span>$\text{word}_I$<span> （它称作上下文） ，观测到它的下一个单词为  </span>$\text{word}_O$<span> 。令输入单词的对应的网络输入为  </span>$\mathbf{\vec x}$<span> ，其隐向量为 </span>$\mathbf{\vec w}_I$<span>，输入输出单词对应的输出单元为 </span>$j^*$<span>，则采用交叉熵的损失函数为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n234" cid="n234" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-20">E(\text{word}_I,\text{word}_O) = -\log  \frac{\exp(u_{j^*})}{\sum_{j^\prime=1}^V \exp(u_{j^\prime})} = - \mathbf{\vec w}^{\prime}_{j^*}\cdot \mathbf{\vec h} + \log \sum_{j=1}^V \exp(\mathbf{\vec w}^{\prime}_{j}\cdot \mathbf{\vec h})\\
     ```

     = - \mathbf{\vec w}^{\prime}*{j^\*}\cdot \mathbf{\vec w}\*I + \log \sum\*{j=1}^V \exp(\mathbf{\vec w}^{\prime}*{j}\cdot \mathbf{\vec w}_I)

     考虑语料库$\mathbb D$中所有的样本，则整体经验损失函数为：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathcal L = \sum_{(\text{word}_I,\text{word}_O) \in \mathbb D} E(\text{word}_I,\text{word}_O)$</div></div><p><span>则网络的优化目标为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n238" cid="n238" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\min \mathcal L = \min_{\mathbf W,\mathbf W^\prime} \sum_{(\text{word}_I,\text{word}_O) \in \mathbb D}\left(- \mathbf{\vec w}^{\prime}_{j^*}\cdot \mathbf{\vec w}_I + \log \sum_{j=1}^V \exp(\mathbf{\vec w}^{\prime}_{j}\cdot \mathbf{\vec w}_I)\right)$</div></div><p><span>设张量 </span>$\mathbf A$<span> 为某个网络参数，则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n240" cid="n240" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\nabla_{\mathbf A} \mathcal L = \sum_{(\text{word}_I,\text{word}_O)} \nabla_{\mathbf A} E$</div></div><p><span>则该参数的单次更新 </span>$\mathbf A \leftarrow \mathbf A -\eta\nabla_{\mathbf A} \mathcal L $<span> ，可以表示为单个样本的多次更新：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n242" cid="n242" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-24">\text{for } (\text{word}_I,\text{word}_O) \in \mathbb D:\\
     ```

     \mathbf A \leftarrow \mathbf A -\eta\nabla_{\mathbf A} E

     因此本节的参数更新推导是关于单个样本的更新：$\mathbf A \leftarrow \mathbf A -\eta\nabla_{\mathbf A} E$。

#### 3.1.2 参数更新

1. 定义$t_j=\mathbb I(j=j^*)$，即第$j$个输出单元对应于真实的输出单词$\text{word}_O$时，它为1；否则为0。定义：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$e_j=\frac{\partial E}{\partial u_j}=y_j-t_j$</div></div><p><span>它刻画了每个输出单元的预测误差：</span></p><ul><li><span>当 </span>$j=j^*$<span> 时： </span>$e_j=y_j-1\lt 0 $<span>，它刻画了输出概率 (</span>$y_j$<span>) 与真实概率 </span>$1$<span> 之间的差距。小于 0  表示预测不足。</span></li><li><span>当 </span>$j\ne j^*$<span> 时：</span>$e_j=y_j \gt 0$<span>，它刻画了输出概率 (</span>$y_j$<span>) 与真实概率 </span>$0$<span> 之间的差距。大于 0  表示预测过量。</span></li></ul></li><li><p><span>根据：</span>$u_j=\mathbf{\vec w}^{\prime}_j\cdot \mathbf{\vec h}\quad  \rightarrow  \quad \frac{\partial u_j}{\partial \mathbf{\vec w}_j^\prime}= \mathbf{\vec h}$<span>  ，则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n257" cid="n257" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\frac{\partial E}{\partial \mathbf {\vec  w}_j^\prime} = \frac{\partial E}{\partial u_j} \times  \frac{\partial u_j}{\partial \mathbf{\vec w}_j^\prime} = e_j \mathbf{\vec h}$</div></div><p><span>则 </span>$\mathbf{\vec w}_j^\prime$<span> 更新规则为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n259" cid="n259" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}^{\prime(new)}_{j } =\mathbf{\vec w}^{\prime(old)}_{j }-\eta e_j\mathbf{\vec h}$</div></div><p><span>其物理意义为：</span></p><ul><li><span>当估计过量 (</span>$e_j\gt 0 \rightarrow y_j\gt t_j$<span>) 时， </span>$ \mathbf{\vec w}^{\prime}_{j } $<span> 会减去一定比例的 </span>$\mathbf{\vec h}$<span> 。 这发生在第 </span>$j$<span> 个输出单元不对应于真实的输出单词时。</span></li><li><span>当估计不足 (</span>$e_j\lt 0 \rightarrow y_j\lt t_j$<span>) 时， </span>$ \mathbf{\vec w}^{\prime}_{j } $<span> 会加上一定比例的 </span>$\mathbf{\vec h}$<span> 。这发生在第 </span>$j$<span> 个输出单元刚好对应于真实的输出单词时。</span></li><li><span>当 </span>$y_j\simeq t_{j}$<span> 时，更新的幅度将非常微小。</span></li></ul></li><li><p><span>定义：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n270" cid="n270" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\overrightarrow {EH}} = \frac{\partial E}{\partial \mathbf{\vec h}} = \left(\frac{\partial \mathbf{\vec u}}{\partial \mathbf{\vec h}} \right)^T\frac{\partial E}{\partial \mathbf{\vec u}}$</div></div><p><span>根据：</span>$\mathbf{\vec u}=\mathbf W^{\prime T}\mathbf{\vec h}  \quad \rightarrow \quad \left(\frac{\partial \mathbf{\vec u}}{\partial \mathbf{\vec h}} \right)^T= \mathbf W^\prime$<span> ，则有：</span>$\mathbf{\overrightarrow {EH}}  = \mathbf W^\prime \mathbf{\vec e} =\sum_{j=1}^Ve_j \mathbf{\vec w}^{\prime }_j$<span>  。</span></p><p>$\mathbf{\overrightarrow {EH}}$<span> 的物理意义为：词汇表 </span>$ \mathbb V $<span> 中所有单词的输出向量的加权和，其权重为 </span>$e_j$<span>  。</span></p></li><li><p><span>考虑到   </span>$\mathbf{\vec h}=\mathbf W^T\mathbf{\vec x}$<span> ，则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n275" cid="n275" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\frac{\partial E}{\partial w_{k,i}}=\frac{\partial E}{\partial h_i}\times\frac{\partial h_i}{\partial w_{k,i}}=EH_i\times x_k$</div></div><p><span>写成矩阵的形式为：</span>$\frac{\partial E}{\partial \mathbf W}=\mathbf{\vec x}\otimes \mathbf{\overrightarrow {EH}}$<span> ，其中 </span>$\otimes$<span> 为克罗内克积。</span></p><p><span>由于 </span>$\mathbf{\vec x}$<span> 是</span><code>one-hote</code><span> 编码，所以它只有一个分量非零，因此 </span>$\frac{\partial E}{\partial \mathbf W}$<span> 只有一行非零，且该非零行就等于 </span>$\mathbf{\overrightarrow {EH}}$<span> 。因此得到更新方程：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n278" cid="n278" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}_I^{(new)}=\mathbf{\vec w}_I^{(old)}-\eta\mathbf{\overrightarrow{EH}}$</div></div><p><span>其中 </span>$\mathbf{\vec w}_I$<span> 为 </span>$\mathbf{\vec x}$<span> 非零分量对应的 </span>$\mathbf W$<span> 中的行，而 </span>$\mathbf W$<span> 的其它行在本次更新中都保持不变。</span></p></li><li><p><span>考虑更新 </span>$\mathbf W$<span> 第 </span>$I$<span> 行的第 </span>$k$<span> 列，则：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n282" cid="n282" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$w_{I,k}^{(new)}=  w_{I,k}^{(old)}-\eta\sum_{j=1}^Ve_jw^\prime_{j,k}$</div></div><ul><li><span>当 </span>$y_j\simeq t_{j}$<span> 时，</span>$e_j$<span> 趋近于 0 ，则更新的幅度将非常微小。</span></li><li><span>当 </span>$y_j$<span> 与 </span>$t_{j}$<span> 差距越大，</span>$e_j$<span> 绝对值越大， 则更新的幅度越大。</span></li></ul></li><li><p><span>当给定许多训练样本（每个样本由两个单词组成），上述更新不断进行，更新的效果在不断积累。</span></p><ul><li><p><span>根据单词的共现结果，输出向量与输入向量相互作用并达到平衡。</span></p><ul><li><p><span>输出向量 </span>$\mathbf{\vec w}^{\prime} $<span> 的更新依赖于输入向量 </span>$\mathbf{\vec w}_I$<span> ：  </span>$\mathbf{\vec w}^{\prime(new)}_{j } =\mathbf{\vec w}^{\prime(old)}_{j }-\eta e_j\mathbf{\vec h}$<span> 。</span></p><p><span>这里隐向量 </span>$\mathbf{\vec h}$<span> 等于输入向量 </span>$\mathbf{\vec w}_I$<span> 。</span></p></li><li><p><span>输入向量  </span>$\mathbf{\vec w}_I$<span> 的更新依赖于输出向量 </span>$\mathbf{\vec w}^{\prime} $<span> ： </span>$\mathbf{\vec w}_I^{(new)}=\mathbf{\vec w}_I^{(old)}-\eta\mathbf{\overrightarrow{EH}}$<span> 。</span></p><p><span>这里 </span>$\mathbf{\overrightarrow{EH}} = \sum_{j=1}^Ve_j \mathbf{\vec w}^{\prime }_j$<span> 为词汇表 </span>$ \mathbb V $<span> 中所有单词的输出向量的加权和，其权重为 </span>$e_j$<span>  。</span></p></li></ul></li><li><p><span>平衡的速度与效果取决于单词的共现分布，以及学习率。</span></p></li></ul></li></ol><h4><a name="3.1.3-多个单词上下文" class="md-header-anchor"></a><span>3.1.3 多个单词上下文</span></h4><ol start='' ><li><p><span>考虑输入为目标单词前后的多个单词（这些单词作为输出的上下文），输入为 </span>$C$<span> 个单词：</span>$\mathbf{\vec x}_1,\mathbf{\vec x}_2,\cdots,\mathbf{\vec x}_C$<span> 。对于每个输入单词，其权重矩阵都为 </span>$\mathbf W$<span>，这称作权重共享。这里的权重共享隐含着：每个单词的表达是固定的、唯一的，与它的上下文无关。</span></p><p><span>隐向量为所有输入单词映射结果的均值：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n307" cid="n307" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec h}=\frac 1C \mathbf W^T(\mathbf{\vec x}_1+\mathbf{\vec x}_2+\cdots+\mathbf{\vec x}_C)=\frac 1C (\mathbf{\vec w}_{I_1}+\mathbf{\vec w}_{I_2}+\cdots+\mathbf{\vec w}_{I_C})$</div></div><p><span>其中：</span>$I_i$<span> 表示第 </span>$i$<span> 个输入单词在词汇表 </span>$\mathbb V$<span> 中的编号，</span>$\mathbf{\vec w}_j $<span> 为矩阵 </span>$\mathbf W $<span> 的第 </span>$j$<span> 行，它是对应输入单词的输入向量。</span></p><p><img src="../imgs/word_representation/word2vec_cbow2.png" width="350px" /></p></li><li><p><span>假设给定一个单词序列 </span>$\text{word}_{I_1},\text{word}_{I_2},\cdots,\text{word}_{I_C}$<span> （它称作上下文），观测到它的下一个单词为  </span>$\text{word}_O$<span> 。  </span>$\text{word}_O$<span> 对应的网络输出编号为 </span>$j^*$<span> 。</span></p><p><span>定义损失函数为交叉熵：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n313" cid="n313" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-33">E=- u_{j^*} +\log \sum_{j=1}^V \exp(u_j)
   ```

   =-\mathbf{\vec w}^{\prime}*{j^\*}\cdot \mathbf{\vec h}+\log \sum*{j=1}^V \exp(\mathbf{\vec w}^{\prime}_{j}\cdot \mathbf{\vec h})

   它的形式与`一个单词上下文`中推导的完全相同，除了这里的$\mathbf{\vec h}$不同。

   考虑语料库$\mathbb D$中所有的样本，则整体经验损失函数为：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathcal L = \sum_{(\text{word}_{I_1},\text{word}_{I_2},\cdots,\text{word}_{I_C},\text{word}_O) \in \mathbb D} E$</div></div><p><span>则网络的优化目标为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n318" cid="n318" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\min \mathcal L = \min_{\mathbf W,\mathbf W^\prime} \sum_{(\text{word}_{I_1},\text{word}_{I_2},\cdots,\text{word}_{I_C},\text{word}_O) \in \mathbb D}\left(- \mathbf{\vec w}^{\prime}_{j^*}\cdot \mathbf{\vec w}_I + \log \sum_{j=1}^V \exp(\mathbf{\vec w}^{\prime}_{j}\cdot \mathbf{\vec w}_I)\right)$</div></div><p><span>.</span></p></li><li><p><span>与</span><code>一个单词上下文</code><span>中推导的结果相同，这里给出参数更新规则：</span></p><ul><li><p><span>更新 </span>$\mathbf W^\prime$<span>  ：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n325" cid="n325" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}^{\prime(new)}_{j } =\mathbf{\vec w}^{\prime(old)}_{j }-\eta e_j\mathbf{\vec h},\quad j=1,2,\cdots,V$</div></div><p><span>其中 </span>$\mathbf{\vec h}=\frac 1C (\mathbf{\vec w}_{I_1}+\mathbf{\vec w}_{I_2}+\cdots+\mathbf{\vec w}_{I_C})$<span>  。</span></p></li><li><p><span>更新 </span>$\mathbf W$<span>：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n329" cid="n329" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}_{I_i}^{(new)}=\mathbf{\vec w}_{I_i}^{(old)}-\frac 1C\eta\mathbf{\overrightarrow{EH}},\quad i=1,2,\cdots,C$</div></div><p><span>其中 ：</span></p><ul><li>$\mathbf{\overrightarrow {EH}}  = \mathbf W^\prime \mathbf{\vec e} =\sum_{j=1}^Ve_j \mathbf{\vec w}^{\prime }_j$<span> ，它是词汇表 </span>$ \mathbb V $<span> 中所有单词的输出向量的加权和，其权重为 </span>$e_j$<span>  。</span></li><li>$I_i$<span> 为第 </span>$i$<span> 个输入单词在词表 </span>$\mathbb V$<span> 中的编号。</span></li></ul></li></ul></li><li><p><span>在更新 </span>$\mathbf W$<span>  时，如果有相同的输入单词（如： </span>$\mathbf{\vec x}_1=\mathbf{\vec x}_2 \rightarrow  \text{word}_{100}$<span> )，则在参数更新时认为它们是不同的。最终的效果就是在 </span>$\mathbf{\vec w}_{I_i}$<span>  中多次减去同一个增量 </span>$\frac 1C\eta\mathbf{\overrightarrow{EH}}$<span> 。</span></p><p><span>你也可以直接减去 </span>$\frac {n_v}{C}\eta\mathbf{\overrightarrow{EH}}$<span>， 其中 </span>$n_v$<span> 为词汇表中单词 </span>$\text{word}_v$<span> 在输入中出现的次数。</span></p></li></ol><h3><a name="3.2-skip-gram" class="md-header-anchor"></a><span>3.2 Skip-Gram</span></h3><ol start='' ><li><code>Skip-Gram</code><span> 模型是根据一个单词来预测其前后附近的几个单词（即：上下文）。</span></li></ol><h4><a name="3.2.1-网络结构" class="md-header-anchor"></a><span>3.2.1 网络结构</span></h4><ol start='' ><li><p><code>Skip-Gram</code><span>  网络模型如下。其中：</span></p><ul><li><p><span>网络输入 </span>$\mathbf{\vec x}=(x_1,x_2,\cdots,x_V)^T \in \mathbb R^V$<span> ，它是输入单词的 </span><code>one-hote</code><span> 编码，其中只有一位为 1，其他位都为 0 。</span></p></li><li><p><span>网络输出 </span>$\mathbf{\vec y}_1,\mathbf{\vec y}_2,\cdots,\mathbf{\vec y}_C$<span> ，其中 </span>$\mathbf{\vec y}_c =(y_1^c,y_2^c,\cdots,y_V^c)^T  \in \mathbb R^V$<span>是第 </span>$c$<span> 个输出单词为词汇表各单词的概率。</span></p></li><li><p><span>对于网络的每个输出 </span>$\mathbf{\vec y}_c$<span> ，其权重矩阵都相同，为 </span>$\mathbf W^\prime$<span>。这称作权重共享。</span></p><p><span>这里的权重共享隐含着：每个单词的输出向量是固定的、唯一的，与其他单词的输出无关。</span></p><p><img src="../imgs/word_representation/word2vec_skipGram1.png" width="400px" /></p></li></ul></li><li><p><code>Skip-Gram</code><span>  网络模型中，设网络第 </span>$c$<span> 个输出的第 </span>$j$<span> 个分量为 </span>$u_{j}^c=\mathbf{\vec w}^{\prime}_j\cdot \mathbf{\vec h}$<span> ，则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n358" cid="n358" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-38">y_{j}^c=p(\text{word}_j ^c\mid \mathbf{\vec x})=\frac{\exp(u_{j}^c)}{\sum_{k=1}^V\exp(u_{k}^c)};
   ```

   \quad c=1,2,\cdots,C;\quad j=1,2,\cdots,V

  $y_{j}^c$表示第$c$个输出中，词汇表$\mathbb V$中第$j$个单词$\text{word}*j$为真实输出单词的概率。*

2. *因为$\mathbf W^\prime$在多个单元之间共享，所以对于网络每个输出，其得分的分布$\mathbf{\vec u}\*c=(u\*{1}^c,u*{2}^c,\cdots,u_{V}^c)^T$是相同的。但是这并不意味着网络的每个输出都是同一个单词。

   并不是网络每个输出中，得分最高的单词为预测单词。因为每个输出中，概率分布都相同，即：$\mathbf{\vec y}_1=\mathbf{\vec y}*2=\cdots=\mathbf{\vec y}\*C$。`Skip-Gram` 网络的目标是：网络的多个输出之间的联合概率最大。

3. 假设输入为单词$\text{word}\*I$，输出单词序列为$\text{word}\*{O_1},\text{word}\*{O_2},\cdots,\text{word}*{O_C}$。定义损失函数为：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$E=-\log p(\text{word}_{O_1},\text{word}_{O_2},\cdots,\text{word}_{O_C}\mid \text{word}_I)=-\log \prod_{c=1}^C\frac{\exp(u_{j_c^*}^c)}{\sum_{j=1}^V\exp(u_{j}^c)}$</div></div><p><span>其中 </span>$ j_1^*,j_2^*,\cdots,j_C^* $<span> 为输出单词序列对应于词典 </span>$ \mathbb V$<span> 中的下标序列。</span></p><p><span>由于网络每个输出的得分的分布都相同，令 </span>$u_j=u_{j}^c=\mathbf{\vec w}^{\prime}_j\cdot \mathbf{\vec h}$<span>，则上式化简为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n368" cid="n368" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$E=-\sum_{c=1}^Cu_{j_c^*}^c+C\log\sum_{j=1}^V \exp (u_j)$</div></div><p><span>.</span></p></li></ol><h4><a name="3.1.2-参数更新" class="md-header-anchor"></a><span>3.1.2 参数更新</span></h4><ol start='' ><li><p><span>定义 </span>$t_{j}^c=\mathbb I(j_c=j_c^*)$<span> ，即网络第 </span>$c$<span> 个输出的第 </span>$j$<span> 个分量对应于第 </span>$c$<span> 个真实的输出单词 </span>$\text{word}_{j_c^*}$<span> 时，它为 1；否则为0。</span></p><p><span>定义：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n375" cid="n375" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$e_{j}^c=\frac{\partial E}{\partial u_{j}^c}=y_{j}^c-t_{j}^c$</div></div><p><span>它刻画了网络第 </span>$c$<span> 个输出的第 </span>$j$<span> 个分量的误差：</span></p><ul><li><span>当 </span>$j_c=j_c^*$<span> 时： </span>$e_{j}^c=y_{j}^c-1 $<span>，它刻画了输出概率 </span>$y_{j}^c$<span> 与真实概率 </span>$1$<span> 之间的差距。小于 0  表示预测不足。</span></li><li><span>当 </span>$j_c\ne j_c^*$<span> 时：</span>$e_{j}^c=y_{j}^c$<span>，它刻画了输出概率  </span>$y_{j}^c$<span> 与真实概率 </span>$0$<span> 之间的差距。大于 0  表示预测过量。</span></li></ul></li><li><p><span>根据：</span>$u_j=\mathbf{\vec w}^{\prime}_j\cdot \mathbf{\vec h}\quad  \rightarrow  \quad \frac{\partial u_j}{\partial \mathbf{\vec w}_j^\prime}= \mathbf{\vec h}$<span> ，则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n384" cid="n384" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\frac{\partial E}{\partial \mathbf {\vec  w}_j^\prime} = \sum_{c=1}^{C}\frac{\partial E}{\partial u_j^c} \times  \frac{\partial u_j^c}{\partial \mathbf{\vec w}_j^\prime} = \sum_{c=1}^C e_j^c \mathbf{\vec h}$</div></div><p><span>定义 </span>$EI_j=\sum_{c=1}^{C}e_{j}^c$<span> ，它为网络每个输出的第  </span>$j$<span>  个分量的误差之和。于是有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n386" cid="n386" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\frac{\partial E}{\partial \mathbf {\vec  w}_j^\prime}=EI_j\times \mathbf{\vec h}$</div></div><p><span>则有更新方程：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n388" cid="n388" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}_j^{\prime(new)}= \mathbf{\vec w}_j^{\prime(old)}-\eta \times EI_j\times \mathbf{\vec h},\quad \quad j=1,2,\cdots,V$</div></div></li><li><p><span>定义：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n391" cid="n391" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\overrightarrow {EH}} = \frac{\partial E}{\partial \mathbf{\vec h}} = \sum_{c=1}^{C}\left(\frac{\partial \mathbf{\vec u}^c}{\partial \mathbf{\vec h}} \right)^T\frac{\partial E}{\partial \mathbf{\vec u}^c}$</div></div><p><span>根据：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n393" cid="n393" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec u}^c=\mathbf W^{\prime T}\mathbf{\vec h}  \quad \rightarrow \quad \left(\frac{\partial \mathbf{\vec u}^c}{\partial \mathbf{\vec h}} \right)^T= \mathbf W^\prime$</div></div><p><span>则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n395" cid="n395" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\overrightarrow {EH}} = \sum_{c=1}^{C} \mathbf W^\prime \mathbf{\vec e}^c = \sum_{j=1}^V EI_j \mathbf{\vec w}^\prime_{j}$</div></div><p>$\mathbf{\overrightarrow {EH}}$<span> 的物理意义为：词汇表 </span>$ \mathbb V $<span> 中所有单词的输出向量的加权和，其权重为 </span>$EI_j$<span> 。</span></p></li><li><p><span>考虑到   </span>$\mathbf{\vec h}=\mathbf W^T\mathbf{\vec x}$<span> ，则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n399" cid="n399" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\frac{\partial E}{\partial w_{k,i}}=\frac{\partial E}{\partial h_i}\times\frac{\partial h_i}{\partial w_{k,i}}=EH_i\times x_k$</div></div><p><span>写成矩阵的形式为：</span>$\frac{\partial E}{\partial \mathbf W}=\mathbf{\vec x}\otimes \mathbf{\overrightarrow {EH}}$<span> ，其中 </span>$\otimes$<span> 为克罗内克积。</span></p><p><span>由于 </span>$\mathbf{\vec x}$<span> 是</span><code>one-hote</code><span> 编码，所以它只有一个分量非零，因此 </span>$\frac{\partial E}{\partial \mathbf W}$<span> 只有一行非零，且该非零行就等于 </span>$\mathbf{\overrightarrow {EH}}$<span> 。因此得到更新方程：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n402" cid="n402" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}_I^{(new)}=\mathbf{\vec w}_I^{(old)}-\eta\mathbf{\overrightarrow{EH}}$</div></div><p><span>其中 </span>$\mathbf{\vec w}_I$<span> 为 </span>$\mathbf{\vec x}$<span> 非零分量对应的 </span>$\mathbf W$<span> 中的行，而 </span>$\mathbf W$<span> 的其它行在本次更新中都保持不变。</span></p></li></ol><h3><a name="3.3-优化" class="md-header-anchor"></a><span>3.3 优化</span></h3><ol start='' ><li><p><span>原始的</span><code>CBOW</code><span>  模型和</span><code>Skip-Gram</code><span> 模型的计算量太大，非常难以计算。</span></p><ul><li><p><span>模型在计算网络输出的时候，需要计算误差 。对于</span><code>CBOW</code><span> 模型，需要计算 </span>$V$<span> 个误差（词汇表的大小），对于 </span><code>Skip-Gram</code><span> 模型，需要计算 </span>$C \times V$<span> 个误差。</span></p><p><span>另外，每个误差的计算需要用到 </span><code>softmax</code><span>  函数，该 </span><code>softmax</code><span> 函数涉及到  </span>$O(V)$<span> 复杂度的运算： </span>$\sum_{j=1}^V \exp (u_j)$<span>  。</span></p></li><li><p><span>每次梯度更新都需要计算网络输出。</span></p></li></ul><p><span>如果词汇表有 </span><code>100万</code><span> 单词，模型迭代 </span><code>100</code><span> 次，则计算量超过 1 亿次。</span></p></li><li><p><span>虽然输入向量的维度也很高，但是由于输入向量只有一位为 1，其它位均为 0，因此输入的总体计算复杂度较小。</span></p></li><li><p><code>word2vec</code><span> 优化的主要思想是：限制输出单元的数量。</span></p><p><span>事实上在上百万的输出单元中，仅有少量的输出单元对于参数更新比较重要，大部分的输出单元对于参数更新没有贡献。</span></p></li><li><p><span>有两种优化策略：</span></p><ul><li><span>通过分层 </span><code>softmax</code><span> 来高效计算 </span><code>softmax</code><span> 函数。</span></li><li><span>通过负采样来缩减输出单元的数量。</span></li></ul></li></ol><h4><a name="3.3.1-分层-softmax" class="md-header-anchor"></a><span>3.3.1 分层 softmax</span></h4><ol start='' ><li><p><span>分层 </span><code>softmax</code><span> 是一种高效计算 </span><code>softmax</code><span> 函数的算法。</span></p><p><span>经过分层 </span><code>softmax</code><span> 之后，计算 </span><code>softmax</code><span> 函数的算法复杂度从 </span>$O(V)$<span> 降低到 </span>$O(\log V)$<span>  ，但是仍然要计算 </span>$V-1$<span>  个内部节点的向量表达 。</span></p></li></ol><h5><a name="a)-网络结构" class="md-header-anchor"></a><span>a) 网络结构</span></h5><ol start='' ><li><p><span>在分层</span><code>softmax</code><span> 中，字典 </span>$\mathbb V$<span> 中的 </span>$V$<span> 个单词被组织成二叉树。</span></p><ul><li><p><span>叶子结点值为某个具体单词的概率（如下图中的白色结点）</span></p></li><li><p><span>中间节点值也代表一个概率（如下图中的灰色结点）。它的值等于直系子节点的值之和，也等于后继的叶子结点值之和，也等于从根节点到当前节点的路径的权重的乘积。</span></p><blockquote><p><span>之所以有这些性质，是由于结点值、权重都是概率，满足和为1的性质</span></p></blockquote></li><li><p><span>根据定义，根节点的值等于所有叶子结点的值之和，即为 1.0 </span></p></li><li><p><span>二叉树的每条边代表分裂：</span></p><ul><li><span>向左的边：表示选择左子节点，边的权重为选择左子节点的概率</span></li><li><span>向右的边：表示选择右子节点，边的权重为选择右子节点的概率</span></li></ul></li></ul><p><img src="../imgs/word_representation/word2vec_softmax.png" width="400px" /></p></li><li><p><span>对于任意一个中间节点 </span>$t$<span>， 假设其向量表达为 </span>$\mathbf{\vec v}_t^\prime$<span>  ，它是待求的参数。</span></p><ul><li><p><span>选择左子节点的概率为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n458" cid="n458" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-50">p(t,left)=\sigma\left(\mathbf{\vec v}_t^\prime\cdot \mathbf{\vec h}\right)\\
   ```

   \sigma(x) = \frac{1}{1+e^x},\quad \sigma(-x) = 1-\sigma(x)

4. 选择右子节点的概率为 :

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$p(t,right)=1-\sigma\left(\mathbf{\vec v}_t^\prime\cdot \mathbf{\vec h}\right)=\sigma\left(-\mathbf{\vec v}_t^\prime\cdot \mathbf{\vec h}\right)$</div></div></li><li><p><span>如果求得所有中间节点的向量表达，则根据每个中间节点的分裂概率，可以很容易的求得每个叶节点的值。</span></p></li></ul></li><li><p><span>在分层</span><code>softmax</code><span> 中，算法并不直接求解输出向量 </span>$\{\mathbf{\vec w}_1^\prime,\mathbf{\vec w}_2^\prime,\cdots,\mathbf{\vec w}_V^\prime\} $<span>，而是求解二叉树的 </span>$V-1$<span> 个中间节点的向量表达 。</span></p><p><span>当需要计算某个单词的概率时，只需要记录从根节点到该单词叶子结点的路径。给定单词 </span>$w$<span> ：</span></p><ul><li><span>定义 </span>$n(w,j)$<span> 为从根节点到单词 </span>$w$<span> 的路径的第 </span>$j$<span> 个节点（从 1 计数）。</span></li><li><span>定义 </span>$L(w)$<span> 为从根节点到单词 </span>$w$<span> 的路径的长度。</span></li><li><span>定义 </span>$ch(t)$<span>  表示节点 </span>$t$<span> 的左子节点。</span></li></ul><p><img src="../imgs/word_representation/softmax.png" width="400px" /></p><p><span>输出为单词 </span>$w$<span> 的概率为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n476" cid="n476" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$p(w)=\prod_{j=1}^{L(w)-1}\sigma\left(g(n(w,j+1)=ch(n(w,j))) \times \mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h} \right)$</div></div><p><span>其中：</span></p><ul><li><p>$n(w,j+1)=ch(n(w,j))$<span> 表示：从根节点到单词 </span>$w$<span> 的路径上，第 </span>$j+1$<span> 个节点是第  </span>$j$<span> 个节点的左子节点。</span></p></li><li><p>$g(x)$<span> 是一个函数。当 </span>$x$<span> 是个事实时，其值为 1；当 </span>$x$<span> 不成立时，其值为 -1。</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n483" cid="n483" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-53">g(x)=\begin{cases}
   ```

   1,& \text{if x*x* is true}\ -1,&\text{if x*x* is false} \end{cases}

5.$g(n(w,j+1)=ch(n(w,j)))$表示：从根节点到单词$w$的路径上：

   - 当第$j+1$个节点是第$j$个节点的左子节点时，函数值为 1
   - 当第$j+1$个节点是第$j$个节点的右子节点时，函数值为 -1

6.$\mathbf{\vec v}^\prime_{n(w,j)}$表示：从根节点到单词$w$的路径上，第$j$个节点的向量表达

7. 对于从根节点到单词$w$的路径上，从第$j$个节点到第$j+1$个节点的概率为：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-54">p(j,j+1)=\begin{cases}
   ```

   \sigma\left( \mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h} \right),&\text{ if j+1*j*+1 is left child of j*j*}\ \sigma\left( - \mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h} \right),&\text{ if j+1*j*+1 is right child of j*j*} \end{cases}

   因此$p(w)$刻画了：从根节点到单词$w$的路径上，每条边的权重（也就是分裂概率）的乘积。

8. 对于所有的叶子节点，有$\sum_{i=1}^Vp(w_i)=1$。

   利用数学归纳法，可以证明：`左子节点的值+右子节点的值=父节点的值`。上式最终证明等于根节点的值，也就是 1.0 。

##### b) 参数更新

1. 为了便于讨论，这里使用`CBOW` 的`一个单词上下文`模型。

   记$g(n(w,j+1)=ch(n(w,j)))$为$g_{n(w,j)}$， 定义损失函数对数似然：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$E=-\log p(w\mid \mathbf{\vec x})=-\sum_{j=1}^{L(w)-1}\log \sigma(g_{n(w,j)}\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h})$</div></div><p><span>则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n507" cid="n507" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-56">\frac{\partial E}{\partial (\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h})}=\left(\sigma(g_{n(w,j)}\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h})-1 \right)g_{n(w,j)}
   ```

   =\begin{cases} \sigma(\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h}) -1&\text{if};g_{n(w,j)}=1\ \sigma(\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h}) &\text{if};g_{n(w,j)}=-1 \end{cases}\ =\sigma(\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h}) -t_{n(w,j)}

   其中：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-57">t_{n(w,j)}=\begin{cases}
   ```

   1,& \text{if node $ j+1$ at path , root $\rightarrow w $ , is left child of node j*j*}\ 0,& \text{if node j+1*j*+1 at path , root \rightarrow w→*w* , is right child of node $ j$} \end{cases}

   定义：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$e_{n(w,j)} = \frac{\partial E}{\partial (\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h})} = \sigma(\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h}) -t_{n(w,j)}$</div></div><ul><li><p>$\sigma(\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h})$<span> 为预测选择 </span>$j$<span> 的左子节点的概率。</span></p></li><li><p>$e_{n(w,j)} $<span> 的物理意义为：从根节点到单词 </span>$w$<span> 的路径上，第 </span>$j$<span> 个节点的选择误差：</span></p><ul><li><span>如果下一个节点选择第 </span>$j$<span> 个节点的左子节点，则  </span>$t_{n(w,j)}=1$<span>， 此时 </span>$e_{n(w,j)} $<span> 表示预测的不足。</span></li><li><span>如果下一个节点选择第 </span>$j$<span> 个节点的右子节点，则  </span>$t_{n(w,j)}= 0$<span>， 此时 </span>$e_{n(w,j)} $<span> 表示预测的过量。</span></li></ul></li></ul></li><li><p><span>考虑内部节点 </span>$n(w,j)$<span>，其向量表达为 </span>$\mathbf{\vec v}^\prime_{n(w,j)}$<span>。则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n524" cid="n524" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\frac {\partial E}{\partial \mathbf{\vec v}^\prime_{n(w,j)}}=\frac{\partial E}{\partial (\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h}) }\times\frac{\partial (\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h})}{\partial \mathbf{\vec v}^\prime_{n(w,j)}}=e_{n(w,j)} \times \mathbf{\vec h}$</div></div><p><span>得到向量表达为 </span>$\mathbf{\vec v}^\prime_{n(w,j)}$<span> 的更新方程：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n526" cid="n526" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec v}^{\prime(new)}_{n(w,j)}=\mathbf{\vec v}^{\prime(old)}_{n(w,j)}-\eta\times e_{n(w,j)}  \times \mathbf{\vec h};\quad j=1,2,\cdots,L(w)-1$</div></div><ul><li><p><span>对于每个单词 </span>$w$<span> ，由于它是叶节点，因此可以更新 </span>$L(w)-1$<span> 个内部节点的向量表达。</span></p></li><li><p><span>当模型的预测概率较准确，即  </span>$\sigma(\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h}) \simeq t_{n(w,j)}$<span>  时，则 </span>$e_{n(w,j)}$<span> 接近0 。此时梯度非常小，</span>$\mathbf{\vec v}^\prime_{n(w,j)}$<span> 的更新幅度也会非常小。</span></p><p><span>当模型的预测概率较不准，则 </span>$e_{n(w,j)}$<span>  较大。此时梯度会较大，</span>$\mathbf{\vec v}^\prime_{n(w,j)}$<span> 的更新幅度也会较大。</span></p></li></ul></li><li><p><span>对于内部结点的向量表达 </span>$\mathbf{\vec v}^\prime_{n(w,j)}$<span>  的更新方程适用于 </span><code>CBOW</code><span> 模型和 </span><code>Skip-Gram</code><span> 模型。但是在 </span><code>Skip-Gram</code><span> 模型中，需要对 </span>$C$<span> 个输出的每一个单词进行更新。</span></p></li><li><p><code>CBOW</code><span> 输入参数更新：对于 </span><code>CBOW</code><span> 模型，定义：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n537" cid="n537" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-61">\mathbf {\overrightarrow {EH}}=\frac{\partial E}{\partial \mathbf{\vec h}}=\sum_{j=1}^{L(w)-1}\frac{\partial E}{\partial (\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h}) }\times \frac{\partial (\mathbf{\vec v}^\prime_{n(w,j)}\cdot \mathbf{\vec h})}{\partial \mathbf{\vec h}}
   ```

   =\sum_{j=1}^{L(w)-1} e_{n(w,j)} \mathbf{\vec v}^\prime_{n(w,j)}

  $\mathbf{\overrightarrow {EH}}$的物理意义为：二叉树中所有内部节点向量表达的加权和，其权重为$e_{n(w,j)}$。

   考虑到$\mathbf{\vec h}=\frac 1C \mathbf W^T(\mathbf{\vec x}_1+\mathbf{\vec x}_2+\cdots+\mathbf{\vec x}_C)$，则有：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\frac{\partial E}{\partial w_{k,i}}=\frac{\partial E}{\partial h_i}\times\frac{\partial h_i}{\partial w_{k,i}}=\frac 1CEH_i\times (x_{(1,k)}+x_{(2,k)}+\cdots,+x_{(C,k)})$</div></div><p><span>写成矩阵的形式为：</span>$\frac{\partial E}{\partial \mathbf W}= \frac 1C \sum_{c=1}^C\mathbf{\vec x}_c\otimes \mathbf{\overrightarrow {EH}}$<span> ，其中 </span>$\otimes$<span> 为克罗内克积。</span></p><p><span>将 </span>$\mathbf W$<span> 的更新分解为 </span>$C$<span> 次，每次对应于一个输入 </span>$\mathbf{\vec x}_c$<span>  。因此得到 </span>$\mathbf W$<span> 的更新方程：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n543" cid="n543" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}_{I_i}^{(new)}=\mathbf{\vec w}_{I_i}^{(old)}-\frac 1C\eta\mathbf{\overrightarrow{EH}},\quad i=1,2,\cdots,C$</div></div><p><span>其中</span>$I_i$<span> 为第 </span>$i$<span> 个输入单词在词表 </span>$\mathbb V$<span> 中的编号。</span></p></li><li><p><code>Skip-Gram</code><span> 输入参数更新：对于 </span><code>Skip-Gram</code><span> 模型，定义：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n547" cid="n547" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-64">\mathbf{\overrightarrow {EH}} = \frac{\partial E}{\partial \mathbf{\vec h}} = \sum_{c=1}^{C}\sum_{j=1}^{L(w_c)-1}\frac{\partial E}{\partial (\mathbf{\vec v}_{n(w_c,j)}\cdot \mathbf{\vec h}) }\times \frac{\partial (\mathbf{\vec v}_{n(w_c,j)}\cdot \mathbf{\vec h})}{\partial \mathbf{\vec h}}\\
   ```

   =\sum_{c=1}^C\sum_{j=1}^{L(w_c)-1}e_{n(w_c,j)}\times \mathbf{\vec v}*{n(w_c,j)}*

   *其中：$w_c$表示网络第$c$个输出的输出单词。*

   *注意：由于引入分层`softmax`，导致更新路径因为输出单词的不同而不同。因此$\sum*{j=1}^{L(w_c)-1}$会因为$c$的不同而不同。

   与`Skip-Gram` 中推导相同，$\mathbf W$的更新方程为：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}_I^{(new)}=\mathbf{\vec w}_I^{(old)}-\eta\mathbf{\overrightarrow{EH}}$</div></div><p><span>其中 </span>$\mathbf{\vec w}_I$<span> 为 </span>$\mathbf{\vec x}$<span> 非零分量对应的 </span>$\mathbf W$<span> 中的行，而 </span>$\mathbf W$<span> 的其它行在本次更新中都保持不变。</span></p></li></ol><h4><a name="3.3.2-负采样" class="md-header-anchor"></a><span>3.3.2 负采样</span></h4><h5><a name="a)-原理" class="md-header-anchor"></a><span>a) 原理</span></h5><ol start='' ><li><p><span>在网络的输出层，真实的输出单词对应的输出单元作为正向单元，其它所有单词对应的输出单元为负向单元。</span></p><ul><li><p><span>正向单元的数量为 1，毋庸置疑，正向单元必须输出。</span></p></li><li><p><span>负向单元的数量为 </span>$V-1$<span>，其中 </span>$V$<span> 为词表的大小，通常为上万甚至百万级别。如果计算所有负向单元的输出概率，则计算量非常庞大。</span></p><p><span>可以从所有负向单元中随机采样一批负向单元，仅仅利用这批负向单元来更新。这称作负采样。</span></p></li></ul></li><li><p><span>负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布。</span></p><p><span>对于真实的输出分布，有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n567" cid="n567" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$y_j=p(\text{word}_j\mid \mathbf{\vec x})=\frac{\exp(u_j)}{\sum_{j^\prime=1}^V \exp(u_{j^\prime})},\quad j=1,2,\cdots,V$</div></div><p><span>对于负采样后的输出分布，假设真实输出单词 </span>$w_O $<span> 对应于输出单元 </span>$j^*$<span> ，负采样的 </span>$K$<span> 个单词对应的输出单元 </span>$\mathcal W_{neg}=\{j_{neg_1},\cdots,j_{neg_K}\}$<span>，则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n569" cid="n569" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-67">\hat y_j=\hat p(\text{word}_j\mid \mathbf{\vec x})=\begin{cases}
   ```

   \frac{\exp(u_j)}{\sum_{j^\prime \in {j^*,j_{neg_1},\cdots,j_{neg_K}}} \exp(u_{j^\prime})},&\quad j\in {j^*,j_{neg_1},\cdots,j_{neg_K}}\ 0, &j \not \in {j^*,j_{neg_1},\cdots,j_{neg_K}} \end{cases}*

   - *在参数的每一轮更新中，负采样实际上只需要用到一部分单词的输出概率。*

   - *对于未被采样到的负向单元$j$，其输出单元的预测误差$e_j = 0$， 则$\mathbf{\vec w}^{\prime}\*j$不会被更新。

   - $\mathbf{\overrightarrow {EH}} = \mathbf W^\prime \mathbf{\vec e} =\sum\*{j=1}^Ve_j \mathbf{\vec w}^{\prime }\*j$中仅有负采样的单元$j\*{neg_1},\cdots,j_{neg_K}$起作用，因此$\mathbf{\vec w}_I^{(new)}$的更新仅仅依赖于正向单元和负采样的单元。*

   - *随着训练的推进，概率分布${y_1,y_2,\cdots,y_V}$逐渐接近于真实的分布${0,\cdots,0,1, 0,\cdots,0}$（第$j^*$位为 1），其绝大部分概率接近 0 、$y_{j^*}$接近 1 。

     而概率分布${\hat y_1,\cdots,\hat y_V}$也有类似的性质，因此用概率分布${\hat y_1,\cdots,\hat y_V}$去模拟概率分布${y_1,y_2,\cdots,y_V}$效果较好。

2. 负采样时，每个负向单元是保留还是丢弃是随机的。负向单元采样的概率分布称作`noise` 分布，记做$P_n(w)$。

  $P_n(w)$可以为任意的概率分布（通常需要根据经验来选择）。谷歌给出的建议是挑选 5~10 个负向单元，根据下面公式来采样：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$P_n(w)=\frac{freq(w)^{3/4}}{\sum_{w^\prime\ne j^*} freq(w^\prime)^{3/4} }$</div></div><p><span>其中：</span>$freq(w)$<span> 为单词在语料库中出现的概率，分母仅考虑负向单元（不考虑正向单元）。</span></p><p>$P_n(w)$<span> 的物理意义为：单词在语料库中出现的概率越大，则越可能被挑中。</span></p></li></ol><h5><a name="b)-参数更新" class="md-header-anchor"></a><span>b) 参数更新</span></h5><ol start='' ><li><p><span>假设输出的单词分类两类：</span></p><ul><li><span>正类：只有一个，即真实的输出单词 </span>$w_O$</li><li><span>负类：从 </span>$P_n(w)$<span> 采样得到的 </span>$K$<span> 个单词 </span>$\mathcal W_{neg}=\{j_{neg_1},\cdots,j_{neg_K}\}$<span> </span></li></ul><p><span>论文</span><code>word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method</code><span> 的作者指出：下面的训练目标能够得到更好的结果：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n596" cid="n596" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$E=-\log\sigma(\mathbf{\vec w^\prime}_{w_O}\cdot \mathbf{\vec h})-\sum_{j\in \mathcal W_{neg}}\log\sigma(-\mathbf{\vec w^\prime}_j\cdot \mathbf{\vec h})$</div></div><p><span>其中：</span></p><ul><li>$\mathbf{\vec w^\prime}_{w_O}$<span> 为真实的输出单词对应的输出向量，</span>$\mathbf{\vec w^\prime}_j$<span> 为负采样的单词得到的输出向量。</span></li><li>$\sigma(\mathbf{\vec w^\prime}_{w_O}\cdot \mathbf{\vec h})$<span>  ：在单词 </span>$w_O$<span> 上输出为正类的概率；</span>$\sigma(-\mathbf{\vec w^\prime}_{j}\cdot \mathbf{\vec h})$<span> ：在单词 </span>$j$<span> 上输出为负类的概率。</span></li></ul><p><span>该目标函数是一个经验公式，而不是采用理论上的交叉熵 </span>$ -\log  \frac{\exp(\mathbf{\vec w^\prime}_{w_O}\cdot \mathbf{\vec h})}{\sum_{j^\prime=1}^V \exp(\mathbf{\vec w^\prime}_{{j^\prime}}\cdot \mathbf{\vec h})} $<span> 。其物理意义为：在正类单词上取正类的概率尽可能大，在负类单词上取负类的概率尽可能大。</span></p><p><span>它是从另一个角度考虑：输出为正向单元的概率 * 输出为负向单元的概率。</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n605" cid="n605" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\sigma(\mathbf{\vec w^\prime}_{w_O}\cdot \mathbf{\vec h}) \prod_{j^\prime\ne w_O}\sigma(\mathbf{\vec w^\prime}_{j^\prime}\cdot \mathbf{\vec h})$</div></div><p><span>其负的对数似然为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n607" cid="n607" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-71">-\log \left(\sigma(\mathbf{\vec w^\prime}_{w_O}\cdot \mathbf{\vec h}) \prod_{j^\prime\ne w_O}\sigma(\mathbf{\vec w^\prime}_{j^\prime}\cdot \mathbf{\vec h}) \right)\\
   ```

   = -\log \sigma(\mathbf{\vec w^\prime}*{w_O}\cdot \mathbf{\vec h}) - \sum*{j^\prime\ne w_O} \log \sigma(\mathbf{\vec w^\prime}*{j^\prime}\cdot \mathbf{\vec h})*

   *仅仅考虑负采样，则可得到：$E=-\log\sigma(\mathbf{\vec w^\prime}*{w_O}\cdot \mathbf{\vec h})-\sum_{j\in \mathcal W_{neg}}\log\sigma(-\mathbf{\vec w^\prime}_j\cdot \mathbf{\vec h})$。

3. 根据$E$的定义，有：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-72">\frac{\partial E}{\partial (\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})}=\begin{cases}
   ```

   \sigma(\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})-1,& \text{if} ;j=w_O\ \sigma(\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h}),& \text{if} ; j \in \mathcal W_{neg} \end{cases} =\sigma(\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})-t_j

   其中$t_j$标记了单词$j$的标签：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-73">t_j=\begin{cases}
   ```

   1,& \text{if j=w_O*j*=*w**O*}\ 0,&\text{esle} \end{cases}

4. 令$e_{j} = \sigma(\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})-t_j$，它刻画了网络在正类单词和负类单词上的预测误差。

   - 当$j =w_O$时，$e_{j}$表示对正类单词预测概率的不足。
   - 当$j \in \mathcal W_{neg}$时，$e_{j}$表示对负类单词预测概率的过量。

   根据：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\frac{\partial E}{\partial \mathbf{\vec w}^\prime_{j}}=\frac{\partial E}{\partial (\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})}\times \frac{\partial (\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})}{\partial \mathbf{\vec w}^\prime_{w_j}}=e_{j} \times \mathbf{\vec h}$</div></div><p><span>则有输出向量的更新方程：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n624" cid="n624" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}^{\prime(new)}_{j}=\mathbf{\vec w}^{\prime(old)}_{j}-\eta\times e_{j} \times \mathbf{\vec h}$</div></div></li><li><p><span>给定一个样本，在更新输出向量时，只有 </span>$K+1$<span> 个输出向量（ 1 个输出单词 </span>$w_O$<span> 、</span>$K$<span> 个负采样单词对应的输出向量）得到更新，其中 </span>$K$<span> 通常数量很小。其它所有单词对应的输出向量未能得到更新。</span></p><p><span>相比较而言：</span></p><ul><li><span>原始算法中，给定一个样本在更新输出向量时，所有输出向量（一共 </span>$V$<span> 个）都得到更新</span></li><li><span>分层</span><code>softmax</code><span> 算法中，给定一个样本在更新输出向量时，</span>$L(w)-1$<span> 个内部节点的向量表达得到更新。</span></li></ul></li><li><p><span>输出向量的更新方程可以用于</span><code>CBOW</code><span> 模型和 </span><code>Skip-Gram</code><span>  模型。</span></p><p><span>若用于</span><code>Skip-Gram</code><span> 模型，则对每个输出依次执行输出向量的更新。</span></p></li><li><p><code>CBOW</code><span> 输入向量参数更新：对于 </span><code>CBOW</code><span> 模型，定义：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n638" cid="n638" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-76">\mathbf{\overrightarrow {EH}}=\frac{\partial E}{\partial \mathbf{\vec h}}=\sum_{j\in \{w_O\}\bigcup \mathcal W_{neg}}\frac{\partial E}{\partial (\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})}\times \frac{\partial (\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})}{\partial \mathbf{\vec h}}
   ```

   =\sum_{j\in {w_O}\bigcup \mathcal W_{neg}} e_{j} \times \mathbf{\vec w}^\prime_{j}

  $\mathbf{\overrightarrow {EH}}$的物理意义为：负采样单词、输出单词对应输出向量的加权和，其权重为$e_{j}$。

   与`分层softmax: CBOW 输入向量参数更新` 中的推导相同，$\mathbf W$的更新方程为：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}_{I_i}^{(new)}=\mathbf{\vec w}_{I_i}^{(old)}-\frac 1C\eta\mathbf{\overrightarrow{EH}},\quad i=1,2,\cdots,C$</div></div><p><span>其中</span>$I_i$<span> 为第 </span>$i$<span> 个输入单词在词表 </span>$\mathbb V$<span> 中的编号。</span></p></li><li><p><code>Skip-Gram</code><span> 输入向量参数更新：对于 </span><code>Skip-Gram</code><span> 模型，定义：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n645" cid="n645" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-78">\mathbf{\overrightarrow {EH}}=\frac{\partial E}{\partial \mathbf{\vec h}}=\sum_{c=1}^C\sum_{j\in \{w_O^c\}\bigcup \mathcal W_{neg}^c}\frac{\partial E}{\partial (\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})}\times \frac{\partial (\mathbf{\vec w}^\prime_{j}\cdot \mathbf{\vec h})}{\partial \mathbf{\vec h}}\\
   ```

   =\sum_{c=1}^C\sum_{j\in {w_O^c}\bigcup \mathcal W_{neg}^c} e_{j} \times \mathbf{\vec w}^\prime_{j}

   其中：$w_O^c$表示网络第$c$个输出的输出单词，$\mathcal W_{neg}^c$表示网络第$c$个输出的负采样单词集。

   注意：由于引入负采样，导致网络每个输出中，对应的输出单词有所不同，负采样单词也有所不同。因此${w_O^c}\bigcup \mathcal W_{neg}^c$会因为$c$的不同而不同。

   与`Skip-Gram` 中推导相同，$\mathbf W$的更新方程为：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}_I^{(new)}=\mathbf{\vec w}_I^{(old)}-\eta\mathbf{\overrightarrow{EH}}$</div></div><p><span>其中 </span>$\mathbf{\vec w}_I$<span> 为 </span>$\mathbf{\vec x}$<span> 非零分量对应的 </span>$\mathbf W$<span> 中的行，而 </span>$\mathbf W$<span> 的其它行在本次更新中都保持不变。</span></p></li></ol><h4><a name="3.3.3-降采样" class="md-header-anchor"></a><span>3.3.3 降采样</span></h4><ol start='' ><li><p><span>对于一些常见单词，比如 </span><code>the</code><span>，我们可以在语料库中随机删除它。这有两个原因（假设使用 </span><code>CBOW</code><span> ）：</span></p><ul><li><span>当 </span><code>the</code><span> 出现在上下文时，该单词并不会为目标词提供多少语义上的信息。</span></li><li><span>当 </span><code>the</code><span> 作为目标词时，该单词从语义上本身并没有多大意义，因此没必要频繁更新。</span></li></ul><p><span>降采样过程中，单词 </span>$w$<span> 被保留的概率为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n661" cid="n661" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$p(w) = \left(\sqrt{\frac{z(w)}{\text{subsampling_rate}}}+1\right)\times \frac{\text{subsampling_rate}}{z(w)}$</div></div><p><span>其中 </span>$z(w)$<span> 为单词 </span>$w$<span> 在语料库中出现的概率，</span>$\text{subsampling_rate}$<span> 为降采样率（默认为 </span><code>0.001</code><span>）。</span></p><p><span>可以看到：随着单词在语料库中出现的词频越来越大，该单词保留的概率越来越低。</span></p><p><img src="../imgs/word_representation/word2vec_subsampling.png" width="400px" /></p></li></ol><h3><a name="3.4-subword-embedding" class="md-header-anchor"></a><span>3.4 subword embedding</span></h3><ol start='' ><li><p><span>论文 </span><code>《Enriching Word Vectors with Subword Information》</code><span> 中，作者提出通过增加字符级信息来训练词向量。</span></p><p><span>下图给出了该方法在维基百科上训练的词向量在相似度计算任务上的表现（由人工评估模型召回的结果）。</span><code>sisg-</code><span> 和 </span><code>sisg</code><span> 模型均采用了 </span><code>subword embedding</code><span>，区别是：对于未登录词，</span><code>sisg-</code><span>  采用零向量来填充，而 </span><code>sisg</code><span> 采用 </span><code>character n-gram embedding</code><span> 来填充。</span></p><p><img src="../imgs/word_representation/word2vec_char.png" width="300px" /></p></li><li><p><span>单词拆分：每个单词表示为一组 </span><code>character n-gram</code><span> 字符（不考虑顺序），以单词 </span><code>where</code><span>、 </span><code>n=3</code><span> 为例：</span></p><ul><li><span>首先增加特殊的边界字符 </span><code>&lt;</code><span> （单词的左边界）和 </span><code>&gt;</code><span> （单词的右边界）。</span></li><li><span>然后拆分出一组 </span><code>character n-gram</code><span> 字符：</span><code>&lt;wh, whe,her,ere,re&gt;</code><span> 。</span></li><li><span>最后增加单词本身：</span><code>&lt;where&gt;</code><span>。</span></li></ul><p><span>为了尽可能得到多样性的 </span><code>character n-gram</code><span> 字符，作者抽取了所有 </span><code>3&lt;= n &lt;= 6</code><span> 的 </span><code>character n-gram</code><span> 。以单词 </span><code>mistake</code><span> 为例：</span></p><pre spellcheck="false" class="md-fences md-end-block ty-contain-cm modeLoaded" lang="c"><div class="CodeMirror cm-s-inner CodeMirror-wrap" lang="c"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 0px; left: 8px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation" style=""><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: 0px; width: 0px;"></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&lt;</span><span class="cm-variable">mi</span>,<span class="cm-variable">mis</span>,<span class="cm-variable">ist</span>,<span class="cm-variable">sta</span>,<span class="cm-variable">tak</span>,<span class="cm-variable">ake</span>,<span class="cm-variable">ke</span><span class="cm-operator">&gt;</span>, &nbsp; <span class="cm-comment">// n = 3</span></span></pre></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&lt;</span><span class="cm-variable">mis</span>,<span class="cm-variable">mist</span>,<span class="cm-variable">ista</span>,<span class="cm-variable">stak</span>,<span class="cm-variable">take</span>,<span class="cm-variable">ake</span><span class="cm-operator">&gt;</span>, <span class="cm-comment">// n = 4</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&lt;</span><span class="cm-variable">mist</span>,<span class="cm-variable">mista</span>,<span class="cm-variable">istak</span>,<span class="cm-variable">stake</span>,<span class="cm-variable">take</span><span class="cm-operator">&gt;</span>, <span class="cm-comment">// n = 5</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&lt;</span><span class="cm-variable">mista</span>,<span class="cm-variable">mistak</span>,<span class="cm-variable">istake</span>,<span class="cm-variable">stake</span><span class="cm-operator">&gt;</span>, &nbsp; <span class="cm-comment">// n = 6</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-operator">&lt;</span><span class="cm-variable">mistake</span><span class="cm-operator">&gt;</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-comment">// 单词本身</span></span></pre></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom-width: 0px; border-bottom-style: solid; border-bottom-color: transparent; top: 110px;"></div><div class="CodeMirror-gutters" style="display: none; height: 110px;"></div></div></div></pre><p><span>注意：这里的 </span><code>take</code><span> 和 </span><code>&lt;take&gt;</code><span> 不同。前者是某个</span><code>character n-gram</code><span>，后者是一个单词。</span></p></li><li><p><span>一旦拆分出单词，则：</span></p><ul><li><span>词典 </span>$\mathbb V$<span> 扩充为包含所有单词和 </span><code>N-gram</code><span> 字符。</span></li><li><span>网络输入包含单词本身以及该单词的所有 </span><code>character n-gram</code><span> ，网络输出仍然保持为单词本身。</span></li></ul><p><span>模型采用 </span><code>word2vec</code><span> ，训练得到每个</span><code>character n-gram embedding</code><span> 。最终单词的词向量是其所有 </span><code>character n-gram embedding</code><span>包括其本身 </span><code>embedding</code><span> 的和（或者均值）。</span></p><p><span>如：单词 </span><code>where</code><span> 的词向量来自于下面</span><code>embedding</code><span> 之和：</span></p><ul><li><span>单词 </span><code>&lt;where&gt;</code><span> 本身的词向量。</span></li><li><span>一组 </span><code>character n-gram</code><span>  字符 </span><code>&lt;wh, whe,her,ere,re&gt;</code><span> 的词向量。</span></li></ul></li><li><p><span>利用字符级信息训练词向量有两个优势：</span></p><ul><li><p><span>有利于低频词的训练。</span></p><p><span>低频词因为词频较低，所以训练不充分。但是低频词包含的 </span><code>character n-gram</code><span> 可能包含某些特殊含义并且得到了充分的训练，因此有助于提升低频词的词向量的表达能力。</span></p></li><li><p><span>有利于获取 </span><code>OOV</code><span> 单词（未登录词：不在词汇表中的单词）的词向量。</span></p><p><span>对于不在词汇表中的单词，可以利用其 </span><code>character n-gram</code><span>  的</span><code>embedding</code><span> 来获取词向量。</span></p></li></ul></li></ol><h3><a name="3.5-应用" class="md-header-anchor"></a><span>3.5 应用</span></h3><ol start='' ><li><p><span>模型、语料库、超参数这三个方面都会影响词向量的训练，其中语料库对训练结果的好坏影响最大。</span></p><p><span>根据论文 </span><code>How to Generate a Good Word Embedding?</code><span> ，作者给出以下建议：</span></p><ul><li><p><span>模型选择：所有的词向量都是基于分布式分布假说：拥有相似上下文的单词，其词义相似。根据目标词和上下文的关系，模型可以分为两类：</span></p><ul><li><span>通过上下文来预测目标词。这类模型更能够捕获单词之间的可替代关系。</span></li><li><span>通过目标词来预测上下文。</span></li></ul><p><span>通过实验发现：简单的模型（</span><code>Skip-Gram</code><span>) 在小语料库下表现较好。复杂的模型在大语料库下略有优势。</span></p></li><li><p><span>语料库：实际上语料库并不是越大越好，语料库的领域更重要。</span></p><ul><li><span>选择了合适的领域，可能只需要 </span><code>1/10</code><span> 甚至 </span><code>1/100</code><span> 的语料就能够得到一个大的、泛领域语料库的效果。</span></li><li><span>如果选择不合适的领域，甚至会导致负面效果，比随机词向量效果还差。</span></li></ul></li><li><p><span>超参数：</span></p><ul><li><p><span>词向量的维度：</span></p><ul><li><span>做词向量语义分析任务时，一般维度越大，效果越好。</span></li><li><span>做具体</span><code>NLP</code><span> 任务时（用作输入特征、或者网络初始化），</span><code>50</code><span> 维之后效果提升就比较少了。</span></li></ul></li><li><p><span>迭代次数：由于训练词向量的目标是尽可能精确地预测目标词，这个优化目标和实际任务并不一致。因此最好的做法是：直接用实际任务的验证集来挑选迭代次数。</span></p><p><span>如果实际任务非常耗时，则可以随机挑选某个简单任务（如：情感分类）及其验证集来挑选迭代次数。</span></p></li></ul></li></ul></li><li><p><code>word2vec</code><span> 还有一些重要的超参数：</span></p><ul><li><span>窗口大小：该超参数通常和语料库中句子长度有关，可以统计句子长度分布来设置。</span></li><li><code>min-count</code><span>：最小词频训练阈值，词频低于该阈值的词被过滤。</span></li><li><span>降采样率 </span><code>subsampling_rate</code><span>：降采样率越低，高频词保留的越少低频词保留的越多。</span></li></ul></li><li><p><code>word2vec</code><span> 结果评估：</span></p><ul><li><span>通过 </span><code>kmeans</code><span> 聚类，查看聚类的簇分布。</span></li><li><span>通过词向量计算单词之间的相似度，查看相似词。</span></li><li><span>通过类比来查看类比词：</span><code>a</code><span> 之于 </span><code>b</code><span>，等价于 </span><code>c</code><span> 之于  </span><code>d</code><span> 。</span></li><li><span>使用 </span><code>tsne</code><span> 降维可视化查看词的分布。</span></li></ul></li><li><p><span>在 </span><code>word2vec</code><span> 中实际上存在两种类型的</span><code>embedding</code><span> 向量：</span>$\mathbf W \in \mathbb R^{V\times N}$<span> 的第 </span>$j$<span> 行 </span>$\mathbf{\vec w}_j ^T$<span> 称作单词 </span>$\text{word}_j$<span> 的输入向量， </span>$\mathbf W^\prime \in \mathbb R^{N\times V}$<span> 的第 </span>$j$<span> 列 </span>$\mathbf{\vec w}_j^{\prime }$<span> 称作单词  </span>$ \text{word}_j$<span> 的输出向量。</span></p><p><span>大多数论文中都采用输入向量 </span>$\mathbf{\vec w}_j $<span> 作为单词 </span>$\text{word}_j$<span> 的表达，而论文 </span><code>Using the Output Embedding to Improve Language Models</code><span>  综合了输入向量和输出向量。在该论文中，作者得出结论：</span></p><ul><li><span>在 </span><code>skip-gram</code><span> 模型中，在常见的衡量词向量的指标上，输出向量略微弱于输入向量。</span></li><li><span>在基于 </span><code>RNN</code><span> 的语言模型中，输出向量反而强于输入向量。</span></li><li><span>通过强制要求 </span>$\mathbf W^T =  \mathbf W^\prime$<span>，这可以使得输入向量等于输出向量。这种方式得到的词向量能够提升语言模型的困惑度</span><code>perplexity</code><span>。</span></li></ul></li><li><p><code>word2vec</code><span> 可以用于计算句子相似度。博客 </span><code>Comparing Sentence Similarity Methods</code><span> 总结了 6 种计算句子相似度的方法：</span></p><ul><li><p><span>无监督方法：</span></p><ul><li><p><span>对句子中所有的词的词向量求平均，获得</span><code>sentence embedding</code><span> 。</span></p></li><li><p><span>对句子中所有的词的词向量加权平均，每个词的权重为 </span><code>tf-idf</code><span> ，获得</span><code>sentence embedding</code><span> 。</span></p></li><li><p><span>对句子中所有的词的词向量加权平均，每个词的权重为 </span><code>smooth inverse frequency:SIF</code><span> ；然后考虑所有的句子，并执行主成分分析；最后对每个句子的词向量加权平均减去</span><code>first principal componet</code><span>，获得</span><code>sentence embedding</code><span> 。</span></p><p><code>SIF</code><span> 定义为： </span>$\frac{a}{a+p(w)}$<span>，其中  </span>$a$<span> 是一个超参数（通常取值为 0.001）， </span>$p(w)$<span>  为数据集中单词 </span>$w$<span> 的词频。</span></p></li><li><p><span>通过 </span><code>Word Mover&#39;s Distance:WMD</code><span> ，直接度量句子之间的相似度。</span></p><p><code>WMD</code><span> 使用两个句子中单词的词向量来衡量一个句子中的单词需要在语义空间中</span><code>移动</code><span>到另一个句子中的单词的最小距离。</span></p></li></ul></li><li><p><span>有监督方法：</span></p><ul><li><p><span>通过分类任务来训练一个文本分类器，取最后一个 </span><code>hidden layer</code><span> 的输出作为 </span><code>sentence embedding</code><span>。</span></p><p><span>其实这就是使用文本分类器的前几层作为 </span><code>encoder</code><span> 。</span></p></li><li><p><span>直接训练一对句子的相似性，其优点是可以直接得到 </span><code>sentence embeding</code><span> 。</span></p></li></ul></li></ul><p><span>最终结论是：简单加权的词向量平均已经可以作为一个较好的 </span><code>baseline</code><span> 。</span></p></li></ol><h2><a name="四、glove" class="md-header-anchor"></a><span>四、GloVe</span></h2><ol start='' ><li><p><span>学习词向量的所有无监督方法最终都是基于语料库的单词共现统计，因此这些模型之间存在共性。</span></p></li><li><p><span>词向量学习算法有两个主要的模型族：</span></p><ul><li><p><span>基于全局矩阵分解的方法，如：</span><code>latent semantic analysis:LSA</code><span> 。</span></p><ul><li><span>优点：能够有效的利用全局的统计信息。</span></li><li><span>缺点：在单词类比任务（如：</span><code>国王 vs 王后</code><span> 类比于</span><code>男人 vs 女人</code><span>）中表现相对较差。</span></li></ul></li><li><p><span>基于局部上下文窗口的方法，如：</span><code>word2vec</code><span>。</span></p><ul><li><span>优点：在单词类比任务中表现较好。</span></li><li><span>缺点：因为</span><code>word2vec</code><span> 在独立的局部上下文窗口上训练，因此难以利用单词的全局统计信息。</span></li></ul></li></ul></li><li><p><code>Global Vectors for Word Representation:GloVe</code><span> 结合了</span><code>LSA</code><span> 算法和</span><code>Word2Vec</code><span> 算法的优点，既考虑了全局统计信息，又利用了局部上下文。</span></p></li></ol><h3><a name="4.1-原理" class="md-header-anchor"></a><span>4.1 原理</span></h3><ol start='' ><li><p><span>设</span><code>单词-单词</code><span> 共现矩阵为 </span>$\mathbf X$<span>  ，其中 </span>$X_{i,j}$<span> 表示在整个语料库中单词 </span>$\text{word}_j$<span> 在单词 </span>$\text{word}_i$<span> 上下文中出现的次数。 令：</span></p><ul><li>$  X_i =\sum_{k=1}^V X_{i,k}$<span> ，它表示：单词 </span>$\text{word}_i$<span> 上下文中出现的所有单词的总数。</span></li><li>$P_{i,j} = P(\text{word}_j\mid \text{word}_i) = \frac {X_{i,j}}{X_i}$<span> ，它表示：单词 </span>$\text{word}_j$<span> 出现在单词 </span>$\text{word}_i$<span> 的上下文中的概率。</span></li><li>$Ratio_{i,j}^k=\frac{P_{i,k}}{P_{j,k}}$<span> ，它表示：单词 </span>$\text{word}_k$<span> 出现在单词 </span>$\text{word}_i$<span> 的上下文中的概率，相对于单词 </span>$\text{word}_k$<span> 出现在单词 </span>$\text{word}_j$<span> 的上下文中的概率的比值。</span></li></ul><p><span>从经验中可以发现以下规律：</span></p><figure><table><thead><tr><th style='text-align:center;' >&nbsp;</th><th style='text-align:center;' ><span>单词 </span>$\text{word}_k$<span> 和单词 </span>$\text{word}_i$<span> 相关</span></th><th style='text-align:center;' ><span>单词 </span>$\text{word}_k$<span> 和单词 </span>$\text{word}_i$<span> 不相关</span></th></tr></thead><tbody><tr><td style='text-align:center;' ><span>单词 </span>$\text{word}_k$<span> 和单词 </span>$\text{word}_j$<span> 相关</span></td><td style='text-align:center;' >$Ratio_{i,j}^k$<span> 趋近于 1</span></td><td style='text-align:center;' >$Ratio_{i,j}^k$<span> 比较小</span></td></tr><tr><td style='text-align:center;' ><span>单词 </span>$\text{word}_k$<span> 和单词 </span>$\text{word}_j$<span> 不相关</span></td><td style='text-align:center;' >$Ratio_{i,j}^k$<span> 比较大</span></td><td style='text-align:center;' >$Ratio_{i,j}^k$<span> 趋近于 1</span></td></tr></tbody></table></figure><p><span>因此 </span>$Ratio_{i,j}^k$<span> 能够反映单词之间的相关性。</span></p></li><li><p><span>假设单词 </span>$\text{word}_i,\text{word}_j,\text{word}_k$<span> 的词向量分别为 </span>$\mathbf{\vec w}_i,\mathbf{\vec w}_j,\mathbf{\vec w}_k$<span> 。</span></p><p><code>GloVe</code><span> 认为：这三个单词的词向量经过某个函数的映射之后等于  </span>$Ratio_{i,j}^k$<span> 。即：词向量中包含了共现矩阵的信息。</span></p><p><span>假设这个映射函数为 </span>$F$<span> ，则有：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n848" cid="n848" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$F(\mathbf{\vec w}_i,\mathbf{\vec w}_j,\mathbf{\vec w}_k) =  Ratio_{i,j}^k =\frac{P_{i,k}}{P_{j,k}}$</div></div><p><span>现在的问题是 </span>$F(\cdot)$<span> 未知，词向量 </span>$\mathbf{\vec w}_i,\mathbf{\vec w}_j,\mathbf{\vec w}_k$<span> 也是未知。如果能够确定 </span>$F(\cdot)$<span> ，则可以求解词向量。</span></p></li><li><p><span>由于 </span>$F(\cdot)$<span>  映射的是向量空间，而向量空间是一个线性空间。因此从右侧的除法 </span>$\frac{P_{i,k}}{P_{j,k}}$<span> 可以联想到对 </span>$\mathbf{\vec w}_i$<span> 和 </span>$\mathbf{\vec w}_j$<span> 做减法。即 </span>$F(\cdot)$<span> 的形式为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n852" cid="n852" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$F(\mathbf{\vec w}_i-\mathbf{\vec w}_j,\mathbf{\vec w}_k) =  \frac{P_{i,k}}{P_{j,k}}$</div></div><p><span>由于 </span>$\mathbf{\vec w}_i-\mathbf{\vec w}_j$<span> 和 </span>$\mathbf{\vec w}_k $<span> 均为向量，而 </span>$ \frac{P_{i,k}}{P_{j,k}}$<span> 为标量。因此可以联想到向量的内积。即 </span>$F(\cdot)$<span> 的形式为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n854" cid="n854" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$F\left((\mathbf{\vec w}_i-\mathbf{\vec w}_j)^T\mathbf{\vec w}_k\right) = F\left(\mathbf{\vec w}_i^T\mathbf{\vec w}_k-\mathbf{\vec w}_j^T\mathbf{\vec w}_k\right)  =  \frac{P_{i,k}}{P_{j,k}}$</div></div><p><span>上式左边为差的形式，右边为商的形式。因此联想到函数 </span>$\exp(\cdot)$<span> 。即 </span>$F(\cdot)$<span> 的形式为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n856" cid="n856" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-84">F(\cdot)=\exp(\cdot)\\
   ```

   \mathbf{\vec w}_i^T\mathbf{\vec w}_k-\mathbf{\vec w}*j^T\mathbf{\vec w}\*k = \log P\*{i,k} - \log P*{j,k}

   要想使得上式成立，只需要令$\mathbf{\vec w}_i^T\mathbf{\vec w}*k = \log P*{i,k},\quad \mathbf{\vec w}_j^T\mathbf{\vec w}*k = \log P*{j,k}$即可。

   - 向量的内积具有对称性，即$\mathbf{\vec w}*i^T\mathbf{\vec w}\*k=\mathbf{\vec w}\*k^T\mathbf{\vec w}\*i$。而$\log \frac{X\*{i,k}} {X_i} \ne \log \frac{X\*{k,i}}{X_k}$，即：$\log P\*{i,k} \ne \log P*{k,i}$。

     为了解决这个问题，模型引入两个偏置项：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\log X_{i,k} =\mathbf{\vec w}_i^T\mathbf{\vec w}_k +b_i+\tilde b_k$</div></div></li><li><p><span>上面的公式仅仅是理想状态，实际上只能要求左右两边尽可能相等。于是设计代价函数为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n865" cid="n865" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$J=\sum_{i,k}\left(\mathbf{\vec w}_i^T\mathbf{\vec w}_k +b_i+\tilde b_k -\log X_{i,k}\right)^2$</div></div><p><span>其中 </span>$\mathbf{\vec w},b,\tilde b$<span> 均为模型参数。</span></p></li></ul></li><li><p><span>根据经验，如果两个词共现的次数越多，则这两个词在代价函数中的影响就应该越大。因此可以设计一个权重来对代价函数中的每一项进行加权，权重为共现次数的函数：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n869" cid="n869" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$J=\sum_{i,k}f(X_{i,k})\left(\mathbf{\vec w}_i^T\mathbf{\vec w}_k +b_i+\tilde b_k -\log X_{i,k}\right)^2$</div></div><p><span>其中权重函数应该符合三个条件：</span></p><ul><li><p>$f(0)=0$<span> 。即：如果两个词没有共现过，则权重为 0 。</span></p><p><span>这是为了确保 </span>$\lim _{x\rightarrow 0} f(x) \log^2 x $<span> 是有限值。</span></p></li><li><p>$f(\cdot)$<span> 是非递减的。即：两个词共现次数越大，则权重越大。</span></p></li><li><p>$f(\cdot)$<span> 对于较大的 </span>$X_{i,k}$<span> 不能取太大的值。即：有些单词共现次数非常大（如单词 </span><code>的</code><span> 与其它词的组合） ，但是它们的重要性并不是很大。</span></p></li></ul><p><code>GloVe</code><span> 论文给出的权重函数 </span>$f(\cdot)$<span> 为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n880" cid="n880" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-88">f(x) = \begin{cases}
     ```

     \left(\frac{x}{x_{\max}}\right)^\alpha& \text{if}\quad x\lt x_{\max}\ 1,&\text{otherwise} \end{cases}

     其中：

     - `GloVe` 论文给出参数$\alpha$和$x_{\max}$的经验值为：$\alpha = \frac 34,x_{\max} = 100$。
     - `GloVe` 论文指出：$x_{\max}$对模型的性能影响较小。

    <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603191614.png?imageslim">
    </p>
    

   - 考虑对所有词向量增加一个常量$\mathbf{\vec c}$，则有：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-89">(\mathbf{\vec w}_i+\mathbf{\vec c})^T(\mathbf{\vec w}_k+\mathbf{\vec c}) +(b_i- \mathbf{\vec c}^T\mathbf{\vec w}_i- \frac{||\mathbf{\vec c}||^2}{2})+(\tilde b_k-\mathbf{\vec c}^T\mathbf{\vec w}_k- \frac{||\mathbf{\vec c}||^2}{2}) -\log X_{i,k}=\\
     ```

     \mathbf{\vec w}_i^T\mathbf{\vec w}*k +b_i+\tilde b_k -\log X*{i,k}

     令$\hat b_i = b_i- \mathbf{\vec c}^T\mathbf{\vec w}_i- \frac{||\mathbf{\vec c}||^2}{2}$，$\hat {\tilde b}_k=\tilde b_k-\mathbf{\vec c}^T\mathbf{\vec w}_k- \frac{||\mathbf{\vec c}||^2}{2}$，则：如果$\mathbf{\vec w}_1,\mathbf{\vec w}_2,\cdots,\mathbf{\vec w}_V$是`Glove` 的解，则$\mathbf{\vec w}_1+\mathbf{\vec c},\mathbf{\vec w}*2+\mathbf{\vec c},\cdots,\mathbf{\vec w}\*V+\mathbf{\vec c}$也是`Glove` 的解。

     因此假设$\mathbf{\vec c}$是一个非常大的值，则会导致几乎所有的词向量都相似。

### 4.2 应用

1. `GloVe` 模型的算法复杂度取决于共现矩阵$\mathbf X$中的非零元素的个数，最坏的情况下为$O(V^2)$。由于词汇表的数量通常很庞大，因此$V^2$会非常大。

   实际上单词共现的次数满足齐普夫定律(`Zipf's Law`)，因此算法复杂度较低，约为$O(|C|)$， 其中$C$为语料库的大小。

   > `Zipf's Law`：如果有一个包含$n$个词的文章，将这些词按其出现的频次递减地排序，那么序号$r$和其出现频次$f$之积$f\times r$，将近似地为一个常数，即$f\times r=const$

2. `GloVe` 模型评估任务：

   - `semantic` 任务： 语义任务。如：`'雅典'之于'希腊' = '柏林'之于'`\*`'?`*
   - *`syntactic` 任务：语法任务。如：`'dance'之于'dancing' = 'fly'之于'`*`'?`

3. `GloVe` 模型性能与语料库大小的关系：

   - 在语法任务中，模型性能随着语料库大小的增长而单调增长。

     这是因为语料库越大，则语法的统计结果越可靠。

   - 在语义任务中，模型性能与语料库绝对大小无关，而与语料库的有效大小有关。

     有效大小指的是语料库中，与目标语义相关的内容的大小。

4. `GloVe` 模型超参数选择：

   - 词向量大小：词向量大小越大，则模型性能越好。但是词向量超过 `200` 维时，维度增加的收益是递减的。

   - 窗口对称性：计算一个单词的上下文时，上下文窗口可以是对称的，也可以是非对称的。

     - 对称窗口：既考虑单词左侧的上下文，又考虑单词右侧的上下文。

     - 非对称窗口：只考虑单词左侧的上下文。

       > 因为语言的阅读习惯是从左到右，所以只考虑左侧的上下文，不考虑右侧的上下文。

   - 窗口大小：

     - 在语法任务中，选择小的、非对称的窗口时，模型性能更好。

       因为语法是局部的，所以小窗口即可；因为语法是依赖于单词顺序的，所以需要非对称窗口。

     - 对于语义任务，则需要选择更大的窗口。

       因为语义是非局部的。

   <p align="center">
     <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603191623.png?imageslim">
   </p>
   

## 五、FastText

1. `fastText` 是 `Facebook AI Research` 在 `2016` 年开源的文本分类器，其提出是在论文 `《Bag of Tricks for Efficient Text Classification》` 中。目前 `fastText` 作为文本分类的基准模型。

   `fastText` 的优点是：在保持分类效果的同时，大大缩短了训练时间。

   - 在 8个 数据集上，不同模型的测试误差：

    <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603191710.png?imageslim">
    </p>
    

   - 单个 `epoch` 的训练时间（`char-CNN` 、`VDCNN` 和 `fastText` ）：

    <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603191639.png?imageslim">
    </p>
    

2. `fastText` 的网络结构与 `word2vec` 的 `CBOW` 非常相似。区别在两个地方：

   - 输入：单篇文档的所有单词都作为网络的输入。因此这里的参数 `C` 是动态的，它等于当前文档的单词数量。
   - 输出：这里网络的输出是各类别的概率。通常文本分类的类别$K$远小于词典大小$V$，因此可以不必进行分层 `softmax`和负采样。


3. 隐向量为所有输入单词映射结果的均值：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec h}=\frac 1C \mathbf W^T(\mathbf{\vec x}_1+\mathbf{\vec x}_2+\cdots+\mathbf{\vec x}_C)=\frac 1C (\mathbf{\vec w}_{I_1}+\mathbf{\vec w}_{I_2}+\cdots+\mathbf{\vec w}_{I_C})$</div></div><p><span>其中：</span>$I_i$<span> 表示第 </span>$i$<span> 个输入单词在词汇表 </span>$\mathbb V$<span> 中的编号，</span>$\mathbf{\vec w}_j $<span> 为矩阵 </span>$\mathbf W $<span> 的第 </span>$j$<span> 行，它是对应输入单词的输入向量。</span></p><p><span>单个样本的损失函数为（</span>$k^*$<span> 为真实类别标签）：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n965" cid="n965" mdtype="math_block">
   		
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;"><script type="math/tex; mode=display" id="MathJax-Element-91">E=- u_{k^*} +\log \sum_{k=1}^K \exp(u_k)
   ```

   =-\mathbf{\vec w}^{\prime}*{k^\*}\cdot \mathbf{\vec h}+\log \sum*{k=1}^K \exp(\mathbf{\vec w}^{\prime}_{k}\cdot \mathbf{\vec h})

   定义每个输出单元的预测误差$e_k=\frac{\partial E}{\partial u_k}=y_k-t_k$，与`CBOW` 多个单词上下文的推导相同：

   - 更新$\mathbf W^\prime$：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}^{\prime(new)}_{k } =\mathbf{\vec w}^{\prime(old)}_{k }-\eta e_k\mathbf{\vec h},\quad k=1,2,\cdots,K$</div></div><p><span>其中 </span>$\mathbf{\vec h}=\frac 1C (\mathbf{\vec w}_{I_1}+\mathbf{\vec w}_{I_2}+\cdots+\mathbf{\vec w}_{I_C})$<span>  。</span></p></li><li><p><span>更新 </span>$\mathbf W$<span>：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n974" cid="n974" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec w}_{I_i}^{(new)}=\mathbf{\vec w}_{I_i}^{(old)}-\frac 1C\eta\mathbf{\overrightarrow{EH}},\quad i=1,2,\cdots,C$</div></div><p><span>其中 ：</span></p><ul><li>$\mathbf{\overrightarrow {EH}}  = \mathbf W^\prime \mathbf{\vec e} =\sum_{k=1}^Ke_k \mathbf{\vec w}^{\prime }_k$<span> ，它是所有类别输出向量的加权和，其权重为 </span>$e_k$<span>  。</span></li><li>$I_i$<span> 为第 </span>$i$<span> 个输入单词在词表 </span>$\mathbb V$<span> 中的编号。</span></li></ul></li></ul></li><li><p><span>如果考虑词序则分类的效果还可以进一步提升，因此在 </span><code>fastText</code><span> 中可以引入 </span><code>N-gram</code><span> 特征。如：</span><code>2-gram</code><span> 合并文档中连续的2个单词作为特征。</span></p></li><li><p><code>fastText</code><span> 生成的词向量嵌入的是分类的信息，而</span><code>word2vec</code><span> 生成的词向量更多的嵌入了通用语义信息。</span></p><ul><li><code>fastText</code><span> 词向量得到的相似度是基于分类类别的相似。如：商品评论情感分类任务中，</span><code>好吃</code><span>  和 </span><code>好玩</code><span> 是相似的，因为它们都是正向情感词。</span></li><li><code>word2vec</code><span> 词向量得到的相似度是基于语义的相似。此时 </span><code>好吃</code><span> 和  </span><code>美味</code><span> 是相似的，因为这二者经常出现在类似的上下文中。</span></li></ul></li></ol><h2><a name="六、elmo" class="md-header-anchor"></a><span>六、ELMo</span></h2><ol start='' ><li><p><code>ELMo:Embeddings from Language Models</code><span> 引入了一种新的单词表示方式，该 表示方式的建模目标是：对单词的复杂特征建模（如：语法特征、语义特征），以及能适应不同的上下文（如：多义词）。</span></p><ul><li><p><code>ELMo</code><span> 词向量是由双向神经网络语言模型的内部多层向量的线性加权组成。</span></p><ul><li><p><code>LSTM</code><span> 高层状态向量捕获了上下文相关的语义信息，可以用于语义消岐等任务。</span></p><p><span>如下图中的左图为语义消岐任务的结果，第一层、第二层分别表示单独使用 </span><code>biLM</code><span> 的</span><code>representation</code><span> 的效果。结果表明：越高层的状态向量，越能够捕获语义信息。</span></p></li><li><p><code>LSTM</code><span> 底层状态向量捕获了语法信息，可以用于词性标注等任务。</span></p><p><span>如下图中的右图为词性标注任务的结果，第一层、第二层分别表示单独使用 </span><code>biLM</code><span> 的</span><code>representation</code><span> 的效果。结果表明：越低层的状态向量，越能够捕获语法信息。</span></p></li></ul><p><img src="../imgs/word_representation/ELMo_different_layers.png" width="500px"/></p></li><li><p><code>ELMo</code><span> 词向量与传统的词向量（如：</span><code>word2vec</code><span> ）不同。在</span><code>ELMo</code><span> 中每个单词的词向量不再是固定的，而是单词所在的句子的函数，由单词所在的上下文决定。因此</span><code>ELMo</code><span> 词向量可以解决多义词问题。</span></p><p><span>下图中，</span><code>GloVe</code><span> 无法区分 </span><code>play</code><span> 这个单词的多种含义。而 </span><code>ELMo</code><span> 由于引入了上下文，因此可以区分其不同含义。</span></p><p><img src="../imgs/word_representation/ELMO_near.png" width="550px"/></p></li><li><p><span>实验表明，</span><code>ELMo</code><span> 在多个任务上取得了广泛的提升。</span></p><p><img src="../imgs/word_representation/ELMO_result.png" width="500px"/></p></li></ul></li><li><p><span>给定一个句子 ： </span>$\{\text{word}_{w_1},\text{word}_{w_2},\cdots,\text{word}_{w_N}\}$<span>，其中 </span>$w_i \in \{1,2,\cdots,V\}$<span>， </span>$N$<span> 为句子的长度。用 </span>$ (w_1,w_2,\cdots,w_N)$<span> 代表该句子， 则生成该句子的概率为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1014" cid="n1014" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$p(w_1,w_2,\cdots,w_N)=\prod_{i=1}^{N}p(w_i\mid w_1,w_2,\cdots,w_{i-1})$</div></div><p><span>可以用一个 </span>$L$<span> 层的前向 </span><code>LSTM</code><span> 模型来实现该概率。其中：</span></p><ul><li><p>$\mathbf{\vec x}_i$<span> 表示输入 </span>$w_i$<span> 的 </span><code>embedding</code><span> 向量， </span>$\mathbf{\vec h}_{i,j}$<span> 表示第 </span>$j$<span> 层 </span><code>LSTM</code><span> 层的第 </span>$i$<span> 个单元的输出隐向量。</span></p></li><li><p><span>第 </span>$L$<span> 层 </span><code>LSTM</code><span> 的输出经过 </span><code>softmax</code><span> 输出层输出对应的条件概率。</span></p><ul><li><code>softmax</code><span> 输出层由一个全连接函数和一个</span><code>softmax</code><span> 函数组成。</span></li><li><span>由于 </span><code>RNN</code><span> 的性质，所有</span><code>softmax</code><span> 输出层的参数都共享。</span></li></ul></li></ul><p><img src="../imgs/word_representation/ELMO_forward.png" width="500px"/></p></li><li><p><code>ELMo</code><span> 模型采用双向神经网络语言模型，它由一个前向</span><code>LSTM</code><span> 网络和一个逆向 </span><code>LSTM</code><span> 网络组成。</span><code>ELMo</code><span> 最大化句子的对数前向生成概率和对数逆向生成概率。</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1029" cid="n1029" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-95">E = \sum_{i=1}^{N}\left[\log p(w_i\mid w_1,w_2,\cdots,w_{i-1};\Theta_x,\overrightarrow\Theta_{LSTM},\Theta_s)\\
     ```

     - \log p(w_i\mid w_{i+1},w_{w+2},\cdots,w_N;\Theta_x,\overleftarrow\Theta_{LSTM},\Theta_s)\right]

     其中：

     - 前向 `LSTM` 网络和逆向 `LSTM`网络共享`embedding` 层的参数$\Theta_x$、共享`softmax` 输出层的参数$\Theta_s$。
     -$\overrightarrow\Theta_{LSTM}$为前向 `LSTM` 网络的参数，$\overleftarrow\Theta_{LSTM}$为逆向 `LSTM` 网络的参数，二者不同。

     `ELMo` 认为单词$w_i$的表达由$2L+1$个向量组成：$\mathbb H_i = {\mathbf{\vec x}*i,\mathbf{\vec h}*{i,j},\mathbf{\overleftarrow h}*{i,j}\mid j=1,2,\cdots,L}$，是这$2L+1$个向量的函数。*

     - *最简单的情况下，`ELMo` 取出第$L$层（或者其它单层）的输出作为词的表达：$\overrightarrow {\text{ELMO}\*i} = \mathbf{\vec h}\*{i,L}:\mathbf{\overleftarrow h}*{i,L}$。其中 `:` 表示向量的拼接。

     - 也可以直接采用这$2L+1$个向量的均值作为单词$w_i$的表达。

     - 可以给每层的向量一个权重，而这些权重（一共$2L+1$个）可以从具体任务中学习到。

       此时`ELMo` 通用的词表达为：这$2L+1$个向量的加权：

       

       ```
         <div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\overrightarrow {\text{ELMO}_i} = \gamma^{task}\sum_{\mathbf{\vec v}_k\in \mathbb H_i} s_k^{\text{task}}\mathbf{\vec v}_k$</div></div><p><span>其中 </span>$s_k^{\text{task}}$<span> 为对应层的权重的 </span><code>softmax</code><span> 归一化结果， </span>$k=0,1,2,\cdots,2L$<span>；而  </span>$\gamma^{task}$<span> 是所有层的缩放因子（与层的位置无关，由具体任务决定）。</span></p></li></ul><p><img src="../imgs/word_representation/ELMO.png" width="900px"/></p></li><li><p><span>应用 </span><code>ELMo</code><span>  时，首先训练无监督的 </span><code>ELMo</code><span> 模型，获取每个单词的 </span>$2L+1$<span> 个中间表示。然后在监督学习任务中，训练这 </span>$2L+1$<span> 个向量的线性组合，方法为：</span></p><ul><li><span>冻结 </span><code>ELMo</code><span> 的模型参数并计算得到  </span>$\overrightarrow {\text{ELMO}_i}$<span> 。</span></li><li><span>拼接 </span>$\mathbf{\vec x}_i^{\text{task}}$<span> 和 </span>$\overrightarrow {\text{ELMO}_i}$<span>，作为监督学习网络的输入，其中  </span>$\mathbf{\vec x}_i^{\text{task}}$<span> 是监督学习网络的单词输入 </span><code>embedding</code><span> 。</span></li><li><span>对于 </span><code>RNN</code><span> 网络，还可以将 </span>$\overrightarrow {\text{ELMO}_i}$<span> 拼接隐层输出  </span>$\mathbf{\vec h}_i^{\text{task}}$<span> ，其中  </span>$\mathbf{\vec h}_i^{\text{task}}$<span> 是监督学习网络的隐向量。</span></li></ul></li><li><p><span>实验表明：在 </span><code>ELMo</code><span> 中添加 </span><code>dropout</code><span> 是有增益的。另外在损失函数中添加  </span>$L_2$<span> 正则化能使得训练到的</span><code>ELMo</code><span>权重倾向于接近所有</span><code>ELMo</code><span>权重的均值。</span></p></li></ol><h2><a name="七、变种" class="md-header-anchor"></a><span>七、变种</span></h2><h3><a name="7.1-item2vec" class="md-header-anchor"></a><span>7.1 Item2Vec</span></h3><ol start='' ><li><p><span>在传统的推荐算法中，协同过滤 </span><code>CF</code><span> 算法是利用 </span><code>item2item</code><span> 关系来计算商品中的相似性：通过</span><code>user-item</code><span> 矩阵来计算 </span><code>item1</code><span> 和 </span><code>item2</code><span> 的相似性。</span></p><p><span>受到 </span><code>word2vec</code><span> 算法的启发，可以将用户在一个 </span><code>session</code><span>  内浏览的商品集合作为一个句子，每个商品作为一个 </span><code>word</code><span> 。</span></p><p><span>出现在同一个集合内的 </span><code>商品对</code><span> 视作正类，</span></p></li><li><p><span>设商品集合为 </span>$\mathbb S = \{\text{item}_{w_1},\text{item}_{w_2},\cdots,\text{item}_{w_K}\}$<span> ，所有商品的全集为 </span>$\mathbb A = \{\text{item}_1,\text{item}_2,\cdots,\text{item}_I\}$<span> 。</span></p><p><span>定义目标函数为：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1070" cid="n1070" mdtype="math_block">
         	
         <div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$L = \frac 1K \sum_{i=1}^K\sum_{j\ne i}^K\log p(\text{item}_{w_j}\mid \text{item}_{w_i})$</div></div><p><span>类似 </span><code>word2vec</code><span>，采用负采样和降采样，使用 </span><code>SGD</code><span> 学习模型则得到每个商品的</span><code>embedding</code><span> 。</span></p><p><span>一旦得到了商品的 </span><code>embedding</code><span>，两两计算 </span><code>cosine</code><span> 即可得到 </span><code>item</code><span> 之间的相似度。下图为 </span><code>Item2Vec</code><span>  和 </span><code>SVD</code><span> 分别得到的词向量经过 </span><code>t-SNE</code><span> 可视化的结果哦。</span></p><p><img src="../imgs/word_representation/item2vec.jpg" width="600px" /></p></li></ol><h3><a name="7.2-sentence2vec" class="md-header-anchor"></a><span>7.2 sentence2vec</span></h3><ol start='' ><li><p><span>获得</span><code>sentence</code><span> 的表达的最简单的方式是：基于句子中词的</span><code>embedding</code><span> 来取简单平均或者加权平均，权重为单词的 </span><code>tf-idf</code><span> 值。</span></p><p><span>研究表明这种简单的方式足以提供一个良好的</span><code>sentence embedding</code><span> 。</span></p></li></ol><h4><a name="7.2.1-skip-thought" class="md-header-anchor"></a><span>7.2.1 Skip-Thought</span></h4><ol start='' ><li><p><span>论文</span><code>《Skip-Thought Vectors》</code><span> 根据 </span><code>word2vec</code><span> 的思想提出了一种通用的、分布式的 </span><code>sentence embedding</code><span> 方法：通过当前 </span><code>sentence</code><span> 来预测前一个 </span><code>sentence</code><span> 和下一个 </span><code>sentence</code><span> 。最终语义和语法属性一致的 </span><code>sentence</code><span> 被映射到相似的向量表示。</span></p></li><li><p><span>设句子 </span>$\mathbf s_i = \{w_i^{(1)},w_i^{(2)},\cdots,w_i^{(\tau_i)}\}$<span> ，</span>$\tau_i$<span> 为句子长度。 </span>$w_i^{(t)}$<span> 为句子 </span>$\mathbf s_i$<span> 的第 </span>$t$<span> 个单词，其</span><code>embedding</code><span> 为 </span>$\mathbf{\vec x}_i^{(t)}$<span> 。给定一组句子 </span>$(\mathbf s_{i-1},\mathbf{s}_i,\mathbf{s}_{i+1})$<span> ，模型分为编码 </span><code>encoder</code><span>、解码 </span><code>dcoder</code><span> 部分。</span></p><p><img src="../imgs/word_representation/skip_thoughts_vector.png" width="700px" /></p><ul><li><p><code>encoder</code><span>：对 </span>$\mathbf s_i$<span> 进行编码，得到其上下文向量：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1089" cid="n1089" mdtype="math_block">
         	
         <div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\mathbf{\vec c}_i = \text{GRU}(\{\mathbf{\vec x}_i^{(1)},\mathbf{\vec x}_i^{(2)},\cdots,\mathbf{\vec x}_i^{(\tau_i)}\})$</div></div><p><span>这里采用 </span><code>GRU</code><span>，也可以采用其它编码方案（</span><code>LSTM</code><span>，双向 </span><code>GRU</code><span> 等）。</span></p></li><li><p><code>decoder</code><span>：根据输入序列的上下文向量分别解码两次，然后分别和 </span>$\mathbf s_i$<span> 和 </span>$\mathbf s_{i+1}$<span> 比较。</span></p><p><span>两个 </span><code>decoder</code><span> 拥有不同的参数，除了 </span>$\mathbf V$<span> 共享之外。</span></p><ul><li><p><span>第一个 </span><code>decoder</code><span>：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1097" cid="n1097" mdtype="math_block">
         	
         <div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-99">\text{update gate:}\quad \mathbf{\vec z}^{(t)}_{i-1}=\sigma(\mathbf{\vec b}^{z}_{i-1}+\mathbf U^{z}_{i-1}\mathbf{\vec c}_{i}+\mathbf W^{z}_{i-1}\mathbf{\vec h}^{(t-1)}_{i-1})\\
       ```

     \text{reset gate:}\quad\mathbf{\vec r}^{(t)}*{i-1}=\sigma(\mathbf{\vec b}^{r}*{i-1}+\mathbf U^{r}*{i-1}\mathbf{\vec c}\*i+\mathbf W^{r}\*{i-1}\mathbf{\vec h}^{(t-1)}*{i-1}) \ \text{cell output:}\quad\mathbf{\vec h}^{(t)}*{i-1}=\mathbf{\vec z}^{(t)}*{i-1}\odot\mathbf{\vec h}^{(t-1)}*{i-1}+(1-\mathbf{\vec z}^{(t)}*{i-1})\odot\tanh(\mathbf{\vec b}*{i-1}+\mathbf U*{i-1}\mathbf{\vec c}*i+\mathbf W*{i-1}\mathbf{\vec r}^{(t)}*{i-1}\odot \mathbf{\vec h}^{(t-1)}*{i-1})\ \mathbf{\vec o}^{(t)}*{i-1}=\text{softmax}\left(\mathbf{\vec c}+\mathbf V\mathbf{\vec h}^{(t)}*{i-1}\right)

   - 第二个 `decoder`：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-100">\text{update gate:}\quad \mathbf{\vec z}^{(t)}_{i+1}=\sigma(\mathbf{\vec b}^{z}_{i+1}+\mathbf U^{z}_{i+1}\mathbf{\vec c}_{i}+\mathbf W^{z}_{i+1}\mathbf{\vec h}^{(t-1)}_{i+1})\\
     ```

     \text{reset gate:}\quad\mathbf{\vec r}^{(t)}*{i+1}=\sigma(\mathbf{\vec b}^{r}*{i+1}+\mathbf U^{r}*{i+1}\mathbf{\vec c}\*i+\mathbf W^{r}\*{i+1}\mathbf{\vec h}^{(t-1)}*{i+1}) \ \text{cell output:}\quad\mathbf{\vec h}^{(t)}*{i+1}=\mathbf{\vec z}^{(t)}*{i+1}\odot\mathbf{\vec h}^{(t-1)}*{i+1}+(1-\mathbf{\vec z}^{(t)}*{i+1})\odot\tanh(\mathbf{\vec b}*{i+1}+\mathbf U*{i+1}\mathbf{\vec c}*i+\mathbf W*{i+1}\mathbf{\vec r}^{(t)}*{i+1}\odot \mathbf{\vec h}^{(t-1)}*{i+1})\ \mathbf{\vec o}^{(t)}*{i+1}=\text{softmax}\left(\mathbf{\vec c}+\mathbf V\mathbf{\vec h}^{(t)}*{i+1}\right)

   - 整体损失函数为：

     

     ```
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$L = - \sum_{t=1}^{\tau_{i-1}}  \log o_{w_{i-1}^{(t)}}^{(t)} - \sum_{t=1}^{\tau_{i+1}}  \log o_{w_{i+1}^{(t)}}^{(t)}$</div></div><p><span>其中 </span>$ o_{w_{i+1}^{(t)}}^{(t)}$<span> 为句子 </span>$\mathbf{s}_{i+1}$<span> 第 </span>$t$<span> 个位置单词 </span>$w_{i+1}^{(t)}$<span> 的输出概率。</span></p></li></ul></li><li><p><span>模型将训练集的所有句子组合加入训练，在测试时将句子所有单词的 </span><code>embedding</code><span> 输入到 </span><code>encoder</code><span> 即可得到句子的 </span><code>embedding</code><span> 。</span></p></li></ul></li><li><p><span>如果测试时某些单词未在训练时见过（未登录词），则可以通过一些广义词向量来解决。如：假设 </span>$\mathcal V_{w2v}$<span> 为更大规模的、通过 </span><code>word2vec</code><span> 训练得到的词向量， </span>$\mathcal V_{rnn}$<span> 为本模型训练得到的词向量。我们学习线性映射 </span>$f: \mathcal V_{w2v} \rightarrow \mathcal V_{rnn}$<span> ，然后对于模型的未登录词 </span>$w_{u}$<span>，得到模型的词向量 ：</span>$\mathbf{\vec x}_u^{rnn} = f(\mathbf{\vec x}_u^{w2v})$<span> 。</span></p><p><span>改进思路：用更深的 </span><code>encoder</code><span> 和 </span><code>decoder</code><span> 网络；用更大的窗口，而不仅仅是预测前后两句；用其它的 </span><code>encoder</code><span> ，如引入 </span><code>attention</code><span> 。</span></p></li></ol><h4><a name="7.2.2-quick-thought" class="md-header-anchor"></a><span>7.2.2 Quick Thought</span></h4><ol start='' ><li><p><span>论文</span><code>《An efficient framework for learning sentence representations》</code><span> 提出了一种新的</span><code>sentence vector</code><span> 方式</span><code>Quick thought Vector</code><span>，相比较</span><code>《Skip-Thought Vectors》</code><span> 而言它的训练速度更快。</span></p></li><li><p><code>Quick thought Vector</code><span> 通过分类器来区分句子的前后句和其它句子，模型结构如下图所示。这将生成问题视为从所有可能的句子中选择一个句子，因此可以看作是对生成问题的判别近似。</span></p><p><img src="../imgs/word_representation/quick_thoughts_vector.png" width="500px" /></p><ul><li><span>判别模型仅仅会捕捉 </span><code>sentence embedding</code><span> 之间的相对关系（是否相近），因此可忽略每个句子与</span><code>embedding</code><span> 无关的信息。</span></li><li><span>生成模型会捕捉</span><code>sentence embedding</code><span> 的绝对位置，因此生成模型约束更强，同时可能捕获了与</span><code>embeding</code><span> 无关的很多信息。</span></li></ul></li><li><p><span>设句子 </span>$\mathbf s_i = \{w_i^{(1)},w_i^{(2)},\cdots,w_i^{(\tau_i)}\}$<span> ，</span>$\tau_i$<span> 为句子长度。 </span>$w_i^{(t)}$<span> 为句子 </span>$\mathbf s_i$<span> 的第 </span>$t$<span> 个单词，其</span><code>embedding</code><span> 为 </span>$\mathbf{\vec x}_i^{(t)}$<span> 。</span></p><p><span>令句子 </span>$\mathbf s_i$<span> 附近窗口大小 </span>$d$<span> 的句子集合为 </span>$\mathcal S_{i,ctx}$<span>、句子 </span>$\mathbf s_i $<span> 的候选样本句子集合为 </span>$\mathcal S_{i,cand}$<span> ，其中满足 </span>$\mathcal S_{i,ctx} \subset \mathcal S_{i,cand}$<span> 。即： </span>$\mathcal S_{i,cand}$<span> 不仅包含了句子 </span>$\mathbf  s_i $<span> 附近的句子，也包含了很多其它句子。</span></p><p><span>使用编码器 </span>$f$<span> 对  </span>$\mathbf s_i$<span> 编码、编码器 </span>$g$<span> 对候选句子 </span>$\mathbf s_{i,cand} \in \mathcal S_{i,cand}$<span> 编码，模型预测候选句子 </span>$\mathbf s_{i,cand} \in \mathcal S_{i,ctx} $<span> 的概率：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1126" cid="n1126" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$p(\mathbf s_{i,cand} \mid \mathbf s_i,\mathcal S_{i,cand}) = \frac{\exp[c(f(\mathbf s_i),g(\mathbf s_{i,cand}))]}{\sum_{\mathbf s^\prime\in \mathcal S_{i,cand}}\exp[c(f(\mathbf s_i),g(\mathbf s^\prime)]}$</div></div><p><span>模型训练的目标函数为：尽可能的识别所有的近邻句子：</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1128" cid="n1128" mdtype="math_block">
     		
     	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$L = \sum_{\mathbf s_{i,ctx} \in \mathcal S_{i,ctx}} \log p(\mathbf s_{i,ctx} \mid \mathbf s_i,\mathcal S_{i,cand})$</div></div><p><span>其中：</span></p><ul><li><span>中 </span>$c(\cdot,\cdot)$<span> 为一个打分函数（或者分类器），在论文中 </span>$c(\mathbf{\vec u},\mathbf{\vec v})$<span> 简单定义为 </span>$c(\mathbf{\vec u},\mathbf{\vec v}) = \mathbf {\vec u} \cdot \mathbf{\vec v}$<span>  。如此简单的分类器是为了迫使编码器学习到更加丰富的</span><code>sentence representation</code><span> 。</span></li><li><span>编码器 </span>$f$<span> 和 </span>$g$<span> 使用不同的参数，这是借鉴了</span><code>word2vec</code><span> 的结构：中心词通过 </span>$\mathbf W$<span> 编码（输入向量）、目标词经过 </span>$\mathbf W^\prime$<span> 映射（输出向量），二者点积得到得分。</span></li><li><span>在测试时给定句子 </span>$\mathbf s_{test}$<span>，其表示是两个编码器输出的拼接：</span>$f(\mathbf s_{test}) : g(\mathbf s_{test}) $<span> 。</span></li></ul></li></ol><h4><a name="7.2.3-infersent" class="md-header-anchor"></a><span>7.2.3 InferSent</span></h4><ol start='' ><li><p><span>论文</span><code>《Supervised learning of universal sentence representations from natural language inference data》</code><span> 通过监督学习来获取句子的</span><code>reprensentation</code><span>，该模型被称作 </span><code>InferSent</code><span> 。相比 </span><code>Skip Thought  Vector</code><span>，该模型的训练速度要快得多并且效果更好。</span></p><ul><li><p><span>论文采用 </span><code>Stanford Natural Language Inference Datasets</code><span>（简称 </span><code>SNLI</code><span>），包含了57万个人类产生的句子对。每个句子对已被人工标记，标签为：蕴含</span><code>emtailment</code><span>、矛盾 </span><code>contradiction</code><span> 、中立 </span><code>neutral</code><span> 。模型。论文假设 </span><code>SNLI</code><span> 的作用类似 </span><code>ImageNet</code><span> ，它学到的句子表达能捕获通用的</span><code>sentence feature</code><span> 。</span></p></li><li><p><span>模型有两种训练方式：输入的两个句子采用 </span><code>encoder</code><span> 分别独立编码，然后获得独立的 </span><code>sentence embedding</code><span> ；采用类似 </span><code>attention</code><span> 的机制来同时对两个句子进行编码，分别获得各自句子的 </span><code>sentence embedding</code><span> 。</span></p><p><span>考虑到论文的目标是获取句子的通用表示，因此采用方式一的结构，并且两个 </span><code>encoder</code><span> 参数共享。</span></p></li></ul></li><li><p><span>给定一对句子 </span>$(\mathbf s_u,\mathbf s_v)$<span>，假设其编码结果分别为 </span>$\mathbf{\vec u},\mathbf{\vec v}$<span> 。有三种方式来提取这两个句子的关系：向量拼接 </span>$\mathbf{\vec u} : \mathbf{\vec v}$<span>，元素级相乘 </span>$\mathbf{\vec u} \odot \mathbf{\vec v}$<span> ，元素级距离：</span>$|\mathbf{\vec u} -  \mathbf{\vec v}|$<span> 。</span></p><p><span>提取结果作为一个 </span><code>softmax</code><span> 输出单元的输入，最终得到3个类别的输出概率。</span></p><p><img src="../imgs/word_representation/infersent.png" width="350px" /></p></li><li><p><span>作者研究了7种不同的编码架构：</span><code>LSTM</code><span>、</span><code>GRU</code><span>、</span><code>BiGRU</code><span>（采用前向、后向最后一个隐单元的拼接）、</span><code>BiLSTM-mean</code><span>（所有隐向量的均值）、</span><code>BiLSTM-max</code><span>（所有隐向量的最大池化）、</span><code>BiLSTM + self-attention</code><span> 、分层卷积模型。最终 </span><code>BiLSTM-max</code><span> 编码器在 </span><code>SNLI</code><span> 和迁移任务上表现效果最好。</span></p><p><span>有些编码器如</span><code>BiLSTM+self-attention</code><span> 在</span><code>NLI</code><span> 上比 </span><code>BiLSTM-mean</code><span> 表现更好，但是在迁移任务中反而表现更差。这是因为 </span><code>attention</code><span> 机制捕捉到了更多的关于具体数据集的信息，导致学到的</span><code>sentence representation</code><span> 是有偏的（而不是通用的）。</span></p><p><img src="../imgs/word_representation/infersent_encoder.png" width="300px" /></p></li><li><p><span>作者在多种任务上比较了 </span><code>InferSent</code><span> 模型和其它模型的结果，下表中下划线表示迁移学习中最佳的模型，† 表示本论文提出的模型。</span></p><p><span>虽然 </span><code>Skip Thought Vector</code><span> 模型的表现效果也比较好，但是该模型是在非常大的有序句子集上训练。如 </span><code>SkitpThought-LN</code><span> 训练了 6400万句子，训练时间一个月。而</span><code>InferSent</code><span> 模型只需要训练 57 万句子，训练时间不到一天（单个 </span><code>GPU</code><span> ）。</span></p><p><img src="../imgs/word_representation/infersent_result.png" width="550px" /></p></li></ol><h4><a name="7.2.4-多任务联合-sentence-vec" class="md-header-anchor"></a><span>7.2.4 多任务联合 sentence-vec</span></h4><ol start='' ><li><p><span>论文</span><code>《LEARNING GENERAL PURPOSE DISTRIBUTED SENTENCE
     ```

     REPRESENTATIONS VIA LARGE SCALE MULTITASK LEARNING》 提出了一个简单、有效的通用句子`representation` 多任务学习框架，它在多个弱相关任务中共享同一个 `sentence encoder` 。

     其背后的思想是：具体任务训练得到的句子表达编码了具体任务的信息，因此是有偏的。如果在多个弱相关任务中训练，则这种偏差会相互减弱，因此能得到更通用的句子表达。

   - 设句子$\mathbf s_i = {w_i^{(1)},w_i^{(2)},\cdots,w_i^{(\tau_i^x)}}$，$\tau_i^x$为句子长度。$w_i^{(t)}$为句子$\mathbf s_i$的第$t$个单词，其`embedding`为$\mathbf{\vec x}_i^{(t)}$。设句子$\mathbf s_i$对应的输出为$\mathbf y_i={y_i^{(1)},y_i^{(2)},\cdots,y_i^{(\tau_i^y)}}$。

     - `encoder` 为一个双向 `GRU`，没有采用 `attention` 机制。因为一旦采用 `attention` 机制，随着 `attention` 权重的不同（该权重与具体任务有关）句子$\mathbf s_i$的表达就不同，因此无法给出通用表达。

       句子$\mathbf s_i$的表达为$\mathbf{\vec c} = [\mathbf{\vec h}_i^{\tau_i^x}: \overleftarrow {\mathbf h}_i^1]$，其中$\mathbf{\vec h}_i^{\tau_i^x}$和$\overleftarrow {\mathbf h}_i^1$分别表示两个方向的最后一个隐向量。

     - `decoder` 为一个单向 `GRU`，其中$\mathbf {\vec c}$作为`decoder` 每个时间步的输入。

       

       ```
       	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display"><script type="math/tex; mode=display" id="MathJax-Element-104">\text{update gate:}\quad \mathbf{\vec z}^{(t)} =\sigma(\mathbf{\vec b}^{z}+\mathbf U^{z} \mathbf{\vec c} +\mathbf W^{z} \mathbf{\vec h}^{(t-1)})\\
       ```

       \text{reset gate:}\quad\mathbf{\vec r}^{(t)}=\sigma(\mathbf{\vec b}^{r}+\mathbf U^{r}\mathbf{\vec c}+\mathbf W^{r}\mathbf{\vec h}^{(t-1)}) \ \text{cell output:}\quad\mathbf{\vec h}^{(t)}=\mathbf{\vec z}^{(t)}\odot\mathbf{\vec h}^{(t-1)}+(1-\mathbf{\vec z}^{(t)})\odot\tanh(\mathbf{\vec b}+\mathbf U\mathbf{\vec c}+\mathbf W\mathbf{\vec r}^{(t)}\odot \mathbf{\vec h}^{(t-1)})\ \mathbf{\vec o}^{(t)}=\text{softmax}\left(\mathbf{\vec c}+\mathbf V\mathbf{\vec h}^{(t)}\right)

     - 论文采用一对多模型：同一个双向 `GRU` 将不同任务的输入句子编码成摘要向量$\mathbf{\vec c}$，然后不同任务采用不同 `GRU` 来解码。


   - 根据论文`《A model of inductive bias learning》` 的结论：

     - 共同学习多个相关任务会产生良好的泛化，这种泛化效果与每个任务的训练样本规模相关。
     - 在足够多的训练任务中学习到的偏差有助于来自相同场景下的新任务的学习。

     因此论文中选择的多任务满足的条件是：任务的多样性足够多、每个任务的训练数据集足够大。

   - 多个任务训练有两种任务切换方式：

     - 在每个任务之间周期性的均匀切换。

     - 在每个任务之间周期性的按比例切换，其中比例因子就是任务的训练集的大小。

       此时训练集越大的任务其参数更新的次数越多。

     论文中选择后一种方式切换：

     - 输入：
       -$k$个任务
       - 所有任务共享的`encoder`$\mathbf E$
       - 每个任务各自的 `decoder` ：$\mathbf D_1,\cdots,\mathbf D_k$
       - 任务参数$\theta=(\theta_1,\cdots,\theta_k)$，其中$\theta_i$为第$i$个任务的参数
       - 每个任务的切换概率$\vec\alpha =(p_1,\cdots,p_k)$，其中$\sum p_i = 1$
       - 每个任务的训练集$\mathbb D_1,\cdots,\mathbb D_k$
       - 总的损失函数$\mathcal L$
     - 任务切换算法步骤：
       - 以概率分布$\vec\alpha$采样一个任务$i$
       - 从训练集$\mathbb D_i$中采样一个样本$(\mathbf{\vec x}^i,y^i) \in \mathbb D_i$
       - 通过编码器编码：$\mathbf{\vec h}*{\mathbf{\vec x}^i} = \mathbf E(\mathbf{\vec x}^i;\theta)$*
       - *获取预测值：$\tilde y^i \leftarrow \mathbf D_i(\mathbf{\vec h}*{\mathbf {\vec x}^i};\theta)$
       - 更新参数：$\theta \leftarrow \text{Adam}(\nabla_\theta \mathcal L(y^i,\tilde y^i))$

   - 下表中是一组模型对十种任务的句子进行评估，评估方式为：将模型输出的句子 `representation` 经过简单的线性模型来输出结果。其中：

     - `STN, Fr, De, NLI, L, 2L, STP & Par` 表示联合了`skip-thought next`，`French translation`, `German translation`, `natural language inference`, `large model`（隐单元的维度更高）, `2-layer large model` （隐单元维度更高，同时`encoder`有多层），`skip-thought previous`， `parsing` 等任务。
     - 最后一列给出了我们的模型相比 `Infersent` 在所有任务上的平均提升。

     实验结果表明：

     - 相比较于其它迁移学习模型，我们的模型在大多数任务上取得最佳效果。
     - 增加更多的任务可以提高模型的迁移学习能力（表格中 `+L` 的行）。
     - 增加编码器的隐单元数量和层数也可以提高模型的迁移学习能力 （表格中 `+2L` 的行）。

     <p align="center">
       <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603191834.png?imageslim">
     </p>
     

   - 下表中比较了我们的迁移学习方法和几种使用注意力机制的、复杂的监督学习方法。

     通过将 `sentence representation` 作为简单的 `MLP`模型的输入，我们的方法就能产生一个足够竞争力的结果。而且我们的模型使用原始训练集的一个很小比例就可以取得很好的效果。

    <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603191827.png?imageslim">
    </p>
    

#### 7.2.5 Universal Sentence Encoder

1. 论文 `《Universal Sentence Encoder》` 提出了两种`encoder` 模型的变体，它们可以在准确性和计算资源之间平衡。

   - `Transformer` 编码器：模型复杂度更高，准确率更高，但是资源消耗更大。

   - `DAN:deep averaging network`编码器：模型复杂度更低，资源消耗更低，但是准确率稍低。

     `DAN` 编码器首先将`word` 和 `bi-gram`的 `embedding` 取均值，然后送入一个 `DNN` 网络中来计算 `sentence embedding` 。

2. `Transformer` 编码器和 `DAN` 编码器采用多任务训练的方式。其中包含了：

   - 类似 `Skip Thougt vector` 的无监督学习任务，但是将该模型的 `LSTM`编码器替换为 `Transformer` 编码器/`DAN` 编码器。

     其训练语料来自 `wiki` 百科语料、网络新闻语料、论坛的讨论语料、网页上的问答语料。

   - 包含了解析好的对话数据的对话`input-response` 任务。

   - 有监督的文本分类任务。

     其训练语料来自 `Stanford Natural Language Inference:SNLI` 。

3. 已训练好的 `Transformer` 编码器和 `DAN`编码器可以在`Tensorflow hub` 上访问。

   一个简单的使用方式为：

   ```
   xxxxxxxxxx
   ```

   ```
   import tensorflow_hub as hub
   ```

   ```
   embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/1")
   ```

   ```
   embedding = embed(["The quick brown fox jumps over the lazy dog."])
   ```

   模型输入一个英文字符串，输出该字符串的固定维度的 `embedding` 。

   输出 `embedding` 可以直接使用，如计算句子的语义相似度；也可以在大型任务中进行微调。

  <p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603191731.png?imageslim">
  </p>
  

4. 迁移学习的效果判断是在一组任务上进行。其中包括：

   - `MR`：电影评论片段的情感分析（5个情感等级）
   - `CR`：客户评论的情感挖掘
   - `SUBI`：电影评论和情节摘要中的情感分析
   - `MPQA`：来自新闻的短句的观点极性分析
   - `TREC`：来自`TREC` 的细粒度问题分类
   - `SST`：二元短语情感分类
   - `STS Benchmark`：文本语义相似度（结果与人工判定的相似文本对进行比较）
   - `Word Embedding Association Test:WEAT` ：词向量偏差测试，用于评估模型的偏差。

   对于分类任务，将 `DAN` 和 `Transformer`的句子编码送入与任务相关的简单神经网络中（如：卷积神经网络`CNN` 或者另外一个 `DAN` ）。对于语义相似度任务，将 `DAN`和 `Transformer` 的句子编码直接计算`arccos` 角度：

   

   ```
   	<div class="md-rawblock-container md-math-container" tabindex="-1"><div class="MathJax_SVG_Display" style="text-align: center;">$\text{sim}(\mathbf{\vec u},\mathbf{\vec v}) = \left(1 - \arccos\left(\frac{\mathbf{\vec u}\cdot \mathbf{\vec v}}{||\mathbf{\vec u}||||\mathbf{\vec v}||} \right)/\pi\right)$</div></div><p><span>模型的表现如下表所示。其中：</span></p><ul><li><p><code>USE_T</code><span>  为采用 </span><code>Transformer</code><span> 编码器的通用句子编码模型，</span><code>USE_D</code><span> 为采用 </span><code>DAN</code><span> 编码器的通用句子编码模型。</span></p></li><li><p><code>w2v w.e.</code><span> 表示采用了</span><code>word2ve</code><span> 预训练模型，</span><code>lrn w.e</code><span> 表示随机初始化词向量。</span></p><p><span>这两种方式是结合了</span><code>sentence embedding</code><span>  和 </span><code>word embedding</code><span> ，将这两个 </span><code>embedding</code><span> 拼接在一起。其区别在于：</span></p><ul><li><code>w2v e.e.</code><span> 的</span><code>word embedding</code><span> 来自于预训练的 </span><code>word2vec skip-gram</code><span> 模型并在训练中微调。</span></li><li><code>lrn w.e</code><span> 的 </span><code>word embedding</code><span> 是随机初始化的，并在训练中调整。</span></li></ul></li></ul><p><img src="../imgs/word_representation/UniversalSentenceEncoder_result1.png" width="500px" /></p></li><li><p><span>迁移效果可以在少量训练样本上获得更好的效果。</span></p><p><span>在 </span><code>SST</code><span> 任务上（全量训练样本 67.3k），采用</span><code>Transformer</code><span> 编码器的通用句子编码模型仅仅使用 1k 个训练样本就能够得到很好的效果。</span></p><p><img src="../imgs/word_representation/UniversalSentenceEncoder_result2.png" width="500px" /></p></li><li><p><span>采用</span><code>Transformer</code><span> 编码器的通用句子编码模型的计算复杂度为句子长度 </span>$n$<span> 的 </span>$O(n^2)$<span> 复杂度，内存消耗也是 </span>$O(n^2)$<span> 复杂度。</span></p><p><span>采用 </span><code>DAN</code><span> 编码器的通用句子编码模型的复杂度分别为 </span>$O(n)$<span> 和 </span>$O(n)$<span> 。但是当句子长度很短时，由于 </span><code>DAN</code><span> 编码器需要考虑 </span><code>unigram</code><span> 和 </span><code>bigram</code><span>，</span><code>DAN</code><span> 的内存消耗可能会超过 </span><code>Transformer</code><span> 的内存消耗。</span></p><p><span>下图中 </span><code>b</code><span> 表示 </span><code>batch size</code><span> 。</span></p><p><img src="../imgs/word_representation/UniversalSentenceEncoder_computation.png" width="900px" /></p></li></ol><h3><a name="7.3-doc2vec" class="md-header-anchor"></a><span>7.3 doc2vec</span></h3><ol start='' ><li><p><span>论文 </span><code>《Distributed Representations of Sentences and Documents》</code><span> 提出了无监督的</span><code>Paragraph Vector</code><span> 算法，用于从可变长度的文本（如：句子</span><code>sentence</code><span>、段落</span><code>paragraph</code><span>、文档</span><code>doc</code><span> ）中学习固定维度的</span><code>representation</code><span> 。它不仅可以编码句子，还可以编码一个段落、整篇文档。</span></p></li><li><p><code>Paragraph Vector</code><span> 算法将每个段落映射到一个唯一的向量，由矩阵 </span>$\mathbf D$<span> 中的一行表示。</span></p><p><span>有两种段落向量模型：</span><code>PV-DM</code><span> 、</span><code>PV-BOW</code><span> ，如下图所示。其中 </span>$N$<span> 为训练集段落数量，</span>$V$<span> 为词汇表的大小，</span>$p$<span> 为</span><code>Paragaph Vector</code><span> 维度，</span>$q$<span> 为词向量维度，</span>$C$<span> 为窗口大小。</span></p><ul><li><p><code>Distributed Memory Model of Paragraph Vectors:PV-DM</code><span> 模型：类似  </span><code>word2vec CBOW</code><span>  思想，联合利用段落向量和词向量来预测窗口单词的下一个单词。其中联合可以采用两个向量的平均或者拼接。</span></p><ul><li><span>每个样本取自段落的一个滑动窗口，通过段落向量和窗口内单词预测窗口的下一个单词。</span></li><li><span>段落向量在该段落生成的所有样本中共享，但是不在段落之间共享。</span></li><li><span>在该模型中段落 </span><code>id</code><span>  被认为是另类的单词，它起到记忆的作用，用于记住当前上下文的主旨。</span></li><li><span>该模型同时也可以生成词向量，词向量在段落之间共享。</span></li></ul></li><li><p><code>Distributed Bag of Words version of Paragraph Vector: PV-DBOW</code><span> 模型：通过段落向量来预测窗口内的单词。</span></p><ul><li><span>每个样本取自段落的一个滑动窗口，通过段落向量来预测窗口内的单词。</span></li><li><span>段落向量在该段落生成的所有样本中共享，但是不在段落之间共享。</span></li><li><span>该模型无法生成词向量，不需要存储词向量矩阵 </span>$\mathbf W$<span>，因此模型更小。</span></li></ul></li><li><p><span>对于测试样本，由于不同的段落分配到了不同的段落 </span><code>id</code><span>，因此测试样本的段落向量是未知的。</span></p><p><span>此时需要固定网络的其它参数，仅仅训练矩阵 </span>$\mathbf D_{N^\prime \times p} $<span> ，其中 </span>$N^\prime$<span> 为测试集的大小。</span></p><p><span>然后将段落向量作为段落的一个</span><code>representation</code><span> 来作为后续模型的输入。</span></p></li><li><p><span>实验结果表明：仅仅使用 </span><code>PV-DM</code><span> 得到的段落向量的效果就比较好，但是联合使用 </span><code>PV-DM</code><span> 和 </span><code>PV-DBOW</code><span> 的段落向量效果更佳。</span></p></li></ul><p><img src="../imgs/word_representation/paragraph_vector.png" width="600px" /></p></li><li><p><span>论文在 </span><code>sentence representation</code><span> 和 </span><code>paragraph representation</code><span> 的效果都较好。</span></p><ul><li><p><code>Stanford Sentiment Treebank Dataset</code><span>  情感分析数据集包含 11855 个样本，每个样本就是一个句子。其中训练集 8544 个样本、验证集 1101 个样本、测试集 2210 个样本。</span></p><ul><li><span>第二列给出了二元情感分析的错误率，最后一列给出了细粒度多元情感分析的错误率。</span></li><li><span>一个有意思的现象是：简单的对句子的词向量取平均并不能得到一个较低的分类错误率。</span></li></ul><p><img src="../imgs/word_representation/paragraph_vector_result1.png" width="350px" /></p></li><li><p><code>IMDB</code><span> 情感分析数据集包含 100k 个样本，每个样本包含多条句子。其中训练集 25k 个样本、测试集 25k 个样本、以及 50k 个未标记样本。</span></p><p><span>论文使用 75k 个样本（25k 个标记样本和  50k 个标记样本）来训练词向量和段落向量，然后测试剩下的 25k 个标记样本。</span></p><p><img src="../imgs/word_representation/paragraph_vector_result2.png" width="350px" /></p></li></ul></li><li><p><span>论文给出一些结论来指导实践：</span></p><ul><li><code>PV-DM</code><span> 模型通常要比 </span><code>PV-DBOW</code><span>  模型更好。单独使用 </span><code>PV-DM</code><span> 的效果已经足够好，但是联合二者的效果更好。</span></li><li><code>PV-DM</code><span> 段落向量和 </span><code>PV-DBOW</code><span> 段落向量拼接的效果通常优于二者相加。</span></li><li><span>最佳窗口大小通常需要通过交叉验证来选取。论文建议窗口大小在 5 到 12 之间选取。</span></li><li><span>虽然测试阶段段落向量的计算代价较大，但是可以并行执行来降低代价。</span></li></ul></li></ol><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p></div>
   ```