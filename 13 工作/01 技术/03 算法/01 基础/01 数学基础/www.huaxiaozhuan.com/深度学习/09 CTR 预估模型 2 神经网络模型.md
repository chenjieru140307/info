# CTR 预估模型：神经网络模型

1. `CTR` 预估模型主要用于搜索、推荐、计算广告等领域的 `CTR` 预估，其发展经历了传统 `CTR` 预估模型、神经网络`CTR` 预估模型。

   - 传统 `CTR` 预估模型包括：逻辑回归`LR` 模型、因子分解机`FM` 模型、梯度提升树 `GBDT` 模型等。

     其优点是：可解释性强、训练和部署方便、便于在线学习。

   - 神经网络 `CTR` 预估模型包括：`DSSM`。

## 一、DSSM

1. 在搜索广告的场景中，`query` 和 `document` 使用不同的单词、同一个单词的不同形态（如：缩写、时态、单复数）来表达同一个概念。如果简单的通过文本的单词匹配来计算 `query` 和 `document` 的相似性，则效果不好。

   - 一种解决方式是：利用潜在语义模型`latent semantic model`（如：`LSA`），将 `query` 和 `document` 都降维到低维语义空间，然后根据二者在低维空间的距离来计算二者相似度。

   - 论文 `"Learning Deep Structured Semantic Models for Web Search using Clickthrough Data"` 提出 `Deep Structured Semantic Model:DSSM` 模型，该模型也是将 `query` 和 `document` 降维到公共的低维空间。

     在该低维空间中，`query` 和 `document` 的相似性也是通过二者的距离来衡量。

     它和 `LSA` 的区别：

     - `LSA` 的低维空间是通过无监督学习，利用单词的共现规律来训练。
     - `DSSM` 的低维空间是通过有监督学习，利用 `(query,document)` `pair` 对的点击规律来训练。

     最终实验表明：`DSSM` 模型要优于 `LSA` 模型。

2. 为解决搜索广告中词汇量大的问题（即：词汇表过于庞大），模型采用了 `word hash` 技术。

### 1.1 模型

1. `DSSM` 模型将原始的文本特征映射到低维的语义空间。

   - 首先将 `query` 和 `document` 表示为词频向量，该向量由每个单词出现的词频组成。

     如：`query = 苹果手机 价格`， `document = Iphone Xs 最低 售价 11399 元 Iphone X 价格 6999 元` 。

     构建词汇表：

     ```
     xxxxxxxxxx
     ```

     1

     1

     ```
     苹果手机 Iphone x Xs 价格 最低 售价 6999 11399 元
     ```

     则得到 `query` 向量和 `document` 向量为：

- 然后将$\mathbf{\vec q}$和$\mathbf{\vec d}$映射到低维语义空间，得到 `query` 语义向量$\mathbf{\vec y}_q$和 `document` 语义向量$\mathbf{\vec y}_d$。

- 计算$\mathbf{\vec y}_q$和$\mathbf{\vec y}_d$的相似度：

 $R(\mathbf{\vec q},\mathbf{\vec d}) = \cos(\mathbf{\vec y}_q,\mathbf{\vec y}_d) = \frac{\mathbf{\vec y}_{\mathbf q}\cdot \mathbf{\vec y}_{\mathbf d}}{||\mathbf{\vec y}_{\mathbf q}||\times ||\mathbf{\vec y}_{\mathbf d}||}$

- 给定 `query i` ，计算所有`document` 与它的相似度，并截取 `top K` 个 `document` 即可得到排序结果：

 $\mathbb L_i =\{\mathbf {\vec d} \mid \max_{top K} R(\mathbf{\vec q}_i, \mathbf {\vec d}) \; \text{and}\; \mathbf {\vec d}\in \mathcal D_i\}$

  其中$\mathbb L_i$是 `query i`的排序结果（根据相似度降序排列），$\mathcal D_i$是所有与 `query i` 有关的文档。

<p align="center">
   <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603210830.png?imageslim">
</p>


1. 假设输入向量为$\mathbf{\vec x}$，输出向量为$\mathbf{\vec y}$，网络一共有$L$层。

   > 对于 `query`，输入就是$\mathbf{\vec q}$，输出就是$\mathbf{\vec y}_q$；对于 `document`，输入就是$\mathbf{\vec d}$，输出就是$\mathbf{\vec y}_d$

   第$l$层的隐向量为：

   其中：

   -$\sigma(\cdot)$为激活函数。论文（2013年）采用 `tanh` 激活函数，但是现在推荐 `relu` 激活函数。
   -$\mathbf W_i,\mathbf{\vec b}_i$为待学习的网络参数。

2. 给定 `query`$\mathbf{\vec q}$和 `document`$\mathbf{\vec d}$，用户点击该文档的概率为：

  $p(\mathbf {\vec d}\mid \mathbf {\vec q}) = \frac{\exp(\gamma\times R( \mathbf {\vec q},\mathbf {\vec d}))}{\sum_{\mathbf {\vec d}^\prime\in \mathcal D}\exp\left(\gamma\times R( \mathbf {\vec q},\mathbf {\vec d}^\prime)\right)}$

   其中$\gamma$为平滑因子，它是一个超参数，需要根据验证集来执行超参数搜索；$\mathcal D$是候选的文档集合。

   实际应用中，给定一对点击样本$(\mathbf {\vec q},\mathbf {\vec d}^+)$，我们从曝光但是未点击的文档中随机选择$K$篇文档作为负样本$(\mathbf {\vec q},\mathbf {\vec d}^-_k),k=1,2,\cdots,K$，则$\mathcal D = \{\mathbf d^+,\mathbf d^-_1,\cdots,\mathbf d^-_K\}$。

   论文中选择$K=4$，并且论文表示： 不同的负采样策略对结果没有显著影响。

   模型训练的目标是：最大化点击样本的对数似然：

  $\mathcal L = -\log \prod_{(\mathbf {\vec q},\mathbf {\vec d}^+)} p(\mathbf {\vec d}^+\mid \mathbf {\vec q})$

   然后基于随机梯度下降优化算法来求解该最优化问题。

   注意：这里并没有计算负样本的概率$p(\mathbf {\vec d}^-_k\mid \mathbf {\vec q})$，负样本的信息在计算概率$p(\mathbf {\vec d}^+\mid \mathbf {\vec q})$时被使用。

### 1.2 word hash

1. 在将 `query/document` 的文本转化为输入向量的过程中，输入向量的维度等于词表的大小。由于实际 `web search` 任务中的词汇表非常庞大，这导致 `DSSM` 网络的输入层的参数太多，模型难以训练。

   假设词汇表有50万，经过 `embedding` 之后的维度为300维，则输入层权重为$\mathbf W\in \mathbb R^{50万\times 300}$，一共 1.5亿参数。

   为解决该问题，`DSSM` 模型在第一层引入 `word hash` 技术。该层是一个线性映射，虽然参数非常多，但是这些参数不需要更新和学习。

2. `word hash` 技术用于降低输入向量的维度。给定一个单词，如：`good`，`word hash` 的步骤为：

   - 首先添加开始标记、结束标记：`#good#`
   - 然后将其分解为字符级的 `n-gram`格式：`#go,goo,ood,od#` （`n=3`时）
   - 最后将文本中的单词 `good` 用一组 `char-level n-gram` 替代。

3. 虽然英语词汇的数量可以是无限的（可以出现大量的、新的合成词），但是英语（或其它类似语言）的字符`n-gram` 数量通常是有限的。因此`word hash` 能够大幅降低词汇表的大小。

   50万规模的词汇表经过 `word hash` 之后降低到3万规模，这使得输入层的参数降低到 900万（假设 `embedding` 维度为 300 维）。相比较于原始的1.5亿，参数降低到原始数量的 1/16 。

   除此之外，`word-hash` 技术还有以下优点：

   - 它能够将同一个单词的不同形态变化映射到 `char-level n-gram` 空间中彼此接近的点。
   - 它能够有效缓解 `out-of-vocabulary:OOV` 问题。在推断期间，虽然有些词汇未出现在训练集中（未登陆词），但是当拆解未 `char-level n-gram` 之后，每个 `n-gram` 都在训练集中出现过。
   - 从单词到 `char-level n-gram` 的映射关系是固定的线性映射，不需要学习。

   `char-level n-gram` 可以视作 `word` 的一个简单的 `representation`，而 `word-hash` 技术就是得到这个 `representation` 。

4. `word-hash` 一个潜在的问题是冲突 `collision`：两个不同的单词可能具有相同的 `char-level n-gram` 表示。

   下表中统计了两个词汇表中的冲突统计信息。可以看到，当采用 `3-gram` 表示时，冲突的占比小于千分之一。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603210841.png?imageslim">
   </p>
   

### 1.3 实验

1. 论文实现的 `DSSM` 模型：

   - 模型结构：包含四层：
     - 第一层为 `word hash` 层，它将 `word` 映射为 `char-level 3-gram` 。其映射规则是固定的，不需要学习参数。
     - 第二层、第三层为中间层，每层输出为 300维。
     - 最后一层为输出层，输出 128维向量。
   - 权重初始化：权重通过在$[-\sqrt{\frac{6}{fan_{in}+fan_{out}}},\sqrt{\frac{6}{fan_{in}+fan_{out}}}]$之间均匀分布的随机变量来初始化。其中$fan_{in},fan_{out}$表示输入单元数量和输出单元数量。
   - 模型通过 `mini-batch` 随机梯度下降法优化，每个 `batch` 包含 1024个样本，一共训练 20 个 `epoch` 。
   - 模型原始词汇表为 50万（即：保留常见的50万词汇），经过 `word hash` 之后降低到 3万。

2. 实验数据集：数据集是从商业搜索引擎的 1年 `query` 日志文件中采样的 16510 个 `query`，平均每个 `query` 有 `15` 个相关的 `document`。

   每对 `(query,document)` 都有人工标注的标签。标签一共5个等级 `0-4`，`0` 表示无关，`4` 表示最相关。

3. `DSSM` 模型和其它模型的比较结果如图所示，其中模型的评估指标为 `NDCG` 。

   - `9~12` 行给出了不同的 `DSSM` 变化：

     - `DNN`：没有采用 `word-hash`的 `DSSM` 。它和第六行的`DAE`结构相同，但是`DAE` 采用无监督学习训练，而`DNN` 采用有监督学习训练。

       为了能够训练`DNN` 模型，我们采用4万规模的词汇表（即：保留常见的4万词汇）。

     - `L-WH linear`：线性的 `word hash` 模型。在经过 `word hash` 之后，直接连接到输出层，且输出层不采用任何非线性函数。因此整个模型都是线性的。

     - `L-WH non-linear`：非线性的 `word hash` 模型。在经过 `word hash` 之后，直接连接到输出层，但是输出层采用非线性函数。

     - `L-WH DNN`：标准的 `DSSM` 模型。

   - 结论：

     - 从 `DNN` 和 `DAE` 的比较结果发现：监督学习普遍比无监督学习效果好

     - `word hash` 允许我们使用更大规模的词汇表。如 `L-WH-DNN` 采用 50万规模的词汇表，而 `DNN` 采用 4万规模的词汇表，但是 `L-WH-DNN` 的模型参数反而更少。

       词汇表越小，则未登陆词越多，这导致文本被丢弃的信息越多。模型的效果越差。

       因此 `word hash` 技术既可以减少模型参数，又能提升模型效果。

     - 深层网络强于浅层网络。

       - 无监督学习： `LSA` 可以看作浅层网络。深层网络的 `DAE` 效果强于浅层网络 `LSA` 。
       - 监督学习：`L-WH non-linear` 可以视为 `L-WH DNN` 的浅层版本，实验结果表明后者效果更好。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603210849.png?imageslim">
   </p>
   

## 二、FNN

1. 传统的 `CTR` 预估模型大多数采用线性模型。线性模型的优点是易于实现，缺点是：模型表达能力较差，无法学习特征之间的相互作用 `interaction` 。

   非线性模型（如：`FM,GBDT`）能够利用不同的组合特征，因此能够改善模型的表达能力。但是这些特征无法利用不同特征的任意组合。

   论文 `Deep Learning over Multi-ﬁeld Categorical Data – A Case Study on User Response Prediction` 提出了 `FNN` 和 `SNN` 两种模型来学习特征的交互作用。

2. 由于视觉、听觉、文本信号在空间或者时间上是局部相关的，因此深度神经网络能够利用这种局部依赖性，并学习特征空间的 `dense representation`，从而使得神经网络能够直接从原始输入中直接有效的学习高阶特征。

   而在 `CTR` 预估任务中，大多数输入特征是离散的 `categorical` 特征，特征之间的局部依赖性是未知的。

   `Factorisation-machine supported Neural Networks: FNN` 和 `Sampling-based Neural Networks: SNN` 两种模型就是从离散的 `sparse feature` 学到 `dense representation` 。

3. `FNN` 和 `SNN` 的主要思路是：

   - 从 `sparse feature`$\mathbf{\vec x}$学到 `dense representation`$\mathbf{\vec z}$
   - 将$\mathbf{\vec z}$作为一个深度前馈神经网络的输入，输出为概率$\hat y$

   二者的区别在于学到$\mathbf{\vec z}$的方式不同。

   另外 `FNN` 和 `SNN` 采用了 “逐层预训练 + 微调” 的训练方式，事实上这种训练方式目前已经不推荐使用，而是直接端到端的训练方式。

### 2.1 模型

#### 2.1.1 FNN

1. `FNN` 模型结合了神经网络和 `FM` 模型，网络分为以下几层：

   - 第 0 层输入层：`categorical` 经过 `one-hot` 编码之后作为输入，该层也被称作 `sparse binary feature` 层。
   - 第1层`embedding` 层：输入层经过局部连接生成`embedding` 向量，该层也被称作 `dense real layer`层。
   - 第2层到第$L$层：全连接层。
   - 最后一层：`sigmoid` 输出层。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603210901.png?imageslim">
   </p>
   

2. `FNN` 的核心在于 `embedding` 向量的生成。假设有$F$个 `field`，`one-hot` 向量为$\mathbf{\vec x}$，`field i` 在向量中的起始位置为$s_i$、终止位置为$e_i$（包含）。

   每个 `field` 生成一个 `embedding` 向量。即 `field i` 生成$\mathbf{\vec z}_i = (w_i,v_1^{(i)},v_2^{(i)},\cdots,v_K^{(i)}) \in \mathbb R^{K+1}$。同时还有全局`bias` 。即：

  $\mathbf {\vec z} = (w_0,w_1,v_1^{(1)},v_2^{(1)},\cdots,v_K^{(1)},\cdots,w_F,v_1^{(F)},v_2^{(F)},\cdots,v_K^{(F)})^T$

   - 输入位置$s_i \sim e_i$仅仅与$\mathbf{\vec z}_i$相连，即：局部连接：

    $\mathbf{\vec z}_i = \mathbf W_0^{(i)} (x_{s_i},x_{s_i+1},\cdots,x_{e_i})^T$

     其中$\mathbf W_0^{(i)} \in \mathbb R^{(K+1)\times (e_i-s_i+1)}$为映射参数。

   -$\mathbf{\vec z}$由 `FM` 模型初始化。由于采用逐层预训练，因此一旦初始化$\mathbf{\vec z}$之后就固定。

     因此求解 `FM` 的过程就是求解$\mathbf W_0$的过程，且一旦初始化后就冻结$\mathbf W_0$，直到最后的微调阶段才更新$\mathbf W_0$。

3. 一旦求解出$\mathbf{\vec z}$，就可以计算后续网络：

   其中$\sigma(\cdot)$为激活函数。

   - 网络每层的隐向量维度可以不同。
   - 网络每层通过 `layer-wise RBM` 逐层预训练来初始化。

4. 一旦进行了 `FM` 预训练和 `layer-wise RBM` 预训练之后，则可以通过监督学习来微调模型。

   模型的损失函数为交叉熵：

  $\mathcal L(y,\hat y) = -y\log\hat y - (1-y)\log(1-\hat y)$

   考虑到：

  $\frac{\partial \mathcal L}{\partial \mathbf W_0^{(i)}} = \sum_{k=1}^{K+1}\frac{\partial \mathcal L}{\partial z_{i,k}} \frac{\partial z_{i,k}}{\partial \mathbf W_0^{(i)}} = \frac{\partial \mathcal L}{\partial \mathbf {\vec z}_i} (x_{s_i},x_{s_i+1},\cdots,x_{e_i})$

   当$x_j = 0$时，对应梯度为0。因此只需要更新$\mathbf{\vec x}$非零的分量对应的参数，这大大降低了参数更新的计算量。

#### 2.1.2 SNN

1. `SNN` 和 `FNN` 的区别在于第一层的网络结构，在 `SNN` 中第一层是全连接的。

  $\mathbf{\vec z} =\sigma(\mathbf W_0\mathbf{\vec x} + \mathbf{\vec b}_0)$

   其中$\sigma(\cdot)$为激活函数，$\mathbf W_0,\mathbf{\vec b}_0$为第一层的网络参数。

   第一层参数可以通过两种预训练方式来预训练：`restricted Boltzmann machine:RBM` 或者 `denoising auto-encoder:DAE` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603210913.png?imageslim">
   </p>
   

### 2.2 实验

1. 数据集：采用 `iPinYou` 数据集。该数据集是一个公开的、真实的展示广告数据集，包含 1950万曝光数据，其中点击数据 14790个。

   所有特征都是 `categorical` 特征，经过`one-hot` 编码之后有 937670 维。

2. 模型比较了全量广告主，以及部分广告主（广告主 `1458,2259,2997,3386` ) 上的预测结果。

   结论：

   - `FM` 模型并没有显著的强于 `LR` 模型，这意味着该任务中二阶特征交叉并没有很好的捕捉到数据的模式。
   - 大多数情况下 `FNN` 的效果最好。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603210922.png?imageslim">
   </p>
   

## 三、PNN

1. 在典型的推荐、广告任务中，大多数特征都是离散的`categorical`。一种常见做法是将这些离散特征进行 `one-hot` 编码，从而转化为稀疏二元特征 `sparse binary feature` 。

   传统模型非常依赖于特征工程从而捕获潜在的高阶模式，而 `FM` 等模型通过自动特征交叉捕获二阶交叉特征。

   论文 `Product-based Neural Networks for User Response Prediction` 提出了 `Product-based Neural Networks:PNN` 模型，该模型构建了一个 `embedding` 层来学习离散特征的分布式`representation`，构建一个 `product` 层来自动捕捉离散特征的潜在高阶模式。

### 3.1 模型

1. 假设有$N$个 `field`，`one-hot`向量为$\mathbf{\vec x}$，`field i` 在向量中的起始位置为$s_i$、终止位置为$e_i$（包含）。

   每个 `field` 生成一个 `embedding` 向量。即 `field i` 生成$\mathbf{\vec f}_i = (f_1^{(i)},f_2^{(i)},\cdots,f_K^{(i)})^T \in \mathbb R^K$。

   模型包含以下几层：

   - 第 0 层输入层：`categorical` 经过 `one-hot` 编码之后作为输入

   - 第1层`embedding` 层：模型从每个 `field` 中学得各 `field` 的 `embedding` 表示。

     输入位置$s_i \sim e_i$仅仅与$\mathbf{\vec f}_i$相连，即：局部连接：

    $\mathbf{\vec f}_i = \mathbf W_0^{(i)} (x_{s_i},x_{s_i+1},\cdots,x_{e_i})^T$

     其中$\mathbf W_0^{(i)} \in \mathbb R^{K\times (e_i-s_i+1)}$为映射参数，它由$\mathbf W_0$的第$s_i$到第$e_i$列组成。

   - 第2层 `product` 层：由`embedding`特征的一阶特征和二阶交叉特征拼接而成。其中$\mathbf z$部分表示一阶特征，$\mathbf p$部分表示二阶特征。为统一生成方式，$\mathbf z$由常数 `1` 和一阶特征交叉生成。

    $g(\cdot,\cdot)$表示成对特征交叉，当定义不同的$g$函数时，就定义了不同的 `PNN`实现。

     该层的输出为：

     其中$d_1$表示对提取的一阶特征和二阶特征通过$\mathbf W_z$和$\mathbf W_p$各自分别进行进一步的特征抽取的数量。$\mathbf W_z^{(i)},\mathbf W_p^{(i)}$类似于 `CNN` 的卷积核，其尺寸为整个图片大小，$d_1$为卷积核的数量，$i$表示第$i$个卷积核。

    $\odot$表示张量的内积，定义为：

    $\mathbf A\odot \mathbf B = \sum_{i,j}A_{i,j}\times B_{i,j}\in \mathbb R$

   - 第3层到第$L$层：全连接层。

   - 最后一层：`sigmoid` 输出层。

    $\hat y = \text{sigmoid}(\mathbf {\vec h}_L)$

   模型的损失函数为 `logloss`：

  $\mathcal L(y,\hat y) = -y\times \log \hat y - (1-y)\times \log(1-\hat y)$

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603210936.png?imageslim">
   </p>
   

#### 3.1.1 IPNN

1. `Inner Product-based neural network:IPNN`：`IPNN` 的特征交叉函数为：

  $p_{i,j} = g(\mathbf{\vec f}_i,\mathbf{\vec f}_j) = \mathbf{\vec f}_i\cdot\mathbf{\vec f}_j$

   则有：

   则计算$\mathbf{\vec h}_2$的复杂度为：

   - 空间复杂度：$O(d_1\times N\times (K+N))$。

> 它们分别是计存储$\mathbf W_z,\mathbf W_p$的空间需求。

- 时间复杂度：$O(N^2\times (d_1+K))$。

1. 为降低复杂度，可以将$\mathbf W_p^{(i)}$分解：

  $\mathbf W_p^{(i)} = \vec\theta^{(i)} \left(\vec\theta^{(i)} \right)^T$

   则有：

   其中$\vec\delta_s^{(i)} = \theta_s^{(i)} \mathbf{\vec f}_s \in \mathbb R^K$。

   则有：

  $\mathbf{\vec h}_p = \left(||\sum_i ||\vec\delta_1^{(i)},\cdots,||\sum_i \vec\delta_{d_1}^{(i)}||\right)^T$

   则计算$\mathbf{\vec h}_2$的复杂度为：

   - 空间复杂度：$O(d_1\times N\times K)$
   - 时间复杂度：$O(d_1\times N\times K)$

2.$\mathbf W_p^{(i)} = \vec\theta^{(i)} \left(\vec\theta^{(i)} \right)^T$仅仅是一阶分解，实际上可以进行更加通用的$M$阶分解：

   其中$\vec\theta_s^{(i)} \in \mathbb R^M$。此时有：

   这种分解的代价更高，同时约束更弱。

#### 3.1.2 OPNN

1. `Outer Product-based neural network:OPNN` ：`OPNN` 的特征交叉函数为：

  $p_{i,j} = g(\mathbf{\vec f}_i,\mathbf{\vec f}_j) = \mathbf{\vec f}_i \mathbf{\vec f}_j ^T$

   与内积产生标量不同，这里的外积产生一个矩阵。

   则$\mathbf p \in \mathbb R^{N\times N\times K\times K}, \mathbf W_p^{(i)} \in \mathbb R^{N\times N\times K\times K}$。

   计算$\mathbf{\vec h}_2$的复杂度为：

   - 空间复杂度：$O(d_1\times N^2\times K^2)$。它完全由$\mathbf W_p$主导。
   - 时间复杂度：$O(d_1\times N^2\times K^2)$。它完全由$\mathbf{\vec h}_p$主导。

2. 为降低复杂度，定义：

   此时$\mathbf W_p^{(i)} \in \mathbb R^{ K\times K}$。

   则计算$\mathbf{\vec h}_2$的复杂度为：

   - 空间复杂度：$O(d_1\times K\times (N+K))$。

     > 它们分别是计存储$\mathbf W_z,\mathbf W_p$的空间需求。

   - 时间复杂度：$O(d_1\times K\times (K+N))$。

#### 3.1.3 讨论

1. 当移除 `product` 层的$\mathbf{\vec h}_p$部分时，`IPNN` 和 `OPNN` 完全相同。

2. 当采用 `IPNN` 时，`IPNN` 和 `FM` 模型非常相似。

   - `FM` 模型将抽取的一阶、二阶特征直接送入分类器
   - `IPMM` 模型将抽取的一阶、二阶特征，首先使用类似 `CNN` 的 “核函数” （由$\mathbf W_z,\mathbf W_p$给出）抽取$d_1$个特征，然后将抽取后的特征送入 `DNN` 。

3. 向量的内积可以视为一系列的 “乘法&加法” 操作，乘法类似于 `AND`、加法类似于 `OR`。向量的外积只有乘法，类似于 `AND` 。

   因此 `product` 层可以视为学习一系列的 `AND/OR` 规则。

### 3.2 实验

1. 数据集：

   - `iPinYou`：该数据集是一个公开的、真实的展示广告数据集，包含 1950万曝光数据，其中点击数据 14790个。
   - `Criteo`：一个著名的广告 `benchmark` 数据集，包含 `1TB` 的点击日志。

   论文采用连续的 7 天来训练，第 8 天数据来评估。

2. 不同模型的评估结果如下所示，其中评估指标为 `AUC,Logloss,RMSE,RIG` 。`RIG`表示相对信息增益 `Relative Information Gain` ，它等于 `1-NE` ，其中 `NE` 为归一化的交叉熵。

   - 模型配置：

     - `FM` 模型的 `embedding` 向量维度为 10 维。
     - `CCPM` 模型有一个 `embedding` 层、2个卷积层、1个最大池化层、1个隐层，一共五层。
     - `FNN` 模型有1个 `embedding`层、3个隐层，一共四层。
     - `PNN`模型有1个 `embedding`层、1个 `produc`t 层、3个隐层，一共五层。
     - `PNN*`模型和 `IPNN,OPNN` 相同，但是 `product` 层是内积和外积的拼接。

     另外：`LR,FM` 使用 `L2` 正则化；`FNN,CCPM,PNN` 使用 `dropout` (0.5的遗忘率) 。

   - 结论：

     - `FM` 模型战胜了 `LR` 模型，说明 `FM` 模型有效的捕捉到了特征交互。
     - 神经网络模型（`FNN,CCPM,IPNN,OPNN,PNN*` ）战胜了 `LR` 模型和 `FM` 模型，说明神经网络模型捕捉到了更高阶的潜在模式 `latent pattern` 。
     - 总体而言 `PNN` 模型效果最好（包括 `IPNN` 和 `OPNN` 模型）。
     - 融合了 `IPNN` 和 `OPNN` 的 `PNN*` 模型并没有显著的强于`IPNN` 和 `OPNN` ，这说明 `IPNN` 和 `OPNN` 捕捉交互特征的能力已经足够强大。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603210946.png?imageslim">
   </p>
   

## 四、DeepCrossing

1. 人工特征组合在很多模型中扮演非常重要的角色，但对于 `web-scale` 任务而言，由于特征数量非常庞大，人工组合特征不太现实。

   论文 `“Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features”` 提出了 `Deep Crossing` 模型，该模型利用深度神经网络来自动组合特征从而生成高阶特征。

2. 虽然 `Deep Crossing` 模型能够自动组合原始特征，但是收集原始数据并提取原始特征仍需要用户的大量精力。

### 4.1 原始特征

1. 在搜索广告`sponsored search` 任务中，我们有大量的原始特征，每个原始特征都用一个向量表示。论文考察的原始特征包括：

   - 用户 `Query`：用户的搜索文本。

   - 广告主竞价 `Keyword`：广告主的竞价关键词。

   - 广告 `Title`：广告的标题文本。

     对于 `Query,Keyword,Title` 等文本特征，论文通过将文本字符串转换为字符级的 `3-gram` 的形式，得到一个 49292 维的向量。其中 49292 为 `3-gram` 词典大小。

   - `MatchType`：广告主指定的关键词匹配类型，分为 `exact,phrase,broad,contextual` 四种。

     论文将其转换为一个 4 维的 `one-hot` 向量。

   - `CampaignID`：营销 `campaign` 的 `ID` 。

2. 由于广告系统中可能有百万级的 `campaign`，因此经过 `one-hot` 之后的 `campaign` 特征维度非常高，这会导致模型尺寸非常大。

   论文的解决方案是：使用一组特征来描述 `campaign` 特征：

   - `CampaignID` 特征：进行 `one-hot`，但是 `ont-hot` 向量仅仅包含点击次数最多的 `10000` 个 `campaign` 的`ID`，剩余的所有 `campaign` 被分配到第 `10001` 个 `slot` 。
   - `CampaignIDCount` 特征：对于分配到第 `10001` 个 `slot` 的剩余 `campaign`，提供每个 `campaign` 的统计特征（如曝光次数、平均`CTR`等），该特征是计数特征 `counting feature` 。

3. 用到的特征如下图所示：

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603210956.png?imageslim">
   </p>
   

### 4.2 模型

1. `DeepCrossing` 模型的输入是原始特征，模型有四种类型的`Layer`：

   - `Embedding Layer`：将原始特征映射成 `embedding` 向量。

     假设原始特征 `one-hot` 向量为$\mathbf{\vec x}$，`field i` 在向量中的起始位置为$s_i$、终止位置为$e_i$（包含）。则 `embedding` 层的输出为：

    $\mathbf{\vec E}^{(i)} = \text{relu}(\mathbf W^{(i)} (x_{s_i},x_{s_i+1},\cdots,x_{e_i})^T + \mathbf{\vec b}^{(i)}) \in \mathbb R^{m_i}$

     其中$\mathbf W^{(i)} \in \mathbb R^{m_i\times (e_i-s_i+1)}, \mathbf{\vec b}^{(i)}\in \mathbb R^{m_i}$为参数，$m_i$为第$i$个 `embedding` 的维度。

     - 通常有$m_i \ll (e_i-s_i+1)$，这使得 `embedding` 之后的维度大大小于原始特征维度。

     - 对于某些维度较小的原始特征（如：维度小于 256），无需进行 `embedding` 层，而是直接输入到 `Stacking Layer` 层。如图中的 `Feature #2` 。

       这是在模型大小和信息保留程度之间的折衷：

       - 完全保留信息（原始输入），则可能使得模型过大
       - 全部使用 `embedding`，则可能信息丢失太多

   - `Stacking Layer`：所有 `embedding` 特征和部分原始特征拼接成一个向量：

    $\mathbf{\vec h}_1 = <\mathbf{\vec E}^{(1)},\cdots,\mathbf{\vec E}^{(K)}>$

     其中$<\cdot >$表示特征拼接，$K$为原始特征的数量，$\mathbf{\vec E}$为 `embedding` 向量。如果是直接输入的原始特征，则$\mathbf{\vec E}$表示该原始特征的 `one-hot` 向量。

   - `Residual Unit Layer`：基于残差单元 `Residual Unit` 构建的残差层，其输出为：

    $\mathbf{\vec h}_2 = \mathcal F(\mathbf{\vec h}_1;\mathbf W_0,\mathbf W_1,\mathbf{\vec b}_0,\mathbf{\vec b}_1) + \mathbf{\vec h}_1$

     其中$\mathcal F(\cdot)$为残差单元：

    $\mathcal F(\mathbf{\vec h}_1;\mathbf W_0,\mathbf W_1,\mathbf{\vec b}_0,\mathbf{\vec b}_1) = \text{relu}\left[\mathbf W_1 (\text{relu}(\mathbf W_0\mathbf{\vec h}_1 + \mathbf{\vec b}_0)) + \mathbf{\vec b}_1\right]$

     注意：在一个 `DeepCrossing` 网络中可以有多个残差层。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603211005.png?imageslim">
     </p>
     

   - `Scoring Layer`：`sigmoid` 输出层。其输出为：

    $\hat y = \text{sigmoid} (\mathbf{\vec w} \cdot \mathbf{\vec h}_{L-1} + b)$

     其中$\mathbf{\vec w},b$为参数，$\mathbf{\vec h}_{L-1}$为前一层的隐向量，$L$为总的层数。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603211023.png?imageslim">
   </p>
   

2. 模型的损失函数为负的 `Logloss`：

  $\mathcal L = - \frac 1N \sum_{i=1}^N\left(y_i \log \hat y_i +(1-y_i)\log(1- \hat y_i) \right)$

3. `DSSM` 模型也可以认为是执行了特征交叉。`DSSM` 模型有两路输入：`Query`、`Ad Text` 。模型分别抽取特征，最后计算二者的 `cosin` 距离，该距离就代表了二者的特征交叉。

   与 `DeepCrossing` 相比，`DSSM` 的特征交叉发生的时间比较靠后。从效果上讲，`DeepCrossing` 的特征交叉发生时间靠前的效果更好。

### 4.3 实验

1. 实验结果表明：是否包含某些原始特征对模型效果影响较大。

   - 下图比较了引入不同特征的情况下，每个训练 `epoch` 结束时的验证集 `logloss`。该 `logloss` 除以 `All_features` 模型的最低 `logloss` 来归一化。

     - `All_features` ：使用所有的特征。
     - `Without_Q_K_T`：不使用 `counting`特征，且不使用 `Query,Keyword,Title` 等文本特征。
     - `Only_Q_K`：仅使用`Query,Keyword` 特征。
     - `Without_position`：不使用 `counting` 特征，且不使用广告的位置信息。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/ntkD74iqJ8St.png?imageslim">
     </p>
     

   - 下图比较了是否包含 `counting` 特征的效果。`logloss` 除以 `All_without_counting` 模型的最低 `logloss` 来归一化。

     - `All_with_counting`：使用包含 `counting` 在内的所有特征
     - `All_without_counting`：使用剔除`counting` 后的所有特征。
     - `Counting_only`：仅仅使用 `counting` 特征。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/cc1GcGCNAr2U.png?imageslim">
     </p>
     

2. `DeepCrossing` 和 `DSSM` 的比较：评估指标为 `AUC`，其中以 `DSSM` 的结果作为基准来归一化。

   `text_cp1_tn_s`、`text_cp1_tn_b` 为论文构造的两个训练集，`text_cp1_tn_s`1.94亿样本，训练集 `text_cp1_tn_b` 29.3亿样本。

   评估在对应的测试集上进行：验证集 0.49亿样本，测试集 0.45亿样本。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/2wtpmxMV4QjO.png?imageslim">
   </p>
   

## 五、Wide&Deep

1. 推荐系统中的一个挑战是：同时实现 `memorization` 和 `generalization`。

   - `memorization`：学到 `item` 或者 `feature` 共现关系，并基于历史数据中的这种相关性来推荐。

     基于`memorization` 的推荐通常更具有话题性，并且和用户已经发生行为的 `item` 直接关联。

   - `generalization`：根据 `item` 或者 `feature` 的共现关系，探索过去从未发生或者很少发生的新特征组合。

     基于 `generalization` 的推荐通常更具有多样性。

2. 广义线性模型通常对特征执行 `one-hot`编码。如 `性别=男` 表示特征：如果用户是 `男性`，则该特征为 1 。

   - 通过特征交叉可以有效的达到 `memorization`。如特征交叉 `AND(性别=男，曾经购买汽车=奇瑞QQ)` ：当用户是男性、且曾经购买奇瑞 `QQ` 汽车时该交叉特征为 1。

   - 如果希望提升泛化能力，则可以提升特征的粒度。如： `AND(性别=男，曾经购买汽车=10万以下汽车)`。

     这种方式的限制是无法推广到训练集中没有出现过的 `query-item` 或者 `feature pair` 。

3. 基于 `embedding` 的模型（如 `FM` 或者 `DNN`）为每个 `query` 和 `item` 学习一个低维的 `dense embedding` 向量，通过 `embedding` 向量来泛化到训练集中未见过的 `query-item feature pair` ，同时也缓解了特征工程的代价。

   但是当 `query-item` 矩阵非常稀疏且矩阵的秩较高时（如：用户具有特定偏好，产品非常小众），很难学到有效的 `query/item` 的低维表达。

   此时大多数 `query-item pair` 之间不应该存在任何交互，但是 `dense embedding` 仍然给出了非零的预测结果。这会导致严重的过拟合，并给出一些不怎么相关的推荐结果。

   在这种场景下，基于特征交叉的广义线性模型能够记住这些特定偏好或者小众产品的 `exception rule` 。

4. 广义线性模型（称为 `wide` 模型）可以通过大量交叉特征来记住特征交互 `feature interaction` ，即 `memorization` 。其优点是可解释性强，缺点是：为了提升泛化能力，需要人工执行大量的特征工程。

   深度神经网络模型（称为 `deep` 模型）只需要执行较少的特征工程即可泛化到未出现的特征组合，即 `generalization` 。其优点是泛化能力强，缺点是容易陷入过拟合。

   论文 `“Wide & Deep Learning for Recommender Systems”` 结合了 `wide` 模型和 `deep` 模型，同时实现了 `memorization` 和 `generalization`。

   > 即：广义线性模型表达能力不强，容易欠拟合；深度神经网络模型表达能力太强，容易过拟合。二者结合就能取得平衡。

### 5.1 模型

1. 一个典型的推荐系统整体架构如下图所示。推荐的流程如下：

   - 当用户访问 `app store` 时产生一个 `query`，它包含用户特征（如用户画像）和上下文特征（如 当前时刻`LBS` 信息、设备信息、页面信息）。

   - 检索 `retrieval` 模块根据用户的 `query` 返回一组相关 `app` 组成的 `app list` 。

     检索算法可以综合利用机器学习模型和人工定义的规则，它用于将百万级别的 `app` 集合降低到几百上千级别的候选 `app list` 。

   - 精排 `ranking` 模块对候选 `app list` 根据用户的行动（如：下载、购买）概率进行排名，返回概率最高的十几个或者几十个 `app` 组成的推荐结果。

   - 返回的推荐结果展示在用户界面，用户可以对这些 `app` 执行各种操作（如：下载、购买）。这些操作行为，以及`query, result app list` 都会被记录并发送到后台日志作为训练数据。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/EQ2DEXvkbhVq.png?imageslim">
   </p>
   

2. `Wide & Deep` 模型主要用于 `ranking` 精排模块，它包含一个 `linear model:LM`部分和一个 `neural network:NN` 部分。

   设模型的输入特征向量为$\mathbf{\vec x}=(x_1,\cdots,x_d)^T$是一个$d$维的特征向量（经过 `one-hot` ），仅包含原始特征。$\phi(\cdot)$表示特征交叉转换函数，$\phi(\mathbf{\vec x})$包含转换后的特征。

   - `LM` 部分：即左侧的 `wide` 子模型，它是一个线性模型：

    $y = \mathbf{\vec w}\cdot <\mathbf{\vec x},\phi(\mathbf{\vec x})> + b$

     其中$<>$表示特征拼接，$\mathbf{\vec w} \in \mathbb R^{d+d^\prime}$是模型参数（$d^\prime$表示交叉特征的数量），$b$为偏置。

   - `NN` 部分：即右侧的 `deep` 子模型，它是一个 `DNN` 模型。

     - 输入层：为了缓解模型的输入大小，`DNN` 的所有离散特征的输入都是原始特征，而没有经过 `one-hot` 编码转换。

     - 第一层 `embedding` 层：将高维稀疏的 `categorical` 特征转换为低维的 `embedding` 向量。论文中的`embedding` 向量维度为 32 维。

     - 第二层特征拼接层：将所有的 `embedding` 向量拼接成一个 `dense feature` 向量。论文中该向量维度为 1200维。

     - 后续每一层都是全连接层：

      $\mathbf{\vec h}^{(l+1)} = \sigma(\mathbf W^{(l)}\mathbf{\vec h}^{(l)} + \mathbf{\vec b}^{(l)})$

       其中$l$为层的编号，$\sigma(\cdot)$为激活函数。

   模型联合了 `wide` 和 `deep` 的输出：

  $\hat y = p(y = 1\mid \mathbf{\vec x}) = \text{sigmoid} (\mathbf{\vec w}_{wide}\cdot <\mathbf{\vec x},\phi(\mathbf{\vec x})>+\mathbf{\vec w}_{deep}\cdot \mathbf{\vec h}^{(L)} + b)$

   其中$\mathbf{\vec w}_{wide}$为 `wide` 部分的权重，$\mathbf{\vec w}_{deep}$为 `deep` 部分的权重，$b$为全局偏置。

   模型的损失函数为负的对数似然，并通过随机梯度下降来训练：

  $\mathcal L = - \frac 1N \sum_{i=1}^N\left(y_i \log \hat y_i +(1-y_i)\log(1- \hat y_i) \right)$

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/UaEQ7EbLXxh2.png?imageslim">
   </p>
   

3. `Wide&Deep` 模型与 `LM & DNN` 的 `ensemble` 集成模型不同。

   - 在集成模型中，每个子模型都是独立训练的，只有预测时才将二者结合在一起。

     在 `Wide&Deep` 模型中，每个子模型在训练期间就结合在一起，共同训练。

   - 在集成模型中，每个子模型必须足够大从而足够健壮，使得子模型集成之后整体的 `accuracy` 等性能足够高。

     在 `Wide&Deep` 模型中，每个子模型都可以比较小，尤其是 `wide` 部分只需要少量的特征交叉即可。

4. `Wide&Deep` 模型的实现如下图所示。模型的 `Pipeline` 分为三个部分：

   - 数据生成 `data generation` 阶段：此阶段把一段时间内的用户曝光数据生成训练样本，每个样本对应一次曝光，标签为用户是否产生行为（如：下载 `app`）。

     在这个阶段执行两个特征工程：

     - 离散的字符串特征（如`app name`）映射成为整数`ID` ，同时生成映射字典`vocabulary`。

       注意：对于出现次数低于指定阈值（如 10此）的字符串直接丢弃，这能够丢弃一些长尾的、罕见的字符串，降低字典规模。

     - 连续特征离散化：

       - 首先将连续特征归一化到 `0~1` 之间，它通过累积分布函数来归一化，计算特征的整体排名（1.0表示排名最高，0.0表示排名最低）

        $f(x) = P(X\le x) = \frac{\sum_{i=1}^NI({X_i}\le x)}{N}$

         其中$I(\cdot)$为示性函数，$N$为总样本数量。

       - 然后将$f(x)$映射到$q$分位。如映射到`10`分位时，假设$f(x) = 0.95$（即排名在最高的 5%），则映射为 `9` 这个等级。

   - 模型训练 `model training` 阶段：此阶段的输入为样本数据、字典数据、标签数据。

     - `wide` 部分的特征由：用户已经安装的 `app` 、给用户曝光的 `app` 的两个特征的交叉组成。

     - `deep` 部分从每个离散特征中学习。

     - 每次有新的训练数据到达时，模型会利用该部分数据重新训练。

       由于重新训练模型的代价太大，因此我们实现了一个 `warm-starting` 系统：基于前一个模型的 `embedding` 和$\mathbf{\vec w}_{wide}$参数来初始化当前模型的这两个参数。

     - 在模型部署到线上之前，还需要验证模型的质量。

   - 模型使用 `model serving` 阶段：训练并验证模型后，将模型部署到服务器上来提供预测服务。

     为满足 `10ms` 量级的响应速度，采用多线程并行来优化性能。方法为：假设一个 `batch` 有 `500` 个候选 `app`，

     - 先将其拆分为更小的一组 `batch`：如 `50` 个 `batch`，每个 `batch` 有 `10` 个候选 `app`。
     - 每个子线程并行的执行推断
     - 将所有子线程的推断结果收集在一起，拼接成整个 `batch` 的推断结果并返回

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/nWW75uXLmSbS.png?imageslim">
   </p>
   

### 5.2 实验

1. 模型从谷歌应用商店的 `app` 下载量，以及 `serving` 性能这两方面来评估。

2. 我们执行三个星期的在线`A/B test`，其中：

   - 对照组1：随机抽取 `1%` 的用户，该部分用户采用旧的精排模型：只有 `wide` 部分的、采取大量特征交叉的 `LR` 模型。
   - 对照组2：随机抽取 `1%` 的用户，该部分用户采用 `DNN` 精排模型：只有 `deep` 部分的 `DNN` 模型。
   - 实验组：随机抽取 `1%` 的用户，该部分用户采用 `Wide&Deep` 精排模型。

   另外我们还在一个离线的留出 `holdout`数据集上评估了这三个模型的离线 `AUC` 指标。最终结果如下表：

   - 在线 `app` 下载量提升：`Wide&Deep`提升幅度较大，达到 `3.9%` 。

     事实上仅 `deep` 模型就能提升 `2.9%`，但是结合了 `wide` 和 `deep`能进一步提升。

   - 离线 `AUC` 提升：`Wide&Deep` 稍微有所提升，但提升幅度不大。

     可能的原因：离线评估固定了曝光和 `label`，即：你给用户推荐的 `app list` 是固定的，用户是否安装的 `label` 也是固定的。

     这种假设实际上是有问题的，因为如果模型发生变化则推送给用户的 `app list` 会有所变化，是否安装的 `label` 也会有所变化。这使得 `模型 --> 特征、label --> 模型`相互依赖。而离线的数据集无法体现这种模型和数据的相互依赖性。

     在线 `A/B test` 中，模型学习从用户最新的反馈中学习，然后推荐出新的 `app list` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/eP0N7kvBAiLC.png?imageslim">
   </p>
   

3. 谷歌应用商店面临高流量，因此提供高吞吐量、低延迟的`model serving` 服务是一个挑战。在流量峰值，我们需要每秒为 `1000万` 个 `app` 打分。

   - 当使用单线程时，一个 `batch` 的样本打分需要 `31ms` 。
   - 当使用多线程并行时，一个 `batch`的样本拆分到多个更小的 `batch`，最终打分需要 `14ms` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/NXBO17OsrYzB.png?imageslim">
   </p>
   

## 六、DCN

1. 人工交叉特征效果很好，但是有两个主要缺点：

   - 人工探索所有的交叉特征是不现实的。
   - 难以推广到未曾出现过的交叉特征。

   虽然 `DNN` 能够自动探索交叉特征并推广到未出现过的交叉特征，但是`DNN` 的学习效率不高，且无法显式的学习特征交叉。

   论文 `《Deep & Cross Network for Ad Click Predictions》` 提出了`Deep&Cross Network:DCN` 模型，该模型保留了 `DNN` 的优点。

   此外，`DCN` 提出了一种新的`cross network` ，该网络能够高效学习高阶特征交互。

   实验结果表明：`DCN` 无论在模型表现以及模型大小方面都具有显著优势。其优点：

   - `cross network` 显式的在每层应用特征交叉，有效的学习了预测能力强的交叉特征，无需人工特征工程。
   - `cross network` 简单高效，可以学习高阶交叉特征。每增加一层，能够学到的交叉特征的阶数就加一阶。
   - 模型表现更好，且参数要比 `DNN` 少一个量级。

2. `Wide&Deep` 模型使用交叉特征作为输入，其成功依赖于选择合适的交叉特征。

   如何选择合适的交叉特征是个指数级问题，目前尚未有明确有效的方法。

### 6.1 模型

1. `DCN` 模型结构如下图所示，模型包含 `embedding and stacking` 层、`cross network`、`deep network` 三个组成部分。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/Sf1ittlImXLf.png?imageslim">
   </p>
   

2. `embedding and stacking` 层：假设输入包含 `sparse` 特征和 `dense` 特征。设原始特征为向量$\mathbf{\vec x}$，其中：

  $\mathbf{\vec x} = <\mathbf{\vec x}_{sparse}^{(1)},\cdots,\mathbf{\vec x}_{sparse}^{(K)},\mathbf{\vec x}_{dense}>$

   其中$\mathbf{\vec x}_{sparse}^{(i)}$为 `field i` 的 `one-hot` 向量，$\mathbf{\vec x}_{dense}$为经过归一化的 `dense` 特征，$<\cdot >$为向量拼接。

   - 首先将 `field i` 的特征映射到 `embedding` 向量：

    $\mathbf {\vec x}_{embed}^{(i)} = \mathbf W_{embed}^{(i)} \mathbf {\vec x}_{sparse}^{(i)}$

     其中$\mathbf {\vec x}_{sparse}^{(i)}\in \mathbb R^{d_i}$为`one-hot` 向量长度，$\mathbf W_{embed}^{(i)}\in \mathbb R^{e_i\times d_i}$为参数。

- 然后将 `embedding` 向量和归一化的 `dense` 特征拼接成向量：

 $\mathbf{\vec x}_0 = <\mathbf {\vec x}_{embed}^{(i)},\cdots,\mathbf {\vec x}_{embed}^{(K)},\mathbf{\vec x}_{dense}> \in \mathbb R^{e_1+e_2+\cdots +e_K + d_s}$

  其中$d_s$为$\mathbf{\vec x}_{dense}$的向量长度。

  `embedding and stacking` 层就是将$\mathbf{\vec x}$转换为$\mathbf{\vec x}_0$。

1. `cross network`：`cross network` 是一个新颖的结构，其核心思想是以高效的方式显式应用特征交叉。

   - `cross network` 由交叉层构成，每层的输入输出为：

    $\mathbf {\vec x}_{l+1} = \mathbf {\vec x}_0 \mathbf {\vec x}_l^T \mathbf {\vec w}_{l+1} + \mathbf {\vec b}_{l+1} + \mathbf {\vec x}_l = f(\mathbf {\vec x}_l,\mathbf {\vec w}_{l+1},\mathbf {\vec b}_{l+1}) + \mathbf {\vec x}_l$

     其中$\mathbf{\vec x}_l$为第$l$层的输出，$\mathbf{\vec w}_l,\mathbf{\vec b}_l \in \mathbb R^d$为第$l$层的参数。 其中$d= e_1+e_2+\cdots +e_K + d_s$。

     每一层的输出都包含两个部分：该层的输入$\mathbf{\vec x}_l$、交叉特征$f$。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/uqb6yw967Koo.png?imageslim">
     </p>
     

   - `cross network` 的特殊结构使得交叉特征的阶数随着网络的深度加深而增加。

     对于$L$层的 `cross network`，其交叉特征的阶数最高可达$L+1$阶。

   - 假设 `cross network` 有$L_c$层，则 `cross network` 部分的参数数量为：$2\times L_c\times d$。

     另外， `cross network` 的时间复杂度、空间复杂度都是输入维度$d$的线性函数。因此`cross network` 相对于右侧的 `deep network` 部分来讲，其代价几乎是可以忽略不计的。因此 `DCN`的整体时间复杂度、空间复杂度与 `DNN` 几乎相同。

   - `cross network` 的本质是用$\mathbf {\vec x}_0 \mathbf {\vec x}_l^T$来捕获所有的特征交叉，这种方式避免了存储整个矩阵以及矩阵乘法运算。

   - 由于`cross network` 的参数很少，所以这部分网络的容量较小，网络过于简单，模型容易陷入欠拟合。

     为了加强模型的学习能力，`DCN` 在右侧引入了并行的 `deep network`。

2. `deep network`：`deep network` 部分是一个简单的全连接前馈神经网络：

  $\mathbf{\vec h}_{l+1} = \sigma(\mathbf W_l \mathbf{\vec h}_l + \mathbf{\vec b}_l^\prime )$

   其中$\sigma(\cdot)$为激活函数，$\mathbf W_l\in \mathbb R^{d_{l+1}\times d_l},\mathbf{\vec b}_l^\prime\in \mathbb R^{d_{l+1}}$，$d_l$为$\mathbf{\vec h}_l$的向量长度。

   假设所有隐层的维度都是$m$，一共$L_d$层，则`deep network`的参数数量为：

  $d\times m + m + (m^2+m)\times (L_d - 1)$

   其中：

   - 输入为$\mathbf{\vec x}_0\in \mathbb R^d$，所以第一层的参数数量为：$d\times m + m$
   - 后续的$L_d-1$层，每一层的参数数量为：$m\times m + m$

3. `DCN` 通过拼接层 `combination layer` 来拼接 `cross network` 和 `deep network`两个网络的输出向量，然后输出到标准的 `sigmoid` 输出层：

  $p = \text{sigmoid}\left(\mathbf W_{logits}<\mathbf {\vec x}_{L_1},\mathbf {\vec h}_{L_2}> + \mathbf{\vec b}_{logits}\right)$

   其中$p$为预测的点击概率，$\mathbf{\vec x}_{L_1}\in \mathbb R^d$为 `cross network` 的输出向量，$\mathbf{\vec h}_{L_2} \in \mathbb R^m$为 `deep network` 的输出向量，$\mathbf W_{logits} \in \mathbb R^{1\times (d+m)},\mathbf{\vec b}_{logits} \in \mathbb R^1$为模型参数。

   模型的损失函数为带正则化的对数似然函数：

  $\mathcal L = -\frac 1N\left(\sum_{i=1}^N \left[y_i\log p_i +(1-y_i)\log(1-p_i)\right]\right) +\lambda \sum_{l}||\mathbf W_l||_2^2$

   其中$N$为样本数量，$y_i$为样本的真实 `label` 。注意：这里只对 `dense network` 的权重进行正则化。

4. `DCN` 训练时联合训练 `cross network` 和 `deep network` 。

### 6.2 cross network

1. `cross network` 可以理解为：多项式逼近 `polynomial approximation`、`FM` 泛化`generalization to FM` 、或者有效投影`efficient projection` 。

2. 为讨论方便，假设$\mathbf{\vec b}_l = \mathbf{\vec 0}$。设$\mathbf{\vec w}_l$的第$i$个元素为$w_{l,i}$。令$\mathbf{\vec x} = (x_{ 1},\cdots,x_{ d})^T$，交叉项为$x_{ 1}^{\alpha_1}\times x_{ 2}^{\alpha_2}\times\cdots \times x_{ d}^{\alpha_d}$，其中$\vec\alpha=(\alpha_1,\cdots,\alpha_d)^T$为每一项的度 `degree`。则该交叉项的度为：

  $|\vec\alpha| = \sum_{i=1}^d \alpha_i,\quad \alpha_i \in \mathbb N$

   如果多项式中有多个交叉项，则多项式的度为所有交叉项中`degree` 的最大值。

3. 多项式逼近：`Weierstrass` 逼近定理表明：在一定条件下，任何函数都可以通过多项式逼近到任意精度。

   定义多项式：

  $P_n(\mathbf{\vec x}) = \left\{\sum_{\vec \alpha}w_\vec\alpha x_1^{\alpha_1}\cdots x_d^{\alpha_d}; 0\le |\vec\alpha|\le n,\vec\alpha\in \mathbb N^d\right\}$

   该多项式的项一共有$O(d^n)$个，即参数数量有$O(d^n)$个。

   - 与之相比，`cross network` 仅仅包含$O(d)$的参数就可以达到与$P_n(\mathbf{\vec x})$相同的度。
   - 可以证明：$L$层的 `cross network`，其输出可以表示为$\mathbf{\vec x}_0$的各交叉特征的多项式，多项式的度不超过$L$。

4. `FM` 泛化：在 `FM` 中，特征$x_i$和向量$\mathbf{\vec v}_i$相关联，交叉特征$x_i\times x_j$的权重为$\mathbf{\vec v}_i\cdot \mathbf{\vec v}_j$。而在 `DCN` 中，特征$x_i$和一组标量$\{w_l^{(i)}\}_{l=1}^L$相关联，交叉特征$x_i\times x_j$的权重从两个集合$\{w_l^{(i)}\}_{l=1}^L，\{w_l^{(j)}\}_{l=1}^L$中的元素相乘项构成。

   - 相同点：二者都是首先学习特征独立的一些参数（如：$\mathbf{\vec v}_i, \{w_l^{(i)}\}_{l=1}^L$），然后交叉项的权重时相应参数的某种组合。

   - 不同点：`FM` 是个浅层网络，其交叉项的度为2，因此表达能力受限。

     `DCN` 网络相对较深，能够表达$L$阶交叉特征，其中$L$为 `cross network` 的深度。

   另外和高阶 `FM` （三阶及其以上）不同，`cross network` 的参数数量是输入特征数量的线性函数，而不是指数函数。

5. 效投影`efficient projection` ：`cross network` 的每一层都是在$\mathbf{\vec x}_0$和$\mathbf{\vec x}_l$之间计算成对交互`pariwise interaction`，然后投影到$d$维。

   其中：

   - 行向量包含了$d^2$各元素，它代表成对交互`pariwise interaction` 。
   -$\mathbf w \in \mathbb R^{d\times d}$是一个对角矩阵，整个矩阵构成了一个分块对角矩阵。

### 6.3 实验

1. `Criteo Display Ads` 数据集：用于预测广告点击率的数据集，包含13个整数特征，26个类别特征，其中每个类别特征的取值集合（即：`cardinality` ）都很大。

   数据包含 7 天的 `11 GB` 用户日志（约 4100万条记录）。我们用前 6 天的数据训练，第7天的数据随机划分为相同大小的验证集和测试集。

   数据集经过预处理：

   - 整数特征：通过`log` 函数来归一化
   - 类别特征：执行 `one-hot` 编码。

   注意：类别特征经过 `embedding` 时，每个特征的 `embedding` 维度为$6\times (carninality)^{1/4}$。

   当拼接所有 `embedding` 特征和整数特征之后，向量维度为 `1026` 维。即：`embedding and stacking` 层的输出维度为$d=1026$。

2. 参数配置：

   - 优化策略：

     - 采用 `Adam` 优化器，`batch size = 512`
     - 对 `deep network` 采用 `batch normalization`
     - 对范数超过100的梯度进行裁剪

   - 正则化：采用早停策略。因为论文未发现$L_2$正则化或者 `dropout` 有效。

   - 超参数：`deep network` 隐向量维度、`deep network`层数、`cross layer` 层数、初始化学习率通过超参数搜索 `grid search` 获得。

     搜索空间：

     - `deep network` 隐向量维度：`32~1024`
     - `deep network`层数：`2~5`
     - `cross layer` 层数：`1~6`
     - 初始化学习率：`0.0001~0.001`

3. `DCN` 模型和其它四个模型的对比，这些模型为：

   - `LR` 模型：所有整数特征通过`log` 映射然后离散化。使用单个特征和交叉特征作为输入，其中交叉特征采用一个复杂的特征选择工具来选择。

     超参数搜索得到最佳超参数：42个交叉特征。

   - `FM` 模型：标准的 `FM` 模型。

   - `DNN` 模型：标准的前馈神经网络模型。

     超参数搜索得到最佳超参数：5层网络，隐向量维度 1024。

   - `Wide&Deep` 模型：其 `wide` 部分依赖领域知识来构建交叉特征。由于没有一个很好的方法来做交叉特征筛选，因此这里跳过了比较。

   - `DeepCrossing` 模型：它没有显式的特征交叉，而是依赖于 `stacking` 和残差单元来构建隐式的特征交叉。

     超参数搜索得到最佳超参数：5层残差单元，输入维度424，隐向量维度 537。

   - `DCN`：超参数搜索得到最佳超参数：`deep network` 为2层，`cross layer` 为6层，`deep network` 隐向量为 1024维。

   基于验证集 `logloss` 的比较结果：

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/SYIe1zJQ0uYx.png?imageslim">
   </p>
   

   达到指定最佳验证集 `logloss` 需要的参数数量：

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/fyBAou6AXFqn.png?imageslim">
   </p>
  

   给定不同的内存限制，模型大小和效果（验证集 `logloss`）

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/KNMXzxYBEybr.png?imageslim">
   </p>
   

4. 考察不同网络结构对于 `DCN` 的影响：

   - `deep network` 深度和隐向量维度对于模型效果的影响：所有的结果都是相对于 `DNN` 的验证集 `logloss`的绝对数量变化（单位为$10^{-2}$），负数表示下降。

     比较的 `DNN` 模型和 `DCN` 模型在 `deep network` 上具有相同的结构，只是 `DNN` 模型没有 `cross network` 而已。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/tGFxNIyl3QHs.png?imageslim">
     </p>
     

   - `cross network` 深度对于模型效果的影响：当`cross network` 深度从 0 到1时，所有`DCN` 模型的验证集 `logloss` 都下降； 当 `cross network` 深度继续增加时，部分 `DCN` 模型的验证集 `loglss` 反而上升。

     图中的 `2 layers,32 nodes` 表示`deep network` 的深度和隐向量维度。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/9p4EeLHGzSOk.png?imageslim">
     </p>
     

## 七、DeepFM

1. 理解用户点击行为背后隐藏的交叉特征对于 `CTR` 预估非常重要。例如，对 `app store` 的研究表明：人们经常在用餐时间下载送餐 `app`。这说明：`app` 类别和时间戳构成的交叉特征可以作为 `CTR` 预估的信号。

   通常用户点击行为背后的特征之间的各种交互非常复杂，其中的低阶交叉特征和高阶交叉特征都能发挥重要作用。根据 `Wide&Deep` 的理解，与仅考虑其中一种情况相比，同时考虑低阶交叉特征和高阶交叉特征能够带来额外的收益。

   但是这个问题的挑战在于：如何有效的构建交叉特征。

   - 有些交叉特征很容易理解，可以由业务专家人工设计。如上面的 `app store` 例子。
   - 大多数交叉特征都隐藏在数据中，很难事先预知，只能通过机器学习自动识别。如，经典的关联规则 `“啤酒 & 尿布”` 是从数据中提取的，而不是专家人工设计的。

2. 目前的模型要么利用低阶特征交叉（如 `FM`模型），要么利用高阶特征交叉（如 `DNN`模型），要么需要专业的特征工程（如 `Deep&Wide`）。

   - 理论上 `FM` 可以建模任意高阶的交叉特征，但是由于代价太大，通常只考虑建模二阶交叉特征。因此实际上 `FM` 只能建模低阶交叉特征。
   - 基于 `CNN` 的模型倾向于捕捉相邻特征之间的交互，而点击率预估任务的相邻特征之间并没有显著的相关性。
   - 基于 `RNN` 的模型更适合具有顺序依赖性的数据，而点击率预估任务的特征之间并不存在顺序依赖性。
   - `PNN` 和 `FNN` 像其它 `DNN` 模型一样，只能捕捉高阶交叉特征，几乎捕捉不到低阶交叉特征。
   - `Deep&Wide` 模型虽然可以同时捕获到低阶交叉特征和高阶交叉特征，但是 `deep` 部分和 `wide` 部分需要分别两个不同的输入，其中 `wide` 部分的输入仍然依赖于专业的特征工程。

   论文`《DeepFM: A Factorization-Machine based Neural Network for CTR Prediction》` 提出了 `DeepFM` 模型，该模型结合了 `FM` 的推荐能力和 `DNN`的特征学习能力，综合利用了低阶交叉特征和高阶交叉特征。其特点有：

   - `DeepFM` 集成了 `FM` 和 `DNN`，可以对低阶交叉特征和高阶交叉特征建模，同时无需对原始输入执行任何特征工程。
   - `DeepFM` 的 `wide` 部分和 `deep` 部分共享输入及`embedding` 。

### 7.1 模型

1. `DeepFM` 模型由两种组件构成：`FM` 组件、`deep` 组件，它们共享输入。这种共享输入使得`DeepFM` 可以同时从原始特征中学习低阶特征交互和高阶特征交互，完全不需要执行特征工程（如 `Wide&Deep` ）。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/dwATeTGe0R6w.png?imageslim">
   </p>
   

2. 假设输入包含 `sparse` 特征和 `dense` 特征。设输入向量$\mathbf{\vec x}$，其中：

  $\mathbf{\vec x} = <\mathbf{\vec x}_{sparse}^{(1)},\cdots,\mathbf{\vec x}_{sparse}^{(K)},\mathbf{\vec x}_{dense}> \in \mathbb R^d$

   其中$\mathbf{\vec x}_{sparse}^{(i)}$为 `field i` 的 `one-hot` 向量，$\mathbf{\vec x}_{dense}$为原始的 `dense` 特征，$<\cdot >$为向量拼接。对于特征 `j` （即$x_j$）：

   - 标量$w_j$用于对它的一阶特征重要性进行建模，即 `FM` 组件左侧的 `+` 部分。
   - 向量$\mathbf{\vec v}_j$用于对它的二阶特征重要性进行建模，即 `FM` 组件右侧的 `x` 部分。
   - 向量$\mathbf{\vec v}_j$也作为 `deep` 组件的输入，从而对更高阶特征交互进行建模，即 `deep`组件。

   最终模型联合了 `FM` 组件和 `deep` 组件的输出：

  $\hat y = \text{sigmoid}(\hat y_{FM} + \hat y_{DNN})$

   其中$\hat y \in (0,1)$为模型预测的`CTR`，$\hat y_{FM}$为 `FM`组件的输出，$\hat y_{DNN}$为 `deep` 组件的输出。

3. `FM` 组件：该部分是一个 `FM` ，用于学习一阶特征和二阶交叉特征。

   `FM` 组件由两种操作组成：加法 `Addition`和内积 `Inner Product`：

  $\hat y_{FM} = \sum_{i=1}^d (w_i\times x_i) + \sum_{i=1}^d\sum_{i=j+1}^d (\mathbf{\vec v}_i\cdot \mathbf{\vec v}_j)\times x_{i} \times x_{j }$

   其中$\mathbf{\vec v}_i\in \mathbb R^k$。

   - 第一项 `Addition Unit` 用于对一阶特征重要性建模
   - 第二项 `Inner Product` 用于对二阶特征重要性建模

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/6KR7QsdT3zrd.png?imageslim">
   </p>
   

4. `deep` 组件：该部分是一个全连接的前馈神经网络，用于学习高阶特征交互。

   假设 `embedding` 层的输出为：$\mathbf{\vec h}^{(0)} = [\mathbf{\vec e}_1,\cdots,\mathbf{\vec e}_m]$，其中$\mathbf{\vec e}_i$为`field i` 的 `embedding` 向量，$\mathbf{\vec h}^{(0)}$为前馈神经网络的输入。则有：

  $\mathbf{\vec h}^{(l+1)} = \sigma(\mathbf W^{(l)}\mathbf{\vec h}^{(l)} +\mathbf{\vec b}^{(l)} )$

   其中$l$为第 `l` 层，$\sigma(\cdot)$为激活函数。

   最终有：

  $y_{DNN} = \sigma(\mathbf {\vec w}_{dnn}\cdot \mathbf{\vec h}^{(L)} + b_{dnn})$

  $L$为`deep` 部分的网络深度。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/DHsArddbPQnc.png?imageslim">
   </p>
   

5. `DeepFM` 和其它模型的区别：

   - `FNN`：`FNN` 虽然也用到了 `FM` 模型，但是它仅使用 `FM` 模型来初始化 `FNN` 然后来微调模型。这使得 `FNN`的 `embedding` 层参数严重受制于 `FM` 模型，从而降低模型效果。

     另外 `FNN` 仅捕捉高阶特征交互。

     与之相比，`DeepFM` 不需要预训练，而是端到端的学习低阶特征交互和高阶特征交互。

   - `PNN`：作者发现 `OPNN` 不如 `IPNN` 可靠，因为外积的近似计算丢失大量信息使得结果不稳定。

     虽然 `IPNN` 更可靠，但是由于 `Product` 层的输出连接到第一个隐层的所有神经元，所以计算复杂度较高。同时 `IPNN` 和 `OPNN` 会忽略低阶特征交互。

     与之相比，`DeepFM` 中的 `Product`层（即 `FM` 组件）的输出仅仅连接到输出层（只有一个神经元），计算复杂度很低。

   - `Wide&Deep`：虽然 `Wide&Deep` 也可以对低阶特征和高阶特征同时建模，但是 `wide` 部分需要人工特征工程，而这需要业务专家的指导。

     与之相比，`DeepFM` 直接处理原始特征，不需要任何业务知识。

     另外，`Wide&Deep` 的一个直接扩展是：使用 `FM` 代替 `wide` 部分的`LR`模型，记作 `FM&DNN` 模型，原始的 `Wide&Deep` 模型记作 `LR&DNN` 模型。

     `FM&DNN` 模型更类似于 `DeepFM` 模型。但是 `DeepFM` 在 `FM` 和 `DNN` 之间共享 `embedding` 特征，这种共享策略有助于特征的 `representation` 学习，使得学到的特征 `representation` 更加精确。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/1risdLfUPWUW.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/4tJFigY1P3Gz.png?imageslim">
   </p>
   

### 7.2 实验

1. 数据集：

   - `Criteo Display Ads` 数据集：用于预测广告点击率的数据集，包含13个整数特征，26个类别特征，其中每个类别特征的取值集合（即：`cardinality` ）都很大。

     数据包含 7 天的 `11 GB` 用户日志（约 4100万条记录）。我们将数据集拆分为 90% 训练集和 10%测试集。

   - 华为公司数据：从华为 `App Store`的游戏中心收集连续 7 天的用户点击数据作为训练集，第八天的数据作为测试集。整个训练集+测试集约10亿条记录。

2. 评估指标 `AUC,logloss` 。

3. 参数配置：对于华为公司数据集，通过超参数搜索来获取最佳参数。对于 `Criteo`数据集，参数为：

   - `FNN` 和 `PNN` 的参数：`dropout = 0.5`、`Adam` 优化器、网络结构`400-400-400`、激活函数`IPNN` 模型为`tanh` 其它为 `relu` 。
   - `DeepFM` 参数：与 `FNN/PNN` 相同。
   - `LR/FM` 参数：优化算法`FTRL & Adam` ，同时 `FM` 的 `embedding` 向量维度为 10。

4. 模型训练效率：评估指标为 `模型训练时间/LR模型的训练时间` 。结论：

   - `FNN` 的预训练步骤拉低了它的训练效率。
   - 由于内积的低效预算，`IPNN` 和 `PNN*` 的训练效率很低。其中 `PNN*`是综合了 内积和外积的 `PNN` 模型。
   - `DeepFM` 在所有模型中，训练效率几乎是最高的。

  <p align="center">
     <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/ycJfSnD0YXJG.png?imageslim">
  </p>
  

5. 不同模型效果比较如下。结论：

   - `LR` 性能比其它模型都差，所以学习特征交互可以提高模型的预测能力。
   - `DeepFM` 优于仅学习低阶特征交互的 `FM` 或者仅学习高阶特征交互的 `FNN,IPNN,OPNN,PNN*` 模型，因此同时学习高阶特征交互和低阶特征交互可以提升模型的预测能力。
   - `DeepFM` 优于 `Wide&Deep` 及其变种，因此低阶特征交互和高阶特征交互共享 `embedding` 层可以提升模型的预测能力。

  <p align="center">
     <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/uKXpWEVVO32P.png?imageslim">
  </p>
  

6. 基于 `Company*` 数据集，研究`DeepFM` 的超参数影响：

   - 激活函数：几乎所有的深度学习模型中，`relu` 比 `tanh` 效果更好。但是 `IPNN` 是例外，可能原始是 `relu`导致很强的稀疏性。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/NQ3A0oCK5ALC.png?imageslim">
     </p>
     

   - `dropout`：当设置正确的 `dropout`比例（从 `0.6~0.9` ）时，模型可以达到最佳性能。这表明向模型添加一定的随机性可以增强模型的鲁棒性。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/DIGtzi3bsNB5.png?imageslim">
     </p>
     

   - `deep` 组件的隐向量维度：当其它条件不变时，单纯增加隐向量维度会增加模型复杂度，提高模型容量。这不一定是有利的，因为可能导致严重过拟合。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/OTv9YqEmO9vQ.png?imageslim">
     </p>
     

   - `deep` 组件的深度：增加网络深度也可以增加模型复杂度、提高模型容量，同样也可能导致严重过拟合。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/YM1gewLpRDna.png?imageslim">
     </p>
     

   - 网络性质：在相同网络深度、总神经元数量的条件下，网络可以呈现四种形状：每层神经元数量相等（恒定）、底层神经元数量多高层神经元数量少（递减）、底层神经元数量少高层神经元数量多（递增）、两头神经元数量少中间神经元数量多（菱形）。

     实验结果表明：恒定网络形状表现最好。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/rBXiLGn7ycIB.png?imageslim">
     </p>
     

## 八、NFM

1. `FM` 是一个有效捕捉二阶特征交互的方案，但是它仅能以线性的方式交互，无法学得真实世界数据的非线性关系，因此模型的表达能力较弱。

   而 `DNN` （如 `Wide&Deep、DeepCrossing`）虽然可以学习非线性特征交互，但是深度网络难以训练。

   论文 `《NeuralFactorizationMachinesforSparsePredictiveAnalytics》` 提出了 `Neural Factorization Machine:NFM`模型，该模型结合了`FM` 建模二阶特征交互中的线性，以及 `DNN` 建模高阶特征交互中的非线性。

   - 和 `FM` 相比，`FM` 可以作为 `NFM` 的一个特例，因此 `NFM` 表达能力比 `FM`更强。
   - 和 `Wide&Deep、DeepCrossing` 相比，`NFM` 网络结构更浅，预测能力更强，更容易训练和调整。

2. 人工特征工程、`FM`、`DNN` 的缺点：

   - 人工特征工程的缺点：

     - 成本很高，因为它需要大量的工程设计、大量的领域知识从而设计有效的特征。
     - 即使针对某个问题开发出有效特征，该方案无法推广到新的问题或领域。

   - `FM` 的缺点：模型使用一阶特征、二阶交叉特征的线性组合，模型简单，表达能力较弱。

   - `DNN` 的缺点：在信息检索和数据挖掘任务中，大多数数据都是天然稀疏的。尽管 `DNN` 具备从 `dense` 数据中学习 `pattern` 的强大能力，但是目前还不清楚如何配置网络从而使得模型能够从 `sparse` 数据中有效的学习 `pattern` 。

     另外，`DNN` 还存在优化困难。大多数 `DNN` 首先学习 `embedding` 向量，然后将 `embedding` 向量拼接起来送入后续的 `deep` 层来学习特征之间的交互。仅仅拼接 `embedding`向量并没有包含多少特征交互信息，必须通过接下来的 `deep` 层来学习特征交互。

     但是`deep` 层太深又容易导致优化问题，如：梯度消失、梯度爆炸、过拟合、网络退化 `degradation` 等问题。

     为说明该问题，我们绘制了 `Wide&Deep` 以及 `DeepCrossing` 在每个 `epoch` 的 `train error` 和 `test error` 。其中 `Wide&Deep` 采用3层双塔结构，`DeepCrossing` 采用 10层残差网络。

     - 从左图可知：从头训练这两个模型比`FM` 模型还要差（测试误差）。

       `Wide&Deep` 测试误差相对较高，可能是因为模型退化问题；`DeepCrossing` 的训练误差最低但是测试误差最高，说明发生了严重的过拟合。

     - 从右图可知：当采用 `FM` 初始化模型的 `embedding` 向量之后（可以视为预训练阶段），两个模型的性能头提升（大约 `11%` ）。

       对于 `Wide&Deep`，模型退化问题得到解决，因为现在它的训练误差要低得多，而且测试误差也比 `FM` 要好。

       对于 `DeepCrossing`，模型仍然遭受严重过拟合，且测试误差比 `FM` 要高。

     以上问题表明：`DNN` 模型的优化困难。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/8G5tHGoznJ5P.png?imageslim">
     </p>
     

3. `NFM` 模型引入了 `Bi-Interaction pooling` 二阶交叉池化层，然后再二阶交叉池化层上方堆叠非线性全连接层。

   - 与 `FM` 方法相比，`NFM` 可以视为深层的 `FM`，它在 `FM` 的基础上提升了对高阶特征交叉和非线性特征交叉建模的能力，提升了模型的能力。
   - 与传统的 `DNN` 方法仅将底层 `embedding` 向量拼接再一起或者取均值相比，二阶交叉池化层对交叉信息进行编码，为后续的全连接层提供更有意义的信息。

4. `NFM` 模型首次在神经网络模型中引入 `Bi-Interaction pooling` 操作，并为 `FM` 提供了基于神经网络的解释。

### 8.1 模型

1. 给定经过`one-hot` 编码之后的输入向量$\mathbf{\vec x}\in \mathbb R^n$，其中特征$x_i=0$表示第$i$个特征不存在。则 `NFM` 的预测结果为：

  $\hat y_{NFM}(\mathbf{\vec x}) = w_0 + \mathbf{\vec w} \cdot \mathbf{\vec x} + f(\mathbf{\vec x})$

   - 类似 `FM`，`NFM` 的第一项为全局偏置，第二项为一阶特征。
   - 与 `FM` 不同，`NFM` 的第三项$f(\mathbf{\vec x})$对交叉特征进行建模，它是一个多层前馈神经网络，包含 `embedding`层、`Bi-Interaction` 层、`Hidden` 层、输出层。如下图所示（仅仅包含$f(\mathbf{\vec x})$）。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/SiMxpD8EiyDF.png?imageslim">
   </p>
   

2. `embedding` 层将每个`feature` 映射到一个 `dense vector representation`，即特征$i$映射到向量$\mathbf{\vec v}_i \in \mathbb R^k$。

   一旦得到 `embedding` 向量，则输入$\mathbf{\vec x}=(x_1,x_2,\cdots,x_n)^T$就可以表示为：

  $\mathcal V_{\mathbf{\vec x}} = \{x_1\mathbf{\vec v}_1,x_2\mathbf{\vec v}_2,\cdots,x_n\mathbf{\vec v}_n\}$

   由于输入$\mathbf{\vec x}$的稀疏性，$\mathcal V_{\mathbf{\vec x}}$只需要保存非零的特征。

   注意：这里使用输入特征$x_i$来调整 `embedding` 向量，而不是简单的从`embedding table` 中查找。这样做的好处是可以统一处理实值特征 `real valued feature` 。

3. `Bi-Interaction` 层对输入的$\mathcal V_{\mathbf{\vec x}}$执行池化操作，将一组 `embedding` 向量（即一个矩阵）转换为一个向量，该操作称作`Bi-Interaction pooling` 操作：

  $f_{BI}(\mathcal V_{\mathbf{\vec x}}) = \sum_{i=1}^n\sum_{j=i+1}^n x_i\mathbf{\vec v}_i\odot x_j\mathbf{\vec v}_j$

   其中：$\odot$是逐元素乘法；$f_{BI}(\mathcal V_{\mathbf{\vec x}})$是一个$k$维向量，它在 `embedding` 空间编码了二阶交叉特征。

   `Bi-Interaction` 层有两个优秀的特性：

   - `Bi-Interaction` 层并没有引入任何额外的模型参数

   - `Bi-Interaction` 层可以在线性时间内有效计算：

    $f_{BI}(\mathcal V_{\mathbf{\vec x}}) = \frac 12 \left[\left(\sum_{i=1}^n x_i\mathbf{\vec v}_i\right)^2 - \sum_{i=1}^n\left(x_i\mathbf{\vec v}_i\right)^2\right]$

     其中$\mathbf{\vec v}^2$表示$\mathbf{\vec v}\odot \mathbf{\vec v}$。

     由于输入$\mathbf{\vec x}$的稀疏性，我们可以在$O(kn_x)$的时间内计算 `Bi-Interaction pooling` ，其中$n_x$表示输入中非零项的数量。

   这些性质意味着 `Bi-Interaction` 层对二阶交叉特征建模的代价很低。

   另外，`Bi-Interaction` 层也支持求导运算：

  $\frac{d}{d \mathbf{\vec v}_i} f_{BI}(\mathcal V_{\mathbf{\vec x}}) = \left(\sum_{j=1}^nx_j\mathbf{\vec v}_j\right)x_i - x_i^2\mathbf{\vec v}_i = \sum_{j=1,j\ne i}^n x_ix_j\mathbf{\vec v}_j$

4. `Hidden` 层是一组全连接层，用于捕获高阶特征交叉：

   其中$L$为隐层数量，$\sigma(\cdot)$为激活函数。

5. 输出层用于输出预测得分：$f(\mathbf{\vec x}) = \mathbf{\vec h}_L\cdot \mathbf{\vec w}_f$。

6. 最终模型为：

  $\hat y_{NFM}(\mathbf{\vec x}) = w_0 + \mathbf{\vec w} \cdot \mathbf{\vec x} + \mathbf{\vec w}_f\cdot \sigma(\mathbf W_L(\cdots\sigma(\mathbf W_1 f_{BI}(\mathcal V_{\mathbf{\vec x}})+ \mathbf{\vec b}_1)\cdots)\mathbf{\vec b}_L)$

   模型的参数为：$\mathbf\Theta=\{w_0, \mathbf{\vec w},\{\mathbf{\vec v}_i\},\mathbf{\vec w}_f,\{\mathbf W_l,\mathbf{\vec b}_l\}\}$。和 `FM` 相比，`NFM` 多了$\{\mathbf W_l,\mathbf{\vec b}_l\}$的部分。

   - 和浅层、线性的 `FM` 模型相比，可以认为 `FM` 是 `NFM` 的特例。当 `NFM`的隐层数量为0、固定$\mathbf{\vec w}_f = (1,1,1,\cdots,1)^T$时，`NFM` 的输出为：

     这是首次在神经网络框架下解释 `FM`模型，这位改进 `FM` 提供了更多思路。尤其是我们可以在 `FM` 上使用各种神经网络技术来提高其学习和泛化能力。如：在 `Bi-Interaction` 层之上使用 `dropout`。与传统的 `L2` 正则化相比，该方法更有效。

   - 和现有的`DNN` 模型相比，`NFM` 结构非常相似，除了 `NFM` 具有 `Bi-Interaction` 层。 如果将 `Bi-Interaction` 层替换为拼接层，则 `NFM` 退化为 `Wide&Deep` 。

     拼接操作的一个明显不足是：它无法处理特征交互关系。因此 `Wide&Deep` 仅依赖于后续的更高层来捕获特征交互。而在实践中发现，深层网络难以优化和训练。

     `Bi-Interaction` 层在底层就能捕获到二阶特征交互，这使得后续层学习更高阶特征交互时更容易。

7. `Bi-Interaction` 层的时间复杂度为$O(kn_x)$，因此主要的时间消耗在隐层计算。

   对于第$l$层隐层，计算复杂度为$O(d_{l-1}\times d_l)$，其中$d_l$表示该层隐向量维度。因此总的时间复杂度为：

  $O(k\times n_x + \sum_{l=1}^L d_{l-1}\times d_l),\quad d_0 = k$

   这与 `Wide&Deep,DeepCrossing` 几乎相同。

8. `NFM` 可以执行不同类型的任务，包括回归、分类、排序 `ranking` 。

   - 对于回归任务，损失函数为平方误差：

    $\mathcal L_{reg} = \sum_{(\mathbf {\vec x},y)\in \mathbb D} (\hat y - y)^2$

     这里没有添加正则化项，因为论文发现：神经网络的某些正则化技术（如：`dropout` ）要比正则化项效果更好。

   - 对于分类任务，损失函数为 `hinge loss` 或者 `logloss`。

   - 对于排序任务，损失函数为`pairwise personalized ranking loss` 或者 `contrastive max-margin loss` 。

9. `NFM` 并未采用常规的$L_1,L_2$正则化，而是采用 `dropout` 和 `Batch Normalization` 。

   - 为了防止 `feature embedding` 出现 `co-adapt` 并过拟合 ，`NFM` 对`Bi-Interaction` 层的输出执行 `dropout` 。即：在 `Bi-Interaction` 层之后接入一个 `Dropout` 层，随机丢弃$\rho$比例的神经元。

     由于不包含任何隐层的 `NFM` 就是 `FM` 模型，因此 `dropout` 也可以认为是 `FM` 模型新的正则化方式。

     - 在测试阶段 `dropout` 要被移除。
     - 理论上，我们也可以在后续的隐层中，每个隐层的输出执行 `dropout` 。

   - 为防止过拟合，`NFM` 也对 `Bi-Interaction` 层和后续层的输出执行 `Batch Normalization` 。

### 8.2 实验

1. 数据集:

   - `Frappe` 数据集：给出了不同上下文时用户的 `app` 使用日志记录，一共包含 `96203` 个 `app` 。

     除了 `userID, appID` 之外，每条日志还包含8个上下文特征：天气、城市、`daytime`（如：早晨、上午、下午） 等。

     采用 `one-hot` 编码之后，特征有 5382 维；`label = 1` 表示用户使用了 `app` 。

   - `MovieLens` 数据集：`GroupLens` 发布的最新 `MovieLens` 数据集的完整版，包含 `17045` 个用户在 `23743`个 `item` 上的 `49657` 类标签。

     将 `userID,movieID,tag` 进行 `one-hot` 编码之后，特征有 `90445`维； `label = 1` 表示用户给 `movie`贴了 `tag` 。

   由于这两个数据集只包含正类（即：`label = 1`），因此需要通过采样来生成负类，负类数量和正类数量的比例为 `2:1`。

   - 对于 `Frappe` 数据集，对每条记录，随机采样每个用户在上下文中未使用的其它两个 `app` 。
   - 对于 `MovieLens` 数据集，对每个用户每个电影的每个 `tag`，随机分配给该电影其它两个该用户尚未分配的 `tag` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/MGOy9tfW6YTQ.png?imageslim">
   </p>
   

   对于每个数据集拆分为训练集（70%）、验证集（20%）、测试集（10%）。

2. 评估标准：验证集或者测试集的均方根误差 `RMSE` 。

   注意：由于论文采用回归模型来预测，因此预测结果可能大于 `+1`（正类的 `label`）护着小于 `-1`（负类的 `label`）。因此如果预测结果超过 `+1` 则截断为 `+1`、低于 `-1` 则截断为 `-1`。

3. 模型比较 `baseline`：

   - `LibFM`：`FM` 的官方实现。
   - `HOFM`：基于 `tensorflow` 实现的高阶 `FM` 。
   - `Wide&Deep`：`Wide&Deep`模型，其中`deep part` 包含三层隐层，每层维度分别为 `1024,512,256` 。
   - `DeepCross`：`DeepCross` 模型，其中包含 5层残差网络，每层维度为 `512,512,256,128,64`。

4. 超参数配置：

   - 优化目标：平方误差。

   - 学习率：所有模型的学习率通过超参数搜索得到，搜索范围`[0.005,0.01,0.02,0.05]`

   - 正则化：

     - 线性模型 `LibFM,HOFM` 使用$L_2$正则化，正则化系数搜索范围 `[1e-6,5e-6,1e-5,...,1e-1]` 。

     - 神经网络模型 `Wide&Deep,DeepCross,NFM`执行 `dropout`，遗忘比例搜索范围 `[0,0.1,0.2,...,0.9]` 。

       实验发现 `dropout` 在 `Wide&Deep,NFM` 中工作良好，但是在 `DeepCross` 工作较差。

     - 所有模型都采用早停策略 `early stopping` 。

   - 优化策略：`libFM` 使用常规`SGD` 优化，其它模型使用 `mini-batch Adagrad` 优化。

     其中 `Frappe` 数据集的 `Batch Size = 128`，`MovieLens` 数据集的 `Batch Size = 4096` 。

     `Batch Size` 的选择要综合考虑训练时间和收敛速度。更大的 `Batch Size` 使得每个 `epoch` 的训练时间更短，但是需要更多的 `epoch` 才能收敛。

   - `embedding size` 默认为 4 。

#### 8.2.1 Bi-Interaction 层

1. 首先研究 `dropout` 对于 `FM` 的作用。当 `NFM` 隐层数量为 0 时就退化为 `FM` 模型，记作`NFM-0` 。

   下图给出了不同数据集上，`FM` 执行$L_2$正则化和 `dropout` 正则化的效果。横坐标为不同的遗忘比例 或者 正则化系数。另外 `LR` 的验证集误差作为 `baseline` 。

   结论：

   - `LR` 效果最差，说明特征交叉非常重要。
   -$L_2$和 `dropout` 都能缓解过拟合，但是 `dropout` 效果更好。这意味着 `dropout` 也能成为 `linear latent-factor` 模型缓解过拟合的方法。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/xQYm89KCeB7A.png?imageslim">
   </p>
   

   从上图观察到：每个数据集 `dropout ratio = 0.3` 时效果最好，因此遗忘比率设定为 0.3 。

   为进一步观察 `dropout` 效果，论文对 `NFM-0` 绘制每个`epoch` 的训练误差和验证误差。相比较于没有`dropout`（`dropout = 0`），采用 `dropout` 后（`dropout=0.3`）的训练误差较高但是验证误差更低。

   这进一步说明了 `dropout` 能够针对 `FM` 模型有效缓解过拟合。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/3PzcNJx8cerf.png?imageslim">
   </p>
   

2. 然后研究 `BatchNormalization` 对于 `FM`的作用。下图给出 `NFM-0` 的 `BI Interaction` 层的输出执行 `BN` 和不执行 `BN` 时，每个 `epoch` 的训练误差和验证误差。其中 `dropout = 0.3`，学习率为 `0.02` 。

   采用了 `BN` 之后：

   - 训练误差更快的收敛，且收敛到更低的水平

   - 验证误差稍微改进，但是改进幅度没有训练误差改进幅度大

   - 学习过程更加不稳定，如：蓝色线的波动更大。这是由于 `dropout` 和 `BN` 一起使用的原因。

     由于 `dropout` 随机丢弃神经元，这导致 `BN` 的输入不断的随机被改变。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/lLwYwEleMDhm.png?imageslim">
   </p>
   

#### 8.2.2 隐层

1. `NFM` 的隐层在捕获特征的高阶交互方面起着关键作用。

2. 首先评估激活函数的影响。模型采用一层隐层，隐层维度和 `embedding` 维度相同。下图给出了不同激活函数、不同 `dropout` 的效果。其中 `LibFM,NFM-0` 作为 `baseline` 。

   结论：非线性激活函数的使用至关重要，能大幅度提升模型效果。这说明使用非线性激活函数来学习高阶特征交互是非常有必要的。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/2xb3rTxNflMe.png?imageslim">
   </p>
   

3. 然后评估隐层的深度的效果。每个隐层都采用 `ReLU` 激活函数，维度为 `embedding`维度相同。

   结论：堆叠更多的隐层，模型性能不会进一步提高。只有一层隐层的 `NFM` 效果最好。

   论文还探索了其它隐层结构：如塔形（隐层维度递减）、残差单元，结果模型的表现仍然没有提高。猜测的原因是： `BI Interaction` 层已经编码了丰富的二阶特征交互。在此基础上，简单的非线性函数足以捕获更高阶的特征交互。

   为证明这一点，论文将 `BI Interaction`层替换为拼接层，这导致网络结构和 `Wide&Deep` 相同）。此时通过简单的增加隐层数量来逐步提高性能，但是最好的性能仍然不如 `NFM-1` 。这说明底层使用更有价值的信息能够有效减轻高层的学习负担，结果使得不一定需要较深的网络就能取得很好的效果。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/G65Bj5AWGz2c.png?imageslim">
   </p>
   

#### 8.2.3 预训练

1. 参数初始化能够极大的影响`DNN` 的收敛和性能，因为基于梯度的优化算法只能为 `DNN` 提供局部最优解。如：使用 `FM` 学习的 `embedding` 向量来初始化模型能够显著提升 `Wide&Deep` 和 `DeepCross` 模型的收敛速度和模型性能。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/a7mpqskMATVl.png?imageslim">
   </p>
   

   下图给出预训练的 `NFM-1` 和非预训练的 `NFM-1` 在每个 `epoch` 的训练误差和验证误差。

   - 预训练的 `NFM-1` 的模型训练很快收敛，仅仅 5个`epoch` 就能达到随机初始化`NFM-1` 的40个 `epoch` （启用 `BN` ）的效果
   - 预训练并不能改善 `NFM` 的验证误差，且随机初始化效果更好。

   这证明`NFM` 的鲁棒性：对于参数初始化不敏感。相比较于预训练对 `Wide&Deep/DeepCross` 产生的巨大影响，`NFM` 更容易训练和优化。这主要归功于 `BI Interaction pooling` 操作的有效性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/fnDQe31h4eMH.png?imageslim">
   </p>
   

#### 8.2.4 模型比较

1. 下图给出不同的 `embedding size` ，不同模型的表现（测试误差）。其中：

   - `Wide&Deep,Deepcross` 使用 `FM` 预训练。
   - `NFM` 使用 `ReLU` 激活函数，`Bi Interaction` 层的 `dropout = 0.5`，隐层的 `dropout` 通过超参数搜索。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/2cRzIBs9GYPh.png?imageslim">
   </p>
   

2. 下表给出了所有模型在两个数据集上的效果（测试误差）以及模型大小。

   结论：

   - 在这两个数据集上，`NFM` 始终以最少的参数（除 `FM` 之外）获得最佳性能，这说明了 `NFM` 针对稀疏数据建模高阶特征交互和非线性特征交互的有效性和合理性。

   - `HOFM` 相对于 `FM` 略有改善，这说明 `FM` 仅建模二阶特征交互的局限性，以及建模高阶特征交互的有效性。

   - `HOFM` 和 `NFM` 之间的巨大差异，反映了以非线性方式构建高阶特征交互的价值。因为 `HOFM` 是通过线性建模高阶特征交互，但是效果比 `NFM` 差距较大，参数反而比 `NFM` 翻倍。

   - `DeepCross` 是这些模型中深度最深的（包含10层），但是相对较差的性能表明：更深的网络并不一定总是更好的。

     论文认为这是由于优化困难和过拟合。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/U0hwPBOthW4i.png?imageslim">
   </p>
   

## 九、AFM

1. `FM` 将所有二阶交叉特征都认为是同等重要的，事实上并非如此。有一些二阶交叉特征是没有价值的，可能会引入噪声并且降低模型性能。

   论文 `《Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks》` 提出了 `Attentional Factorization Machine:AFM` 模型，该模型通过 `attention` 机制来自动学习每个二阶交叉特征的重要性。

   与 `Wide&Deep` 以及 `DeepCross` 等模型相比，`AFM` 结构简单、参数更少、效果更好。同时 `AFM` 具有很好的可解释性：通过注意力权重可以知道哪些交叉特征对于预测贡献较大。

### 9.1 模型

1. `AFM` 模型和 `NFM` 模型一脉相承，其底层架构基本一致。

   给定经过`one-hot` 编码之后的输入向量$\mathbf{\vec x}\in \mathbb R^n$，其中特征$x_i=0$表示第$i$个特征不存在。则 `AFM` 的预测结果为：

  $\hat y_{NFM}(\mathbf{\vec x}) = w_0 + \mathbf{\vec w} \cdot \mathbf{\vec x} + f(\mathbf{\vec x})$

   - 类似 `FM`，`AFM` 的第一项为全局偏置，第二项为一阶特征。
   - 与 `FM` 不同，`AFM` 的第三项$f(\mathbf{\vec x})$对交叉特征进行建模，它是一个多层前馈神经网络，包含 `embedding`层、`Pair-wise Interaction` 成对交叉层、`Attention-based Pooling` 层、输出层。如下图所示（仅仅包含$f(\mathbf{\vec x})$）。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/1b6VHwgr59vP.png?imageslim">
   </p>
   
2. `embedding` 层将每个`feature` 映射到一个 `dense vector representation`，即特征$i$映射到向量$\mathbf{\vec v}_i \in \mathbb R^k$。

   一旦得到 `embedding` 向量，则输入$\mathbf{\vec x}=(x_1,x_2,\cdots,x_n)^T$就可以表示为：

  $\mathcal V_{\mathbf{\vec x}} = \{x_1\mathbf{\vec v}_1,x_2\mathbf{\vec v}_2,\cdots,x_n\mathbf{\vec v}_n\}$

   由于输入$\mathbf{\vec x}$的稀疏性，$\mathcal V_{\mathbf{\vec x}}$只需要保存非零的特征。

   注意：这里使用输入特征$x_i$来调整 `embedding` 向量，而不是简单的从`embedding table` 中查找。这样做的好处是可以统一处理实值特征 `real valued feature` 。

3. 和 `NFM` 的`Bi-Interaction` 层不同，`AFM`的 `Pair-wise Interaction` 层将$m$个向量扩充为$m\times (m-1)/2$个交叉向量，每个交叉向量是两个 `embedding` 向量的逐元素积。$m\le n$为$\mathbf{\vec x}$中非零元素数量。

   假设输入$\mathbf{\vec x}$的非零元素下标为$\mathcal X$，对应的 `embedding` 为$\mathcal E = \{x_i\mathbf{\vec v}_i\}_{i\in \mathcal X}$，则 `Pair-wise Interaction` 层的输出为：

  $f_{PI}(\mathcal E) = \{(\mathbf{\vec v}_i\odot \mathbf{\vec v}_j)x_ix_j\}_{(i,j)\in \mathcal R_x}$

   其中$\odot$表示逐元素乘积，$\mathcal R_x = \{(i,j)\}_{i\in \mathcal X,j \in \mathcal X,j>i}$表示成对下标集合。

   与之相比，`NFM` 的`Bi-Interaction` 层输出为：

  $f_{BI}(\mathcal V_{\mathbf{\vec x}}) = \sum_{(i,j)\in \mathcal R_x} (\mathbf{\vec v}_i\odot \mathbf{\vec v}_j)x_ix_j$

4. 一旦得到`Pair-wise Interaction` 层的$m\times (m-1)/2$个交叉向量，则可以通过一个 `sum pooling` 层来得到一个池化向量：

  $\mathbf{\vec v}_{pooling} = \sum_{(i,j)\in \mathcal R_x} (\mathbf{\vec v}_i\odot \mathbf{\vec v}_j)x_ix_j$

   它刚好就是 `Bi Interaction` 层的输出 。因此 `Pair-wise Interaction层` + `sum pooling 层` = `Bi Interaction 层`。

5. `Attention-based Pooling` 层：与 `Bi Interaction pooling` 操作不同，`Attention-based Pooling` 操作采用了 `attention` 机制：

  $f_{Att}(f_{PI}(\mathcal E)) = \sum_{(i,j)\in \mathcal R_x}x_ix_j\times a_{i,j}\times (\mathbf{\vec v}_i\odot \mathbf{\vec v}_j)$

   其中$a_{i,j}$是交叉特征$(i,j)$的`attention score` ，可以理解为交叉特征$(i,j)$的权重。

   学习$a_{i,j}$的一个方法是直接作为模型参数来学习，但这种方法有个严重的缺点：对于从未在训练集中出现过的交叉特征，其 `attentioin score` 无法训练。

   为解决该问题，论文使用一个 `attention network` 来训练$a_{i,j}$。`attention network` 的输入为$m\times m(-1)/2$个交叉特征向量，输出为$a_{i,j}$。

   其中$\mathbf W\in \mathbb R^{t\times k},\mathbf{\vec b}\in \mathbb R^t,\mathbf{\vec h}\in \mathbb R^t$都是模型参数，$t$为 `attention network` 的隐向量维度，称作 `attention factor` 。

6. 输出层用于输出预测得分：

  $f(\mathbf{\vec x}) = \mathbf{\vec w}_f \cdot \left(\sum_{(i,j)\in \mathcal R_x}x_ix_j\times a_{i,j}\times (\mathbf{\vec v}_i\odot \mathbf{\vec v}_j)\right)$

7. 最终模型为：

  $\hat y_{AFM}(\mathbf{\vec x}) = w_0 + \mathbf{\vec w} \cdot \mathbf{\vec x} + \mathbf{\vec w}_f\cdot\left(\sum_{(i,j)\in \mathcal R_x}x_ix_j\times a_{i,j}\times (\mathbf{\vec v}_i\odot \mathbf{\vec v}_j)\right)$

   模型的参数为：$\mathbf\Theta=\{w_0, \mathbf{\vec w},\{\mathbf{\vec v}_i\},\mathbf{\vec w}_f,\mathbf W,\mathbf{\vec h},\mathbf{\vec b}\}$。

   - 当移除 `attention network` 时，`AFM` 模型退化为 `NFM-0` 模型，即标准的 `FM` 模型。
   - 和 `NFM` 相比，`AFM` 模型缺少隐层来提取高阶特征交互。

8. `NFM` 可以执行不同类型的任务，包括回归、分类、排序 `ranking` 。

   - 对于回归任务，损失函数为平方误差：

    $\mathcal L_{reg} = \sum_{(\mathbf {\vec x},y)\in \mathbb D} (\hat y - y)^2$

   - 对于分类任务，损失函数为 `hinge loss` 或者 `logloss`。

   - 对于排序任务，损失函数为`pairwise personalized ranking loss` 或者 `contrastive max-margin loss` 。

9. 为缓解过拟合，`AFM` 采用$L_2$正则化和 `dropout` 正则化。

   - `Pair-wise Interaction` 层的输出执行 `dropout` 。

   - `attention network` 层的权重执行$L_2$正则化：

    $\mathcal L = \sum_{\mathbf{\vec x} \in \mathbb D} (\hat y - y )^2 + \lambda ||\mathbf W||^2$

   论文并未对 `attention network` 层的输出执行 `dropout`，因为实验发现：在 `Pair-wise Interaction` 层和`attention network` 层都采用 `dropout`会导致模型性能下降。

### 9.2 实验

1. 数据集:

   - `Frappe` 数据集：给出了不同上下文时用户的 `app` 使用日志记录，一共包含 `96203` 个 `app` 。

     除了 `userID, appID` 之外，每条日志还包含8个上下文特征：天气、城市、`daytime`（如：早晨、上午、下午） 等。

     采用 `one-hot` 编码之后，特征有 5382 维；`label = 1` 表示用户使用了 `app` 。

   - `MovieLens` 数据集：`GroupLens` 发布的最新 `MovieLens` 数据集的完整版，包含 `17045` 个用户在 `23743`个 `item` 上的 `49657` 类标签。

     将 `userID,movieID,tag` 进行 `one-hot` 编码之后，特征有 `90445`维； `label = 1` 表示用户给 `movie`贴了 `tag` 。

   由于这两个数据集只包含正类（即：`label = 1`），因此需要通过采样来生成负类，负类数量和正类数量的比例为 `2:1`。

   - 对于 `Frappe` 数据集，对每条记录，随机采样每个用户在上下文中未使用的其它两个 `app` 。
   - 对于 `MovieLens` 数据集，对每个用户每个电影的每个 `tag`，随机分配给该电影其它两个该用户尚未分配的 `tag` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/76t9d8b0l6Yf.png?imageslim">
   </p>
   

   对于每个数据集拆分为训练集（70%）、验证集（20%）、测试集（10%）。

2. 评估标准：验证集或者测试集的均方根误差 `RMSE` 。

   注意：由于论文采用回归模型来预测，因此预测结果可能大于 `+1`（正类的 `label`）护着小于 `-1`（负类的 `label`）。因此如果预测结果超过 `+1` 则截断为 `+1`、低于 `-1` 则截断为 `-1`。

3. 模型比较 `baseline`：

   - `LibFM`：`FM` 的官方实现（基于 C++），采用 `SGD` 学习器。
   - `HOFM`：基于 `tensorflow` 实现的高阶 `FM` ，这里设置为3阶。因为 `MovieLens` 数据集只有三个特征：`User,Item,Tag` 。
   - `Wide&Deep`：`Wide&Deep`模型，其中`deep part` 包含三层隐层，每层维度分别为 `1024,512,256` ；`wide`部分和 `FM` 的线性部分相同。
   - `DeepCross`：`DeepCross` 模型，其中包含 5层残差网络，每层维度为 `512,512,256,128,64`。

4. 超参数配置：

   - 优化目标：平方误差。

   - //学习率：所有模型的学习率通过超参数搜索得到，搜索范围`[0.005,0.01,0.02,0.05]`

   - 正则化：

     - 线性模型 `LibFM,HOFM` 使用$L_2$正则化，正则化系数搜索范围 `[1e-6,5e-6,1e-5,...,1e-1]` 。

     - 神经网络模型 `Wide&Deep,DeepCross,NFM`执行 `dropout`，遗忘比例搜索范围 `[0,0.1,0.2,...,0.9]` 。

       实验发现 `dropout` 在 `Wide&Deep,NFM` 中工作良好，但是在 `DeepCross` 工作较差。

     - 所有模型都采用早停策略 `early stopping` 。

   - 优化策略：`libFM` 使用常规`SGD` 优化，其它模型使用 `mini-batch Adagrad` 优化。

     其中 `Frappe` 数据集的 `Batch Size = 128`，`MovieLens` 数据集的 `Batch Size = 4096` 。

     `Batch Size` 的选择要综合考虑训练时间和收敛速度。更大的 `Batch Size` 使得每个 `epoch` 的训练时间更短，但是需要更多的 `epoch` 才能收敛。

   - `embedding size` 默认为 256。

   - `attention factor` 默认为 256。

#### 9.2.1 超参数探索

1. 首先考察 `dropout` 超参数。设置$\lambda =0$从而使得 `attention network` 没有正则化。

   另外移除 `attention network` 使得网络退化为 `FM` 模型来做比较。`libFM` 作为 `baseline` 。

   结论：

   - 选择合适的 `dropout` 值，`AFM` 和 `FM` 都能够得到显著的提升。这证明了在 `Pair-wise Interaction` 层执行 `dropout` 的有效性。
   - 我们的 `FM` 模型比 `libFM` 效果更好，有两个原因：
     - `libFM` 采用`SGD` 来优化，学习率是固定的。我们的 `FM` 是通过 `Adagrade` 来优化，采用自适应学习率因此优化效果更好。
     - `LibFM` 采用$L_2$正则化来缓解过拟合，而我们的 `FM` 采用 `dropout`。后者缓解过拟合的效果更好。
   - `AFM` 的效果比 `FM` 和 `libFM` 好得多，即使是当 `dropout = 0` 时（即：未采取任何正则化）`AFM` 的效果仍然很好。这充分证明了 `attention network` 的效果。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/PxNjAatzwpU9.png?imageslim">
   </p>
   

2. 然后考察$L_2$正则化系数。当选择合适的 `dropout` 参数之后，我们考察对 `attention network` 执行$L_2$正则化的效果。

   结论：当$\lambda \gt 0$时，模型效果得到改善。这证明了 `attention network` 正则化的效果，能进一步改善模型的泛化能力。同时说明了：仅仅 `Pair-wise Interaction` 层执行 `dropout` 对于缓解过拟合是不够的。

   注意：`FM` 和 `LibFM` 均没有 `attention network` ，因此它们在图上都是直线。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/BoyGX7eJh0G8.png?imageslim">
   </p>
   

#### 9.2.2 attention network

1. 首先考察 `attention factor` 的影响。

   下图给出 `attention network` 不同的隐向量维度(`attention factor` )下，模型的性能。其中每种隐向量维度都各自独立的选择了最合适的$\lambda$。

   结论：`AFM` 对于 `attention factor` 的变化比较稳定。极端情况下，即使 `attention factor =1`，此时$\mathbf W$退化为一个向量、 `attention network` 退化为一个广义线性模型，`AFM` 的效果仍然很好。

   这证明了 `AFM` 设计的合理性：通过 `attention network` 来评估特征的`representation vector` 之间交互的重要性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/OUVhnu6Hbl78.png?imageslim">
   </p>
   

2. 然后评估 `AFM` 和 `FM` 在每个 `epoch` 的训练误差和测试误差。 可以看到：`AFM` 收敛速度比 `FM` 收敛速度更快。

   - 对于`Frappe`，`AFM` 训练误差、测试误差都比 `FM` 下降很多，说明 `AFM`对于已知数据、未知数据都拟合较好。
   - 对于 `MovieLens`，尽管 `AFM` 训练误差更大，但是它的测试误差更小，说明了 `AFM` 泛化能力更强。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/1LrPHJa4DPTV.png?imageslim">
   </p>
   

3. 最后可以通过 `attention score` 来解释每个交叉特征的重要性。

   - 首先固定所有的$a_{i,j} = \frac{1}{\mathcal R_x}$（即：每个交叉特征都是同等重要的），训练模型 `A` 。该模型等价于 `FM` 模型，在下表中记作 `FM`。

   - 然后在模型 `A` 的基础上，固定所有的 `embedding` 向量，仅训练 `attention network`，得到模型 `B`。该模型在下表中记作 `FM + A`（`A` 指的是 `Attention` ） 。

     在这一步发现模型 `B` 比模型 `A` 提升了大约 3%，这也证明了 `attention network` 的有效性。

   - 最后用模型 `B` 预测从测试集中随机挑选的 3 个正样本，观察 `attention score` 。

   下表中，表格中每一项表示的是 `attention sore x interaction score` 。其中 `attention score` 总和为1， `interaction score` 由$x_i \mathbf{\vec v}_i$来决定。

   从表中可知：

   - `FM` 模型中，不同交叉特征的重要性都是相同的。
   - `AFM` 模型中，`Iter-Tag` 交叉特征更为重要。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/RLQkWCVE6o79.png?imageslim">
   </p>
   

#### 9.2.3 模型比较

1. 下表给出了所有模型在两个数据集上的效果（测试误差）以及模型大小。其中 `embedding = 256`，`M` 表示百万。

   结论：

   - `AFM` 参数最少，同时效果最好。这证明了 `AFM` 模型的有效性，尽管它是一个浅层模型，但是其性能优于深层模型。

   - `HOFM` 相对于 `FM` 略有改善，这说明 `FM` 仅建模二阶特征交互的局限性，以及建模高阶特征交互的有效性。

   - `HOFM` 和 `NFM` 之间的巨大差异，反映了以非线性方式构建高阶特征交互的价值。因为 `HOFM` 是通过线性建模高阶特征交互，但是效果比 `NFM` 差距较大，参数反而比 `NFM` 翻倍。

   - `DeepCross` 模型效果最差。论文发现 `dropout` 在 `DeepCross` 上效果很差，原因可能是 `Batch Normalization` 的影响。

     由于 `DeepCross` 是最深的模型，但是相对较差的性能表明：更深的网络并不一定总是更好的。论文认为这是由于优化困难和过拟合。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/u3uRJenRSPsy.png?imageslim">
   </p>
   

## 十、xDeepFM

1. 在 `CTR` 预估任务中，常用的人工特征工程、`FM` 模型、`DNN` 模型都存在不足。

   - 人工特征工程缺点：

     - 高质量的交叉特征往往代价太高。数据科学家需要花费大量时间探索数据中的潜在模式，然后才能了解问题领域并提取有意义的交叉特征。
     - `web-scale` 规模的系统中，原始特征数量太大，无法人工分析所有的特征交叉组合。
     - 人工交叉特征无法推广到训练数据中从未出现的特征组合。

   - `FM` 模型缺点：

     - 只能对低价、线性的特征交互建模。
     - 对所有的特征交互建模，无论该是有用的还是无用的组合。事实上，无用的特征交互可能引入噪声并降低模型性能。

   - `DNN` 模型缺点：

     - `DNN` 隐式的在 `bit-wise` 级别对特征交互建模，这意味着：即使是同一个特征的 `embedding` 向量内部的元素之间也会相互影响。

       > 与之相反，`FM` 在 `vector-level` 级别对特征交互建模。

     - 另外，`FNN/PNN` 更多的关注高阶特征交互而不关注低阶特征交互；`Wide&Deep/DeepFM` 虽然同时对低阶、高阶特征交互建模，但是它们是隐式建模而不是显式建模。

   针对以上不足，论文 `《xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems》` 提出了 `xDeepFM` 模型，该模型引入了一种新的网络 `Compressed Interaction Network:CIN`，该网络显式的在 `vector-wise` 级别建模特征交互。

   该模型被称作 `eXtreme Deep Factorization Machine:xDeepFM` ，其优点有：

   - 通过 `CIN` 网络显式的在 `vector-wise` 级别学习高阶特征交互。

     其中 `CIN` 网络每增加一层，能学到的特征交叉的阶数就增加一阶。

   - 通过 `DNN` 网络，`xDeepFM` 也能够隐式的学习任意低阶和高阶的特征交互。

### 10.1 模型

1. 如果一个 `field` 中只有一个取值（如：用户性别），则该 `field` 的 `embedding` 就是对应 `one-hot` 中 `1` 对应的 `embedding`的取值。

   如果一个 `field` 中有多个取值（如：用户最近一个月看过的电影），则该 `field` 的 `embedding` 就是对应 `one-hot` 中所有 `1`对应的 `embedding` 的累加。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/EBvtg6wsU3r9.png?imageslim">
   </p>
   

2. `DCN` 的 `cross network` 虽然对高阶特征交互显式建模，但是它学到的高阶特征交互非常特殊，仅仅是某种非常特殊的形式。

   假设一个$k$层的 `cross network`，我们忽略偏置项，第$i+1$层定义为：

  $\mathbf{\vec x}_{i+1} = \mathbf{\vec x}_0\mathbf{\vec x}_i^T\mathbf{\vec w}_{i+1} + \mathbf{\vec x}_i$

   则可以用数学归纳法证明：`cross network` 的输出$\mathbf{\vec x}_k$是$\mathbf{\vec x}_0$的一个标量乘积。

   即：

  $\mathbf{\vec x}_{k} = \alpha_{k} \times \mathbf{\vec x}_{0}$

   其中：

   注意：标量乘积仅仅意味着向量的方向相同，但是并不意味着线性关系。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/99wLB2UYrSmL.png?imageslim">
   </p>
   

3. `cross network` 能够有效的学到高阶特征交互，其计算复杂度相对于 `DNN` 来讲可以忽略不计，但是有两个严重不足：

   - 网络输出形式过于单一，仅仅是$\mathbf{\vec x}_0$的标量乘积。
   - 基于 `bit-wise` 级别学习特征交互。

   `xDeepFM` 的 `CIN` 参考了 `cross network`的思想，但是具有以下特点：

   - 和 `cross network` 相同，`CIN` 也可以显式建模高阶特征交互，且网络复杂度并没有随着交互阶数的增加而指数增长。
   - 和 `cross network` 不同，`CIN`基于 `vector-wise` 级别学习特征交互，且网络的表达能力更强输出形式更多。

#### 10.1.1 CIN

1. 假设所有的 `embedding` 向量维度为$D$，假设 `field i` 的 `embedding` 为$\mathbf{\vec e}_i \in \mathbb R^{D }$。假设有$m$个 `field`，将所有 `embedding` 拼接成矩阵：

   矩阵的第$i$行就是`field i` 的 `embedding`：$\mathbf{\vec x}^{(0)}_i = \mathbf{\vec e}_i$。

   `CIN` 的第$k$层输出也是一个矩阵$\mathbf X^{(k)} \in \mathbb R^{H_k\times D}$，其中$H_k$为输出向量的数量，其中$H_0 = m$：

  $\mathbf{\vec x}^{(k)}_{h} = \sum_{i=1}^{H_{k-1}}\sum_{j=1}^m W_{i,j}^{(k,h)}(\mathbf{\vec x}_{i}^{(k-1)}\odot \mathbf{\vec x}^{(0)}_j),\quad 1\le h\le H_k$

   其中：

   -$\odot$为向量的逐元素积
   -$\mathbf W ^{(k,h)} \in \mathbb R^{H_{k-1}\times m}$为权重向量，它用于为 `vector-wise` 的交叉特征$(\mathbf{\vec x}_{i}^{(k-1)}\odot \mathbf{\vec x}^{(0)}_j)$赋予不同的权重。

   由于$\mathbf X^{(k)}$是通过$\mathbf X^{(k-1)}$和$\mathbf X^{(0)}$计算得到，因此 `CIN`显式的建模特征交互，并且特征交互的阶数随着`CIN` 网络的深度加深而加深。

   另外在计算过程中，所有的特征交互都是基于向量的运算，因此整个建模过程是 `vector-wise` 的。

2. `CIN` 的建模过程非常类似卷积神经网络`CNN` 。

   - 首先引入临时三维张量$\mathbf Z^{(k+1)}\in \mathbb R^{H_k\times m \times D}$，它是$\mathbf X^{(k)}$和$\mathbf X^{(0)}$的外积。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/r1NimNQVxGBL.png?imageslim">
     </p>
     

   - 然后将三维张量$\mathbf Z^{(k+1)}$视为一张图片，将$\mathbf W^{(k+1,h)}\in \mathbb R^{H_{k}\times m}$视为一个卷积核，沿着 `embedding`维度进行卷积得到 `featuremap` ，即向量$\mathbf{\vec x}^{(k+1)}_{h} \in \mathbb R^D$。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/LdmaTP4Y3olJ.png?imageslim">
     </p>
     

   - 使用$H_{k+1}$个卷积核执行卷积，得到的 `featuremap` 组成输出张量$\mathbf X^{(k+1)}\in \mathbb R^{H_{k+1}\times D}$。

   因此 `CIN` 网络将$H_k\times m$个交叉向量压缩到$H_{k+1}$个向量，这就是网络名称中的 `compressed`的由来。

3. `CIN` 网络的整体结构如下图所示。令$T$表示网络深度，每层输出$\mathbf X^{(k)} \in \mathbb R^{H_k\times D},1\le k\le T$都和输出单元相连。

   - 首先对每层的 `feature map` 应用 `sum pooling`：

    $p_h^{(k)} = \sum_{j=1}^D \mathbf{\vec x}_{h}^{(k)},\quad h=1,2,\cdots,H_k$

     这里池化仍然是沿着 `embedding` 维度进行。

     因此得到该层的池化向量：$\mathbf{\vec p}^{(k)} = (p_1^{(k)},p_2^{(k)},\cdots,p_{H_k}^{(k)})^T$。

   - 拼接所有层的输出池化向量，则有：

    $\mathbf{\vec p}^+ = <\mathbf{\vec p}^{(1)},\mathbf{\vec p}^{(2)},\cdots,\mathbf{\vec p}^{(T)}> \in \mathbb R^{\sum_i^T H_i}$

     该向量作为 `CIN` 网络的输出向量。

   - 输出向量输入到 `sigmoid` 输出层，得到 `CIN` 网络的输出得分：

    $\hat y = \frac{1}{1+\exp(\mathbf{\vec p}^+\cdot \mathbf{\vec w}^+)}$

     其中$\mathbf{\vec w}^+$为网络参数。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/akI1bipCu5cr.png?imageslim">
   </p>
   

4. 卷积、池化操作都是沿着 `embedding` 维度进行，而不是沿着其它方向。原因是：我们希望对特征之间的高阶特征交叉显式建模。

   - 根据 `CIN` 网络的基本原理，卷积必须对$\mathbf Z^{(k+1)}$的 `embedding` 维度进行。

    $\mathbf{\vec x}^{(k)}_{h} = \sum_{i=1}^{H_{k-1}}\sum_{j=1}^m W_{i,j}^{(k,h)}(\mathbf{\vec x}_{i}^{(k-1)}\odot \mathbf{\vec x}^{(0)}_j)$

   - 为了得到显式的交叉特征，池化也必须对$\mathbf X^{(k)}$的`embedding` 维度进行。

5. 设第$k$层第$h$个`feature map` 的参数$\mathbf W^{(k,h)}\in \mathbb R^{H_{k-1}\times m}$，因此第$k$层的参数数量为$H_k\times H_{k-1}\times m$。

   另外最后一层 `sigmoid` 输出层的参数数量为$\sum_{k=1}^T H_k$，因此总的参数为：

  $\sum_{k=1}^T H_k\times (1 + H_{k-1}\times m)$

   可以看到，`CIN` 的空间复杂度和 `embedding` 维度$D$无关。

   - 一个 `T` 层的、第$k$层隐向量长度为$H_k$的 `DNN`网络的参数数量（包括`sigmoid` 输出层）为：

    $m\times D\times H_1+\sum_{k=2}^TH_k\times H_{k-1} + H_T$

     可以看到 `DNN` 的空间复杂度和 `embedding` 维度$D$有关。

   - 通常$m$和$H_{k-1}$不会太大，所以$\mathbf W^{(k,h)}$规模不会太大。一旦有必要，可以对$\mathbf W^{(k,h)}$进行$L$阶分解，将其分解为两个小矩阵：

    $\mathbf W^{(k,h)} = \mathbf U^{(k,h)} (\mathbf V^{(k,h)} )^T$

     其中$\mathbf U^{(k,h)} \in \mathbb R^{H_{k-1}\times L}, \mathbf V^{(k,h)} \in \mathbb R^{m\times L}$，$L\ll H,\text{and}\;L\ll m$。

     假设$H_k =\cdots = H_1= H$，则分解之后的空间复杂度从$O(mTH^2)$下降到$O(mTHL+TH^2L)$。

6. 假设$H_k =\cdots = H_1= H$，则计算$\mathbf Z^{(k+1)}$的时间复杂度为$O(mHD)$。由于有$H$层，则总的时间复杂度为$O(mH^2DT)$。

   常规 `DNN` 网络的时间复杂度为$O(mHD + H^2D)$，因此 `CIN` 的时间复杂度太高。

7. 假设所有的 `feature map` 都是$m$维度，即：$H_{k} = \cdots = H_0 = m$。

   - 第一层第$h$个 `feature map` 为：

    $\mathbf{\vec x}_h^{(1)} = \sum_{i=1}^m\sum_{j=1}^m W_{i,j}^{(1,h)}(\mathbf{\vec x}_i^{(0)}\odot \mathbf{\vec x}_j^{(0)}),\quad h=1,2,\cdots,m$

     因此第一层的 `feature map` 为$O(m^2)$对交叉特征建模。

   - 第二层第$h$个`feature map` 为：

     其中下标$l,k$的计算已经在第一层完成，因此第二层使用$O(m^2)$个额外的参数对三阶特征交叉建模。

   - 一个典型的$k$阶特征交叉有$O(m^k)$个参数，但是`CIN` 网络只需要$O(km^3)$个参数就可以建模$k$阶特征交叉。

   .

#### 10.1.2 xDeepFM

1. `xDeepFM` 结合了 `CIN` 网络和 `DNN` 网络，分别对特征交互显式、隐式的建模，二者互补。

   - 模型输出为：

    $\hat y = \sigma(\mathbf{\vec w}_{linear}\cdot \mathbf{\vec x} + \mathbf{\vec w}_{dnn}\cdot \mathbf{\vec x}_{dnn} + \mathbf{\vec x}_{cin}\cdot\mathbf{\vec x}_{cin} + b)$

     其中$\sigma(\cdot)$为激活函数；$\mathbf{\vec w}_{linear},\mathbf{\vec w}_{dnn},\mathbf{\vec w}_{cin}$分别为线性部分、`DNN` 部分、`CIN`部分的输出层权重参数；$\mathbf{\vec x}, \mathbf{\vec x}_{dnn}, \mathbf{\vec x}_{cin}$分别为模型的原始输入特征、`DNN`网络提取的特征、`CIN` 网络提取的特征。

   - 模型损失函数为负的对数似然函数：

    $\mathcal L = -\frac 1N \left(\sum_{i=1}^Ny_i\log \hat y_i+(1-y_i)\log(1-\hat y_i)\right)$

     其中$N$为样本数量。

   - 模型训练目标：损失函数 + 正则化项

    $\mathcal J = \mathcal L + \lambda_*||\mathbf\Theta||$

     其中$\lambda_*$为正则化系数；$\mathbf\Theta$为所有参数，包括线性部分、`CIN`部分、`DNN` 部分。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/yiTFuFA8xh9X.png?imageslim">
   </p>
   

2. `xDeepFM` 可以退化为 `DeepFM` 模型和 `FM`模型。

   - 当`CIN` 网络的深度为1，且所有卷积核的元素都固定为 `1` 时，`xDeepFM`退化为 `DeepFM` 模型。

     注意：`DeepFM` 中，`FM` 层直接连接到输出单元，不需要任何参数。

   - 当进一步移除 `DNN` 部分时，`xDeepFM` 退化为 `FM` 模型。

### 10.2 实验

1. 数据集：

   - `Criteo Display Ads` 数据集：用于预测广告点击率的数据集，包含13个整数特征，26个类别特征，其中每个类别特征的取值集合（即：`cardinality` ）都很大。

     数据包含 7 天的 `11 GB` 用户日志（约 4100万条记录），训练集、验证集、测试集的比例为 `8:1:1` 。

   - `DianPing` 数据集：大众点评网（中国最大的消费者评论网站）提供的数据集，包含很多特征，如：评论、`check-in`、商店元信息（包括地理位置、商店属性）。

     数据包含6个月的用户 `check-in` 记录，目标是餐厅推荐。即：给定用户的个人资料、餐厅属性、用户最近访问过的三个 `POI`，我们希望预测用户访问每个餐厅的概率。

     - 由于只有`check-in` 记录而没有负反馈记录，所以我们只有正样本。为了获取负样本，我们对用户`check-in` 的每个餐厅，取该餐厅附近3公里内随机抽样的四个热门餐厅作为负样本。
     - 训练集、验证集、测试集的比例为 `8:1:1` 。

   - `BingNews` 数据集：`Bing News` 是微软 `Bing` 搜索引擎中的一部分。

     数据包含连续五天的曝光日志，其中前三天的数据用于训练和验证，后两天的数据用于测试。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/H0BzGPjdOtOw.png?imageslim">
   </p>
   

2. 模型的评估指标为 `AUC,logloss`。

3. 对照模型包括`LR,FM,DNN,PNN,Wide&Deep,DCN,DeepFM` 模型。

   - 为验证模型能够自动学习有效的特征，我们不包含任何人工设计的交叉特征。

   - 每个模型的超参数通过验证集获取最佳的超参数，最后在测试集上比较各模型的评估指标。

   - 神经网络模型的一些默认配置：

     - 采用 `Adam` 优化器，初始化学习率为 `0.001`，`batch-size = 4096` 。

     - 对于 `DNN,DCN,Wide&Deep,DeepFM,xDeepFM`，采用$L_2$正则化，正则化系数为$\lambda = 0.0001$。

       对于 `PNN`，采用 `dropout` 正则化，`dropout` 系数为 `0.5`。

     - 每个 `field` 的 `embedding`向量维度为 `10` 。

     - `DNN` 层隐向量的维度为`400`，`CIN` 层的影响力的维度为：`Criteo` 数据集 `200`，`Dianping/BingNews` 数据集 `100` 。

#### 10.2.1 CIN 网络

1. 理论表明：`FM` 显式建模二阶特征交互；`DNN` 隐式建模高阶特征交互；`cross network` 试图用少量参数显式建模高阶特征交互（已被证明无效）；`CIN` 显式建模高阶特征交互。

   实际上模型的表现严重依赖于数据集。如果某个数据集 `A` 上模型 `M1` 优于模型 `M2`，那么无法从理论上保证在其它数据集上也是如此。如：如果数据集不需要高阶特征交互，则 `FM` 可能就是最佳的模型。

   下表给出了各模型在各数据集上的表现。其中 `Depth` 表示网络的最佳深度（通过超参数搜索得到）。

   注意：这里 `CIN` 是 `xDeepFM` 的 `CIN` 网络，不包含 `xDeepFM` 的 `DNN` 部分；`CrossNet` 是 `DCN` 的 `cross network` 部分，也不包含 `DNN` 部分。

   结论：

   - `DNN,CrossNet,CIN` 在所有数据集上明显优于 `FM`，说明对于这三个数据集，高阶特征交互是必要的。
   - 在所有的数据集上，`CIN` 都优于其它模型，说明了 `CIN` 在显式建模高阶特征交互方面的有效性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/sD2DaWI9fTqw.png?imageslim">
   </p>
   

#### 10.2.2 xDeepFM

1. `xDeepFM` 集成了 `CIN` 网络和 `DNN` 网络，因此我们将它和其它模型比较。

   结论：

   - `LR` 模型远比其它模型差，这表明对特征交互建模非常重要。
   - `Wide&Deep,DCN,DeepFM,xDeepFM`明显强于 `FM`，这说明混合结构对于模型提升非常重要。
   - `xDeepFM` 在所有数据集上都表现最好，说明必须将显式、隐式高阶特征交互进行组合。
   - 所有基于神经网络的模型都不需要非常深，就可以获得最佳性能。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/kuhpEdHHweFi.png?imageslim">
   </p>
   

#### 10.2.3 超参数探索

1. 这里对 `xDeepFM` 的 `DNN` 部分的超参数选择最佳的配置并固定不动，然后对 `CIN` 部分的超参数进行探索。

2. 首先探索`CIN`网络的深度。可以看到：`xDeepFM` 的性能随着网络深度增加而增加，但是当深度大于3时模型性能下降。

   这是由于过拟合导致，因为当添加更多的隐层时训练集损失仍在下降。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/zNXYS8UwTepV.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/sjavcusbkdus.png?imageslim">
   </p>
   

3. 然后探索 `CIN` 网络隐向量维度。实验中我们将隐层数量设为 `3` 。

   - 当隐向量维度从`20` 增加到 `200`时，`Bing News` 数据集的模型性能稳定提高。
   - 在大众点评数据集上，隐向量最佳维度为 `100` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/aA7SKk5m5MU3.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/uwMyAONO0lqq.png?imageslim">
   </p>
   

4. 最后评估`CIN` 网络的激活函数。可以看到：当采用线性激活函数`identify`（即不采用任何激活函数）时，模型表现最佳。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/RDxAtKtACC80.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/KGl1qtrs6398.png?imageslim">
   </p>
   

    

## 十一、ESMM

1. 在 “曝光 -> 点击 -> 转化” 链路上，`CTR`预估任务预估每个曝光被用户点击的概率，`CVR` 预估任务预估每个点击被转化的概率。

   和 `CTR` 相比，`CVR` 更关注预测用户有没有商品购买、游戏注册、表单登记等转化行为。

2. 常规的 `CVR` 预估模型采用和 `CTR` 预估模型类似的深度学习方法，但是在具体应用中面临几个特殊的问题：

   - 样本选择偏差 `sample selection bias:SSB` 问题：传统的 `CVR` 模型是在点击样本上训练的，但是推断是在曝光样本上进行的。这导致训练样本和推断样本不一致，降低了模型的泛化能力。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/z2mKumQd7qhY.png?imageslim">
     </p>
     

   - 数据稀疏 `data sparsity:DS` 问题：点击数据通常要比曝光数据少得多，因此 `CVR` 任务的样本数通常远小于 `CTR` 任务的样本数。这导致 `CVR` 模型很容易陷入过拟合。

   有一些方法试图解决这些问题：

   - 基于不同特征构建层次化分类器 `hierarchical estimators`，然后用逻辑回归模型融合这些分类器来预测。

     该方法可以解决数据稀疏问题，但是依赖于先验知识来构建层次化的分类器。

   - 过采样 `Oversampling` 拷贝正样本从而有助于缓解数据的稀疏性，但是这种方式对采样率非常敏感。

   - `All Missing As Negative:AMAN`方法随机抽取一定比例的未点击的曝光样本作为负样本。

     该方法可以一定程度上消除 `SSB` 问题，但是会导致 `pCVR` 被低估。因为在未点击的曝光样本中，假如所有曝光都被用户点击，则可能存在一定比例的转化行为（正样本）。而 `AMAN` 方法假设所有未点击的曝光都是负样本。

   - 无偏方法采用拒绝抽样 `rejection sampling` 来使得观测样本（即：点击分布）拟合真实的数据分布（即：曝光分布），从而解决了 `SSB` 问题。

     这种方式在除以拒绝概率 `rejection probability` 时可能会遇到数值稳定性问题。

3. 论文 `Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate` 提出了 `Entire Space Multi-task Model:ESMM` 模型，该模型充分利用了用户的操作顺序（“曝光 -> 点击 -> 转化”），通过新的视角来建模 `CVR` 。

   同时：

   - `ESMM` 模型通过在整个曝光空间对 `CVR` 建模，这解决了样本选择偏差问题。
   - `ESMM` 模型采用特征表达的迁移学习策略，这解决了数据稀疏问题。

### 11.1 模型

1. 在转化链路中用户遵循 “曝光 -> 点击 -> 转化” 的顺序。`ESMM` 模型借鉴了多任务学习的思想，基于该顺序引入两个辅助任务：`CTR` 预测任务、`CTCVR` 预测任务。

   - 模型由两个子网络构成：左侧的 `CVR` 网络和右侧的 `CTR` 网络，二者均采用 `BASE` 模型（即下图左侧的基准 `DNN` 模型）相同的结构。

     论文指出：这两个子网络可以用最先进的其它模型替代，从而获取更好的整体性能。

   - 模型同时输出`pCTR,pCVR,pCTCVR`三路输出，其中：

     - `pCVR` 输出由左侧的 `CVR` 网络输出
     - `pCTR` 输出由右侧的 `CTR` 网络输出
     - `pCTCVR` 输出将 `CVR` 和 `CTR`网络输出的乘积作为输出

   - `ESMM` 模型不会直接使用点击样本来预测 `CVR`，而是将 `CVR` 作为一个中间变量：

    $\text{pCTR}\times \text{pCVR} = \text{pCTCVR}$

     其中 `PCTR` 和 `pCTCVR` 都是在整个曝光样本空间进行估计的，因此派生的 `pCVR` 也适用于整个曝光样本空间。这就消除了样本选择偏差问题。

   - 另外，`CVR` 网络的 `embedding` 参数和 `CTR` 网络的 `embeddin` 参数共享，而后者具有更多的训练样本。因此这种参数的迁移学习可以有效缓解数据稀疏问题。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/N8i5jQNfuCPU.png?imageslim">
   </p>
   

2. 假设观察到的所有样本集合为：

  $\mathcal S = \{(\mathbf{\vec x}_1,y_1, z_1),\cdots,(\mathbf{\vec x}_N,y_N,z_N)\}$

   其中$\mathbf{\vec x}\in \mathcal X$为特征，$y\in \mathcal Y$点击`label`，$z\in \mathcal Z$为转化`label`。

   - `CVR` 建模是预估$\text{pCVR} = p(z=1\mid y=1,\mathbf{\vec x})$
   - `CTR` 建模是预估$\text{pCTR} = p(y=1\mid \mathbf{\vec x})$
   - `CTCVR` 建模是预估$\text{pCTCVR} = p(y=1,z=1\mid \mathbf{\vec x})$

   它们之间满足：

  $p(y=1,z=1\mid \mathbf{\vec x}) = p(y=1\mid \mathbf{\vec x})\times p(z=1\mid y=1,\mathbf{\vec x})$

   即：$\text{pCTCVR} = \text{pCTR}\times \text{pCVR}$。

3. 大多数传统 `CVR` 模型是上图左侧所示的 `DNN` 模型，这种结构作为我们实验对比的基准模型。

   传统 `CVR` 模型直接估计$p(z=1\mid y=1,\mathbf{\vec x})$，模型训练样本仅包含点击样本：

  $\mathcal S_c = \{(\mathbf{\vec x}_1,y_1,z_1\mid y_1=1),\cdots,(\mathbf{\vec x}_M,y_M,z_M\mid y_M=1)\}$

   其中 `M` 为所有点击样本数量，$\mathcal S_c\sub \mathcal S$。

   这会带来以下问题：

   - `SSB`：设$\mathcal S_c$的特征空间为$\mathcal X_c$，则 `CVR` 模型近似转化为：

    $p(z=1\mid y=1,\mathbf{\vec x};\mathbf{\vec x}\in \mathcal X)\simeq q(z=1\mid \mathbf{\vec x}; \mathbf{\vec x}\in \mathcal X_c)$

     因此训练期间都是在$\mathcal X_c$上训练。

     而推断期间给定一个特征$\mathbf{\vec x}\in \mathcal X$，我们需要计算：假设该曝光被点击的条件下其转化率。即计算$q(z=1\mid \mathbf{\vec x}; \mathbf{\vec x}\in \mathcal X)$。

     这里存在两个问题：

     -$\mathcal X_c$仅仅是$\mathcal X$的一个很小的部分，它很容易受到一些随机的噪声点击的影响。因此其分布很不稳定。
     - 空间$\mathcal X_c$的分布和$\mathcal X$差异较大，这使得训练样本的分布偏离了预测样本的分布（即：真实样本分布），降低了`CVR` 模型的泛化能力。

   - `DS`：由于点击事件发生次数比曝光事件发生次数少得多，因此 `CVR` 训练样本极其稀疏。

     论文的实验数据统计表明：`CVR` 任务样本数仅仅是 `CTR` 任务样本数的 `4%` 。

   - 反馈延迟 `delayed feedback`：即单次曝光的点击和转化之间可能间隔很长时间。

     如：给用户推荐一款商品，用户点击之后可能过了两周才购买。

4. 与基准 `DNN` 模型不同，`ESMM` 在整个曝光空间建模。根据：

  $p(z=1\mid y=1,\mathbf{\vec x}) = \frac{p(y=1,z=1\mid \mathbf{\vec x})}{p(y=1\mid \mathbf{\vec x})}$

   其中$p(y=1,z=1\mid \mathbf{\vec x})$和$p(y=1\mid \mathbf{\vec x})$在所有曝光的数据空间$\mathcal S$上建模，因此可以有效解决样本选择偏移问题。

5. 在 `ESMM` 中，`CVR` 网络的 `embedding` 和 `CTR` 网络的 `embedding` 字典共享，这是典型的特征表达迁移学习。

   由于 `CTR` 任务的训练样本要比 `CVR` 任务多得多，这种参数共享机制使得 `CVR` 网络能够从未点击的曝光样本中学习，有效缓解了数据稀疏性问题。

6. `ESMM` 模型的损失函数综合考虑了 `CTR` 和 `CTCVR` 两个辅助任务：

  $\mathcal L(\theta_{cvr},\theta_{ctr}) = \sum_{i=1}^N l(y_i,f(\mathbf{\vec x}_i;\theta_{ctr}))+\sum_{i=1}^N l(y_i \& z_i,f(\mathbf{\vec x}_i;\theta_{ctr})\times f(\mathbf{\vec x}_i;\theta_{cvr}))$

   其中$\theta_{ctr},\theta_{cvr}$为 `CTR`和 `CVR` 网络参数，$l(\cdot)$为交叉熵损失函数。

7. 表面上看，我们可以分别独立的建立两个模型分别估计 `pCTR` 和 `pCTCVR`，然后根据：

  $\text{pCVR} = \frac{\text{pCTCVR} }{\text{pCTR} }$

   来计算 `pCVR` 。但这种做法存在两个问题：

   - 由于 `pCTR` 通常是一个非常小的数（如$10^{-9}$），除以一个很小的数很容易导致数值不稳定。
   - 除法的结果无法确保是在 `0~1` 之间。

   在 `ESMM` 模型中，`pCVR` 是一个中间变量，通过乘法的形式使得这三个概率可以同时被训练。

   另外模型结构也保证了 `pCVR` 一定是在 `0~1` 之间。

### 11.2 实验

1. 数据集：`CVR` 任务目前没有公开的数据集。为了评估模型，论文从淘宝的推荐系统中收集了曝光日志，并发布了整个数据集的 `1%` 随机抽样数据（高达 `38GB` ）作为公共数据集 `Public Dataset`。整个数据集称作产品数据集 `Product Dataset`。

   下表给出了这两个数据集的统计信息。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/u0Qqtoax562T.png?imageslim">
   </p>
   

2. 参与比较的模型包括：

   - 基准模型：即前面介绍的简单 `DNN`模型，不采取任何额外的策略。
   - `AMAN`：执行负采样策略的基准模型，负采样比例从 `10%,20%,50%,100%` 中搜索。
   - `OVERSAMPLING`：执行正样本过采样策略的基准模型，正样本拷贝比例从 `2,3,5,10` 中搜索。
   - `UNBIAS`：执行 `rejection` 采样策略的基准模型，通过拒绝采样使得训练分布（即点击数据集的数据分布）和真实分布（即曝光数据集的数据分布）保持一致。
   - `DIVISION`：通过基准模型分别训练 `pCTR` 和 `pCTCVR`，然后计算 `pCVR = pCTCVR/pCTR` 。
   - `ESMM-NS`：`ESMM` 模型的一个变种，该模型没有共享 `embedding` 参数。

   前面四个模型是分别直接独立建模 `CVR` 、 `CTR` ，然后根据 `CTCVR = CTR x CVR` 来计算 `CTCVR` ；`DIVISION, ESMM-NS,ESMM` 在整个曝光样本空间间接建模`CVR` 。

   为了公平起见，包括 `ESMM` 在内的所有模型都采取和 `BASE` 基准模型相同的网络结构和超参数，包括：

   - 采用 `ReLU` 激活函数
   - `embedding` 向量维度为 `18`
   - 嵌入层后续的全连接层有4层，每层的隐向量维度分别为 `360,200,80,2`
   - 采用 `adam` 优化器，优化器的参数为：$\beta_1=0.9,\beta_2=0.999,\epsilon = 10^{-8}$

3. 模型性能评估在两个任务上进行：对点击数据集评估 `pCVR` 任务、对曝光数据集评估 `pCTCVR` 任务。评估指标为 `AUC` 。

   - 每个模型分别评估 `pCVR` 任务
   - 每个模型通过计算 `pCTR x pCVR`来评估 `pCTCVR` 任务

   这两个任务的数据集都根据时间顺序拆分：前 `1/2` 数据作为训练数据，后 `1/2`数据作为测试数据。

   所有实验重复 10次，取10次的平均结果。

4. 在公共数据集上不同模型的表现如下图所示。

   - 在基准模型的三个变种之间，只有 `AMAN` 在 `CVR` 任务的表现稍差，这可能是由于随机采样的敏感性导致。

     `OVERSAMPLING` 和 `UNBIAS` 在 `CVR`和 `CTCVR` 这两个任务上相对基准模型均有所改进。

   - `DIVISION` 和 `ESMM-NS` 对整个曝光空间进行估计，因此它们相对基准模型取得显著提升。

     由于 `ESMM-NS` 避免了数值不稳定性，因此其性能强于 `DIVISION` 。

   - `ESMM` 利用迁移学习从未点击数据中学习，因此进一步强于 `ESMM-NS` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/RhN1UYnYLpJy.png?imageslim">
   </p>
   

5. 在产品数据集上评估`ESMM` 模型，该数据集比公开数据集大两个量级。

   我们通过对产品数据集执行不同采样率来生成不同规模的训练数据集，然后评估训练集规模对于模型性能的影响。结论：

   - 随着训练样本的增加，所有方法都得到改善。这说明了数据稀疏性的影响。
   - 除了`AMAN 1%` 这组搭配以外，所有其它模型在所有训练集规模上都超越了基准模型。
   - `ESMM-NS` 和 `ESMM` 在所有数据集规模上超越了所有其它模型。
   - `ESMM` 模型在 `CVR` 任务和 `CTCVR` 任务上的所有数据集规模都超越了其它模型一大截。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/8TtPdop5eF9y.png?imageslim">
   </p>
   

## 十二、DIN

1. 在电商领域，每个用户都有丰富的历史行为数据，从这些历史行为数据中提取特征对于 `CTR` 预估模型非常重要。

   这些用户历史行为数据具有两个显著特点：

   - 多样性：用户在访问电商网站时，可能对各种各样的商品感兴趣。如：一个年轻的母亲可能同时对包包、鞋子、耳环、童装等感兴趣。

   - 局部激活 `local activation` ：用户是否点击当前商品仅取决于历史行为数据中的部分数据，而不是全部历史行为数据。

     如：一个游泳用户会点击推荐的护目镜，主要是因为曾经购买过泳衣，而不是因为最近曾经购买的手机。

   如，从淘宝线上收集的用户行为案例：

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/a6dfkssAkuVY.png?imageslim">
   </p>
   

   常见的 `DNN` 模型缺乏针对性的深入了解、利用用户历史行为数据的结构。这些模型通常都是在 `embedding` 层之后添加一个池化层，通过使用`sum pooling` 或者 `avg pooling` ，从而将用户的一组历史行为 `embedding` 转化为固定尺寸的`embedding`向量。这将导致部分信息丢失，从而无法充分利用用户丰富的历史行为数据。

   论文 `《Deep Interest Network for Click-Through Rate Prediction》` 提出了深度兴趣网络 `Deep Interest network:DIN` 模型，该模型通过兴趣分布来刻画用户的不同兴趣，并针对候选广告（淘宝的广告也是一种商品）设计了类似 `attention` 的网络结构来激活局部的相关兴趣，使得与候选广告相关性更高的兴趣获得更高的权重。

   另外，论文为了解决过拟合问题提出了一种有效的自适应正则化技术。

2. 展示广告系统 `display advertising system` 的应用场景如下所示。当用户访问电商网站时：

   - 首先检查用户的历史行为数据
   - 然后根据 `matching` 模块召回候选广告集
   - 接着根据 `ranking` 模块预测用户对每个广告的点击率，挑选出点击率最高的一批广告
   - 最后曝光广告并记录用户的行为

   注意：电商领域的广告也是一种商品。即：被推广的目标就是广告主期望售卖的商品。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/Tul3YtYBfXsu.png?imageslim">
   </p>
   

3. 与搜索广告不同，大部分用户进入展示广告系统时并没有明确的兴趣意图。因此我们需要从用户的历史行为数据中有效抽取用户兴趣。

### 12.1 模型

1. 样本原始特征由稀疏 `ID` 组成，我们将其划分为四组：

   - 用户画像特征：包括用户基础画像如年龄、性别等。
   - 用户行为特征：包括用户历史访问的商品`id`、历史访问的店铺`id`、历史访问的商品类别`id` 等。
   - 广告特征：包括广告的商品`id`、店铺`id`、商品类别`id` 等。
   - 上下文特征：包括访问时间等。

   这里不包含任何交叉特征，交叉特征由神经网络来捕获。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/tRgwBpdvIfBo.png?imageslim">
   </p>
   

2. `DIN` 模型（右图）和基准模型 `Base Model` （左图）如下图所示。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/E0F9zDdNwtjH.png?imageslim">
   </p>
   

3. 基准模型采用 `embedding&MLP` 结构，它由两个部分组成：

   - `embedding` 部分：将稀疏 `id` 特征映射到 `embedding` 向量。
   - `MLP` 部分：将 `embedding` 向量馈入 `MLP` 来预测输出。

   由于输入中包含长度可变的行为序列`ID`，因此基准模型采用一个池化层（如 `sumpooling` ）来聚合 `embedding` 向量序列，从而获得固定尺寸的向量。

   基准模型在实践中表现良好，但是在池化过程中丢失了很多信息。即：池化操作破坏了用户行为数据的内部结构。

4. 假设用户 `u` 的分布式表达为$\mathbf{\vec v}_u$，广告 `a` 的分布式表达为$\mathbf{\vec v}_a$。我们通过内积来衡量用户和广告的相关性：$f(u,a) = \mathbf{\vec v}_u\cdot \mathbf{\vec v}_a$。

   假设用户 `u` 和广告 `a,b` 的相关性都很高，则$f(u,a),f(u,b)$的值都会较大。那么位于$\mathbf{\vec v}_a,\mathbf{\vec v}_b$之间的点都会具有很高的相关性得分。这给用户和广告的分布式表达建模带来了硬约束。

   - 我们可以通过增加向量空间的维数来满足约束条件，但是这会导致模型参数的巨大增长。

   - `DIN` 通过让$\mathbf{\vec v}_u$称为$\mathbf{\vec v}_a$的函数来解决这个问题：

    $\mathbf{\vec v}_u = f(\mathbf{\vec v}_a) = \sum_{i=1}^Nw_i\times \mathbf{\vec v}_i= \sum_{i=1}^Ng(\mathbf{\vec v}_i,\mathbf{\vec v}_a)\times \mathbf{\vec v}_i$

     其中：

     -$\mathbf{\vec v}_i$是用户第 `i` 个行为`id` （如商品`id`、店铺 `id` ）的 `embedding` 向量
     -$\mathbf{\vec v}_u$是用户所有行为 `id` 的 `embedding` 向量的加权和，权重$w_i$是用户第 `i` 个行为对候选广告 `a` 的 `attention score` 。

     因此用户的 `embedding` 向量根据不同的候选广告而有所不同，即：局部激活。而通过 `sum pooling` 融合历史行为来实现兴趣多样性。

### 12.2 Dice 激活函数

1. `PReLU` 激活函数是一种广泛应用的激活函数，其定义为：

   其中$a_i$是一个很小的数。

   `PReLU` 激活函数类似 `Leaky ReLU`，它用于避免零梯度。研究表明：`PReLU`激活函数虽然能提升准确率，但是会有引入一些额外的过拟合风险。

   为进一步提高模型的收敛速度和预测能力，论文设计了一个新的、依赖于数据的激活函数，称作 `Dice` ：

   其中：

   -$\mathbb E[y_i]$和$Var[y_i]$是训练期间基于 `mini-batch` 统计得到的均值和方差，$\epsilon$为一个小的正数从而平滑结果。

   - 在推断期间我们使用$\mathbb E[y_i]^\prime$和$Var[y_i]^\prime$：

     其中$\mathbb E[y_i]_{t+1}, Var[y_i]_{t+1}$分别为第$t+1$个 `mini-batch` 统计得到的均值和方差，$\alpha$为超参数（入 `0.99` ）。

     最后一个迭代步的$\mathbb E[y_i]^\prime$和$Var[y_i]^\prime$就是推断期间用到的均值和方差。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/OhoHMtiw05t7.png?imageslim">
     </p>
     

2. `Dice` 可以视为一个 `soft rectifier` ，它带有两个通道$a_iy_i$和$y_i$，通过$p_i$控制这两个通道的流量。

3. `Dice` 的核心思想是：根据数据来自适应的调整整流点 `rectifier point`，而不是像 `PReLU` 一样整流点在零点。

### 12.3 自适应正则化

1. 实验显示：当添加细粒度的、用户访问过的商品`id` 特征时，模型的性能（验证`auc`）在第一个 `epoch` 后迅速下降。这表明模型遇到严重过拟合。

   由于 `web-scale` 级别的用户行为数据遵从长尾分布，即：大多数行为特征`ID` 在训练样本中仅出现几次。这不可避免地将噪声引入到训练过程中，并加剧过拟合。

   缓解该问题地一种简单方式是：过滤掉低频地行为特征`ID` 。这可以视为一种人工正则化策略。

   这种过滤策略太过于粗糙，因此论文引入了一种自适应地正则化策略：根据行为特征`ID` 出现地频率对行为特征`ID` 施加不同地正则化强度。

2. 定义$\mathbb B$为 `size = b`的 `mini batch` 样本集合，$n_i$表示训练集中行为特征`ID = i` 出现的频次，$\lambda$为正则化系数。

   定义参数更新方程为：

  $\mathbf{\vec w}_i\leftarrow \mathbf{\vec w}_i - \eta\left[\frac 1b \sum_{(\mathbf{\vec x}_j,y_j)\in \mathbb B} \nabla _{\mathbf{\vec w}_i} L(f(\mathbf{\vec x}_j),y_j) + \lambda \frac {1}{n_i}\mathbf{\vec w}_i\times I_i\right]$

   其中：

   -$\mathbf{\vec w}_i$表示特征 `ID = i` 对应的 `embedding` 向量，它也是`embedding` 参数。
   -$I_i$用于指示：$\mathbb B$中是否有特征 `i` 非零的样本。

   该正则化项惩罚了低频特征。

### 12.4 XDL

1. 实践中发现：大多数`DNN` 网络都是基于以下两个部分来构建：

   - 采用 `embedding` 技术将高维的原始稀疏特征转换为低维的 `embedding`向量
   - 将得到的 `embedding` 向量馈入 `MLP/RNN/CNN` 等网络。

   模型参数主要集中在 `embedding` 部分，这部分需要在多台机器上分配；第二个部分可以在单台机器上处理。

   基于该思想，论文提出了支持多`GPU`分布式训练的 `X-Deep Learning:XDL` 平台，该平台支持模型并行和数据并行，其目标是解决大规模稀疏输入特征、百亿级参数的工业级深度学习网络的挑战。

   `XDL`平台主要有三个组件：

   - 分布式`embedding` 层：它是一个模型并行模块，`embedding` 层的参数分布在多个 `GPU` 上。

     `embedding` 层作为一个预定义的网络单元来使用，提供前向、反向传播两种工作模式。

   - 本地后端`local backend` 模块：它是一个独立的模块，用于处理本地网络的训练。

     其优点是：

     - 采用统一的数据交换接口和抽象，我们可以轻松的集成和切换不同的框架。
     - 论文复用了开源的深度学习框架，如 `tensorflow,mxnet,theano`等。因此可以方便的跟进开源社区并利用最新的网络结构和算法的好处。

   - 通信模块：它是基础模块，用于帮助分布式`embedding` 层和本地后端实现并行。

     论文的第一版中，它基于 `MPI` 实现的。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/rcDwqJRpW8Lx.png?imageslim">
   </p>
   

2. `DIN` 在 `XDL` 平台上训练，并采用了 `common feature` 技巧。

   由于 `XDL` 平台的高性能和灵活性，训练速度加快了10倍，从而以更高的效率优化超参数。

### 12.5 实验

1. 评估指标：`AUC` 和 `Group AUC : GAUC` 。

   `GAUC` 是 `AUC` 的推广，它是 `AUC` 的加权平均：

  $\text{GAUC} = \frac{\sum_{i=1}^n w_i\times \text{AUC}_i}{\sum_{i=1}^n w_i} = \frac{\sum_{i=1}^n \text{imp}_i\times \text{AUC}_i}{\sum_{i=1}^n \text{imp}_i}$

   其中：$\text{AUC}_i$表示用户 `i`的所有样本对应的 `auc` ；$\text{imp}_i$是用户 `i` 的所有样本数。

   事实证明：`GAUC` 在展示广告中更具有指导意义。 `AUC` 考虑所有样本的排名，而事实上在线上预测时，对于给定用户我们只需要考虑候选广告的排名。

   `GAUC` 对每个用户单独进行处理，先考虑每个用户的预测结果，再对所有用户进行加权。这可以消除用户 `bias` 的影响。

   如：模型倾向于对女性用户打高分（预测为正的概率较高），对男性用户打低分（预测为正的概率较低）。

   - 如果采用`AUC` 则指标效果一般，因为男性正样本打分可能低于女性负样本。
   - 如果采用 `GAUC` 则指标效果较好，因为男性用户、女性用户各自的 `AUC` 都较高，加权之后的 `GAUC` 也较高。这和线上投放的方式一致。

#### 12.5.1 可视化

1. `DIN` 模型中，稀疏`ID` 特征被编码为`embedding` 向量。这里随机选择 `9` 个类别（服装、运动鞋、箱包等），每个类别选择 `100` 种商品。这些商品的 `embedding`向量通过 `t-SNE` 可视化如下图。

   图中具有相同形状的点对应于同一类别颜色和点击率预测值相对应。从图中可以清楚看到 `DIN embedding` 的聚类特性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/kAmx3479O1VV.png?imageslim">
   </p>
   

2. `DIN` 基于 `attention` 单元来局部激活与候选广告相关的历史行为。下图展示了候选广告相关的注意力得分。

   可以看到：与候选广告高度相关的历史行为获得了很高的注意力得分。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/C4yu94byNPe9.png?imageslim">
   </p>
   

#### 12.5.2 自适应正则化

1. 基准模型和`DIN` 模型都会遇到过拟合问题。下图展示了具有细粒度商品`ID` 特征时的训练过程，图中清楚的看到过拟合问题（`without reg` ）。

   论文比较了几种正则化技术：

   - `dropout`：在每个样本中，随机丢弃 `50%` 的历史行为商品`ID` 。

   - 人工频率过滤：在每个样本中，人工去掉历史行为中的低频商品`ID`，保留高频商品`ID` 。

   -$L_2$正则化：采用正则化系数$\lambda = 0.01$的$L_2$正则化。

   - `DiFacto` 正则化：正则化系数$\lambda = 0.01$：

    $\mathbf{\vec w}_i \leftarrow \mathbf{\vec w}_i-\eta\left[\frac 1b \sum_{(\mathbf{\vec x}_j,y_j)\in \mathbb B} \nabla_{\mathbf{\vec w}_i} L(f(\mathbf{\vec x}_j),y_j) +\lambda n_i\mathbf{\vec w}_iI_i\right]$

   - 自适应正则化：：正则化系数$\lambda = 0.01$。

   比较结果如下图所示：

   - 不带任何正则化时(`without reg`），模型发生严重过拟合。
   - 带 `dropout` 正则化会缓解过拟合，但是同时会导致收敛速度在第一个`epoch` 变慢。
   - 人工频率过滤正则化也会缓解过拟合，同时在第一个`epoch` 保持同样的收敛速度（和无任何正则化方法相比），但是最终模型性能比 `dropout` 效果更差。
   - `DiFactro` 正则化对于用户历史行为中的高频商品设置更大惩罚。但是在我们的任务中，高频商品更能代表用户的兴趣，而低频商品代表噪声。因此这种形式的正则化效果效果甚至不如$L_2$正则化。
   - 自适应正则化方法在第一个`epoch`之后几乎看不到过拟合，在第二个`epoch` 验证集的 `loss` 和 `GAUC` 几乎已经收敛。这证明了自适应正则化方法的有效性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/0uBFmq1F8ohD.png?imageslim">
   </p>
   

#### 12.5.3 模型对比

1. 在淘宝线上展示广告系统中比较了不同的模型效果。

   - 训练数据和测试数据采集自系统日志（曝光日志和点击日志）。论文收集了两周的样本作为训练集，下一天的样本作为测试集。

   - 基准模型和 `DIN` 模型的参数各自独立调优，并且报告各自最佳结果。

   - 结论：

     - 使用自适应正则化的 `DIN` 使用基准模型一半的迭代次数就可以获得基准模型最好的 `GAUC` 。

       最终 `DIN` 模型相比基准模型获得了 `1.01%` 的绝对 `GAUC`增益。

     - 采用`Dice` 激活函数之后，`DIN` 模型提高了 `0.23%` 的绝对 `GAUC` 增益。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/SlHPWIS75EkI.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/SunIHXJlcmgY.png?imageslim">
   </p>
   

## 十三、DIEN

1. 在 `CTR` 预估模型中，捕获用户行为数据背后的潜在用户兴趣非常重要。目前捕获用户兴趣的模型有两个主要缺陷：

   - 包括`DIN` 在内的大多数兴趣模型都将用户行为直接视为兴趣。事实上，用户的显式行为不等于用户的潜在兴趣。

     因此这些模型都忽略了挖掘用户显式行为背后的真正用户兴趣。

   - 考虑到外部环境和用户自身认知的变化，用户兴趣会随着时间动态变化，大多数模型都未考虑这一点。

   基于这些观察，论文 `《Deep Interest Evolution Network for Click-Through Rate Prediction》` 提出了深度兴趣演化模型 `Deep Interest Evolution Network:DIEN` 。`DIEN` 有两个关键模块：

   - 兴趣抽取层 `interest extractor layer` ：用于从用户的历史行为序列中捕获潜在的时序兴趣 `latent temporal interest`。

     正确的兴趣表示是 `DIEN` 模型的基础。在兴趣提取层，`DIEN` 选择利用 `GRU` 来建模用户行为之间的依赖关系。

     考虑到一段连续的行为是由某个兴趣直接引起的，因此`DIEN` 引入辅助损失函数。辅助损失函数使用下一个阶段的行为来监督当前阶段隐状态（称作兴趣状态）的学习。这些额外的监督信息有助于模型捕获兴趣表示的更多语义信息，从而推动`GRU` 的隐状态更有效的表达兴趣。

   - 兴趣演化层 `interest evolving layer` ：用于建模用户的兴趣演变过程。

     用户的兴趣是多种多样的，这导致产生兴趣漂移现象：相邻的两次访问中，用户的意图可能完全不同。并且用户的当前行为可能取决于很久之前的行为，而不是前几次行为。

     同时，用户对不同目标商品的点击行为可能受到不同兴趣的共同影响。

     在兴趣演变层，`DIEN` 对与目标商品有关的兴趣演变轨迹进行建模。基于从兴趣提取层得到的兴趣序列，`DIEN` 采用带注意力更新门的 `GRU` ( `GRU with attentional update gate:AUGRU` ）来建模针对不同目标商品的特定兴趣演变过程。

     `AUGRU` 使用兴趣状态和目标商品来计算相关性，从而增强了相关兴趣对于兴趣演变的影响，同时减弱由于兴趣漂移产生的无关兴趣的影响。

   `DIEN` 模型已经被部署到淘宝的在线展示广告系统中，其点击率提高了 `20.7%` 。

2. `DIN` 模型在捕获用户历史行为序列之间的依赖性方面很弱；传统的 `RNN` 虽然可用于对用户历史行为序列建模，但有两个缺陷：

   - `RNN` 直接将序列结构中的隐状态视为潜在的用户兴趣，但是这些隐状态缺乏对兴趣表达的专门的监督。
   - `RNN` 连续且不加区分的处理相邻行为之间的所有依赖，实际上不同的行为对于目标商品的权重是不同的。

### 13.1 模型

#### 13.1.1 BaseModel

1. 在线展示广告系统中，有四类 `category`特征：

   - 用户画像特征 `user profile` ：包括性别、年龄等。
   - 用户行为特征 `user behavior`：包括用户访问的商品`ID` 列表，用户访问的店铺`ID` 列表、用户访问的商品类别`ID` 列表等。
   - 广告特征 `AD` ：广告本身也是商品，因此称作目标商品。它包含商品`ID`、店铺`ID` 等特征。
   - 上下文特征 `Context` ：包含当前访问时间等特征。

   每个特征都可以进行`one-hote` 编码。假设用户画像、用户行为、广告、上下文这四个`field` 的不同特征 `one-hot` 编码拼接之后分别为$\mathbf{ x}_p,\mathbf{ x}_b,\mathbf{ x}_a,\mathbf{ x}_c$。

   通常用户行为特征是一个行为序列，因此有：

  $\mathbf{x}_b = [\mathbf{\vec b}_1,\cdots,\mathbf{\vec b}_T] \in \mathbb R^{K\times T}, \mathbf{\vec b}_t\in \{0,1\}^K$

   其中$T$为用户行为序列的长度，$K$为所有商品的数量，$\mathbf{\vec b}_t$是第$t$个行为的 `one-hot` 向量。

2. 大多数 `deep CTR` 模型基于 `embedding &MLP` 结构，因此 `BaseModel` 主要由以下部分组成：

   - `embedding` 部分：在 `embedding`层，每个稀疏`ID` 特征被映射为 `embedding` 向量。

     假设商品`ID`$k$被映射到向量$\mathbf{\vec m}_k$，则对于行为$\mathbf{\vec b}_t$假设它对应于商品$j_t$（即第$j_t$个分量为 `1` 其它分量为 `0` ），则它对应于 `embedding` 向量$\mathbf{\vec m}_{j_t}$。

     则用户行为序列$\mathbf{x}_b$对应于`embedding` 向量序列：

    $\mathbf{\vec e}_b=[\mathbf{\vec m}_{j_1},\cdots,\mathbf{\vec m}_{j_T}]$

     同样的，我们用$\mathbf{\vec e}_a$来表示广告 `ID` 为 `a` 的 `embedding` 向量。

   - `MLP` 部分：将来自同一个`field` 的所有 `embedding` 向量馈入一个池化层，得到一个固定长度的向量。然后将来自不同`field` 的固定长度向量拼接在一起，馈入 `MLP` 网络中得到最终预测结果。

3. 采用负对数似然函数作为损失函数，即：

  $\mathcal L_{target} = -\frac 1N \sum_{(\mathbf{ x},y)\in \mathbb D}^N (y\times \log p(\mathbf x) + (1-y) \times \log (1-p(\mathbf x)))$

   其中：

   -$\mathbf x = [\mathbf{ x}_p,\mathbf{ x}_b,\mathbf{ x}_a,\mathbf{ x}_c] \in \mathbb D$，$\mathbb D$为训练集，$N$为训练集大小。
   -$y\in \{0,1\}$表示用户是否点击目标商品。
   -$p(\mathbf x)$为模型预测用户点击目标商品的概率。

#### 13.1.2 DIEN

1. 与搜索广告不同，在线展示广告系统中，用户并不能清楚的表明其意图，因此捕获用户的兴趣及其动态变化对于 `CTR` 预估模型非常重要。

   `DIEN` 致力于捕获用户兴趣，并为用户兴趣演变过程建模。`DIEN` 主要由四部分组成：`Embedding` 层、`Interest Extractor Layer` 兴趣抽取层、`Interest Evolving Layer` 兴趣演化层、`MLP` 网络。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/32THEdNPfGJi.png?imageslim">
   </p>
   

#### 13.1.3 兴趣抽取层

1. 在电商领域，用户行为是用户潜在兴趣的载体，而用户兴趣将导致用户行为的一系列变化。而兴趣抽取层就是从用户行为序列中提取背后的一系列兴趣状态。

   在电商领域用户的行为非常丰富，即使在很短时间内(如两周)，用户历史行为序列的长度也很长。为了在效率和性能之间平衡，论文采用 `GRU` 对行为之间的依赖关系建模：

   其中：

   -$\sigma$为 `sigmoid` 激活函数，$\odot$为逐元素积。

   -$\mathbf W^u,\mathbf W^r,\mathbf W^h\in \mathbb R^{ n_H \times n_I}, \mathbf U^z,\mathbf U^r,\mathbf U^h\in \mathbb R^{n_H\times n_H}$为参数矩阵。其中$n_H$为隐向量维度，$n_I$为输入维度。

    $\mathbf{\vec b}^u,\mathbf{\vec b}^r,\mathbf{\vec b}^h\in \mathbb R^{n_H}$为偏置参数。

   -$\mathbf{\vec i}_t = \mathbf{ e}_b[t]$为用户行为序列中的第$t$个行为的 `embedding`，它作为 `GRU` 的输入；$\mathbf{\vec h}_t$为第$t$个隐状态。

2. 实际上隐状态$\mathbf{\vec h}_t$无法有效的表达用户兴趣。由于目标商品的点击是由最终兴趣触发的，因此损失函数$\mathcal L$中使用的 `label` 仅仅监督了最后一个兴趣状态$\mathbf{\vec h}_{T}$，历史兴趣状态$\mathbf{\vec h}_{t},t\le T$没有得到合适的监督。

   在电商领域，用户每一步的兴趣状态都将导致连续的行为，因此 `DIEN` 提出辅助损失。它利用第$t+1$步的输入（即第$t+1$个行为）来监督第$t$步的兴趣状态$\mathbf{\vec h}_t$的学习。

   除了采用下一个实际产生行为的商品作为正样本之外，`DIEN` 还从正样本之外采样了一些商品作为负样本。因此得到$N$对行为`embedding` 序列：

  $\{\mathbf{ e}_b^i,\hat{\mathbf{ e}}_b^i\}\in \mathbb D_\mathcal B,i=1,2,\cdots,N$

   其中：

   -$i$为训练样本编号，$N$为训练样本总数。

   -$\mathbf{ e}_b^i\in \mathbb R^{T\times n_E}$表示用户的历史行为序列，$T$为用户历史行为序列长度，$n_E$为行为 `embedding` 维度。

    $\mathbf e_b^i[t]\in \mathcal G$表示用户$i$历史行为序列的第$t$个商品的 `embedding` 向量，$G$表示全部的商品集合。

   - 而$\hat{\mathbf{ e}}_b^i$表示负样本采样序列。

    $\hat {\mathbf e}_b^i[t] \in G- \mathbf e_b^i[t]$表示从用户$i$历史行为序列第$t$个商品以外的所有商品中采样得到的商品的 `embedding`。

   辅助损失函数为：

  $\mathcal L_{aux} = -\frac 1N\left[\sum_{i=1}^N\sum_t\log \sigma(\mathbf{\vec h}_t^i,\mathbf e_b^i[t+1])+ \log(1-\sigma(\mathbf{\vec h}_t^i,\mathbf {\hat e}_b^i[t+1]))\right]$

   其中：

   -$\sigma(\mathbf{\vec x}_1,\mathbf{\vec x}_2) = \frac{1}{1+\exp(- \mathbf{\vec x}_1\cdot \mathbf{\vec x}_2)}$为 `sigmoid` 激活函数。
   -$\mathbf{\vec h}_t^i$表示用户$i$的第$t$个隐状态。

   考虑辅助损失之后，`DIEN` 模型的整体目标函数为：

  $\mathcal L = \mathcal L_{target} + \alpha \times \mathcal L_{aux}$

   其中：$\mathcal L_{target}$为模型的主损失函数；$\alpha$为超参数，用于平衡兴趣表达和`CTR` 预测。

3. 通过引入辅助函数，每个隐状态$\mathbf{\vec h}_t$就具有足够的表达能力来表达行为$\mathbf{ e}_b[t]$背后的兴趣。所有$T$个兴趣状态$[\mathbf{\vec h}_1,\mathbf{\vec h}_2,\cdots,\mathbf{\vec h}_T]$一起构成了兴趣序列，从而作为兴趣演化层的输入。

4. 引入辅助函数具有多个优点：

   - 从兴趣学习的角度看，辅助损失函数的引入有助于`GRU` 的每个隐状态学到正确的兴趣表示。
   - 从 `GRU` 优化的角度看，辅助函数的引入有助于缓解 `GRU` 的长距离依赖问题，降低反向传播的难度。
   - 还有不怎么重要的一点：辅助损失函数为 `embedding` 层的学习提供了更多的语义信息，从而产生更好的 `embedding` 表达。

#### 13.1.4 兴趣演化层

1. 随着外部环境和用户自身认知的共同作用，用户的各种兴趣随着时间的推移也在不断演化。

   以用户对衣服的兴趣为例，随着人口趋势和用户品味的变化，用户对衣服的偏好也在改变。这种兴趣演化过程直接决定了候选衣服商品的点击率预测。

   因此对兴趣演化过程进行建模，有以下优点：

   - 可以为最终兴趣表达提供更多的历史信息
   - 可以更好的预测目标商品的`CTR`

2. 在演化过程中，用户兴趣呈现两个特性：

   - 由于兴趣的多样性，兴趣可能会发生漂移。

     兴趣漂移对用户行为产生的影响是：用户可能在某个时间段对书籍感兴趣，但是另一个时间段可能对衣服感兴趣。

   - 尽管兴趣之间会相互影响，但是每种兴趣都有自己的演化过程。如：书籍和衣服的兴趣演化过程几乎是独立的。

     我们只关注和目标商品相关的兴趣演化过程。

3. 在兴趣抽取层，借助辅助损失我们获得了兴趣序列的表达形式。结合兴趣演化过程的特点，`DIEN` 结合了注意力机制的局部激活能力以及`GRU` 的序列学习能力来建模兴趣演化过程。在 `GRU` 的每个时间步，局部激活都可以强化相关兴趣的影响，并减弱无关兴趣（兴趣漂移）的干扰。这有助于目标商品相关的兴趣演化过程的建模。

   令$\mathbf{\vec i}_t^\prime,\mathbf{\vec h}_t^\prime$为兴趣演化模块的输入向量和隐向量。其中：

   - 兴趣演化模块的输入就是兴趣抽取模块的隐向量：$\mathbf{\vec i}_t^\prime = \mathbf{\vec h}_t$。
   - 最后一个隐向量$\mathbf{\vec h}_T^\prime$就是最终的兴趣状态。

   注意力得分函数定义为：

  $a_t = \frac{\exp(\mathbf{\vec h}_t \mathbf W \mathbf{\vec e}_a)}{\sum_{j=1}^T \exp(\mathbf{\vec h}_j\mathbf W \mathbf{\vec e}_a)}$

   其中：

   -$\mathbf{\vec e}_a \in \mathbb R^{n_A}$是广告 `ad`各 `field` 的 `embedding` 向量的拼接向量，$n_A$为拼接向量维度。
   -$\mathbf W\in \mathbb R^{n_H\times n_A}$为参数矩阵，$n_H$为隐向量维度。

   注意力得分反映了广告 `a` 和输入的潜在兴趣$\mathbf{\vec h}_t$之间的关系，关系约紧密则得分越高。

4. 有多种注意力机制来建模兴趣演化过程。

   - `GRU with attentional input : AIGRU` ：最简单直接的方式是采用注意力得分来影响兴趣演化层的输入，这被称作 `AIGRU` 。

    $\mathbf{\vec i}_t^\prime = \mathbf{\vec h}_t\times a_t$

     理想情况下，和目标商品相关性较低的输入可以被降低到 0 。

     但是 `AIGRU` 效果不是很好，因为即使是零输入也可以改变 `GRU` 的隐状态。即：即使相对于目标商品的兴趣较低，也会影响后面兴趣演化过程的学习。

   - `Attention based GRU : AGRU`：通过使用注意力得分来替代 `GRU` 的更新门，并直接更改隐状态：

    $\mathbf{\vec h}_t^\prime = (1-a_t)\times \mathbf{\vec h}_{t-1}^\prime + a_t \times \tilde {\mathbf{\vec h}}_t^\prime$

     `AGRU` 将注意力机制嵌入到 `GRU`中，从而降低了兴趣演化过程中与目标商品无关兴趣的影响，克服了 `AIGRU` 的缺陷。

   - `GRU with attentional update gate : AUGRU`：在 `AGRU` 中我们用一个标量$a_t$替代了更新门向量$\mathbf{\vec u}_t$，这会忽略不同维度的差异。因此可以考虑通过注意力得分调整更新门：

     其中$\mathbf{\vec u}_t^\prime$为原始更新门。

     这里我们保留更新门的维度差异，并对每个维度按照注意力得分进行统一缩放。这使得目标商品无关兴趣对于隐状态影响较小。

     `AUGRU` 有效避免了兴趣漂移带来的影响，并推动目标商品相关的兴趣平稳演化。

### 13.2 实验

1. 数据集：

   - 公共数据集 `public Dataset`：`Amazon` 数据集。该数据集由商品评论和商品元数据组成。我们使用该数据集的两个子集：`Books, Electronics` 。

     在这些数据集中我们将评论视为行为，并按照时间对用户的评论排序。假设用户$u$有$T$个行为，我们的目标是使用前$T-1$个行为来预测用户$u$是否会点评第$T$个评论中的商品。

   - 工业数据集 `Industrial Dataset`：由淘宝在线展示广告系统中的曝光日志和点击日志组成。

     我们将连续 `50` 天点击的广告作为样本，用户点击目标商品之前 `14`天的点击行为作为历史行为序列。其中前 `49` 天的样本作为训练集，第 `50` 天的样本作为测试集。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/DtXJ8Ap5Xpk6.png?imageslim">
   </p>
   

2. 对照模型为`BaseModel`、`Wide&Deep`、`PNN`、`DIN`、`Two layer GRU Attention`。其中：

   - `BaseModel` 和 `DIEN` 具有同样的 `embedding` 和 `MLP` 以及对应的参数配置，但是`BaseModel` 使用 `sum pooling` 来聚合所有用户的行为`embedding` 。
   - `Wide&Deep` 的 `deep part` 和 `BaseModel` 相同。
   - `Two layer GRU Attention` 通过双层 `GRU` 来建模用户行为序列，并采用注意力机制来关注目标商品相关的行为。

#### 13.2.1 离线结果

1. 在 `public dataset` 上的离线评估结果如下所示。评估指标为离线 `AUC` ，每个实验重复 `5` 次并取均值和标准差。结论：

   - 人工设计特征的 `Wide&Deep` 性能不佳。

   - 自动特征交叉的 `PNN` 效果相对较好。

   - 旨在捕获兴趣的模型可以明显改善 `AUC`：

     - `DIN` 激活了相对于目标广告的用户兴趣

     - `Two Layer GRU Attention`进一步有效的捕获了兴趣序列中的目标广告的相关兴趣

     - `DIEN` 不仅可以有效捕获兴趣序列，还可以对与目标商品相关的兴趣演化过程建模。

       兴趣演化模型可以帮助 `DIEN`获取更好的兴趣表达，并准确捕获兴趣的动态演化从而大幅提升性能。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/S3XuQ3XWkxRo.png?imageslim">
   </p>
   

2. 在 `Industrial Dataset` 工业数据集上，`DIEN` 的 `MLP` 部分使用 6 个 `FCN` 层，维度分别为 `600,400,300,200,80,2` ；历史行为序列的最大长度为 `50` 。结论：

   - `Wide&Deep,PNN` 比 `BaseModel` 效果更好。
   - 与 `Amazon` 数据集只有一种商品不同，阿里在线广告数据集同时包含多种类型的商品，因此基于注意力的方法可以像 `DIN` 一样大幅提升性能。
   - `DIEN` 捕获了目标商品相关的兴趣演化过程，效果最好。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/K374PzGuokJV.png?imageslim">
   </p>
   

3. 不同兴趣演化方式的比较如下所示。评估指标为离线 `AUC` ，每个实验重复 `5` 次并取均值和标准差。结论：

   - 与 `BaseModel` 相比`Two layer GRU Attention`获得了改进，但是它缺乏对兴趣演化进行建模。
   - `AIGRU` 直接对兴趣演化建模。尽管它效果得到提升，但是注意力和演化过程之间的分裂使得建模过程中丢失了注意力的信息。
   - `AGRU` 进一步试图融合注意力和演化过程，但是它没有充分利用更新门针对多个维度更新的优势。
   - `AUGRU` 效果最好，它理想的融合了注意力机制和序列学习，有效捕捉了目标商品相关兴趣的演化过程。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/aFBhywmOqXr7.png?imageslim">
   </p>
   

4. 在使用 `AUGRU` 的基础上我们进一步评估辅助损失的效果。

   - 公共数据集中，用于辅助损失的负样本都是从当前评论商品之外的商品集中随机抽样的。
   - 工业数据集中，用于辅助损失的负样本是所有已曝光但是未点击的商品。

   从损失函数的趋势可见：整体损失函数$\mathcal L$和辅助损失$\mathcal L_{aux}$都保持相似的下降趋势。这意味着 `CTR` 预测过程中，整体损失和辅助损失对于兴趣表达都起作用。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/EUEya2X8xENz.png?imageslim">
   </p>
   

   最终效果如下表所示：

   - 辅助损失对于两个公共数据集都带来了很大改进。它反应了监督信息对于学习兴趣序列和 `embedding` 表示的重要性。

   - 辅助损失对于工业数据集的改进不如公共数据集明显，区别在于以下几点：

     - 工业数据集具有大量的样本来学习 `embedding` 层，这使得它从辅助损失中获得的收益更少。

     - 不同于公共数据集的商品全部属于同一个类别，工业数据集包含阿里所有场景的所有类别商品。

       辅助损失的目标是预测单场景，因此这和工业数据集不匹配。这导致辅助损失对于工业数据集的效果较小。此时 `AUGRU` 的效果得到得到增强。

      <p align="center">
         <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/c35KfKkarGn5.png?imageslim">
      </p>
     

5. `AUGRU` 中隐状态的动态变化可以反映出兴趣演化过程，我们可以将这些隐状态可视化从而探索不同目标商品对于兴趣演化的影响。

   我们通过 `PCA` 将 `AUGRU` 的隐状态投影到二维空间中，投影对应的隐状态按照行为发生的顺序连接。其中历史行为来自于以下类别：电脑音箱、耳机、汽车 `GPS`、`SD`卡、`micro SD` 卡、外置硬盘、手机套。

   - 图`a` 给出了不同目标商品的隐状态移动路径。
     - 无目标商品的黄色曲线表示原始兴趣序列（所有注意力得分都相等），它表示不受目标商品影响的兴趣演化。
     - 蓝色曲线表示目标商品为屏幕保护类别商品激活的隐状态，目标商品与所有历史行为相关性较小，因此表现出类似黄色曲线的路径。
     - 红色曲线表示目标商品为手机套类别商品激活的隐状态，目标商品和历史行为密切相关，因此导致较长的路径。
   - 图`b` 中，和目标商品相关的历史行为获得了更大的注意力得分。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/ORhr6y8iYQAw.png?imageslim">
   </p>
   

#### 13.2.2 在线实验

1. 论文在淘宝的展示广告系统进行在线 `A/B testing` ，测试时间从 `2018-06-07 ~ 2018-07-02` ，测试结果如下。

   与 `BaseModel` 相比，`DIEN` 的 `CTR` 提升 `20.7%`、`eCPM` 提升 `17.1%` 、`pay per click:PPC` 降低 `3%` 。

   目前 `DIEN` 已经在线部署并服务于主要流量。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/s8eclm2H04Ex.png?imageslim">
   </p>
    

2. `DIEN` 的在线服务 `online serving` 是一个巨大挑战。在线系统流量非常高，峰值可超过 `100` 万用户/秒。为保证低延迟和高吞吐量，论文采用了几种重要技术来提高 `serving` 性能：

   - `element parallel GRU` 以及 `kernel fusion`：融合尽可能多的独立 `kernel` 计算，同时 `GRU` 隐状态的每个元素并行计算（元素级并行）。
   - `Batching`：不同用户的相邻请求合并为一个 `batch`，从而充分利用 `GPU` 的并行计算能力。
   - 模型压缩：通过 `Rocket Launching` 方法来压缩模型从而训练一个更轻量级的网络，该网络尺寸更小但是性能接近原始大模型。

   通过这些技术，`DIEN serving` 的延迟可以从 `38.2ms` 降低到 `6.6ms` ，每个 `worker` 的 `query per second:QPS` 可以达到 `360` 。

## 十四、DSIN

1. `CTR` 预测任务中，一个持续研究的主题是：如何从用户行为序列中捕获用户不断变化的兴趣。

   大多数现有的研究都忽略了用户行为序列的内在结构：用户行为序列是由会话 `session` 组成的。其中会话是在给定时间内用户行为的列表。

   - 会话是根据时间来划分的。如：如果两次用户行为超过一定时间间隔（如 1小时），则它们分别属于两个独立的会话。
   - 同一个会话中的用户行为是高度同构的，不同会话中的行为是高度异构的。

   如下图所示，论文从淘宝中抽取了一名真实用户，根据其行为序列发生的时间划分为三个会话，每个会话的时间间隔超过30分钟。

   - 横坐标为时间，纵坐标为会话`ID` 。图片下方的数字为时间，代表了点击当前商品和点击第一个商品之间的时间间隔（单位秒）。
   - 用户在`Session 1` 中浏览裤子、在 `Session 2` 中浏览戒指、在 `Session 3` 中浏览外套。

   这个现象是普遍的，它反映了一个事实：

   - 用户通常在一个会话中具有明确的特定意图。
   - 当用户开始新的会话时，用户兴趣可能会急剧变化（相对于旧会话的用户兴趣）。

   基于此现象，论文 `《Deep Session Interest Network for Click-Through Rate Prediction》` 提出了模型 `Deep Session Interest Network：DSIN` ，该模型利用用户的历史会话来建模用户行为序列。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/CP8MCgGpM7sy.png?imageslim">
   </p>
   

### 14.1 模型

1. `DSIN` 包含三个关键部分：

   - 第一部分：将用户行为划分为会话，然后使用 `bias encoding` 的自注意力机制对每个会话建模。

     自注意力机制可以捕获会话内部的相关性，然后提取用户在每个会话的兴趣。这些不同的会话兴趣可能会彼此相关，甚至会遵循某种序列模式。

   - 第二部分：利用双向 `LSTM` 来捕获用户不同的历史会话兴趣的交互和演变。

   - 第三部分：考虑到不同的会话兴趣对目标商品有不同的影响，论文设计了局部激活单元来聚合这些会话兴趣，从而生成用户行为序列的最终表达。

2. `BaseModel` 基准模型：基准模型采用 `Embedding &MLP` 结构，其中：

   - 特征：`BaseModel` 中采用三组特征，每组特征都由一些稀疏特征构成，包括：

     - 用户画像：包括性别、城市等。
     - 商品画像：包含商品`id`、卖家 `id` 、品牌 `id` 等。
     - 用户历史行为：包含用户最近点击商品的商品 `id`。注意：商品的附带信息可以拼接起来从而表示商品。

   - `embedding`：通常我们采用 `embedding` 技术来将稀疏特征映射到低维的 `dense vector` 。

     - 用户画像的 `embedding` 为$\mathbf X^U \in \mathbb R^{N_u\times d_{model}}$，其中$N_u$表示用户画像的稀疏特征数量，$d_{model}$为`embedding` 向量维度。

     - 商品画像的 `embedding` 为$\mathbf X^I\in \mathbb R^{N_i\times d_{model}}$，其中$N_i$表示商品画像的稀疏特征数量，$d_{model}$为`embedding` 向量维度。

     - 用户历史行为的 `embedding`为：

      $\mathbf S=[\mathbf{\vec b}_1\cdots,\mathbf {\vec b}_i,\cdots,\mathbf{\vec b}_{T}]\in \mathbb R^{T\times d_{model}}$

       其中$T$为用户历史行为数量，$\mathbf{\vec b}_i$为第$i$个行为的 `embedding` 向量，$d_{model}$为`embedding` 向量维度。

   - `MLP`：我们将用户画像`embedding`、商品画像`embedding`、用户历史行为`embedding` 拼接、展平然后馈入`MLP` 网络中。

     在 `MLP` 中，我们采用 `ReLU` 激活函数并使用 `softmax` 输出单元来预测用户点击商品的概率。

   - 损失函数：我们采用负的对数似然作为损失函数：

    $\mathcal L = - \frac 1 N \sum_{(\mathbf x,y)\in \mathbb D} (y\times \log p(\mathbf x) + (1-y) \times \log (1-p(\mathbf x)))$

     其中：

     -$\mathbb D$为训练集，$N$为样本数量
     -$\mathbf x = [\mathbf X^U,\mathbf X^I,\mathbf S]$为样本经过 `embedding`的特征，$y\in \{0,1\}$标记用户是否点击，$p(\cdot)$表示网络预测用户点击的概率。

3. 由于用户在不同的会话中表现出不同的兴趣，以及用户不同会话的兴趣之间存在关联，因此`DSIN` 采用专门的组件来提取会话兴趣、捕获会话兴趣序列的关系。

   `DSIN` 的基础架构类似 `BaseModel` 的 `Embedding&MLP` 架构，但是`DSIN` 模型在 `Embedding` 和 `MLP` 之间还有四个`layer`，从上到下依次为：

   - 会话划分层 `session division layer`：将用户行为序列划分为不同的会话。
   - 会话兴趣提取层 `sessioin interest extractor layer`：抽取用户的会话兴趣。
   - 会话兴趣交互层 `sesion interest interaction layer` ： 捕获用户的会话兴趣序列关系。
   - 会话兴趣激活层 `session interest activating layer`： 采用局部激活单元来建模会话兴趣对目标商品的影响。

   最后会话兴趣激活层的输出和用户画像`embedding`、商品画像`embedding` 一起馈入 `MLP` 来执行预测。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/uzPpCb2yn2FS.png?imageslim">
   </p>
   

#### 14.1.1 会话划分层

1. 为了更精确的抽取用户的会话兴趣，我们将用户行为序列$\mathbf S=[\mathbf{\vec b}_1\cdots,\mathbf {\vec b}_i,\cdots,\mathbf{\vec b}_{T}]$划分为会话序列$\mathbf Q$，其中第$k$个会话为：

  $\mathbf Q_k = [\mathbf{\vec b}_{S_{k-1}+1},\cdots,\mathbf{\vec b}_{S_{k-1}+i},\cdots,\mathbf{\vec b}_{S_{k-1}+T_k}] \in \mathbb R^{T_k\times d_{model}}$

   其中：

   -$T_k$为第$k$个会话中的行为数量，$S_k= \sum_{i=1}^k T_{k}$表示从第$1$到第$k$个会话的累积行为数量。
   -$\mathbf{\vec b}_{S_{k-1}+i}$表示第$k$个会话中第$i$个用户行为`embedding` 向量。

2. 通常拆分依据是：如果两个相邻的用户行为时间间隔超过，比如说30分钟，则将它们划分到两个会话中。否则它们同属于同一个会话。

   这个时间间隔根据不同的应用场景而选择不同的值。

#### 14.1.2 会话兴趣提取层

1. 同一会话的行为彼此密切相关，但是也有一些偶然行为与会话整体行为关系不大。

   为了捕获同一会话行为之间的内在关系，并且减少无关行为的影响，论文在每个会话中采用 `multi-head self-attention`机制。

   另外，论文还对`self-attention` 机制做了一些改进，从而更好地实现会话兴趣提取这一目标。

2. `Bias Encoding`：自注意力机制将位置编码应用于 `input embedding` ，但是在 `DSIN` 中我们还需要捕获会话之间的位置关系和`bias`，因此论文提出了`bias encoding` 来融合`session bias`、`posision bias`、`unit bias` 。为了讨论方便，假设所有会话的序列长度均为$\tilde T$、会话数量为$K$。则 `bias encoding` 为：

  $BE_{k,t,c} = w_k^K+w_t^{\tilde T} +w_c^C$

   其中：

   -$\mathbf {\vec w}^K \in \mathbb R^K$是会话的 `bias`向量；$w_k^K$为第$k$个分量，代表第$k$个会话的 `bias` ；$K$为会话总数。
   -$\mathbf {\vec w} ^{\tilde T} \in \mathbb R^{\tilde T}$为用户行为的 `bias` 向量；$w_t^{\tilde T}$为第$t$个分量，代表会话$k$中第$t$个用户行为的 `bias` ；$\tilde T$为会话$k$中用户行为总数 。
   -$\mathbf {\vec w} ^C \in \mathbb R^{d_{model}}$为用户行为`embedding` 的 `unit` 的`bias`向量；$w_c^C$为第$c$个分量，代表`embedding` 向量第$c$个`unit` 的 `bias`；$C$等于$d_{model}$。

   采用`bias encoding` 之后，用户的会话更新为：

  $\mathbf Q = \mathbf Q + \mathbf{BE}$

   其中$\mathbf {BE} \in \mathbb R^{K\times \tilde T\times C}$。

3. `Multi-head self-attention`：在推荐系统中，用户点击行为会受到多种因素的影响（如颜色、款式、价格）。`multi-head self-attention` 可以捕获这类不同的表示子空间。

   假设将第$k$个会话沿着维度方向拆分为$H$个 `head`：

  $\mathbf Q_k = \begin{bmatrix}
   \mathbf Q_{k,1}\\
   \vdots\\
   \mathbf Q_{k,h}\\
   \vdots\\
   \mathbf Q_{k,H}
   \end{bmatrix}$

   其中：$\mathbf Q_{k,h} = [\mathbf{\vec b}_{S_{k-1}+1}^h,\cdots,\mathbf{\vec b}_{S_{k-1}+i}^h,\cdots,\mathbf{\vec b}_{S_{k-1}+T_k}^h] \in \mathbb R^{T_k\times d_{H}}$表示第$h$个 `head`的输入；$H$为`head` 数量，$d_H = \frac{d_{model}}{H}$；$\mathbf{\vec b}_{S_{k-1}+i}^h\in \mathbb R^{d_H}$表示$\mathbf{\vec b}_{S_{k-1}+i}\in \mathbb R^{d_{model}}$的第$h$个`head` 。

   则第$h$个`head` 的输出为：

  $\mathbf{head}_{k,h} = \text{Attention}(\mathbf Q_{k,h}\mathbf W^Q, \mathbf Q_{k,h}\mathbf W^K,\mathbf Q_{k,h}\mathbf W^V)\\
   =\text{softmax}\left(\frac{\mathbf Q_{k,h}\mathbf W^Q(\mathbf Q_{k,h}\mathbf W^K)^T}{\sqrt{d_{model}}}\right)\mathbf Q_{k,h}\mathbf W^V$

   其中$\mathbf W^Q,\mathbf W^K,\mathbf W^V$为对应的映射矩阵。

   - 拼接不同 `head` 的输出然后馈送到一个前馈神经网络：

    $\mathbf I_k^Q = FFN(Concat(\mathbf{head}_{k,1},\cdots,\mathbf{head}_{k,H})\mathbf W^O)$

     其中$FFN(\cdot)$为前馈神经网络，$\mathbf W^O$为一个映射矩阵。

     在前馈神经网络中，论文也采用了残差连接和 `layer normalization` 。

   - 对输出的一组向量进行池化操作，则得到第$k$各会话兴趣为：

    $\mathbf{\vec I}_k= \text{Avg}(\mathbf I_k^Q)$

     其中$\text{Avg}(\cdot)$为平均池化操作。

   注意：不同会话的`self-attention` 机制中的权重是共享的。

#### 14.1.3 会话兴趣交互层

1. 针对会话兴趣的动态变化建模可以丰富会话兴趣的表示。论文通过 `Bi-LSTM` 对 `DSIN` 中的会话兴趣交互进行建模：

  $\mathbf{\vec i}_t = \sigma(\mathbf W_{x,i} \mathbf {\vec I}_t + \mathbf W_{h,i}\mathbf{\vec h}_{t-1} + \mathbf W_{c,i}\mathbf{\vec c}_{t-1} + \mathbf{\vec b}_i)\\
   \mathbf{\vec f}_t = \sigma(\mathbf W_{x,f} \mathbf {\vec I}_t + \mathbf W_{h,f}\mathbf{\vec h}_{t-1} + \mathbf W_{c,f}\mathbf{\vec c}_{t-1} + \mathbf{\vec b}_f)\\
   \mathbf{\vec c}_t = \mathbf{\vec f}_t\odot \mathbf{\vec c}_{t-1} + \mathbf{\vec i}_i\odot\tanh(\mathbf W_{x,c}\mathbf{\vec I}_t + \mathbf W_{h,c}\mathbf{\vec h}_{t-1} + \mathbf{\vec b}_c)\\
   \mathbf{\vec o}_t = \sigma(\mathbf W_{x,o}\mathbf{\vec I}_t + \mathbf W_{h,o}\mathbf{\vec h}_{t-1} + \mathbf W_{c,o}\mathbf{\vec c}_t + \mathbf{\vec b}_o)\\
   \mathbf{\vec h}_t = \mathbf{\vec o}_t\odot \tanh(\mathbf{\vec c}_t)$

   其中$\sigma(\cdot)$为`sigmoid`函数；$\mathbf{\vec i,\vec f,\vec o,\vec c}$为输入门、遗忘门、输出门、`cell` 向量，其维度和$\mathbf{\vec I}_t$维度相同。

   上述只是前向状态，考虑到前向、反向两个方向，隐向量计算为：

  $\mathbf{\vec H}_t = [\mathbf{\vec h}_{ft},\mathbf{\vec h}_{bt}]$

   其中$\mathbf{\vec h}_{ft}$为 `Bi-LSTM` 前向隐向量，$\mathbf{\vec h}_{bt}$为 `Bi-LSTM` 反向隐向量。

#### 14.1.4 会话兴趣激活层

1. 和目标商品相关性更强的会话兴趣对于用户的点击意愿的影响更大。因此对于每个目标商品，用户的会话兴趣需要重新分配权重。这里论文采用注意力机制分别将会话兴趣提取层、会话兴趣交互层的输出向量序列进行注意力融合：

   - 会话兴趣提取层：

    $a_{k}^I = \frac{\exp\left(\mathbf{\vec I}_k \cdot (\mathbf W^I {\mathbf {\vec p} })\right)}{\sum_{k}^K \exp\left(\mathbf{\vec I}_k \cdot (\mathbf W^I {\mathbf {\vec p}})\right)}\\
     \mathbf{\vec U}^I = \sum_{k}^K a_{k}^I \mathbf{\vec I}_k$

     其中$\mathbf{\vec p}$为目标商品的 `embedding` 向量，$\mathbf W^I$为对应权重，$a_k^I$为第$k$个会话的会话兴趣对目标商品的注意力得分。

   - 会话兴趣交互层：

    $a_{k}^H = \frac{\exp\left(\mathbf{\vec H}_k \cdot (\mathbf W^H {\mathbf {\vec p} })\right)}{\sum_{k}^K \exp\left(\mathbf{\vec H}_k \cdot (\mathbf W^H {\mathbf {\vec p} })\right)}\\
     \mathbf{\vec U}^H = \sum_{k}^K a_{k}^H \mathbf{\vec H}_k$

     其中$\mathbf{\vec p}$为目标商品的 `embedding` 向量，$\mathbf W^H$为对应权重，$a_k^h$为第$k$个会话的会话交互结果对目标商品的注意力得分。

   注意：这里计算注意力得分时没有进行维度缩放处理，标准的注意力得分为：

  $\text{score} = \frac{\mathbf{\vec q}\cdot \mathbf{\vec k}}{\sqrt{d_k}}$

2. 将用户画像`embedding` 向量、商品画像`embedding` 向量、$\mathbf{\vec U}^I$、$\mathbf{\vec U}^H$一起拼接作为 `MLP` 层的输入。

### 14.2 实验

1. 数据集：

   - `Advertising Dataset`：由阿里妈妈发布的公共数据集。数据集包含连续 `8` 天的用户曝光、点击日志，共覆盖`100`万用户、`80`万广告，包含`2600`万条记录。数据集中记录了用户最近的 `200` 个历史用户行为。

     我们采用前 `7` 天的数据为训练集，第八天数据为测试集。

   - `Recommender Dataset`：阿里巴巴线上推荐系统数据集。该数据集包含连续 `8` 天的 `60` 亿个曝光、点击日志，分别有 `1` 亿用户和 `7000` 万商品。数据集中也记录了用户最近的 `200` 个历史用户行为。

     我们采用前 `7` 天的数据为训练集，第八天数据为测试集。

2. 比较模型：`YoutubetNet、Wide&Deep、DIN、DIN-RNN、DIEN` 。其中 `DIN-RNN` 和 `DIN` 结构相似，但是 `DIN-RNN` 采用 `Bi-LSTM` 对用户的历史行为进行建模。

3. 评估指标：`AUC` 。

4. 模型评估结果如下表。其中：

   - `YoutubeNet-NO-UB` 表示不带用户行为信息的 `YoutubeNet`
   - `DSIN-PE`表示带 `positional encoding` 的 `DSIN`
   - `DSIN-BE-No-SIIL` 表示带 `bias encoding`但是没有会话兴趣交互层及对应的激活单元的 `DSIN`
   - `DSIN-BE` 表示带`bias encoding`以及其它完整结构的 `DSIN`

   结论：

   - 由于用户行为的原因，`YoutubeNet`的表现要优于 `YoutubeNet-No-User-Behavior`

   - `Wide&Deep` 由于结合了 `wide` 侧的记忆功能从而获得更好的结果。

   - 由于用户行为序列的不连续性，两个数据集中的 `DIN-RNN` 结果都比 `DIN` 差。

   - `DIEN` 获得更好的结果，但是辅助损失和经过特殊设计的 `AUGRU` 产生了对行为原始表达的偏离 `deviating`。

   - `DSIN` 在两个数据集上获得最佳结果。

     `DSIN` 将用户的历史行为提取到会话兴趣层，并对会话兴趣的动态演变过程进行建模。这两者都丰富了用户的表达。

     另外，`DSIN` 的本地激活单元有助于获得针对目标商品的用户会话兴趣的自适应表达。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/9ud1SWuXIp7I.png?imageslim">
   </p>
   

5. 如上表所示，`DIN-RNN` 在`DIN` 基础上增加了序列建模，但是效果比 `DIN` 差；而 `DSIN-BE` 也在`DSIN-BE-NO-SIIL` 基础上增加了序列建模，但是效果比 `DSIN-BE-NO-SIIL` 好。

   为什么同样增加了序列进行建模，而前者效果差后者效果好？

   - 在 `DIN-RNN` 中，行为的快速跳变和突然结束导致用户的行为序列噪声大。这将导致 `RNN` 中信息传输过程中有效信息丢失，从而进一步扰乱了用户行为序列的正确表达。

   - 在 `DSIN` 中，我们将用户行为序列划分为多个会话，其中：

     - 用户行为在每个会话中通常都是同质的
     - 用户的会话兴趣遵循序列模式，因此更适合序列建模

     由于这两个因素，`DSIN` 提高了性能。

6. 如上表所示，`DSIN-BE` 比 `DSIN-BE-NO-SIIL` 仅仅多了会话兴趣交互层及对应的激活单元。结果表明 `DSIN-BE` 效果更好。

   通过会话兴趣交互层，用户的会话兴趣和上下文信息（历史兴趣上下文，如上一个兴趣、上上一个兴趣等等）融合在一起从而更具有表达能力，从而提高了 `DSIN` 的性能。

7. 如上表所示，`DSIN-BE` 和`DSIN-PE` 相比，前者采用`Bias Encoding`后者采用 `Position Encoding` 。结果表明 `DSIN-BE` 效果更好。

   与二维的位置编码不同，`Bias Encoding`除了捕获位置`bias` 还能够捕获会话`bias`。

   经验上讲，`Bias Encoding` 可以成功捕获会话的次序信息从而提高 `DSIN` 性能。

8. 如下图所示，我们给出了局部激活单元和自注意力机制中的注意力权重。图的下半部分给出了自注意力机制的权重、上半部分给出了激活单元的权重，颜色越深权重越大。

   注意：自注意力机制的注意力权重是每个`head` 的注意力权重之和。

   - 我们以第一个会话为例：用户主要浏览与裤子有关的商品，偶尔浏览外套有关的商品。

     可以看到：和裤子有关商品的权重通常都较高。经过自注意力机制之后，大部分和裤子有关的行为表达都被保留并提取到会话兴趣中。

   - 局部激活单元的工作方式是：使得与目标商品有关的用户会话兴趣更突出。

     图中的目标商品是黑色裤子，因此模型为与裤子相关的用户会话兴趣分配更大的权重，从而使得这部分会话兴趣对结果预测的影响更大。

   - 第三个会话与外套有关，但是用户对黑色的颜色偏好也有助于对目标黑色裤子的点击率预测。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/cgFmbmozr1z4.png?imageslim">
   </p>
   

## 十五、DICM

1. 淘宝的每个商品通常会展示一个商品图片以及相关的一小段描述文字。当用户对商品感兴趣时，用户可以点击图片从而查看更多的详情信息。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/gqA6p2Pv3R3U.png?imageslim">
   </p>
   

   用户点击的商品图片被称作行为图片 `behavior image` 。通过引入用户的历史行为图片可以引入用户的视觉偏好，从而增强用户的行为`representation`，有助于 `CTR` 的预测。

2. 利用行为图片的一个简单方式是：为每张图片分配一个图片 `ID`，然后通过模型自动学习这些图片 `ID` 的 `representation` 。

   但是图片 `ID` 仅仅告诉我们：这些图片是不同的。图片`ID` 无法给出图片的语义信息。

   使用图片 `ID` 有两个主要缺点：

   - 对于出现频率非常低的行为图片，其参数无法得到充分的训练，从而使得其 `representation` 表现较差。
   - 在线上预测时，对于新的、从未见过的行为图片，无法得到其 `representation`，泛化能力很差。

   另一种方式是：利用行为图片的语义信息，如经过`CNN` 模型抽取的 `feature map`。

   这种方式可以提供商品本身的视觉描述，带有视觉语义信息，从而为模型带来更强的泛化能力。

   在线上预测时，对于新的、从未见过的行为图片，我们仍然可以抽取其 `feature map` 从而引入模型。 而这些 `feature map` 是模型在训练中曾经见过或者能够处理的。

3. 目前已有一些`DNN` 模型引入商品图片特征，但是它们引入的是广告侧的图片特征而不是用户侧的行为图片特征。这二者区别在于：

   - 广告侧的图片特征：图片作为目标广告的特征，它刻画了广告的视觉信息。

     因为每个目标广告通常只有一张商品图片，所以在训练和预测时每个样本只有一张图片。

   - 用户侧的行为图片特征：图片作为用户的特征，它刻画了用户的视觉偏好信息。

     因为每个用户历史上可能点击很多商品的图片，所以在训练和测试时每个样本有很多张图片。

   事实上，可以同时引入广告侧的商品图片特征和用户侧的用户行为图片特征，从而结合广告视觉特征和用户视觉偏好，进而提高模型 `CTR` 预测能力。

4. 在模型中对用户的行为图片建模非常有挑战性。

   在淘宝中，平均每个用户的行为图片数量在两百多个。考虑到淘宝的用户规模在亿级、商品规模在数十亿（因此对应的商品图片也在数十亿）。因此用户侧的行为图片特征的训练非常困难。

   在传统的参数服务`PS` 架构中，训练图片存储的位置是个难点：

   - 如果图片存储在 `worker` 中则大幅增加了训练数据的规模。在淘宝的业务场景中，每个 `mini-batch` 的数据量从 `134M` 增加到 `5.1G`，增加大约 40倍。这使得网络 `IO` 无法承受。
   - 如果将图片存储在 `server` 中，并在训练期间由 `worker` 访问，则训练时需要传输大量原始图片，所以导致训练过程的通信负载难以接受。

   因此传统的 `PS` 架构不适合该场景。

5. 论文`《 Image Matters: Visually modeling user behaviors using Advanced Model Server 》` 提出深度图像点击模型 `Deep Image CTR Model:DICM`，该模型可以有效的对用户行为图片建模。

   - 论文提出了 `Advanced Model Server:AMS` 这种新的、高效的分布式机器学习架构。

     在 `AMS` 中，前向传播、反向传播也可以发生在 `server` 端，并且 `server` 仅将行为图片的 `feature map` 传递给 `worker` 。相比与原始的行为图片，`feature map` 的数据规模要小得多。这大大降低了通信负载。

   - 在 `AMS` 的基础上，论文提出了 `DICM` 模型来有效对`ID` 类特征、用户行为图片特征一起建模。

     `DICM` 模型充分利用了用户视觉偏好以及广告视觉特征之间的联系，从而大幅提升模型性能。

### 15.1 广告系统

1. 淘宝展示广告系统 `display advertising system` 每天响应数十亿次 `pv` 请求，每个请求都需要系统在几毫秒内从数千万个广告中选择 `eCPM` 排名最高的广告。

   广告系统以类似漏斗的方式完成此任务。系统主要由三个串联的模块组成：

   - `Match` 模块：该模块根据用户行为预测出用户的偏好，并从全量候选广告集中检索大约 `4000` 个广告。

     该模块也被称作检索模块或者召回模块。

   - `Pre-rank` 模块：该模块采用轻量级的 `CTR` 模型将检索到的广告集根据预估的 `CTR` 进一步缩减至大约 `400` 个广告。

     该模块也被称作粗排模块。

   - `Rank` 模块：该模块使用复杂的`CTR`模型准确预测粗排的广告集中每个广告的`CTR`，通过 `eCPM` 对这些广告进行排序并选择最佳的广告。这一步缩减到 `1` 个广告。

     该模块也被称作精排模块。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/iPpvzuMH6tYR.png?imageslim">
   </p>
   

2. `DICM` 模型及其它类似的模型通常都作用在 `Rank` 模块。但论文作者指出：经过调整的 `DICM` 模型也可以应用于 `Pre-rank`模块。

### 15.2 整体结构

1. `DICM` 利用视觉信息扩展了 `Embedding&MLP` 模型。如下所示：用户行为图片`User Behavior Images`和广告商品图片`Ad Image`作为两个特殊的特征。

   - 这些图片首先馈入一个可训练的子模型来得到更高级的`representation` 。类似`embedding`，该子模型将图片`embed` 到一个向量（也可以视作 `feature map` ），因此我们将这个子模型称作 `embedding model` 。
     - 该 `embedding model` 可以认为是传统的 `id embedding` 的扩展，因为它可以对训练期间从未见过的新图片进行`embed` 。
     - 该 `embedding model` 实际上是独立的，它不依赖于其它模块。因此 `embedding model` 可以独立的前向、反向传播。
   - 由于用户的行为图片的数量可变，因此需要将可变数量的图片`embedding` 聚合成固定长度的`user representation`。然后将该固定长度的向量馈入 `MLP` 。
   - 该架构不仅适用于用户行为图片，也适用于用户行为文本、用户行为视频。

    

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/oXy19Ab9esP9.png?imageslim">
   </p>
   

#### 15.2.1 AMS

1. 引入用户行为图片的模型的训练时，主要挑战是大量的图片。图片不仅本身数据量大从而导致存储、通信的巨大开销，而且在提取图片的语义信息时涉及复杂的计算从而导致巨大的计算开销。

   为解决该问题，论文提出了 `AMS` 架构。

2. `AMS` 不仅可以通过`key-value` 来 `lookup`查找 `embedding` 向量，还可以对图片进行 `embed` 。

3. 和 `PS` 架构一样，`AMS` 也包括 `server` 和 `worker`，此外 `AMS` 还包含 `task scheduler` 。

   `AMS` 同时训练两个模型 `worker model`$\mathcal W$和 `embedding model`$\mathcal E$。其中各组件的执行过程为：

   - `Task Scheduler`：

     - 首先初始化模型$\mathcal W,\mathcal E$。
     - 然后执行 `mini-batch` 训练。对于每个 `mini-batch t` ，在所有 `worker` 上执行过程 `WORKERITERATIOIN(t)`。

   - `Worker`：对所有的 `worker` 执行过程 `WORKERITERATIOIN(t)` 。

     对于第 `r` 个 `worker` ($1 \le r\le M$)，`WORKERITERATIOIN`步骤为：

     - 加载第 `t` 个`mini-batch` 的特征、标签：$\mathbf{\vec x}_r^t,y_r^t$。
     - 从 `Server` 请求$\mathbf{\vec x}_r^t$中的 `ID` 和图片的 `embedding`（`SERVEREMBED` 过程） 。其中图片通过图片的、索引来向 `server` 发生请求。
     - 从 `Server` 获取所有的 `embedding` 向量$\mathbf e_r^t$。
     - 基于$\mathbf e_r^t$对$\mathcal W$进行前向传播和反向传播，并得到梯度：
       - `worker` 参数的梯度：$\delta^t_{\mathcal W_r} = \nabla_{\mathbf w}\mathcal L(\mathbf{\vec x}_r^t,y_r^t,\mathbf e_r^t)$。
       - `embedding` 参数的梯度：$\delta_{\mathbf e_r}^t = \nabla_{\mathbf e} \mathcal L(\mathbf{\vec x}_r^t,y_r^t,\mathbf e_r^t)$
     - 向 `Server` 推送梯度$\delta^t_{\mathbf e_r}$，并且执行 `server` 的 `SERVERUPDATE` 过程。
     - 向所有 `worker` 同步梯度$\delta^t_{\mathcal W_r}$并更新模型$\mathcal W$。

   - `Server`：对所有的 `Server` 执行过程 `SERVEREMBED(t)` 和 `SERVERUPDATE(t)` 。

     对于第$s$个 `Server` ($1 \le s\le N$)，`SERVEREMBED` 步骤为：

     - 从 `Server` 本地获取图片数据 `I`
     - 计算图片的 `embedding`：$e = \mathcal E(I)$

     对于第$s$个 `Server` ($1 \le s\le N$)，`SERVERUPDATE` 步骤为：

     - 计算梯度：$\delta^t_{\mathcal E_s}= \nabla \mathcal E(I)\times \delta^t_e$
     - 向所有 `server` 同步梯度$\delta^t_{\mathcal E_s}$并更新模型$\mathcal E$。

4. 在 `AMS` 架构中，所有的行为图片在 `worker` 中使用索引来记录，而这些图片在 `server` 中存储和计算，并通过 `embedding` 模型获取其语义向量。

   - 图片以 `key-value` 的格式存储在`server` 之间，`key` 为图片索引`value` 为图片数据。

     样本数据保存图片索引而不是图片数据本身，同时样本数据在所有 `worker` 之间分配。

   - `sparse ID` 特征和 `embedding model` 在 `server` 中运行，`MLP` 和 `aggregator` 在 `worker` 中运行。

   - 每一轮迭代，`worker` 都读取一个 `mini-batch` 样本并从 `server` 请求 `ID` 特征和图片特征的 `embedding` 。

     注意：该 `worker` 的请求会被发送到存储对应 `ID` 和图像的 `server` 。

     - 对于 `ID` 特征，`server` 直接通过 `lookup` 查找对应的 `embedding` 。
     - 对于图片特征，`server` 首先从本地获取图片数据，然后通过 `embedding` 模型$\mathcal E$获取 `embedding` 向量$\mathbf{\vec e}$。

   - 反向梯度传播在 `worker` 部分和 `server` 部分被分别处理，这保证了原始图片特征端到端的训练。

   - 这种方法不仅可以用于行为图片，也可以用于用户历史商品评论等文本语义的抽取。

5. `AMS` 优势：

   - 每张图片在 `server` 中仅存储一次（而不是对每个用户存储一次），显著减少图片的存储（减少大约40倍）。

   - 在 `worker` 和 `server` 之间传递的 `embedding` 向量的大小远小于图片本身的大小，这降低了通信需求（减少大约340倍）。

   - `server` 中多个图片可以并行处理（在 `GPU` 中），这可以降低计算成本。

     实际应用中，`server` 和 `worker` 都部署在同一个 `GPU` 中，因此这可以最大程度的利用 `GPU` 。

   基于 `AMS` ，我们可以将数十亿样本在18小时内完成训练，使得模型实现天级更新从而达到工业级需求。连续18天日志数据在不同`GPU` 配置上的训练时间如下图所示。考虑到成本和时间之间的折衷，论文选择20`GPU` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/k7eT7irzuUea.png?imageslim">
   </p>
   

#### 15.2.2 embedding model

1. `embedding model` 用于从原始图片数据中语义 `embedding` 向量。论文采用预训练的 `VGG16` 模型：冻结`VGG16` 的底层部分，仅训练其高层部分。

   论文冻结`VGG16` 的前14层（从 `Conv1` 到 `FC6` ），固定部分输出一个 `4096` 维的向量。对于可训练的部分，论文采用 `3` 层的全连接网络（每层的神经元数量分别为 `4096,64,12` ），并输出一个 `12` 维的向量。

2. 训练 `VGG top` 多少层涉及到效果和效率之间的折衷：

   - 如果冻结的部分太多，如冻结到 `FC8` ，则固定部分输出一个 `1000`维的向量。这会导致计算效率较高（可训练的部分减少），但是导致 `3%` 的相对性能损失。
   - 如果冻结的部分太少，则可训练部分增加，计算量会显著增加，同时模型性能并不会显著改进。

#### 15.2.3 aggregator

1. `aggregator` 用于将用户大量的行为图片`embedding` 向量聚合成一个固定长度的向量。论文探索了大量聚合方式：

   - 最直接的方式：将所有用户行为图片`embedding` 向量串联在一起，然后填充或截断到固定长度。

     当用户行为图片数量较大，或者用户行为图片的顺序更改时，这种方式容易受到影响。

   - 另一种直接方式：采用池化操作（如：最大池化、均值池化、或者 `sum` 池化）。

     这种方式给每个图片赋予相同的权重，无法侧重相关的行为图片。

   - 类似`DIN` 模型引入注意力机制，它根据目标广告自适应的捕获最相关的行为。

     考虑到视觉相关性，论文将广告图片作为 `query`，这种方式称作 `Attentive Pooling` 。

2. 广告的商品类别 `T shirt` 和用户行为图片中的 `T shirt` 图片是有关联的，因此我们可以捕获 `ID` 特征和行为图片 `embedding` 之间的 `attention` 。

   因此论文提出了 `MultiQueryAttentive Pooling`，它考虑了所有的图片、`ID` 特征的 `attentive weight` 。

   作者设计了两个注意力通道，这两个通道的输出向量进行拼接得到 `aggregator` 的输出向量。：

   - 将广告图片作为 `query`、将用户行为图片作为 `Values/Keys` 的通道。
   - 将 `ID` 特征作为 `query`、将所有图片作为 `Values/Keys` 的通道。

   注意：和 `multi-head` 技术不同， `MultiQueryAttentive Pooling` 对每个注意力通道采取不同的 `query`，这互补的探索了不同的相关性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/doWPVRR5GIRY.png?imageslim">
   </p>
   

#### 15.2.4 推断

1. 在大型工业广告系统中，推断效率对于`CTR` 模型的在线部署至关重要。

   对于常规 `DNN` 模型，通常 `embedding` 参数存放在全局 `key-value` 共享存储中，`MLP` 参数存储在本地 `ranking server`中。对于每个`inferrence` 请求，`ranking server` 会从 `key-value` 共享存储提取 `ID embeding`，并向`MLP` 馈入该 `embedding` 从而执行 `CTR` 预测。

   实践证明该方案在生产中具有高吞吐、低延迟。

2. 当涉及图片尤其是大量的用户行为图片时，提取图片的 `embedding` 向量可能带来大量的计算开销和通信开销。

   收益于图片的独立性，图片`embedding` 向量可以离线计算并作为常规 `ID` 特征进行全局存储。因此`ranking server` 无需任何修改即可适配图片的引入。

   注意：对于已有的图片可以通过其图片`ID`查找离线计算好的图片 `embedding` 向量。对于新的图片，系统可以直接计算其 `embedding` 来使用并缓存起来。这可以有效缓解 `ID` 特征的冷启动问题。

   对于每个`pv` 请求，`DICM` 的响应时间仅仅从 `21ms`（传统 的、不带图片的`DNN` 模型）增加到 `24ms` 。符合在线推断的需求。

3. `DICM` 模型也可以用于 `Pre-rank` 阶段。为加快在线推断效率，论文设计了类似 `DSSM` 的架构。

   - 首先对广告和用户分别建模，得到维度相同的 `ad vector` 和 `user vector` 。

     注意：为了避免广告和用户特征过早融合，用户的行为图片`embedding` 向量采用 `sum` 池化。

   - 然后通过计算 `ad vector` 和 `user vector` 的点击来执行预测。

   直到点击之前，用户和广告之间都没有交互。因此 `ad vector` 和 `user vector` 可以离线预计算并存储。在线 `serving` 只需要读取预计算好的向量并执行向量点击操作，这大幅降低了计算量。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/Ac0zmppOooHk.png?imageslim">
   </p>
   

### 15.3 实验

1. 数据集：所有数据来自于淘宝的展示广告系统收集自 `2017-07` 月连续 `19` 天的日志数据，其中前 `18` 天为训练集、最后一天为测试集。

   数据集包含 `39` 亿训练样本、`2.19` 亿测试样本。样本有 `27` 种 `ID` 特征，包括用户画像、用户行为、广告基础信息和描述信息。

2. 评估方式：离线评估模型的 `AUC` 和 `GAUC`。

   `GAUC` 是所有用户的 `AUC` 加权平均。在实际广告系统中，`GAUC` 比 `AUC` 和交叉熵损失更有效。因为 `GAUC` 是个性化的，专注于每个用户的预测。

3. 训练技巧：

   - 为加快模型训练速度和降低存储成本，论文采用 `common feature` 技巧：将相同用户的样本放置在一起，从而将用户的相关特征构成 `common feature` 。

   - 论文选择每个用户过去14天点击的图片作为用户行为图片。

     由于用户的点击行为具有太多的噪音，论文认为用户浏览了一定时长之后的点击才是有效点击。根据这种简单的过滤策略，模型的效果更好。同时用户平均点击图片的数量从200降低到32.6。

   - 模型采用 `PReLU` 激活函数，这是实践中的经验；采用 `Adam` 优化器，初始学习率为 `0.001` ，学习率每隔 `24000` 个 `batch` 降低到之前的 `90%`。

     模型在2个 `epoch` 之后收敛，大约有 `12.8万` 次迭代。

   - 收益于模型的天级更新，可以使用前一天的模型来初始化当前模型的参数。

     注意：`DICM` 的各部分以不同的速度收敛。

     - 由于 `ID` 的稀疏性以及参数规模较大，`ID` 的 `embedding`部分很容易陷入过拟合。
     - 图像 `embedding model` 需要足够多的训练才能捕获视觉信息和用户意图之间的高度非线性关系。

     因此论文提出了 `partial warm-up`技术：

     - 使用预训练模型（前一天的模型）为除了`ID embedding`之外的所有部分来初始化，需要通过预训练模型初始化的部分包括：图片`embedding model`、`extractor`、`MLP` 部分。
     - `ID embedding` 部分的参数采用随机初始化。

4. 基准模型 `baseline`： 基准模型采用仅具有稀疏 `ID` 特征的 `embedding&MLP` 模型。

   注意：基准模型还采用了两个特殊的`ID` 字段：广告图片`ID` 字段、用户行为图片的`ID` 字段。

   这两个字段对于公平比较是必不可少的，因为这两个字段可以引入图片的部分信息，从而在与`DICM` 模型比较过程中清晰的看到图片语义信息带来的改进。

   如果没有这两个字段则无法引入任何图片信息，因此无法和 `DICM` 模型比较，因为后者引入了更多的信息（图片信息）。

   另外论文使用自适应正则化技术（参考 `DIN` 模型）来解决`ID` 特征过拟合的问题。

#### 15.3.1 AMS 效率

1. 首先研究 `AMS` 架构相对于 `PS` 架构的效率优势。

2. 论文比较了 `PS` 架构的两种图片保存方式：

   - `PS-worker`：在 `worker` 结点存储图片，同时也存储其它训练数据。
   - `PS-server`：在 `server` 结点存储图片，在 `worker` 结点存储其它训练数据。

   与它们进行比较的是 `AMS` 架构。

   具体参数：

   - `20` 个 `GPU` 结点组成的 `GPU` 集群，样本数量 `39`亿，平均每个用户包含 `32.6` 个行为图片。
   - `mini-batch` 设置为每个 `GPU` 结点 `3000` 个样本，因此有效 `mini-batch` 大小为 `60000` 。
   - 受益于 `common-feature` 技术，每个 `mini-batch` 包含 `32` 万张图片、以及 `140` 万 `ID` （不包括图片的索引 `ID` ）。
   - 总计有 `1.2` 亿张不同的图片参与训练，每张图片被预处理为 `4096` 维的向量来作为训练输入。

   实验结果如下表所示，表中的结果是 `mini-batch` 的平均数据规模，单位为`Bytes` 。其中：

   - `storage` 指标：存储 `mini-batch`图片和 `ID` 数据需要的存储规模，单位为 `Bytes` 。

   - `communication` 指标：在 `worker`和 `server` 之间传输 `mini-batch`数据的传输规模，单位为 `Bytes` 。

     `ALL` 指的是传输所有数据（图片 + `ID` 数据），`Image` 指的是仅传输图片数据。

   从实验结果可以看到：`AMS` 架构达到了很好的系统效率，而`PS-worker` 和 `PS-server` 架构的效率损失（如：存储效率、通信效率）很大。

   - `PS-worker` 需要的存储量是 `AMS` 架构的 `31` 倍（`5.1G vs 164M`）
   - `PS-server` 需要的通信负载是 `AMS`架构的 `32` 倍（`5.1G vs 158M` ）

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/sH782HAgAI8d.png?imageslim">
   </p>
   

#### 15.3.2 消融研究

1. 默认情况下，所有消融研究都取消了 `partial warm-up` 策略。

2. 我们从`baseline` 开始，分别使用广告图片特征、用户行为图片特征、以及同时使用这两个特征。

   注意：`baseline` 模型中已经有广告图片`ID` 和用户行为图片`ID`，但是并没有引入图片的语义信息。这里评估的是引入图片语义信息带来的效果。

   从实验结果可知：

   - 用户行为图片、广告图片都可以提升模型效果。因此通过在用户侧和广告侧引入视觉特征具有积极效果。

   - 对用户行为图片、广告图片共同建模将显著提升模型性能。这种提升的效果远远大于二者独立收益之和。

     因此该结果表明：通过对用户视觉信息和广告视觉信息联合建模具有协同效应。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/3A0NkBMOYQTd.png?imageslim">
   </p>
   

3. 前面我们探讨了不同 `aggregator` 的聚合策略，这里通过实验观察其效果。

   - `Concatenation` 聚合：对用户行为图片的 `embedding` 向量进行拼接聚合，其性能较差，因此并不是一个好的聚合方式。
   - `Maxpooling/Sumpooling` 聚合：这两种聚合方式具有一定的改进。
   - `AttentivePooling`：通过将广告图像作为注意力`query` ，这种聚合方式表现出明显的增益。
   - `MultiQeuryAttentivePooling`：收益于稀疏`ID` 和图片语义信息之间的交互，这种聚合方式的效果最佳。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/em43HMmUokoq.png?imageslim">
   </p>
   

4. `DICM` 可以应用不同的 `basic` 网络，并带来图片特征的持续改进。为验证这一点，论文使用经典的 `LR` 模型、基准的 `Embedding&MLP` 模型、近期提出的 `DIN`模型来作比较。

   可以看到：

   - 具有图片特征的模型始终比仅具有`ID` 特征的模型效果更好。
   - 具有图片特征的`DIN` 模型表现最佳，并大大超越了经典的 `DIN` 模型。
   - 采用图片特征时，`LR` 模型的改进不如其它两种模型。这是因为 `LR` 模型无法充分利用图片的高级语义信息。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/9XEss7CFOe2A.png?imageslim">
   </p>
   

5. 论文比较了采用 `non warm-up`、`partial warm-up`、`full warm-up` 三种预训练策略的效果。

   - `partial warm-up` 效果最佳。
   - `full warm-up` 效果最差，这是由于 `ID embedding` 参数严重过拟合。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/Pxk1NAwSkHyR.png?imageslim">
   </p>
   

#### 15.3.3 离线/在线评估

1. 最佳配置的 `DICM` 模型（采用 `partial warm-up` 策略、`MultiQeuryAttentivePooling` 、以及其它最佳配置）和 `baseline` 模型的离线效果对比、在线 `A/B test` 对比：

   - 离线对比结果如下。结果表明：`DICM` 比 `baseline` 效果更好。同时，`DICM` 和 `baseline` 的差距在训练过程中是一致的，这证明了 `DICM`的健壮性。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/nt0IyLgFcByr.png?imageslim">
     </p>
     

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/FQpxbaPk1xLn.png?imageslim">
     </p>
     

   - 在线 `A/B test` 对比结果如下。为了和线上生产环境保持一致，我们用生产中最先进的网络（具有更精细的人工设计特征的 `Embedding&MLP` 版本）替代 `DICM`的 `basic` 网络。

     下面给出了连续 7天的在线 `A/B test` 中，`DICM` 取得的相对增益。评估的指标为广告系统的三个关键指标：`CTR/eCPM/GPM(gross merchandise value per mile)`。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/LcI5lWEgI8SR.png?imageslim">
     </p>
     

#### 15.3.4 Pre-rank

1. 前文提到我们可以在 `Pre-rank` 阶段应用调整过的 `DICM` 模型。

   实验结果表明：`DICM` 模型再次明显优于 `baseline` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/XH8FTmuaT4U2.png?imageslim">
   </p>
   