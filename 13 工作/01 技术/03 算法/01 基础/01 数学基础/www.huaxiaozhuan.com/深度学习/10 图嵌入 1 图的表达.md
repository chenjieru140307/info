# 图的表达

1. `Graph Embedding` 可用于可视化、结点分类、链接预测 `link prediction`、推荐等任务中。
2. 网络表示学习 `network representation learning :NRL` ：将网络的每个顶点编码到一个统一的低维空间中。

## 一、DeepWalk

1. 定义图 `network` 为$G=(V,E)$，其中$V$为图中所有的顶点（也称为网络的成员 `member` ），$E \sube (V\times V)$为所有的边。

   定义带标签的社交网络 `labeled social network` 为$G_L = (V,E,X,Y)$，其中：

   - 特征$X\in \mathbb R^{|V|\times S}$，$S$为每个成员的特征空间的大小，$|V|$为顶点数量。
   -$Y\in \mathbb R^{|V|\in |\mathcal Y|}$，$\mathcal Y$为 `label` 空间，$|\mathcal Y |$为分类类别数量。

   在传统机器学习算法中，我们仅仅学习从$X \rightarrow Y$的映射，忽略了成员之间的社交关系。而基于图的算法中，我们可以利用$G$中色社交关系来提升分类准确性，在文献中这个问题被称作关系分类`relational classfication` 问题。

   传统的关系分类算法将这个问题视作无向马尔可夫网络中的一个`inference`，然后使用迭代近似推理算法在给定网络结构的情况下计算标签的后验分布。这些算法融合了标签空间作为特征空间的一部分。

   论文`《DeepWalk: Online Learning of Social Representations》` 提出了 `DeepWalk` 算法，该算法基于无监督方法直接学习网络的结构特征，学得一个与标签无关的网络`representation` 。

2. `DeepWalk` 将图$G$作为输入来学习网络中顶点的潜在表达 `latent representation`，这种表达可以将社交关系编码到一个连续向量空间中，从而给后续模型使用。

   以 `Karate network` 为例，图`a` 给出了典型的图，图`b` 给出了该图中所有顶点的 `2`维潜在表达。可以看到：二者非常相似，且图 `b` 可以对应于图 `a` 的聚类。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/42TG3iqkXNI1.png?imageslim">
   </p>
   

3. `DeepWalk` 的目标是学习每个顶点的表达$\mathbf X_e \in \mathbb R^{|V|\times d}$。其中，$d$是一个较小的数，表示低维表达空间的维度大小。这个低维空间是分布式表达的，每个维度或者几个维度代表某种社交现象。

4. `DeepWalk` 首次将深度学习技术引入到 `network` 分析中，它通过深度学习技术学习图顶点的社交表达 `social representation` 。顶点社交表达可以在低维连续空间编码社交关系`social relation`，从而获得网络顶点的相似性和社区成员关系 `community membership`。

   这种社交表达 `social representation`需要满足以下特点：

   - 适应性 `Adaptability`：社交网络不断演变，新的社交关系不应该要求重新训练整个网络。
   - 社区相关`Community aware`：低维表示空间中的距离应该代表网络成员之间的社交相似性。
   - 低维`Low dimensional`：低维模型泛化效果更好，训练速度和推理速度更快。
   - 连续`Continuous` ：连续的表示更方便建模（函数可微），且使得社区 `community` 之间的边界更平滑。

### 1.1 模型

1. 定义以顶点$v_i$开始的一次随机游走序列为$\tilde {\mathcal W}_{v_i} = (\mathcal W_{v_i}^1,\cdots,\mathcal W_{v_i}^k,\cdots, \mathcal W_{v_i}^t)$，其中：

   -$t$为每个序列的长度，它是算法的超参数。
   -$\mathcal W_{v_i}^k$是序列中的第$k$个顶点，它是从$\mathcal W_{v_i}^{k-1}$的邻居中随机选择一个顶点而生成。

2. 如果一个图中顶点的度`degree` 满足幂律分布`power law`，则随机游走序列中顶点出现的频次也满足幂律分布。

   一个事实是：自然语言中单词出现的频次也满足幂律分布。由于语言模型是解决这种模式的工具，因此这些工具理论上也可以应用于`network`的 `community structure` 建模。

   `DeepWalk` 就是参考了神经语言模型的思想：将随机游走序列视为文本序列，从而学习顶点的潜在表达。

   下图给出了两个典型分布：

   - 图 `a` 给出了`short random walk`中，顶点出现频次的分布。

     因为随机游走序列最长的长度不超过$t$，因此称为`short`。

   - 图 `b` 给出了英文维基百科的 `10` 万篇文章中，单词的频次分布。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/QDxSWL3kivDY.png?imageslim">
   </p>
   

3. 语言模型的输入是语料库$\mathbb D$和词汇表$V$，而 `DeepWalk` 的输入是一组`short random walk` 序列和图的顶点$V$。

#### 1.1.1 算法

1. `DeepWalk` 算法由两部分组成：随机游走序列生成部分、参数更新部分。

   - 随机游走序列生成：随机游走序列生成器从图$G$中均匀随机采样一个顶点$v_i$作为序列$\tilde {\mathcal W}_{v_i}$的起点，然后生成器从上一个访问顶点的邻居节点中均匀随机采样一个顶点作为序列的下一个点。
     - 论文中将序列的长度设置为最长为$t$，但是论文也提到：理论上序列的长度不一定是固定的，没有任何限制。
     - 随机游走可以选择随机重启，即以一定的概率回到起始点。但是论文的实验结果显示：随机重启并没有任何优势。
   - 参数更新部分：

2. 对于每一个顶点$v_i$，`DeepWalk` 随机生成以$v_i$为起点的$\gamma$条随机游走序列。每条随机游走序列都是独立采样的，它们都是以顶点$v_i$为起点。

3. `DeepWalk` 算法：

   - 输入：

     - 图$G(V,E)$
     - 窗口尺寸$w$
     - 顶点的 `embedding size`$d$
     - 每个顶点作为起始点的游走轮次$\gamma$
     - 游走序列长度$t$

   - 输出：所有顶点的`representation` 矩阵$\mathbf\Phi \in \mathbb R^{|V|\times d}$

   - 算法步骤：

     - 从$\mathcal U^{|V|\times d}$中随机采样来初始化$\mathbf \Phi$

     - 从$V$构建一颗二叉树 `binary tree`$T$

       > 建立二叉树是为了后续 `SkipGram` 算法采用 层次 `Softmax`

     - 迭代：$i=0,1,\cdots,\gamma$，迭代步骤：

       - 随机混洗顶点：$\mathcal O = shuffle(V)$。

         > 每次遍历的开始，随机混洗顶点。但是这一步并不是严格要求的，但是它可以加快随机梯度下降的收敛速度。

       - 对每个顶点$v_i\in \mathcal O$，执行：

         - 利用随机游走生成器生成随机游走序列：$\tilde {\mathcal W}_{v_i} = RandomWalk(G,v_i,t)$
         - 采用 `SkipGram`算法来更新顶点的表达：$SkipGram(\mathbf\Phi,\tilde {\mathcal W}_{v_i},w)$

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/O4VsHCOYYbkR.png?imageslim">
   </p>
   

4. `DeepWalk` 算法有两层循环：

   - 外层循环指定应该在每个顶点开始随机游走的轮次。
   - 内存循环遍历图$G$中的所有顶点。

5. 对于每个随机游走序列中的顶点$v_i$，我们基于 `SkipGram` 的思想最大化$v_i$邻居结点的概率。

   `SkigGram` 算法：

   - 输入：

     - 顶点的 `representation` 矩阵$\mathbf \Phi$
     - 随机游走序列$\tilde {\mathcal W}_{v_i}$
     - 窗口大小$w$

   - 输出：更新后的顶点 `representation`矩阵$\mathbf \Phi$

   - 算法步骤：

     对随机游走序列$\tilde {\mathcal W}_{v_i}$的每个顶点：$v_j\in \{\mathcal W_{v_i}^1,\cdots,\mathcal W_{v_i}^k,\cdots, \mathcal W_{v_i}^t\}$迭代，迭代步骤：

     - 对$v_j$窗口内的所有邻居顶点$u_k\in \{\mathcal W_{v_i}^{(j-w)},\cdots, \mathcal W_{v_i}^{(j+w)}\}$，执行：

       其中$P(u_k\mid v_j;\mathbf\Phi)$表示$u_k$是顶点$v_j$邻居结点的概率。

6. 由于类别标签数量等于$V$，这可能是百万或者数十亿，因此每次迭代过程中计算$P(u_k\mid v_j;\mathbf\Phi)$。为了加快训练速度，通常采用 `Hierarchical Softmax`。这也是前面为什么要对顶点建立二叉树$T$的原因。

   假设到达顶点$u_k$经过的二叉树中间结点依次为$b_0\rightarrow b_1\rightarrow \cdots\rightarrow b_{\lceil \log |V|\rceil}$，其中$b_0=\text{root}, b_{\lceil \log |V|\rceil}=u_k$，则有：

  $P(u_k\mid v_j;\mathbf\Phi) = \prod_{l=1}^{\lceil \log |V|\rceil} P(b_l\mid v_j;\mathbf\Phi)$

   而$P(b_l\mid v_j;\mathbf\Phi)$可以直接用二分类建模，因此计算$P(u_k\mid v_j;\mathbf\Phi)$的整体复杂度从$O(|V|)$降低到$O(\log|V|)$。

   如果对更频繁出现的顶点赋予更短的路径，则得到霍夫曼编码树，这可以进一步降低计算代价。

#### 1.1.1 训练和优化

1. `DeepWalk` 算法的参数为$\mathbf\Phi$和$T$，参数采用随机梯度下降法来优化。其中学习率$\alpha$初始值为 `0.025`，然后随着已见过的顶点数量线性衰减。

2. `DeepWalk` 算法有两个理想的优点：

   - 支持增量学习，因此是一种在线学习算法。

     当图$G$发生变化时，我们可以在发生变化的局部区域用随机游走生成器生成新的随机游走序列来更新模型，从而可以用较小的代价来增量更新模型，无需重新计算整个图$G$。

   - 容易并行化，不同的随机游走生成器可以并行探索图$G$的不同部分。

     此外，在训练时我们可以使用异步随机梯度下降 `ASGD` 方法。在 `ASGD`中我们访问共享参数时不需要加锁，这可以实现更快的收敛速度（见图`a`）并且没有预测性能的损失（见图`b`）。

     之所以如此时因为：社交网络随机游走序列的顶点频次分布和`NLP`单词频次分布都遵循幂律分布，因此会出现长尾效应，大多数顶点出现次数较少。这使得对$\mathbf \Phi$的更新是稀疏的，即：同时对同一个顶点进行更新的概率很低。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/1t0pAs9DNXYD.png?imageslim">
     </p>
     

3. `DeepWalk` 可以挖掘图$G$的局部结构，但是无法挖掘图$G$的整体结构。

4. `DeepWalk` 算法还有两个变种模式：

   - 流式计算模式：在不知道图$G$整体信息的情况下实现 `DeepWalk`。在这种模式下，从图中得到的一小段随机游走序列直接传给学习器来更新模型。此时算法需要进行以下修改：

     - 学习率$\alpha$不再是衰减的，而是固定为一个小的常数。这使得算法的训练需要更长的时间。
     - 不必再建立二叉树$T$。但是如果顶点数量$|V|$已知，则也可以建立二叉树。

   - 非随机游走模式：在某些图中，有些路径的产生并不是随机的，如：用户浏览网页元素的路径。对于这些图，我们可以直接使用用户产生的路径而不是随机生成的路径来建模。

     通过这种方式，我们不仅可以建模网络结构，还可以建模这些路径的热度。

### 1.2 实验

1. 数据集：

   - `BlogCatalog` 数据集：由博客作者提供的社交关系网络。标签代表作者提供的主题类别。
   - `Flickr` 数据集：`Flickr`网站用户之间的关系网络。标签代表用户的兴趣组，如“黑白照片”。
   - `YouTube` 数据集：`YouTube` 网站用户之间的社交网络。标签代表用户的视频兴趣组，如“动漫、摔跤”。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/UmxKH4yViC2t.png?imageslim">
   </p>
   

2. 对比模型：

   - 谱聚类 `SpectralClustering`：从图$G$的 `normalized graph Laplacian` 矩阵$\tilde{\mathcal L}$中计算到的$d$个最小的特征向量，构成图的表达。

   - `Modularity`：从图$G$的 `Modularity`$B$计算得到 `top-d` 个特征向量，构成了图的表达。

   - `EdgeCluster`：基于 `k-means` 聚类算法对图$G$的邻接矩阵进行聚类。事实证明它的性能可以和 `Modularity` 相比，并且可以扩展到那些超大规模的图，这些图太大而难以执行谱分解。

   - `wvRN`： `weighted-vote Relational Neighbor` 是一个关系分类器 `relational classier`。给定顶点$v_i$的邻居$\mathcal N_i$，`wvRN` 通过邻居结点的加权平均来预估$P(y_i\mid \mathcal N_i)$：

    $P (y_i\mid \mathcal N_i) = \frac 1Z \sum_{v_j\in \mathcal N_i} w_{i,j}P (y_j\mid \mathcal N_j)$

     其中$Z$为归一化系数。

     该方法在真实网络中表现良好，并被推荐作为一个优秀的关系分类基准。

   - `Majority`：选择训练集中出现次数最多的标签作为预测结果（所有样本都预测成一个值）。

3. 评估指标：对于多标签分类问题 `multilabel classication`，我们采用 `Macro-F1` 和 `Micro-F1` 作为评估指标。

   - `Macro-F1` ：根据整体的混淆矩阵计算得到的$F_1$值。
   - `micro-F1`：先根据每个类别的混淆矩阵计算得到各自的$F_1$值，然后对所有$F_1$值取平均。

   另外我们采用模型提取的特征训练 `one-vs-rest` 逻辑回归分类器，根据该分类器来评估结果。

#### 1.2.1 模型结果

1. 实验条件：

   - 随机采样一个比例为$T_R$的带标签顶点作为训练数据，剩下顶点作为测试数据。
   - 所有流程执行 `10` 次并汇报平均性能。
   - 对于 `DeepWalk`，我们汇报$\gamma=80,w=10,d=128$的结果；对于`SpectralClustring,Modularity,EdgeCluster` ，我们汇报$d=500$的结果。

2. `BlogCatalog`数据集：我们评估$T_R= 10\% \sim 90\%$的结果。结果如下表，粗体表示每一列最佳结果。

   结论：

   - `DeepWalk` 性能始终优于 `EdgeCluster,Modularity,wvRN`。

     事实上当 `DeepWalk` 仅仅给出 `20%`标记样本来训练的情况下，模型效果优于其它模型 `90%` 的标记样本作为训练集。

   - 虽然 `SpectralClustering` 方法更具有竞争力，但是当数据稀疏时 `DeepWalk` 效果最好：

     - 对于 `Macro-F1` 指标，$T_R \le 20\%$时 `DeepWalk` 效果更好。
     - 对于`Micro-F1` 指标，$T_R\le 60\%$时 `DeepWalk` 效果更好。

   - 当仅标记图中的一小部分时，`DeepWalk` 效果最好。因此后面实验我们更关注稀疏标记图`sparsely labeled graph` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/1C55YLMa3F32.png?imageslim">
   </p>
   

3. `Flickr` 数据集：我们评估$T_R= 1 \% \sim 10\%$的结果。结果如下表，粗体表示每一列最佳结果。结论与前面的实验一致。

   - 在 `Micro-F1` 指标上，`DeepWalk`比其它基准至少有 `3%` 的绝对提升。
   - 在 `Micro-F1` 指标上，`DeepWalk`只需要 `3%` 的标记数据就可以打败其它方法 `10%` 的标记数据，因此 `DeepWalk` 的标记样本需求量比基准方法少 `60%` 。
   - 在 `Macro-F1` 指标上，`DeepWalk`性能接近 `SpectralClustring`，击败了所有其它方法。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/KyMBv2X81WDw.png?imageslim">
   </p>
   

4. `YouTube` 数据集：我们评估$T_R= 1 \% \sim 10\%$的结果。结果如下表，粗体表示每一列最佳结果。

   由于 `YouTube` 规模比前面两个数据集大得多，因此我们无法运行两种基准方法 `SpecutralClustering,Modularity` 。

   - `DeepWalk` 性能远超 `EdgeCluster`基准方法：
     - 当标记数据只有 `1%` 时，`DeepWalk` 在 `Micro-F1` 指标上相对 `EdgeCluster` 有 `14%`的绝对提升，在 `Macro-F1`指标上相对 `EdgeCluster` 有 `10%` 的绝对提升。
     - 当标记数据增长到 `10%` 时，`DeepWalk` 提升幅度有所下降。`DeepWalk` 在 `Micro-F1`指标上相对 `EdgeCluster` 有 `3%` 的绝对提升，在 `Macro-F1` 指标上相对 `EdgeCluster` 有 `4%` 的绝对提升。
   - `DeepWalk` 能扩展到大规模网络，并且在稀疏标记环境中表现出色。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/nsBhK7MHfGg6.png?imageslim">
   </p>
   

#### 1.2.2 超参数效果

1. 为评估超参数的影响，论文在 `Flickr` 和 `BlogCatalog` 数据集上进行实验。我们固定窗口大小$w=10$和游走长度$t=40$。

2. 考察不同$d$的效果：

   - 图 `a1` 和 图 `a3` 考察不同$d$和不同$T_R$的效果。

     `Flickr` 和 `BlogCatalog` 数据集上模型的表现一致：模型最佳尺寸$d$取决于训练样本的数量。

     注意：`Flickr` 的 `1%` 训练样本和 `BlogCatalog` 的 `10%` 训练样本，模型的表现差不多。

   - 图 `a2` 和图 `a4` 考察不同$d$和不同$\gamma$的效果。

     在不同$\gamma$取值上，模型性能对于不同$d$的变化比较稳定。

     - 从$\gamma = 30$开始继续提高$\gamma$时模型的效果提升不大，因此$\gamma = 30$几乎取得最好效果。继续提升效果，计算代价上升而收益不明显。
     - 不同的数据集中，不同$\gamma$值的差异非常一致，尽管 `FLICKR` 包含边的数量比 `BLOGCATALOG` 多一个数量级。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/WsB8ajGt0voQ.png?imageslim">
   </p>
   

3. 考察不同$\gamma$的影响。图`b1` 和 `b3` 考察不同$d$的效果，图 `b2` 和 `b4` 考察了不同$T_R$的效果。

   实验结果表明，它们的表现比较一致：增加$\gamma$时开始可能有巨大提升，但是当$\gamma \gt 10$时提升效果显著下降。

   这些结果表明：仅需要少量的随机游走，我们就可以学习有意义的顶点潜在表示。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/34apG4qlebGd.png?imageslim">
   </p>
   

## 二、LINE

1. 定义信息网络`information network` 为$G=(V,E)$，其中：

   -$V$为顶点集合，对应于一个个实体。

   -$E$为边的集合，对应于实体之间的关系。

     更具体地，每条边$e\in E$对应于一个有序对$e=(u,v)$，并关联到一个权重$w_{u,v}\gt 0$。边$e$就对应于实体$u$和$v$地关系。

     - 如果$G$是无向图，则有$(u,v) = (v,u)$；如果$G$是有向图，则有$(u,v)\ne (v,u)$。

     - 如果$w_{u,v} \in \{0,1\}$则这种边称为二元边，表示相连/不相连；如果$w_{u,v}\in \mathbb R^+\cup \{0\}$则边的取值是实数，表示连接的紧密程度。

       这里所有权重都是非负的，并不考虑负权重。

2. 顶点对$(u,v)$之间的一阶邻近度`first-order proximity` 为权重$w_{u,v}$，它刻画了顶点$u$和$v$之间的邻近程度 。

   - 如果顶点$u$和$v$之间没有连接，则它们的一阶邻近度为 `0` 。

   - 一阶邻近度通常反应了两个顶点的相似性。如：

     - 社交网络中，如果两个成员是好友关系，则他们倾向于有相似的兴趣。
     - 在 `web` 网络中，如果两个网页存在链接关系，则它们通常会提到类似的话题。

     由于这种重要性，很多已有的`Graph Embedding` 算法（如：`IsoMap,LLE,Laplacian eigenmap, graph factorization` ）的目标函数都利用了一阶邻近度。

3. 真实世界的信息网络中，能观察到的直接链接仅占很小的比例，大部分链接都因观察不到而缺失。如：社交网络中，很多线下的关系链并没有百分之百同步到线上。

   如果顶点$u$和$v$的链接发生缺失，则其一阶邻近度为 `0`，即使实际上它们关系非常密切。因此仅仅依靠一阶邻近度不足以描述网络的全局结构，我们需要寻找方法来解决这种因为大部分链接缺失导致的网络稀疏问题。

   一个直观的想法是：如果两个顶点共享相同的一组邻居，则这两个顶点往往彼此相似。如：

   - 社交网络中，如果两个成员有相同的朋友圈，则他们很有可能也是好友关系。
   - 单词共现网络中，如果两个单词的上下文相同，则这两个单词往往具有相近的含义。

   如下图所示：

   - 顶点`6` 和 `7` 是直接相连，拥有较高的一阶邻近度，因此相互之间关系密切。映射到低维空间时，这两个顶点的相似度应该较高。
   - 顶点 `5` 和 `6` 有相同的邻居结点，拥有较高的二阶邻近度，因此关系也很密切。映射到低维空间时，这两个顶点的相似度也应该较高。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/sUYfM5FyMNJt.png?imageslim">
   </p>
   

   因此我们采用二阶邻近度`second-order proximity` 。顶点对$(u,v)$之间的二阶邻近度定义为：它们邻居结构的相似程度。

   令$\mathbf{\vec p}_u=(w_{u,1},\cdots,w_{u,|V|})^T$为顶点$u$和其它所有顶点的一阶邻近度，那么$u$和$v$的二阶邻近度定义为：$p_u$和$p_v$的相似度。如果$u$和$v$之间没有任何共同的邻居，则其二阶邻近度为 `0` 。

4. 给定一个大规模网络$G=(V,E)$，`Graph Embedding` 的目标是给出图中每个顶点$v\in V$的低维空间$\mathbb R^d$`representation` （$d \ll |V|$），在这个低维空间中保留顶点之间的一阶邻近度和二阶邻近度。

   一个理想的 `Graph Embedding` 具有以下要求：

   - 必须能够保留顶点之间的一阶邻近度和二阶邻近度。
   - 必须能够扩展到超大规模网络，如数百万顶点和数十亿条边。
   - 可以处理任何类型的边：有向的/无向的，带权重的/无权重的。

   现有的一些方法并不足以满足这些要求：

   - 传统的`Graph Embedding` 方法，如：`IsoMap,LLE,Laplacian eigenmap, graph factorization`，它们通常仅能处理小规模的网络，无法处理真实世界大规模网络。

     并且这些方法仅仅采用一阶邻近度，并没有考虑二阶邻近度。

   - `DeepWalk` 方法虽然能够处理超大规模网络，但是它仅适合无权图，无法处理带权重的图。

     而且 `DeepWalk` 并没有明确表示它保留的是顶点之间的一阶邻近度还是二阶邻近度。

   论文 `《LINE: Large-scale Information Network Embedding》` 提出了 `LINE` 模型，该模型能够同时满足以上需求。`LINE` 模型还提出了一种边采样算法来解决经典的随机梯度下降的局限性，提高了采样效率和效果。

   实验表明`LINE` 模型在各种现真实网络（语言网络、社交网络、引用网络）上有效，并且算法非常高效：在单机上几个小时内学习具有数百万顶点、数十亿边的 `Graph Embedding` 。

### 2.1 模型

#### 2.1.1 一阶邻近度

1. 为建模一阶邻近度，对每个无向边$(i,j)$我们定义顶点$v_i$和$v_j$之间的联合概率分布为：

  $p_1(v_i,v_j) = \frac{1}{1+\exp\left(-\mathbf{\vec u}_i \cdot \mathbf{\vec u}_j \right)}$

   其中$\mathbf{\vec u}_i\in \mathbb R^d$为顶点$v_i$的低维表达向量。

  $p(\cdot,\cdot)$是定义在空间$V\times V$上的概率分布函数，定义其经验概率分布为：

  $\hat p_1(i,j) = \frac{w_{i,j}}{W}$

   其中$W=\sum_{(i,j)\in E} w_{i,j}$表示所有边的权重，$w_{i,j}$表示边$(i,j)$的权重。

   为了在低维空间中保留一阶邻近度，一个简单直接的方法是最小化目标函数：

  $O_1 = \sum_{(i,j)\in E}d(\hat p_1(i,j),p_1(i,j))$

   其中$d(\cdot,\cdot)$为两个概率分布的距离。论文采用 `KL` 散度作为两个分布的距离函数，因此最小化目标函数：

  $O_1 = - \sum_{(i,j)\in E} w_{i,j}\times \log p_1(v_i,v_j)$

   通过求解该最优化问题，我们得到每个顶点的表达$\{\mathbf{\vec u}_1,\cdots,\mathbf{\vec u}_{|V|}\}$。

   注意：这里定义的一阶邻近度模型仅适合无向图。

#### 2.1.2 二阶邻近度

1. 给定一个有向图，二阶邻近度假设：有很多共同邻居的两个顶点彼此相似。

   注意：如果是无向图，则可以将每条无向边视为方向相反、权重相等的两条有向边。此时二阶邻近度模型也适用于无向图。

2. 在二阶邻近度模型中，每个顶点会充做其它顶点的 “上下文” 。因此每个顶点在计算 `representation` 时存在两个角色：需要计算`representation` 的顶点本身、计算其它顶点`representation` 时的上下文。

   对于顶点$v_i$我们引入两个 `representation` 向量：

   - 当作为需要计算 `representation`的顶点时，其表达向量为$\mathbf{\vec u}_i$
   - 当作为计算其它顶点 `representation` 的上下文时，其表达向量为$\mathbf{\vec u}_i^\prime$

   借鉴 `SkipGram` 的思想，对每条有向边$(i,j)$，我们定义由顶点$v_i$生成上下文$v_j$的概率为：

  $p_2(v_j\mid v_i) = \frac{\exp(\mathbf{\vec u}_j^\prime \cdot \mathbf{\vec u}_i)}{\sum_{k=1}^{|V|}\exp(\mathbf{\vec u}_k^\prime \cdot \mathbf{\vec u}_i)}$

   其中$|V|$为顶点数量（也是上下文数量）。

   因此对于每个顶点$v_i$，我们定义了一个条件概率分布$p_2(\cdot\mid v_i)$，这个条件概率分布的经验分布函数为：

  $\hat p_2(\cdot\mid v_i) = \frac{w_{i,j}}{d_i}$

   其中$w_{i,j}$为边$(i,j)$的权重，$d_i$为顶点$i$的出度`out degree` ：$d_i = \sum_{k\in \mathcal N(i)} w_{i,k}$，$\mathcal N(i)$为顶点$i$的邻域。

   为了在低维空间中保留二阶邻近度，我们最小化目标函数：

  $O_2 = \sum_{i\in V}\lambda_i d(\hat p_2(\cdot\mid v_i),p_2(\cdot\mid v_i))$

   其中

   -$\lambda_i$来表示顶点$v_i$在网络中的重要性，它可以通过顶点的 `degree` 来衡量，也可以通过 `PageRank` 之类的算法来估计。
   -$d(\cdot,\cdot)$为两个概率分布的距离。

   论文采用$\lambda_i = d_i$、距离采用 `KL` 散度，则最小化目标函数：

  $O_2 = -\sum_{(i,j)\in E} w_{i,j} \times \log p_2(v_j\mid v_i)$

   通过求解该最优化问题，我们得到每个顶点的表达$\{\mathbf{\vec u}_1,\cdots,\mathbf{\vec u}_{|V|}\}$，每个顶点的上下文表达$\{\mathbf{\vec u}_1^\prime,\cdots,\mathbf{\vec u}_{|V|}^\prime\}$。

#### 2.1.3 融合

1. `LINE` 用两个模型分别训练一阶邻近度模型和二阶邻近度模型，得到一阶`embedding` 向量和二阶 `embedding` 向量。

   - 对于监督学习任务，`LINE` 将一阶`embedding` 向量和二阶 `embedding`向量拼接成一个向量作为每个顶点的最终`embedding` 向量，然后通过一个逻辑回归模型来自动调整向量中每个单元的权重。

     这相当于间接的调整了一阶`embedding` 向量、二阶`embedding`向量的融合权重。

   - 对于无监督学习任务，`LINE` 只能单独使用一阶`embedding` 向量或者二阶`embedding` 向量。因为无监督学习任务没有任何方法来确定它们之间的融合权重，所以无法对二者进行拼接。

2. `LINE` 未来探索的一个方向是：将一阶邻近度目标函数、二阶邻近度目标函数融合到一个目标函数中，然后使用一个模型来训练。

3. `LINE` 采用二阶邻近度模型来补充一阶邻近度模型的稀疏性，从而更好的捕捉图的全局结构。

4. `DeepWalk` 通过随机游走来扩展顶点的领域，类似深度优先搜索；`LINE` 通过一阶邻近度、二阶邻近度来扩展顶点的领域，采用广度优先搜索。

   另外 `DeepWalk` 仅适用于无权图，而 `LINE` 适用于带权图、无权图。

#### 2.1.4 最优化问题

1. 对于目标函数$O_2$，当计算$p_2(\cdot\mid v_i)$时我们需要加总所有的顶点$V$：

  ${\sum_{k=1}^{|V|}\exp(\mathbf{\vec u}_k^\prime \cdot \mathbf{\vec u}_i)}$

   当在每个更新`step` 中进行计算时，计算代价非常大。

   为解决该问题论文采用负采样的思路：对于每条边$(i,j)$，将最大化对数概率

  $\log p_2(v_j\mid v_i) = \log \left(\frac{\exp(\mathbf{\vec u}_j^\prime \cdot \mathbf{\vec u}_i)}{\sum_{k=1}^{|V|}\exp(\mathbf{\vec u}_k^\prime \cdot \mathbf{\vec u}_i)}\right)$

   替换为：最大化正的定点对$$的概率、以及最小化若干对负的定点对的概率：

  $\log \sigma(\mathbf{\vec u}_j^\prime \cdot \mathbf{\vec u}_i) + \sum_{k=1}^K \mathbb E_{v_k\sim P_n(v)}\left[\log\sigma(-\mathbf{\vec u}_k^\prime\cdot \mathbf{\vec u}_i)\right]$

   其中：

   -$\sigma(x) = 1/(1+\exp(-x))$为 `sigmoid` 函数。

   - 第一项为正的顶点对$$，表示观察到的边$(i,j)$的概率。

   - 第二项为采样的若干对负的顶点对的概率，表示负的$K$组负的顶点对$$。其中随机采样的$K$个顶点的采样概率分布为：

    $P_n(v) \propto d_v^{3/4}$

    $d_v$为顶点$v$的出度。

     从采样概率可知：越 “热门” 的顶点，它被作为负顶点的概率越高。

2. 对于目标函数$O_1$，存在一个平凡解：

  $\mathbf{\vec u}_i = (+\infty,+\infty,\cdots,+\infty)^T,\quad i=1,2,\cdots,|V|$

   此时$p_1(v_i,v_j) = \frac{1}{1+\exp\left(-\mathbf{\vec u}_i \cdot \mathbf{\vec u}_j \right)} = 1$，使得$O_1 = 0$取最小值。

   实际上这样的一组解是无意义的。为了避免这样的结果，论文也采用负采样策略：

  $\log \sigma(\mathbf{\vec u}_j \cdot \mathbf{\vec u}_i) + \sum_{k=1}^K \mathbb E_{v_k\sim P_n(v)}\left[\log\sigma(-\mathbf{\vec u}_k \cdot \mathbf{\vec u}_i)\right]+ \sum_{k=1}^K \mathbb E_{v_k\sim P_n(v)}\left[\log\sigma(-\mathbf{\vec u}_k \cdot \mathbf{\vec u}_j)\right]$

   - 第一项为正的顶点对$$，表示观察到的边$(i,j)$的概率。
   - 第二项为采样的若干对负的顶点对的概率，表示负的$K$组负的顶点对$$。
   - 第三项为采样的若干对负的顶点对的概率，表示负的$K$组负的顶点对$$。

   因为一阶邻近度模型是无向图，因此这里分别以$i$为顶点、$j$为顶点独立的进行负采样。

#### 2.1.5 边采样

1. 即使找到了合适的最优化目标，针对超大规模的图的最优化过程也是一项挑战。虽然可以采用随机梯度下降法进行优化，但是真实 `Graph` 表明：直接采用随机梯度下降法是有问题的，因为很多真实 `Graph` 是带权重的，而且权重呈现高度分散的特点。

   如：考虑一个单词共现网络，顶点为单词，边的权重为单词之间的共现次数。有的单词共现频次很低，只有一次；有的单词共现频次很高，达到几万甚至几十万次。

   我们采用异步随机梯度下降`ASGD` 方法来优化负采样的目标函数。在每一个更新`step` 中，对于每个`mini-batch` 的每条边有：

  $\nabla_{\mathbf{\vec u}_i} O_2 = w_{i,j}\times \nabla_{\mathbf{\vec u}_i} \log p_2(v_j\mid v_i)$

   可以看到：这里的梯度会乘以边的权重。当边的权重很分散时，优化过程的学习率非常难以选择。

   - 如果选择一个较大的学习率，则它对于权重较小的边的参数更新比较友好，但是对于权重较大的边容易导致梯度爆炸。
   - 如果选择一个较小的学习率，则它对于权重较大的边的参数更新比较友好，但是对于权重较小的边容易导致学习缓慢。

2. 如果所有的边的权重都是相等的（如二元边），则选择合适的学习率是很容易的。因此一种简单的解决方案是：将权重为$w$的边扩展为$w$条二元边。

   这种方式虽然解决了问题，但是显著增加了内存需求，尤其是当$w$很大时。

3. 为解决该问题，可以采用原始边权重成正比的概率从原始边采样，从而得到二元边。然后用二元边进行模型参数更新。这称为边采样 `edge-sampling` 策略。

   问题是：如何根据边的权重对其进行采样。

   - 朴素方案：设$W=(w_1,\cdots,w_{|E|})$表示这些边的权重。

     - 首先按计算$w_{sum} = \sum_{i=1}^{|E|} w_i$，然后在$[0,w_{sum}]$之间随机采样一个随机数$S$

     - 观察$S$落入哪个区间$[\sum_{j=0}^{i-1}w_j,\sum_{j=0}^i w_j)$，即寻找满足条件的$i$：

      $\sum_{j=0}^{i-1}w_j\le S \lt \sum_{j=0}^i w_j$

       则对应的边就是被采样的边。

     这种方法需要$O(|E|)$时间复杂度来采样一条边，当边的数量$|E|$很大时时间代价太大。

   - `alias table` 方案：采用 `alias table` 之后，平均采样一条边的平摊时间复杂度为$O(1)$。

4. 从 `alias table` 采样一条边平均花费$O(1)$时间，计算这条边的损失函数的时间复杂度为$O(d(K+1))$，因此每个更新`step` 的时间复杂度为$O(dK)$。

   实践过程中我们发现最优化更新`step` 数量正比于边的数量，为$O(|E|)$。因此 `LINE` 算法的整体时间复杂度为$O(dK|E|)$，它是边数量的线性关系并且与顶点数量无关。

5. 边采样策略可以在不改变目标函数的情况下，提高随机梯度下降的效率，且不会影响最优化结果。

#### 2.1.6 二阶邻居

1. 当顶点的 `degree` 很小，即邻居很少的那些顶点，参数更新的机会相对较少因此很难得到其准确的 `representation` 。尤其是二阶邻近度模型严重依赖于顶点的这些邻居。

   一个简单的解决方案是：通过添加更高阶的邻居（如邻居的邻居）来扩充这些顶点的邻居集合。

   论文中仅考虑二阶邻居，即邻居的邻居。此时顶点$i$和它的二阶邻居$j$的权重为：

  $w_{i,j} = \sum_{k\in \mathcal N(i)} w_{i,k} \frac{w_{k,j}}{d_k}$

   其中$\mathcal N(i)$为顶点$i$的邻居集合，$d_k$为邻居顶点$k$的 `degree` 。

   对于顶点$i$，我们并不是添加所有的二阶邻居。我们添加部分二阶邻居使得顶点$i$的扩展邻居集合规模达到某个阈值，如`500` 。添加时采用$w_{i,j}$较大的那部分二阶邻居。

#### 2.1.7 新顶点

1. 对于`Graph` 中新加入的顶点，如何计算其`representation` ？

   - 如果新顶点$i$和图中已有的其它顶点产生链接，则可以计算经验分布$\hat p_1(\cdot,v_i),\hat p_2(\cdot\mid v_i)$。

     根据$O_1$目标函数和$O_2$目标函数，我们可以直接优化：

     从而得到一阶 `embedding` 向量和二阶 `embedding` 向量。其中$\mathcal N(i)$为顶点$i$的邻居顶点集合，这些邻居都是图中已有的顶点。

     在优化过程中，已有顶点的 `embedding` 保持不变，仅需要更新顶点$i$的 `embedding` 。

   - 如果新顶点$i$不与图中已有的其它顶点相连，则必须求助于其它信息，如顶点的文本信息。这也是论文未来探索的方向。

### 2.2 实验

1. 数据集：

   - 语言网络 `Language Network` 数据集：英文维基百科抽取的单词共现网络，顶点为单词、边为单词是否共现、边的权重为单词共现频次。

     论文采用窗口大小为`5` 的滑动窗口进行单词共现统计，窗口内的单词被认为是共现的。共现次数低于 `5`次的边被过滤掉。

   - 社交网络 `Social Network` 数据集：和`DeepWalk` 一致，这里也采用 `Flickr` 和 `Youtube` 数据集。

   - 引文网络 `Citation Network` 数据集：使用 `DBLP` 数据集构建的作者引用网络、论文引用网络。

   这些数据集的统计见下表，其中：

   - 所有这些网络囊括了有向图/无向图，带权图/无权图。
   - 每个网络至少包含一百万顶点和数百万边，最大的网络包含`200` 万顶点和十亿边。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/2M8cP7iYqt7k.png?imageslim">
   </p>
   

2. 论文将`LINE` 模型和其它现有的 `Graph Embedding` 方法进行比较。由于 `MDS,IsoMap,Laplacian eigenmap` 等方法无法处理大规模网络，因此它们并没有参与比较。

   - `Graph Factorization:GF` ：构建网络的亲和矩阵 `affinity matrix`，并通过矩阵分解来获取每个顶点的低维表达。

     `GF` 方法通过随机梯度下降法进行优化，因此可以处理大型网络。但是它仅适合无向图。

   - `DeepWalk`：对每个顶点使用从该顶点开始的截断随机游走来获取上下文信息，基于上下文信息建模来获取每个顶点的低维表达。

     `DeepWalk` 也利用了二阶邻近关系，但是它仅适用于无权网络。

   - `LINE-SGD`：直接通过`SGD` 来优化目标函数的 `LINE` 模型。在优化过程中并没有使用边采样策略。

     该方法有两个变种：

     - `LINE-SGD(1st)`：`LINE-SGD`的一阶邻近模型，它的优化目标是$O_1$。该模型仅适用于无向图。
     - `LINE-SGD(2nd)`：`LINE-SGD`的二阶邻近模型，它的优化目标是$O_2$。该模型可用于无向图和有向图。

   - `LINE`：标准的 `LINE` 模型。在优化过程中使用了边采用策略。

     该方法也有两个变种，分别为：

     - `LINE(1st)`：`LINE`的一阶邻近模型，它的优化目标是$O_1$。该模型仅适用于无向图。
     - `LINE(2nd)`：`LINE`的二阶邻近模型，它的优化目标是$O_2$。该模型可用于无向图和有向图。

   - `LINE(1st+2nd)`：同时拼接了`LINE(1st)` 和 `LINE(2nd)` 学到的 `representation` 向量，得到每个顶点的一个拼接后的、更长的表示向量。

     需要对拼接后的向量各维度进行加权从而平衡一阶表示向量和二阶表示向量的关系。在监督学习任务中，可以基于训练数据自动得到权重；在无监督学习任务中，无法配置这种权重。

     因此 `LINE(1st+2nd)` 仅用在监督学习任务中。

3. 参数配置：

   - 所有方法都统一的配置：
     - 随机梯度下降的 `batch-size = 1`，即每个批次一个样本。因此迭代的样本数量就等于更新的 `step` 数量。
     - 学习率$\rho_t = \rho_0(1-t/T)$，其中：$\rho_0 = 0.025$为初始化学习率；$t$表示第$t$个更新`step` ；$T$为总的更新`step` 数量。
     - 所有得到的 `embedding` 向量都进行归一化：$||\mathbf{\vec w}||_2 = 1$。
     - 为公平比较，语言网络数据集的 `embedding` 向量维度设为 `200`（因为 `word2vec` 方法采用该配置）；其它网络数据集的 `embedding` 向量维度默认设为 `128` 。
   - 对于 `LINE` 及其变种，负采样比例$K=5$。
   - 对于 `LINE(1st)` 、`LINE(2nd)` 及其变种，总迭代步数$T$等于 `100亿` ；对于 `GF` ，总迭代步数$T$等于 `200亿` 。
   - 对于 `DeepWalk`窗口大小 `win=10`，游走序列长度 `t=40`，每个顶点出发的序列数量$\gamma = 40$。

#### 2.2.1 语言网络数据集

1. 语言网络数据集包含200万顶点和10亿条边，评估任务为：

   - 单词类比 `word analogy`：给定一对单词 `(a,b)` 和一个单词 `c` ，该任务的目标是寻找单词`d`，使得`c` 和 `d`的关系类比于 `a` 和 `b` 的关系。记作：

    $a : b \rightarrow c: \;?$

     如：`(中国，北京 )` 对应于 `(法国，?)` ，这里的目标单词为 `巴黎`。

     因此给定单词 `a,b,c` 的 `embedding`向量，该任务的目标是寻找单词$d^*$，使得该单词的 `embedding` 尽可能与$\mathbf{\vec u}_b - \mathbf{\vec u}_a + \mathbf{\vec u}_c$相似：

    $d^* = \arg\max_d\cos\left(\mathbf{\vec u}_b - \mathbf{\vec u}_a + \mathbf{\vec u}_c ,\mathbf{\vec u}_d\right)$

   - 文档分类：从维基百科种选择7种不同的类别 `艺术,历史,人类,数学,自然,技术,体育`，对每个类别随机抽取 1万篇文章，其中每篇文章都只属于单个类别（如果文章属于多个类别就丢掉）。所有这些文章及其类别就构成了一个带标记的多类别语料库。

     我们利用文档中所有单词的 `embedding` 均值来表示文档`embedding` ，然后利用`one-vs-rest` 逻辑回归分类器进行分类，并评估分类结果的 `Micro-F1` 和 `Macro-F` 指标。

     虽然这种文档表示方式比较粗糙，但是这里的重点是比较不同的单词`embedding` ，而不是寻找一个良好的文档 `embedding` 方法。

2. 在维基百科语言网络上的单词类比结果如下表所示，采用了两种方式的类比（语义类比 `Semantic`、句法类比 `Sytactic`）。其中：

   - 对`GF` 方法，边的权重定义为单词共现次数的对数，这比直接采用单词共现次数更好的性能。
   - 对 `DeepWalk` 方法，尝试使用不同的截断阈值从而将网络权重二元化`binarize` 。最终当所有边都被保留下来时，模型性能最好。
   - `SkipGram` 直接从原始维基百科页面内容而不是语言网络来学习词向量。
   - 所有模型都是在单机上运行 `16` 个线程来计算，机器配置：`1T` 内存、`2.0GHZ` 的 `40` 个 `CPU` 。

   结论：

   - `LINE(2nd)` 优于所有其它方法。

     - `LINE(2nd)` 优于 `GF,LINE(1st)` 等一阶方法。这表明：和一阶邻近度相比，二阶邻近度更好的捕获了单词的语义。

       因为如果单词 `a` 和 `b` 的二阶邻近度很大，则意味着可以在相同上下文中可以相互替换单词`a` 和 `b` 。这比一阶邻近度更能说明相似语义。

     - 虽然 `DeepWalk` 也探索了二阶邻近度，但是其性能不如 `LINE(2nd)`，甚至不如一阶方法的 `GF,LINE(1st)` 。

       这是因为 `DeepWalk` 忽略了单词共现次数的原因，事实上语言网络中单词共现频次非常重要。

     - 在原始语料上训练的 `SkipGram` 模型也不如 `LINE(2nd)`，原因可能是语言网络比原始单词序列更好的捕获了单词共现的全局信息。

   - 采用 `SGD` 直接优化的 `LINE` 版本效果要差得多。这是因为语言网络的边的权重范围从个位数到上千万，波动剧烈。这使得最优化过程受到严重影响。

     在梯度更新过程中进行边采样处理的`LINE` 模型有效解决了该问题，最终效果要好得多。

   - `LINE(1st)` 和 `LINE(2nd)` 的训练效率很高，对于百万结点、十亿边的网络只需要不到3个小时。

     - `LINE(1st),LINE(2nd)` 训练速度比`GF` 至少快 `10%`，比 `DeepWalk` 快5倍。
     - `LINE-SGD` 版本要稍慢，原因是在梯度更新过程中必须应用阈值截断技术防止梯度爆炸。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/T9z3DuHUfETP.png?imageslim">
   </p>
   

3. 在维基百科语言网络上的文本分类结果如下表所示。其中我们分别抽取 `10%~90%`的标记样本作为训练集，剩余部分作为测试集。对每一种拆分我们随机执行 `10` 遍取平均结果。

   结论：

   - `GF` 方法优于 `DeepWalk`，因为 `DeepWalk` 忽略了单词共现次数。
   - `LINE-SGD` 性能较差，因为边权重波动剧烈所以`LINE-SGD` 的梯度更新过程非常困难。
   - 采用边采样的 `LINE` 模型优于 `LINE-SGD`，因为梯度更新过程中使用边采样能解决边权重波动剧烈带来的学习率选择问题。
   - `LINE(1st) + LINE(2nd)` 性能明显优于其它所有方法，这表明一阶邻近度和二阶邻近度是互补的。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/m2G5UXD3w2Cj.png?imageslim">
   </p>
   

4. 下表对给定单词，分别采用一阶邻近度模型和二阶邻近度模型召回其最相似的 `top`单词。可以看到：

   - 二阶邻近度召回的最相似单词都是语义相关的单词。
   - 一阶邻近度召回的最相似单词是语法相关、语义相关的混合体。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/sbdLiLPOSzCq.png?imageslim">
   </p>
   

#### 2.2.2社交网络数据集

1. 相比语言网络，社交网络更为稀疏，尤其是`YouTube` 网络。论文通过多标签分类任务来评估每个模型的`embedding` 效果。多标签分类任务将每个顶点分配到一个或者多个类别，每个类别代表一个社区`community` 。最终评估分类结果的 `Micro-F1` 和 `Macro-F` 指标。

   论文分别抽取 `10%~90%` 的标记样本作为训练集，剩余部分作为测试集。对每一种拆分随机执行 `10` 遍取平均结果。

2. `Flickr Network` 数据集采用最热门的5个类别作为分类的类别，评估结果如下表。

   - `LINE(1st)` 模型优于 `GF` 模型，这表明`LINE(1st)` 模型比 `GF` 模型具有更好的一阶邻近度建模能力。
   - `LINE(2nd)` 模型优于 `DeepWalk` 模型，这表明`LINE(2nd)` 模型比 `DeepWalk` 模型具有更好的二阶邻近度建模能力。
   - `LINE(1st)` 模型略微优于 `LINE(2nd)` 模型，这和语言网络的结论相反。原因有两个：
     - 社交网络中，一阶邻近度比二阶邻近度更重要，因为一阶关系表明两个顶点的关系更紧密。
     - 当网络非常稀疏并且顶点的平均邻居数太小时，二阶邻近度可能不太准确。
   - `LINE(1st) + LINE(2nd)` 性能明显优于其它所有方法，这表明一阶邻近度和二阶邻近度是互补的。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/668BlGwuufQa.png?imageslim">
   </p>
   

3. `YouTube` 数据集非常稀疏，每个顶点的平均`degree` 小于 5 。对该数据集的评估结果如下表。

   - 在不同规模训练集上，`LINE(1st)`一致性的优于 `LINE(2nd)`。这和 `Flickr` 数据集一致。

   - 由于极度稀疏性，`LINE(2nd)` 性能甚至不如 `DeepWalk` 。

   - `LINE(1st) + LINE(2nd)` 优于 `128` 维的`DeepWalk` ，并逐渐超越`256` 维的 `DeepWalk` 。

     这表明一阶邻近度和二阶邻近度是互补的，并能够缓解网络稀疏问题。

   考察 `DeepWalk` 是如何通过截断的随机游走来解决网络稀疏性问题的。随机游走类似于深度优先搜索，这种方式可以通过引入间接邻居来迅速缓解邻居稀疏的问题。但是这种方式也可能引入距离较远的顶点，距离较远意味着相关性不大。

   一种更合理的方式是采用广度优先搜索策略来扩展每个稀疏顶点的邻居，如二阶邻居策略。下表中，括号中的指标是二阶邻居策略的表现。其中我们对邻居数量少于 `1000` 的顶点扩展其二阶邻居，直到扩展邻居集合规模达到 `1000` 。采用二阶邻居策略的网络称作重构网络 `reconstructed network` 。

   之所以阈值设定为 `1000` 是因为实验发现：继续添加顶点并不会添加性能。

   - 采用二阶邻居策略之后`GF,LINE(1st),LINE(2nd)` 的性能都得到提升，其中 `LINE(2nd)` 的性能提升最为明显。

   - 采用二阶邻居策略之后 `LINE(2nd)`大多数情况下都优于 `DeepWalk` 。

   - 采用二阶邻居策略之后 `LINE(1st+2nd)` 性能并没有提升太多。这意味着原始网络的一阶邻近度和二阶邻近度组合已经捕获了大部分信息。

     因此，`LINE(1st+2nd)` 是一个非常有效的`Graph Embedding` 方式，适用于`dense` 网络和 `sparse` 网络。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/pbPCmdcloSfu.png?imageslim">
   </p>
   

#### 2.2.3 引文网络数据集

1. 引文网络数据集包括作者引用网络、论文引用网络，它们都是有向图。由于`GF` 和 `LINE(1st)` 都无法应用于有向图，因此这里仅仅比较 `DeepWalk` 和 `LINE(2nd)` 。

   论文通过多标签分类任务评估顶点 `embedding` 的效果。论文采用 7 个热门的会议（`AAAI,CIKM,ICML,KDD,NIPS,SIGIR,WWW`）作为分类类别，在会议中发表的作者或者论文被标记为对应类别。最终评估分类结果的 `Micro-F1` 和 `Macro-F` 指标。

   论文分别抽取 `10%~90%` 的标记样本作为训练集，剩余部分作为测试集。对每一种拆分随机执行 `10` 遍取平均结果。

2. 作者引用网络数据集的评估结果如下表所示。

   - 由于网络稀疏，因此 `DeepWalk` 性能优于 `LINE(2nd)` 。
   - 通过二阶邻居策略重构网络，邻居规模的阈值设定为 `500`，最终 `LINE(2nd)` 性能大幅提升并且优于 `DeepWalk` 。
   - `LINE-SGD(2nd)` 性能较差，因为边权重波动剧烈所以`LINE-SGD` 的梯度更新过程非常困难。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/f1uTIvfKh7Qx.png?imageslim">
   </p>
   

3. 论文引用网络数据集的评估结果如下所示。

   - `LINE(2nd)` 的性能明显优于 `DeepWalk`。这是由于在论文引用网络中的随机游走只能沿着引用路径来游走，这使得一些时间比较久远的、相关性不大的论文被作为上下文。

     相比之下，`LINE(2nd)` 的上下文都是最近的、密切相关的参考文献，因此更为合理。

   - 通过二阶邻居策略重构网络，邻居规模的阈值设定为 `200`，最终 `LINE(2nd)` 性能得到进一步提升。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/jnXw9rIPPNzt.png?imageslim">
   </p>
   

#### 2.2.4 可视化

1. `Graph Embedding` 的一个重要用途是输出有意义的`Graph` 可视化。论文选择作者引用网络数据集来可视化，选择了三个领域的不同会议：

   - 数据挖掘`data mining` 领域的 `WWW,KDD` 会议
   - 机器学习 `machine learning` 领域 的 `NIPS,IM`L 会议
   - 计算机视觉 `computer vision` 领域的 `CVPR,ICCV` 会议

   作者引用网络基于这些会议公开发表的文章来构建，丢弃 `degree < 3` 的作者（表明这些作者不怎么重要），最终得到一个包含 `18561` 个顶点、`207074` 条边的`Graph` 。

   论文首先通过不同的模型来得到 `Graph Embedding`，然后将顶点的 `embedding`向量通过 `t-SNE` 进行可视化。下图中不同颜色代表不同的领域：红色代表数据挖掘，蓝色代表机器学习，绿色代表计算机视觉。

   - `GF` 模型的可视化结果不是很有意义，因为不同领域的顶点并没有聚集在一起。

   - `DeepWalk` 模型的可视化结果要好得多，但是很多不同领域的顶点密集的聚集在中心区域，其中大多数是`degree` 较高的顶点。

     这是因为： `DeepWalk` 使用基于截断随机游走的方式来生成顶点的邻居，由于随机性这会带来很大的噪声。

     尤其是`degree` 较高的顶点，因为对于`degree` 较高的顶点每次随机选择的路径几乎都不同，这使得`degree` 较高的顶点每次都出现在不同的低频上下文中。

   - `LINE(2nd)` 模型的可视化结果更好、更具有意义。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/3sSvGkNg6lh2.png?imageslim">
   </p>
   

#### 2.2.5 参数探索及其它

1. 以社交网络为例，论文分析了模型性能和`Graph` 稀疏性的影响。

   - 图`a` 给出了 `Flickr` 网络链接的百分比和`LINE` 模型性能的关系。选择 `Flickr` 的原因是它比 `YouTube` 网络更密集。论文从原始网络中随机采样不同比例的链接从而构建具有不同稀疏性的网络。

     - 一开始网络非常稀疏时，`LINE(1st)` 的性能好于 `LINE(2nd)` 。
     - 当网络逐渐密集时，`LINE(2nd)` 的性能超越了 `LINE(1st)` 。

     这表明：当网络及其稀疏时二阶邻近度模型受到影响；当每个顶点附近有足够的邻居时，二阶邻近度模型优于一阶邻近度模型。

   - 图 `b` 给出了`YouTube` 数据集原始网络和二阶邻居策略重构网络中，顶点`degree` 和顶点性能指标的关系。

     论文将顶点根据 `degree` 来分组：：$(0,1],[2,3],[5,6],[1,12],[13,30],[31,+\infty)$。然后评估每个分组的顶点分类结果指标。

     - 当顶点的 `degree` 增加时，所有模型的效果都得到提升。
     - 在原始网络中除第一个分组之外，`LINE(2nd)` 的性能始终优于 `LINE(1nd)` 。这表明二阶邻近度模型不适用于 `degree` 较小的点。
     - 在重构网络中，`LINE(1st)`和 `LINE(2nd)` 都得到了改善，尤其是 `LINE(2nd)` 。
     - 在重构网络中，`LINE(2nd)`始终优于 `DeepWalk` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/87qturPSthNR.png?imageslim">
   </p>
   

2. 论文考察了不同的`embedding` 维度$d$和 `LINE` 模型性能的关系，以及模型的收敛速度。

   - 图 `a` 给出了不同维度$d$时模型的性能。可以看到：当$d$过大时，`LINE(1st)`和 `LINE(2nd)` 性能有所下降。

   - 图 `b` 给出了 `LINE` 和 `DeepWalk` 的性能和收敛速度。可以看到：`LINE(2nd)` 始终优于 `LINE(1st)`和 `DeepWalk`，并且`LINE(1st)` 和 `LINE(2nd)` 收敛速度都快于 `DeepWalk` 。

     由于 `batch-size = 1`，因此每个`sample` 需要一个迭代`step`。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/b94KsJBRuPFm.png?imageslim">
   </p>
   

3. 最后论文给出了采用边采样和异步随机梯度下降法的`LINE` 模型的可扩展性 `scalability` 。

   - 图 `a` 给出了在 `YouTube` 数据集上的线程数及其加速比。可以看到线程加速比接近线性。
   - 图`b` 给出了使用多线程进行模型更新时，模型性能的波动。可以看到采用多线程训练模型时，模型的最终性能保持稳定。

   这两个结果表明：`LINE` 模型具有很好的可扩展性（即并行度）。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/JD5LHu0jPsIW.png?imageslim">
   </p>
   

## 三、GraRep

1. 目前的 `Graph Embedding` 算法都存在局限性。

   - `DeepWalk` 算法虽然在经验上有效，但是它在学习过程中使用的损失函数在`Graph` 上的明确含义尚未可知。另外其采样过程又慢又复杂。
   - `LINE` 模型显式定义了损失函数来捕获一阶邻近度和二阶邻近度，它们分别代表了一阶局部关系和二阶局部关系。但是它无法扩展到二阶以上的邻近关系。

   论文 `《GraRep: Learning Graph Representations with Global Structural Information》` 提出了 `GraRep` 模型，该模型显式定义了损失函数，并捕获了图的全局结构信息。

   通过在语言网络、社交网络、引用网络等数据集上的实验表明，`GraRep` 模型学到的全局 `representation` 信息可以有效作用于聚类、分类以及可视化等任务中。

2. 当构建全局结构信息时，需要捕获各种$k$阶关系。如下图所示给出了顶点 `A1` 和 `A2` 之间的不同$k$阶关系。粗线表示关系很强，细线表示关系较弱。

   - 图`a` 和 `e` 展示了一阶关系的重要性，其中顶点 `A1` 和 `A2` 直接相连。

     - 图 `a` 中这两个顶点具有较强的一阶关系。
     - 图 `e` 中这两个顶点具有较弱的一阶关系。

   - 图 `b` 和 `f` 展示了二阶关系的重要性，其中顶点 `A1` 和 `A2` 不相连但是有共同的邻居：

     - 图`b` 中这两个顶点具有多个共同邻居，因此具有较强的二阶关系。
     - 图 `f` 中这两个顶点只有一个共同邻居。，因此具有较弱的二阶关系。

     显然二阶关系对于捕获两个顶点的关系很重要，共同邻居越多则关系越强。

   - 图 `c` 和 `g` 展示了三阶关系的重要性，其中顶点 `A1` 和 `A2` 不相连：

     - 图`g` 中尽管 `A1` 和 `B` 之间关系较强，但是由于 `B` 到 `C` 之间、以及 `C` 到 `A2` 之间的关系较弱，因此 `A1` 到 `A2` 的整体关系也较弱。
     - 图 `c` 中由于 `B` 到 `A2` 之间有大量的共同邻居，因此 `A1` 到 `A2` 的整体关系较强。

     显然在学习图的全局`representation` 时，必须捕获这些三阶信息。

   - 图 `d` 和 `h` 展示了四阶关系的重要性，其中顶点 `A1` 和 `A2` 不相连：

     - 图`d` 中，因为 `A1` 和 `A2` 之间存在多条路径，因此其关系较强
     - 图 `h` 中，因为`A1` 和 `A2` 之间不存在路径，因此其关系较弱。

     显然在学习图的全局`representation` 时，也必须捕获这些四阶信息。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/k7BiWVhoIaR6.png?imageslim">
   </p>
   

3. 在学习图的表达时，除了必须引入不同的$k$阶信息，也必须区别性的对待不同的$k$阶信息。

   如下图`a` 所示，顶点 `A` 接收到两类信息：

   - 从顶点 `B` 获得的一阶信息。
   - 从顶点 `C1,C2,C3,C4` 获得的二阶信息。

   如果这些信息不佳区分（比如都映射到同一个子空间），则完全可以构建图 `b` 来替代图 `a` 。但是二者是完全不同的结构。

   因此我们的模型应该将不同的$k$阶信息映射到不同的子空间中。与此相反，`SkipGram` 模型就将所有的$k$阶关系（如一阶关系、二阶关系 .....）映射到同一个子空间。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/DMrlwLs9epTc.png?imageslim">
   </p>
   

4. `GraRep` 模型同时采用了不同的$k$阶关系从而捕获图的全局结构信息，同时不同的$k$阶关系映射到不同的子空间中。

   - `LINE` 模型虽然将不同的$k$阶关系映射到不同的子空间，但是仅考虑一阶关系和二阶关系，未考虑更高阶的关系。
   - `SkipGram` 虽然考虑了不同的$k$阶关系，但是它将所有这些关系都映射到同一个子空间。

### 3.1 模型

1. 给定图$G=(V,E)$，其中：

   -$V=\{v_1,\cdots,v_n\}$为图的顶点集合，$v_i$表示图的一个顶点。
   -$E=\{e_{i,j}\}$为图的边集合，$e_{i,j}$表示顶点$v_i,v_j$之间的边，其权重为$w_{i,j}$。权重衡量了边的重要性。

   `Graph Embedding` 的任务是：学习全局 `representation` 矩阵

   其中第$i$行的向量$\mathbf{\vec w}_i \in \mathbb R^d$代表顶点$v_i$的表达，该向量捕获了顶点$v_i$的全局结构信息。

2. 定义邻接矩阵$\mathbf S$为：

  $S_{i,j} = \begin{cases}w_{i,j},&\text{if exist edge from $v_i$ to $v_j$} \\0,&\text{else}\end{cases}$

   这里假设所有的权重都是正的。

   定义度矩阵`degree matrix`$\mathbf D$为一个对角矩阵：

  $D_{i,j} = \begin{cases}\sum_{p\ne i, 1\le p\le n}S_{i,p},&\text{if $i = j$}\\0,&\text{if $i \ne j$}\end{cases}$

   假设顶点$v_i$转移到顶点$v_j$的概率正比于$S_{i,j}$，则定义转移概率矩阵 `probability transition matrix`：$\mathbf A = \mathbf D^{-1}\mathbf S$。其中$A_{i,j}$定义了从顶点$v_i$经过一步转移到顶点$v_j$的概率，因此也称为一阶转移概率矩阵。显然$\mathbf A$可以认为是对$\mathbf S$按 ”行“ 进行的归一化。

3. 考虑顶点$w$和顶点$c$之间的关系。这里要考虑两个问题：

   - 顶点$w$和$c$之间是否有关，即是否存在从顶点$w$到$c$之间的路径？
   - 如果有关则关系有多紧密，即如果从顶点$w$开始随机转移则能够到达顶点$c$的可能性有多大？

   定义从顶点$w$刚好经过$k$步到达顶点$c$的概率为$p_k(c\mid w)$，称作$k$阶转移概率。则有：

  $p_1(j\mid i) = A_{i,j}$

   定义$k$阶转移概率矩阵为：

  $\mathbf A^k =\underbrace{ \mathbf A\cdots\mathbf A}_{k}$

   可以证明有：$p_k(j\mid i) = A_{i,j}^k$。其中$A_{i,j}^k$表示$\mathbf A^k$的第$i$行第$j$列。

   首先考虑特定的$k$（比如$k=2$）。对于给定的顶点$w$称作当前顶点 `current vertex`，从$w$开始的所有$k$阶路径的终点集合$\mathbb C = (c_1,c_2,\cdots)$称作上下文，上下文中的每个顶点称作上下文顶点 `context vertex` 。

   受`Skip-Gram` 模型启发，我们采用 `noise contrastive estimation:NEC` 来定义目标函数。对给定顶点$w$，其损失函数定义为：

  $L_k(w) = \left(\sum_{c\in V}p_k(c\mid w)\log\sigma(\mathbf{\vec w}\cdot \mathbf{\vec c})\right) + \lambda \mathbb E_{c^\prime \sim p_k(V)}[\log\sigma(-\mathbf{\vec w}\cdot \mathbf{\vec c}^\prime )]$

   其中：

   - 第一项给出了图中存在的、所有以$w$开始的$k$阶顶点对（正样本），第二项给出了以$w$开始的、终点随机采样的$k$阶顶点对（负样本）。

   -$p_k(c\mid w)$表示$w$和$c$之间的$k$阶关系。

   -$\sigma(\cdot)$为 `sigmoid` 函数：$\sigma(x) = (1+e^{-x})^{-1}$

   -$\lambda$为超参数，给出负样本数量。

   -$\mathbf{\vec w},\mathbf{\vec c},\mathbf{\vec c}^\prime$分别为顶点$w,c,c^\prime$的`representation` 向量。

   -$p_k(V)$为图中所有顶点的概率分布，$\mathbb E_{c^\prime \sim p_k(V)}[\cdot]$给出了当$c^\prime$服从分布$p_k(V)$时的期望。这里$c^\prime$为负采样的顶点。

     其中：

    $p_k(c) = \sum_{w^\prime}q(w^\prime)p_k(c\mid w^\prime) = \frac 1 {|V|}\sum_{w^\prime}A^k(w^\prime,c)$

    $p_k(c)$的物理意义为：从所有满足$k$阶关系的顶点中选择顶点$c$的概率。其中$|V|$为顶点数量，$q(w^\prime)$为选择$w^\prime$作为路径的第一个顶点的概率，假设它服从均匀分布$q(w^\prime) = \frac 1{|V|}$。

  $\mathbb E_{c^\prime \sim p_k(V)}[\cdot]$可以化简为：

  $\mathbb E_{c^\prime \sim p_k(V)}[\log\sigma(-\mathbf{\vec w}\cdot \mathbf{\vec c}^\prime )] = \sum_{c^\prime \in V} p_k(c^\prime ) \times \log\sigma(-\mathbf{\vec w}\cdot \mathbf{\vec c}^\prime )$

   因此得到：

  $L_k(w) = \sum_{c\in V}\left[p_k(c\mid w)\log\sigma(\mathbf{\vec w}\cdot \mathbf{\vec c}) + \lambda p_k(c) \log\sigma(-\mathbf{\vec w}\cdot \mathbf{\vec c})\right]$

   这里对于给定的顶点对$(w,c)$，定义其局部损失函数为：

   则$k$阶损失函数为：

  $\mathcal L_k = \sum_{w\in V}\sum_{c\in V} L_k(w,c)$

   为最小化$\mathcal L$则只需要最小化每个成分，即求解：

  $\min L_k(w,c)$

   令$e= \mathbf{\vec w}\cdot \mathbf{\vec c}$，求解：

  $\frac{\partial }{\partial e} L_k(w,c) = 0$

   得到最优解：

  $e= \mathbf{\vec w}\cdot \mathbf{\vec c} = \log\left(\frac{A^k_{w,c}}{\sum_{w^\prime}A^k_{w^\prime,c}}\right) -\log(\beta)$

   其中$\beta = \frac{\lambda }{|V|}$。

4. 定义矩阵$\mathbf Y^k$为：

  $Y_{i,j}^k = \log\left(\frac{A^k_{w,c}}{\sum_{w^\prime}A^k_{w^\prime,c}}\right) -\log(\beta)$

   定义矩阵：

   则有：$\mathbf Y^k = \mathbf W^k \mathbf C^k$。因此我们将最优化问题转化为矩阵分解问题。

   为降低噪音我们将$\mathbf Y^k$中的所有负数都替换为正数。因为若存在负数，则图中存在的$k$阶定点对的概率$\sigma(\mathbf{\vec w}\cdot \mathbf{\vec c}) \lt \frac 12$。则有：

  $\mathbf X^k = \max(\mathbf Y^k, 0 )$

   虽然有很多矩阵分解技术，这里论文采用 `SVD` 矩阵分解。给定矩阵$\mathbf X^k$其 `SVD` 分解为：

  $\mathbf X^k = \mathbf U^k\Sigma^k(\mathbf V^k)^T$

   其中$\mathbf U^k,\mathbf V^k$均为正交矩阵；$\Sigma^k$为对角矩阵，对角线原始为按降序排列的矩阵奇异值。

   取最大的 `d` 个奇异值以及$\mathbf U^k,\mathbf V^k$的前面 `d` 列，分别对应$\Sigma^k_d, \mathbf U^k_d,\mathbf V^k_d$。其物理意义为：$\mathbf X^k (\mathbf X^k )^T$最大的 `d` 个特征值及其对应的特征向量。

   此时有：

  $\mathbf X^k\simeq \mathbf X_d^k = \mathbf U_d^k\Sigma_d^k(\mathbf V_d^k)^T$

   令$\mathbf W^k = \mathbf U_d^k(\Sigma_d^k)^{1/2},\mathbf C^k=(\Sigma_d^k)^{1/2}(\mathbf V_d^k)^T$，因此解决该矩阵分解问题。

   -$\mathbf W^k$的每一行给出了各顶点作为当前顶点的 `representation` 向量。

   -$\mathbf C^k$的每一列给出了各顶点作为上下文顶点的 `representation` 向量。

   - 除了 `SVD` 方法进行矩阵分解之外，还可以采用其它方法如 `incremental SVD,ICA,dnn` 。

     这里主要聚焦于学习 `Graph Embedding` 而不是矩阵分解技术。事实上如果采用更高级的 `dnn` 技术来分解矩阵，则最终效果很难说明是由于 `Graph Embedding` 模型的效果还是 `dnn` 降维的效果。

5. `GraRep` 模型考虑所有的$k$阶关系来捕获图的全局信息，即$k=1,2,\cdots,K$。事实上当$k$足够大时，$\mathbf A^k$就收敛到一个固定值，因此通常$K$选择一个不大的值。

   最终 `GraRep` 模型将每个顶点的$1,2,\cdots,K$阶 `representation` 拼接成一个整体作为顶点的 `representation` 。因为不同的$k$阶`representation` 反应了不同的局部信息。

6. `GraRep` 算法：

   - 算法输入：

     - 图的邻接矩阵$\mathbf S$
     - 最大转移阶数$K$
     - 对数偏移系数$\beta$
     - 维度$d$

   - 算法输出：每个顶点的表达矩阵$\mathbf W$

   - 算法步骤：

     - 计算$k=1,2,\cdots,K$阶转移概率矩阵$A^k$。计算流程：首先计算$\mathbf A = \mathbf D^{-1} \mathbf S$；然后依次计算$\mathbf A^1,\mathbf A^2,\cdots,\mathbf A^k$。

       > 对于带权图，$\mathbf S$是一个实数矩阵；对于无权图，$\mathbf S$是一个`0-1` 矩阵。算法都能够处理。

     - 对$k=1,2,\cdots,K$迭代计算每个顶点的$k$阶表达，计算步骤为：

       - 获取正的对数概率矩阵$\mathbf X^k$：首先计算$\Gamma_1^k,\Gamma_2^k,\cdots,\Gamma_N^k$，其中$\Gamma_j^k=\sum_p A_{p,j}^k$。然后计算：

      $X_{i,j}^k = \max\{\log\left(\frac{A_{i,j}^k}{\Gamma_j^k}\right)-\log\beta,0\}$

       - 计算`representation`矩阵$\mathbf W^k$：

       - 拼接所有的$k$阶表达：

         其中$<>$表示向量的拼接。

### 3.2 GraRep vs SGNS

1. 事实上 `SGNS` 是 `GraRep` 的特例。

2. 定义$\mathbf M = \mathbf A^1 + \mathbf A^2 +\cdots + \mathbf A^K$，其中$M_{w,c} = \sum_{k=1}^K A_{w,c}^k$。考虑所有的$k=1,2,\cdots,K$阶损失：

   同样的推导过程可以解得最优解：

  $e= \mathbf{\vec w}\cdot \mathbf{\vec c} = \log\left(\frac{M_{w,c}}{\sum_{w^\prime}M_{w^\prime,c}}\right) -\log(\beta)$

   其中其中$\beta = \frac{\lambda }{|V|}$。

3. 设随机游走序列长度为$K$，顶点$w$出现在$\gamma$条序列中。考虑所有的 `vertex-context` 集合$\mathbb D$，则顶点$w$出现在$\mathbb D$中的此时为$n(w) = \gamma\times K$。

   设当前顶点为$w$，则：

   - 顶点$c$为其一阶上下文的次数为：

    $n_1(w,c) = \gamma\times K\times p_1(c\mid w) = \gamma K A_{w,c}^1$

   - 顶点$c$为其二阶上下文的次数：

    $n_2(w,c) = \gamma\times K\times p_2(c\mid w) = \gamma\times K\times\sum_{c^\prime}\left(p_1(c\mid c^\prime)\times p_1(c^\prime\mid w)\right)= \gamma K A_{w,c}^2$

   - ...

   - 顶点$c$为其$K$阶上下文的次数：

    $n_K(w,c) = \gamma\times K\times p_K(c\mid w)= \gamma K A_{w,c}^K$

   则有顶点$c$位于$w$的上下文中的次数为：

  $n(w,c) = \sum_{k=1}^K n_k(w,c) = \gamma K M_{w,c}$

   则`context`$c$出现在$\mathbb D$中的次数为：

  $n(c) = \sum_w n(w,c) = \gamma K \sum_w M_{w,c}$

   根据论文 `《NeuralWord Embedding as Implicit Matrix Factorization》` ， `SGNS` 等价于矩阵分解。因此`SGNS` 的解为：

  $e = \mathbf{\vec w}\cdot \mathbf{\vec c} = \log\left(\frac{n(w,c)\times |\mathbb D|}{n(w)\times n(c)}\right) - \log \lambda$

   其中$|\mathbb D|$为所有观测到的 `vertext-context` 组合的数量，因此有$|\mathbb D| = \gamma \times K\times |V|$，$|V|$为图中所有顶点数量。

   因此有：

  $e = \mathbf{\vec w}\cdot \mathbf{\vec c} = \log\left(\frac{M_{w,c} }{\sum_{w^\prime\in |V|} M_{w^\prime,c}}\right) - \log \beta$

   其中其中$\beta = \frac{\lambda }{|V|}$。

   这刚好就是 `GraRep` 的解。

### 3.3 实验

1. 数据集：

   - 语言网络数据集`20-Newsgroup` ：包含2万篇新闻组文档，并按照20个不同的组分类。每篇文档由文档内单词的 `tf-idf` 组成的向量表示，并根据余弦相似度计算得到文档的相似度。根据文档的这些相似度构建语言网络，该网络是一个全连接的带权图，用于聚类效果的评估。

     为验证模型的鲁棒性，论文分别从`3/6/9` 个不同的新闻组构建了三个更小规模的语言网络(`NG` 代表 `Newsgroups` ）：

     - `3-NG`：由`comp.graphics, comp.graphics and talk.politics.guns` 这三个新闻组的文档构成。
     - `6-NG`：由 `alt.atheism, comp.sys.mac.hardware, rec.motorcycles, rec.sport.hockey, soc.religion.christian and talk.religion.misc`这六个新闻组的文档构成。
     - `9-NG`：由 `talk.politics.mideast, talk.politics.misc, comp.os.ms- windows.misc, sci.crypt, sci.med, sci.space, sci.electronics, misc.forsale, and comp.sys.ibm.pc.hardware` 这九个新闻组的文档构成。

     这些小网络分别使用所有文档 `all data`，以及每个主题随机抽取`200`篇文档两种配置。

   - 社交网络数据集 `Blogcatalog`：每个顶点表示一个博客作者，每条边对应作者之间的关系。

     每个作者都带有多个标签信息，标签来自39种主题类别。该网络是一个无权图，用于多标签分类效果的评估。

   - 引文网络数据集 `DBLP Network`：每个顶点代表一个作者，边代表一个作者对另一位作者的引用次数。

     论文选择六个热门会议并分为三组：数据挖掘`data mining` 领域的 `WWW,KDD` 会议、机器学习 `machine learning` 领域 的 `NIPS,IM`L 会议、计算机视觉 `computer vision`领域的 `CVPR,ICCV` 会议。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/Bwxm25anE8qW.png?imageslim">
   </p>
   

2. 评估模型：`LINE` 模型、`DeepWalk` 模型、`E-SGNS` 模型、谱聚类模型。

   其中 `E-SGNS` 模型可以视为 `GraRep` 模型的特例，而谱聚类模型和 `E-SGNS` 模型的区别在于它们不同的损失函数。

3. 参数配置：

   - 对于 `LINE` 模型：`batch-size=1`，学习率为 `0.025`，负采样系数为 `5`，迭代`step` 总数为百亿级。

     论文拼接了一阶邻近度模型和二阶邻近度模型的 `representation`，并针对`degree` 较小的顶点执行重构策略来达到最佳性能。

   - 对于 `DeepWalk` 和 `E-SGNS` 模型：窗口大小为 `10`，序列最长长度为 `40`，每个顶点开始的游走序列数量为 `80` 。

   - 正则化：

     - `LINE,GraRep` 模型进行$L_2$正则化可以达到最佳效果。
     - `DeepWalk,E-SGNS` 模型没有正则化也能达到最佳效果。

   - `embedding` 维度$d$：为公平比较，`Blogcatalog` 和 `DBLP`数据集的$d=128$；而 `20-NewsGroup` 数据集的$d=64$。

   - `GraRep` 模型：$\beta = \frac 1{|V|}$；`Blogcatalog` 和 `DBLP`数据集的$K=6$， `20-NewsGroup` 数据集的$K=3$。

#### 3.3.1 语言网络数据集

1. 对语言网络数据集通过聚类任务评估各模型的性能。论文将各模型学到的 `representation` 执行 `k-means` 算法并评估归一化互信息`Normalized Mutual Information (NMI)` 得分。

   为确保结论可靠，每种`representation`重复运行 `10` 次 `k-means` 并报告平均 `NMI`得分。

   - 对于 `LINE` 模型这里采用了重构策略，邻居数量阈值分别设定为 `k-max = 0,200,500,1000` 取最佳阈值（`200`）。
   - 对于 `DeepWalk,E-SGNS` 以及谱聚类，论文除了给出$d=64$之外还给出了$d=192$的结果以评估不同维数的影响。

   最终结果如下表所示，每列的最佳结果用粗体突出显示。结论：

   - `GraRep` 模型在所有实验种都优于其它方法。
   - 对于 `DeepWalk,E-SGNS` 和谱聚类，增加维度$d$把那个不能有效提高性能。论文认为：较高的维度并不会为`representation` 提供额外的补充信息。
   - 对于`LINE` 模型采用重建策略确实有效，因为重建策略可以补充一阶邻近度、二阶邻近度以外的三阶结构信息。
   - `GraRep` 和 `LINE` 模型在很小的 `Graph` 上就可以得到良好的性能，论文认为：即使在图很小的条件下，这两个模型也可以捕获丰富的局部关系信息。
   - 当$d=16$时谱聚类达到最佳性能，但是对其它算法$d=64$达到最佳性能。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/CrUKsnf3MqwS.png?imageslim">
   </p>
   

#### 3.3.2 社交网络数据集

1. 对社交网络数据集通过多类别分类任务来评估各模型`representation` 的性能，评估指标为 `one-vs-rest` 逻辑回归分类器分类结果的 `Micro-F1` 和 `Macro-F1` 指标。

   论文随机采样 `10%~90%` 的顶点来训练，剩下的顶点作为测试集 。为确保结论可靠，每一种拆分评估10次取平均。

   这里`LINE` 模型采用了重构策略，邻居数量阈值分别设定为 `k-max = 0,200,500,1000` 取最佳阈值（`500`）。

   最终结果如下表所示，每列的最佳结果用粗体突出显示。结论：总体上 `GraPep` 性能远超过其它方法，尤其是仅使用 `10%` 样本训练。

   这表明`GraRep` 学到的局部结构信息可以相互补充从而捕获到全局结构信息。这在数据稀疏时具有显著优势。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/XAqouxfhfP5u.png?imageslim">
   </p>
   

#### 3.3.3 引文网络数据集

1. 对引文网络数据集通过 `t-SNE` 可视化来评估各模型`representation` 的效果。

   来自同一个研究领域的作者使用相同的颜色，其中绿色表示数据挖掘、紫色表示计算机视觉、蓝色表示机器学习。

   结论：

   - 使用谱聚类的效果一般，因为不同颜色的顶点相互混合。

   - `DeepWalk` 和 `E-SGNS` 结果看起来好得多，因为大多数相同颜色的顶点似乎构成了分组，但是分组的边界似乎不是很明显。

   - `LINE` 和 `GraRep`结果种，每个分组的边界更加清晰，不同颜色的顶点出现在明显可区分的区域中。

     与 `LINE` 相比 `GraRep` 的结果似乎更好，每个区域的边界更清晰。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/3ElNn9Nx3ysG.png?imageslim">
   </p>
   

2. 引文网络可视化的 `KL` 散度如下表所示。其中 `KL` 散度捕获了 ”成对顶点的相似度” 和 “二维投影距离” 之间的误差。

   更低的 `KL` 散度代表更好的表达能力。可以看到：`GraRep` 模型的顶点 `representation` 效果最好。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/P3JgRQBbuiuM.png?imageslim">
   </p>
   

#### 3.3.4 参数探索及其它

1. 考察不同的$K$值的效果。下图给出 `Blogcatelog` 数据集上不同$K$值对应得 `Micro-F1` 和 `macro-F1` 得分。

   为阅读方便，这里仅给出$K=1,2,5,6,7$的结果。因为实验发现：$K=4$的结果略好于$K=3$，而$K=5$的效果与$K=4$相当。

   结论：

   -$K=2$比$K=1$有明显改善，而$K=3$的性能进一步优于$K=2$。

     这表明：不同的$k$阶信息可以学到互补的局部信息。

   -$K=7$的结果并不比$K=6$好多少。

     这是因为当$k$足够大时，学到的$k$阶信息会减弱并趋于一个稳定的分布。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/JTmAFIfqAQbC.png?imageslim">
   </p>
   

2. 考察不同$d$值的效果。下图给出了 `3NG` 和 `9NG` 在不同$d$配置下不同模型的 `NMI` 得分。

   结论：

   - `GraRep` 模型结果始终优于其它模型。
   - 几乎所有算法都在$d=64$时取得最佳性能，当$d$继续增加到更大值时所有算法效果都开始下降。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/olW5bRpEh0wm.png?imageslim">
   </p>
   

3. 下图给出不同总维度下模型的运行时间。总维度等于$d\times K$，它代表最终 `GraRep` 拼接得到的`representation` 向量。

   - 图 `a` 给出了$d=128$且$K=1,2,\cdots,7$时，模型在 `Blogcatalog` 数据集上的模型训练时间。

     可以看到：模型训练时间随着总维度的增加而几乎线性增加。

   - 图 `b` 给出了在 `20-NewsGroup` 数据集不同规模的网络上训练的时间。

     可以看到：随着 `Graph` 规模的增长，模型训练时间显著增加。原因是随着 `Graph` 增长，矩阵幂运算$\mathbf A^k$和矩阵 `SVD` 分解涉及到时间复杂度较高的运算。

     后续可以考虑采用深度学习来替代 `SVD` 技术来做矩阵分解。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/aXxA1Am6H9hR.png?imageslim">
   </p>
   

## 四、TADW

1. 大多数网络表示学习方法仅仅研究网络结构。事实上，网络顶点包含了丰富的信息（如文本内容和其它元数据），而这些方法都无法利用到这些信息。

   利用顶点的文本信息的一个直接方法是：分别独立学习文本特征`representation`和网络顶点`representation`，然后将二者拼接在一起。这种方法没有考虑网络结构和文本信息之间的复杂交互，因此通常效果一般。

   论文`《Network Representation Learning with Rich Text Information》` 提出了 `text-associated DeepWalk:TADW` 模型，该模型在矩阵分解的框架下将顶点的文本信息纳入网络表示学习中。

### 4.1 模型

1. 给定网络$G=(V,E)$，`NRL` 的目标是对每个顶点$w\in V$得到一个低维表达$\mathbf{\vec w} \in \mathbb R^d$，其中$d\ll |V|$。

2. 根据论文 `《NeuralWord Embedding as Implicit Matrix Factorization》` 以及 `GraRep` 的结论，基于 `SGNS` 的 `DeepWalk` 等价于矩阵分解：

  $e= \mathbf{\vec w}\cdot \mathbf{\vec c} = \log\left(\frac{M_{w,c}}{\sum_{w^\prime}M_{w^\prime,c}}\right) -\log(\beta)$

   其中：

   -$\mathbf A = \mathbf D^{-1}\mathbf S$为转移概率矩阵，$\mathbf S$为邻接矩阵，$\mathbf D$为度矩阵。

    $A_{i,j}$定义了从顶点$v_i$经过一步转移到顶点$v_j$的概率，因此也称为一阶转移概率矩阵。

   -$\mathbf A^k =\underbrace{ \mathbf A\cdots\mathbf A}_{k}$为$k$阶转移矩阵，$A_{i,j}^k$定义了顶点$v_i$经过$k$步转移到顶点$v_j$的概率。

   -$\mathbf M = \mathbf A^1 + \mathbf A^2 +\cdots + \mathbf A^K$，$M_{w,c} = \sum_{k=1}^K A_{w,c}^k$。

   -$\beta = \frac{\lambda }{|V|}$。

   进一步的，基于该思路类似可以证明：基于`softmax` 的 `SkipGram` 模型等价于矩阵分解：

  $e= \mathbf{\vec w}\cdot \mathbf{\vec c} = \log M_{w,c} = \log \sum_{k=1}^K A_{w,c}^k$

   定义矩阵$\mathbf Y,Y_{w,c} = \log M_{w,c}$，则可以对$\mathbf Y$进行低秩分解：$\mathbf Y = \mathbf W \mathbf H ^T$，其中$\mathbf W\in \mathbb R^{|V|\times d},\mathbf H\in \mathbb R^{|V|\times d}$，$d\ll |V|$。

   这个问题是 `NP` 难的，因此研究者将这个问题转化为一个带约束的最优化问题：

  $\arg\min_{\mathbf W,\mathbf H} \sum_{(i,j)\in \mathbf \Omega }\left(Y_{i,j}- (\mathbf W\mathbf H^T)_{i,j}\right)^2 + \frac{\alpha } 2\left(||\mathbf W||_F^2+ ||\mathbf H||_F^2\right)$

   其中$\mathbf \Omega$是$\mathbf Y$观察到的部分，$||\cdot||_F$为 `Frobenius` 范数，$\alpha$为正则化系数。

3. 假设我们还有额外的特征，则可以使用 `inductive matrix completion` 技术来利用这些额外特征。

   假设每个顶点还有额外的特征$\mathbf T \in \mathbb R^{|V|\times f_t}$，则可以进行矩阵分解：

  $\mathbf Y = \mathbf W (\mathbf T\mathbf H) ^T$

   其中$\mathbf W\in \mathbb R^{|V|\times d},\mathbf H\in \mathbb R^{f_t\times d} ,\mathbf T\in \mathbb R^{|V|\times f_t}$。

   目标函数为：

  $\arg\min_{\mathbf W,\mathbf H} ||\mathbf Y - \mathbf W(\mathbf T\mathbf H)^T||_F^2 + \frac{\alpha } 2\left(||\mathbf W||_F^2+ ||\mathbf H||_F^2\right)$

   优于目标函数是$\mathbf W$或$\mathbf H$的凸函数，因此可以通过交替最小化$\mathbf W$和$\mathbf H$来求解该问题。实验中经过`10` 轮迭代即可收敛。

   - 尽管优化算法最终收敛到局部极小值而不是全局极小值，但是经验表明 `TADW` 效果较好。
   - 这里的$\mathbf T$就是每个顶点的文本`representation`矩阵，它是预训练好的。
   - 最终我们拼接$\mathbf W$和$\mathbf T\mathbf H$作为每个顶点的$2d$维 `representation` 。

4. 考虑到计算$\mathbf Y$的计算复杂度为$O(|V|^3)$，因此`TADW`采用近似计算在速度和准确性之间折衷：

  $\mathbf Y = \log(\mathbf A + \mathbf A^2+\cdots \mathbf A^K)\simeq \frac{\mathbf A + \mathbf A^2}{2}$

   有两个好处：

   - 计算复杂度降低到$O(|V|^2)$
   -$\frac{\mathbf A + \mathbf A^2}{2}$是稀疏矩阵，其非零项远远少于$\log(\mathbf M)$。

5. 算法复杂度：

   - 计算$\mathbf Y$的复杂度为$O(|V|^2)$。
   - 在最优化求解迭代过程中：
     - 最小化一次$\mathbf W$的复杂度为$O(Y_{nz} d + |V|d^2)$
     - 最小化一次$\mathbf H$的复杂度为$O(Y_{nz}d + |V| f_t d)$。

   总的算法复杂度为$O(|V|^2 + Y_{nz}d+|V|d^2 + |V|f_td)$。其中$Y_{nz}$为$\mathbf Y$中非零元素的数量。

### 4.2 实验

1. 数据集：

   - `Cora` 数据集：包含来自七个类别的 `2708` 篇机器学习论文。

     - 论文之间的链接代表引用关系，共有 `5429`个链接。

     - 从标题、摘要中生成短文本作为文档，并经过停用词处理、低频词处理（过滤文档词频低于 10个的单词），并将每个单词转化为 `one-hot`向量。

       最终每篇文档映射为一个 `1433` 维的`binary` 向量，每一位为`0/1` 表示对应的单词是否存在。

   - `Citeseer`数据集：包含来自六个类别的 `3312` 篇公开发表出版物。

     - 文档之间的链接代表引用关系，共`4732`个链接。

     - 从标题、摘要中生成短文本作为文档，并经过停用词处理、低频词处理（过滤文档词频低于 10个的单词），并将每个单词转化为 `one-hot`向量。

       最终每篇文档映射为一个 `3703` 维的`binary` 向量，每一位为`0/1` 表示对应的单词是否存在。

   - `Wiki` 数据集：包含来自十九个类别的 `2405`篇维基百科文章。

     - 文章之间的链接代表超链接，共 `17981` 个链接。我们移除了没有任何超链接的文档。
     - 每篇文章都用内容单词的 `TFIDF` 向量来表示，向量维度为 `4973` 维。

   其中`Cora、Citeseer` 数据集平均每篇文档包含 `18~32` 个单词，数据集被认为是有向图；`Wiki` 数据集平均每篇文档包含 640 个单词，数据集被认为是无向图。

2. 对比模型及其配置：

   - `TADW` ：通过`SVD` 分解 `TFIDF` 矩阵到 `200` 维的文本特征矩阵$\mathbf T\in \mathbb R^{200\times |V|}$，这是为了降低矩阵$\mathbf H$的规模。

     对于 `Cora,Citeseer` 数据集$d = 80,\alpha = 0.2$；对于维基百科数据集$d = 100,200,\alpha = 0.2$。

     注意：最终每个顶点的`representation` 向量的维度为 `2k`。

   - `DeepWalk`：窗口尺寸$t=10$，每个顶点的游走序列数量$\gamma = 80$。对于`Cora,Citeseer` 数据集维度$d=100$，对于维基百科数据集维度$d=200$。

   - `pLSA`：利用文档的主题分布作为文档的 `representation` 。

   - `Text Features`：使用文本特征矩阵$\mathbf T$作为每篇文档的 `200` 维 `representation`，这代表了仅使用文本特征的效果。

   - `Naive Combination`：直接拼接 `DeepWalk` 的`embedding` 向量和文本特征向量。对于 `Cora,Citeseer`数据集这将得到一个`300` 维向量；低于维基百科数据集这将得到一个 `400` 维向量。

   - `NetPLSA` ：引入文档之间的连接作为正则化项的主题模型。

     对于 `Cora,Citeseer` 数据集主题数量为 `160`；对于维基百科数据集主题数量为 `200` 。

3. 评估方式：我们对抽取的 `Graph Embedding` 分别采用线性 `SVM` 和 `TSVM` 来评估其监督学习和半监督学习的性能。评估方法是基于 `one-vs-rest` 来进行多分类任务。对于线性 `SVM` 我们分别随机选择 `10%~50%` 的样本作为训练集，剩下样本作为测试集；对于`TSVM` 我们分别随机选择 `1%~10%` 的样本作为训练集，剩下样本作为测试集。

   我们评估测试集的分类 `accuracy` 。每种拆分方式随机重复10次取均值作为结果。

4. 评估结果如下所示：

   - `-` 表示 `TSVM` 训练12个小时都未收敛。

   - 我们并未在维基百科上给出半监督学习的结果。因为在该数据集上`TADW` 在监督学习的小数据集上已经证明其优越性，无需继续验证。

   - 结论：

     - `TADW` 在所有三个数据集上始终优于其它所有模型。

       在 `Cora,Citeseer` 数据集上，即使 `TADW` 模型的训练数据减少一半，其效果然仍然超过其它所有模型。这证明了 `TADW` 是有效的。

     - `TADW` 在半监督学习方面显著提升。在 `Cora` 数据集的半监督学习任务中，`TADW` 超越了剩余的最佳模型 `4%`；在 `Citeseer` 数据集的半监督学习任务中，`TADW` 超越了剩余的最佳模型 `16%`。

       这是因为 `Citeseer` 数据集的噪声更大，而 `TADW` 对学习的噪声鲁棒性更好。

     - 当训练集较小时，`TADW` 效果更为明显。

       大多数模型的效果随着训练集比例的下降而迅速下降，因为这些模型的顶点`representation` 训练不充分，噪声较多。而`TADW` 从网络和文本中共同学习的 `representation` 噪音更少。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/T7vIr0gUBYao.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/vYcOIKE2mKR8.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/ySlDPFj4EpmE.png?imageslim">
   </p>
   

5. 固定训练集比例为 `10%` ，我们评估超参数$d$（维度）和$\alpha$（正则化系数）的影响。

   对于 `Cora,Citeseer` 数据集$d = 40 \sim 120, \alpha = 0.1\sim 1.0$；对于维基百科数据集$d = 100 \sim 200, \alpha = 0.1\sim 1.0$。

   可以看到：

   - 当固定维度$d$时，`TADW`预测结果随着$\alpha$的增加基本保持不变。
   - 对于不同的数据集，最佳的维度$d$有所不同。

   > 如下图所示，原始论文用$k$表示维度，用$\lambda$表示正则化项。这里为了表达方便采用了不同的符号。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/vf52QXm9h68Y.png?imageslim">
   </p>
   

## 五、DNGR

1. 虽然 `DeepWalk` 可以有效的学习无权图的顶点`representation` ，但是它存在两个缺点：

   - 获得`vertex-context` 的采样过程效率太低，且无法对带权图进行采样。

   - `SGNS` 等价于对 `PPMI` 矩阵进行矩阵分解，目前广泛使用的 `SVD` 本质是一种降维工具。

     事实上 `SVD` 是一种线性降维，无法捕获原始高维空间和`representation` 低维空间之间的非线性关系。`Levy` 也证明了：从 `SVD` 方法中学到的`representation` 不一定优于 `PPMI`矩阵本身作为 `representation`（参考 `word representation` 章节）。

   针对这两个问题，论文 `《Deep Neural Networks for Learning Graph Representations》` 提出了 `DNGR` 模型，该模型主要做了两点改进：

   - 采用基于随机游走模型 `random surfing model` 来直接构建`Graph`的结构信息，抛弃了基于随机游走`random walk`策略生成线性序列的方式。
   - 引入`stacked denoising autoencoder:SDAE` 堆叠式降噪自编码器来分解 `PPMI` 矩阵，从而进行非线性降维，从而捕获顶点之间的潜在复杂的非线性关系。

### 5.1 模型

1. 给定带权图$G=(V,E)$，其中$V=\{v_1,\cdots,v_{|V|}\}$为所有顶点集合，$E=\{e_{i,j}\},e_{i,j}\ge 0$为所有边的集合。$e_{i,j}$表示顶点$v_i$和$v_j$之间边的权重，如果顶点$v_i$和$v_j$之间不存在边则$e_{i,j} = 0$。

2. 如`GraRep` 章节所示，可以证明 `SGNS` 等价于 `PPMI` 矩阵分解。

  $\mathbf{\vec w}\cdot \mathbf{\vec c} = \log\left(\frac{n(w,c)\times |\mathbb D|}{n(w)\times n(c)}\right) - \log \lambda$

   其中：

   -$\mathbb D$为所有观测到的 `vertext-context` 组合，$|\mathbb D|$为它的数量
   -$n(w)$为$\mathbb D$中`vertex`$w$的数量，$n(c)$为$\mathbb D$中`contex`$v$的数量，$n(w,c)$为$\mathbb D$中`vertex-contex`$(w,c)$的数量
   -$\lambda$为负采样过程中负样本数量

   如`word representation` 章节`SGNS vs 矩阵分解` 部分所示，可以对 `PPMI` 矩阵分解来获得每个顶点的 `representation` 。

   因此这里的关键是：

   - 如何生成 `PPMI` 矩阵
   - 如何分解 `PPMI` 矩阵

3. `DNGR` 模型主要包含三个部分：

   - 随机游走模型：该部分主要用于捕获图结构信息，并生成概率共现矩阵。
   - `PPMI` 矩阵：根据概率共现矩阵计算 `PPMI` 矩阵。
   - 堆叠式降噪自编码器：该部分主要用于执行非线性降维来获得顶点的低维 `representation` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/6JRJhwLNAnrF.png?imageslim">
   </p>
   

4. 随机游走模型基于 `PangeRank` 模型。

   假设顶点$v_i$经过一步到达顶点$v_j$的概率为$p_{i,j}$，定义转移矩阵为：

   考虑`random surfing model`：每个顶点以$\alpha$的概率随机游走，但以$1-\alpha$的概率返回源点并重启随机游走。

   考虑顶点$v_i$为源点：定义行向量$\mathbf{\vec p}_k$，其中第$j$项表示顶点$v_i$经过$k$步转移之后到达顶点$v_j$的概率。则有递归公式：

   -$\mathbf{\vec p}_0$是一个`one-hot` 向量，其第$i$项为`1`、其它项为 `0`。
   -$\mathbf{\vec p}_k = \alpha \mathbf{\vec p}_{k-1}\mathbf A + (1-\alpha)\mathbf{\vec p}_0$

5. 堆叠式降噪自编码器用于对 `PPMI` 矩阵进行非线性降维。

   - 降噪自编码器：与常规自编码器不同，降噪自编码器在训练之前通过随机重置部分输入为零从而破坏输入数据，目标是最小化重建误差：

     其中$l(\cdot)$为平方误差；$\tilde{\mathbf{\vec x}}_i$为$\mathbf{\vec x}_i$的破坏形式；$f(\cdot )$为编码函数，$\theta_1$为对应参数；$g(\cdot)$为解码函数，$\theta_2$为对应参数；$\sigma$为 `sigmoid` 函数，用于建模非线性关系。

     降噪自编码器能够有效降低噪音并提高鲁棒性。

     如上图中：$X_2,X_5,Y_2$被破坏从而以红色突出显示。

   - 堆叠式：采用逐层训练的多层降噪自编码器来学习不同 `level` 上的 `representation`。作者认为：不同层学到的`representation` 代表了不同`level` 的抽象，网络层越高抽象层次越高。

     如上图中：$X_i$对应于原始输入，$Y_i$对应于第一层 `representation`，$Z_i$对应于第二层`representation`

### 5.2 实验

1. 数据集：

   - 语言网络数据集`20-Newsgroup` ：包含2万篇新闻组文档，并按照20个不同的组分类。每篇文档由文档内单词的 `tf-idf` 组成的向量表示，并根据余弦相似度计算得到文档的相似度。根据文档的这些相似度构建语言网络，该网络是一个全连接的带权图，用于聚类效果的评估。

     为验证模型的鲁棒性，论文分别从`3/6/9` 个不同的新闻组构建了三个更小规模的语言网络(`NG` 代表 `Newsgroups` ）：

     - `3-NG`：由`comp.graphics, comp.graphics and talk.politics.guns` 这三个新闻组的文档构成。
     - `6-NG`：由 `alt.atheism, comp.sys.mac.hardware, rec.motorcycles, rec.sport.hockey, soc.religion.christian and talk.religion.misc`这六个新闻组的文档构成。
     - `9-NG`：由 `talk.politics.mideast, talk.politics.misc, comp.os.ms- windows.misc, sci.crypt, sci.med, sci.space, sci.electronics, misc.forsale, and comp.sys.ibm.pc.hardware` 这九个新闻组的文档构成。

     这些小网络分别使用所有文档 `all data`，以及每个主题随机抽取`200`篇文档两种配置。

   - `Wine` 数据集：包含意大利同一地区种植的三种不同品种的葡萄酒化学分析的13种成分的数量（对应13各特征及其对应取值），数据包含 178个样本。

     论文将样本作为顶点、采用不同样本之间的余弦相似度作为边来构建`Graph` 。

   - 维基百科数据集：包含 `2010` 年 `4`月的快照作为训练集，包含 `2000`万条文章和 `9.9` 亿个 `token` 。

     由于 `SVD` 算法复杂性论文选择 `1` 万个最常见的单词构建词典。

2. 基准模型：

   - `DeepWalk`：使用截断的随机游走将图结构转化为线性结构，然后使用层次 `softmax` 的 `SkipGram` 模型处理序列。
   - `SGNS`：使用带负采样的 `SkipGram`模型。
   - `PPMI`：直接基于单词的共现来构建 `PPMI` 矩阵，并用顶点的 `PPMI` 信息构建顶点的稀疏、高维 `representation` 。
   - `SVD`：构建 `PPMI`矩阵，并通过 `SVD`降维来获取顶点的低维 `representation` 。

3. 模型参数：

   - 随机游走序列长度$\eta = 40$，每个顶点开始的序列数量为$\gamma = 80$。

   - 对于 `DeepWalk,SGNS` ，负采样个数$\lambda = 5$，上下文窗口大小为$K=10$。

   - 对于 `DNGR`：

     - 使用 `dropout` 缓解过拟合，`dropout` 比例通过调参来择优。

     - 所有神经元采用 `sigmoid` 激活函数。

     - 堆叠式降噪自编码器的层数针对不同数据集不同。

      <p align="center">
         <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/c6y50wkOF0lx.png?imageslim">
      </p>
      

4. 论文在 `20-NewsGroup` 上执行聚类任务，该任务通过 `K-means` 对每个算法生成的顶点 `representation` 向量进行聚类，并以归一化互信息 `normalized mutual information: NMI` 作为衡量指标。

   每种算法随机执行`10` 次并报告平均结果。为了验证不同维度的影响，下面还给出了 `DeepWalk,SGNS` 在维度为 `128、512` 时的结果。

   结论：

   - 在 `DeepWalk` 和 `SGNS` 中，增加维度使得效果先提升后下降。
   - `DNGR` 明显优于其它基准算法。
     - 对比 `DNGR` 和 `PPMI` 可知，对于高维稀疏矩阵降维并提取有效信息可以提升效果。
     - 对比 `SVD`、`PPMI`可知，`SVD`并不一定总是优于 `PPMI`。
     - 对比 `DNGR`、`SVD` 可知，`DNGR`是一种更有效的降维方法。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/M4WHeDU2dVdY.png?imageslim">
   </p>
   

5. 论文在 `Wine` 数据集上执行可视化任务，该任务采用 `t-SNE` 将 `DNGR,SVD,DeepWalk,SGNS` 输出的顶点`representation` 映射到二维空间来可视化。在二维空间中，相同类型的葡萄酒以相同颜色来展示。

   结论：在所有可视化结果中，`DNGR` 效果最好。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/aWlI5vuorpOj.png?imageslim">
   </p>
   

6. 论文在维基百科数据集上执行单词相似度任务，该任务直接从训练语料库中计算 `word-context` 组合因此不需要随机游走模型来生产 `PPMI` 矩阵。

   论文采用 `Spearman’s rank correlation coefficient` 来评估算法的结果。

   为了获得最佳结果，论文将 `SGNS,DNGR,SVD` 负采样的样本数分别设置为 `5,1,1` 。

   结论：

   - `SVD,SGNS,DNGR` 都比 `PPMI` 效果更好，这表明降维在该任务中的重要性。
   - `DNGR` 效果最好，超越了 `SVD` 和 `SGNS` ，这表明学习`representation` 时捕获非线性关系的重要性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/loORniQkU839.png?imageslim">
   </p>
   

7. 论文在通过评估 `20-NewsGroup` 的逐层 `NMI` 值，验证了堆叠式降噪自编码器深层架构的重要性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/ITPWdHSzv4KK.png?imageslim">
   </p>
   

## 六、Node2Vec

1. `feature learning` 的挑战是如何定义恰当的目标函数，这涉及计算效率和预测准确率之间的平衡。

   - 一方面可以直接优化下游监督任务的目标函数。
     - 优点：预测准确率高。
     - 缺点：需要估计的参数数量激增，训练时间复杂度较高；且学到的`feature representation` 是任务相关的，无法广泛应用于其它类型的任务。
   - 另一方面可以精心设计独立于下游监督任务的目标函数。
     - 优点：计算效率较高；且`feature representation`是任务无关的，可以广泛应用于下游各种类型的任务。
     - 缺点：应用于下游监督学习任务时预测准确率稍低。

2. 当前的模型和方法无法为无监督学习 `graph feature learning` 定义一个合理目标函数。

   - 基于线性和非线性的经典无监督学习算法，如 `PCA,MDS,IsoMap` 及其扩展算法，它们最大化的目标函数为：原始数据在低维空间`representation` 的方差。

     这些算法具有两个主要缺点：

     - 算法涉及矩阵的特征分解，这对于大型`Graph` 代价太大，因此可扩展性很差。

     - 这些算法的目标函数隐含着对 `Graph` 结构的各种假设，如同质性 `homophily` 或者结构对等性 `structural equivalence` ，这些假设不一定适合各种类型的 `Graph`。

       例如谱聚类的目标函数就有很强的同质假设：图的割`cut`有助于分类。该假设在很多场景下是合理的，但是无法推广到所有的 `Graph` 。

   - 基于邻域关系的无监督学习算法，如 `DeepWalk, LINE` 及其扩展算法，它们最大化的目标函数为：尽可能在低维空间中保留原始空间中的顶点邻域关系 `neighborhood` ，即在低维空间中尽可能相似。

     不同的采样策略将导致不同的邻域关系，因此学到不同的顶点表达。实际上并没有一个明确的、更好的采样策略使得采样到的领域关系适合所有网络以及所有任务。这也是 `DeepWalk,LINE` 等工作的主要缺点：无法为采样过程提供任何的灵活性。

     如下图中顶点$u$和顶点$s_1$都属于同一个社区 `community` ，根据 `DeepWalk,LINE` 算法这两个顶点是相似的。

     事实上顶点$u$和顶点$s_6$虽然属于不同的社区，但是它们属于网络的同一种特殊连接模式，可以认为它们也是 ”相邻“的，因此它们也是相似的。

     因此必须要有一个灵活的算法来学到两种规则的顶点 `representation`，从而使得算法能够学到泛化能力更强的特征表达：

     - 将来自同一个社区的顶点映射的低维`embedding` 向量尽可能相似。
     - 将来自同一类特殊连接模式的顶点映射的低维`embedding` 向量尽可能相似。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/1odTfDTonSmT.png?imageslim">
     </p>
     

3. 论文 `《node2vec: Scalable Feature Learning for Networks》` 提出了 `node2vec` 模型，该模型的目标函数也是：尽可能在低维空间中保留原始空间中的顶点邻域关系 `neighborhood` 。但是 `node2vec` 重新定义了邻居这一概念，认为灵活地探索顶点领居是学习更加丰富的顶点表达的关键。

   `node2vec` 通过一个有偏随机游走过程`biased random walk procedure` 来探索各种类型的邻居，从而能够灵活的根据顶点所属的社区或者顶点在网络中的角色来学习顶点表达。

   `node2vec` 是 `DeepWalk,LINE` 等算法的进一步扩展。与 `DeepWalk,LINE` 等死板的搜索方法相比，`node2vec` 可以通过调节超参数来控制搜索空间，从而产生更加灵活的算法。该超参数具有直观的解释，并决定了不同不同的搜索策略。

   - 在多标签分类任务中`node2vec` 优于最新的方法最高达 `26.7%`；在链接预测任务中 `node2vec` 优于最新的方法最高达 `12.6%` 。
   - 在计算效率上 `node2vec` 主要计算阶段很容易并行化，可以在几个小时内计算扩展到数百万个结点的大型网络。

4. `node2vec` 还将单个顶点的表达扩展到成对顶点的表达，即边的表达，使得`node2vec` 能够同时进行基于顶点的预测任务、基于边的预测任务。

### 6.1 模型

1. 给定网络$G(V,E)$，`node2vec`的目标是学习映射$f: V \rightarrow \mathbb R^d$，该映射将每个顶点$v$映射到低维空间表达$\mathbf{\vec w}_v$，该低维空间表达用于下游任务，其中$d\ll |V|$。

   这里图$G$可以是有向图也可以是无向图，可以是无权图也可以是带权图。

   对于每个顶点$u\in V$定义其基于采样策略$S$的网络邻居 `network neighborhood` 为$\mathcal N_S(u) \sub V$。类似 `SkipGram`，`node2vec` 的优化目标是：给定顶点$u$，在低维空间中最大化其邻居$\mathcal N_S(u)$的对数似然函数。即：

  $\max_f \sum_{u\in V}\log P(\mathcal N_S(u) \mid \mathbf{\vec w}_u)$

   其中$\mathbf{\vec w}_u = f(u) \in \mathbb R^d$。

   为求解该最优化问题，`node2vec` 做了两个关键假设：

   - 条件独立性：给定顶点$u$，对于其邻居$\mathcal N_S(u)$内的任意两个顶点$x,y$，假设$x$成为$u$的邻居和$y$成为$u$的邻居无关。即有：

    $P(\mathcal N_S(u) \mid \mathbf{\vec w}_u) = \prod_{v\in \mathcal N_S(u)} P(\mathbf{\vec w}_v\mid \mathbf{\vec w}_u)$

   - 空间对称性：给定顶点$u$，它作为源顶点和作为邻居顶点时共享同一个 `embedding` 向量。因此可以通过 `softmax` 函数来建模条件概率：

    $P(\mathbf{\vec w}_v\mid \mathbf{\vec w}_u) = \frac{\exp(\mathbf{\vec w}_v\cdot\mathbf{\vec w}_u )}{\sum_{v^\prime\in V}\exp(\mathbf{\vec w}_{v^\prime}\cdot\mathbf{\vec w}_u)}$

   因此`node2vec` 的目标函数为：

  $\mathcal J = \sum_{u\in V}\left[-\log Z_u + \sum_{v\in \mathcal N_S(u)} \mathbf{\vec w}_v\cdot\mathbf{\vec w}_u \right]$

   其中$Z_u = \sum_{v^\prime\in V}\exp(\mathbf{\vec w}_{v^\prime}\cdot\mathbf{\vec w}_u)$。

   对于大型网络$Z_u$的计算代价很高，因此通常采用负采样策略。

2. `SkipGram` 架构是基于 `NLP` 开发的，由于文本的线性特点 `SkipGram` 可以直接基于文本上的连续滑动窗口来自然的得到单词$u$的邻居$\mathcal N_S(u)$。

   但是网络不是线性的，因此难以直接定义邻居。为解决该问题，`node2vec` 给出了一个随机采样过程，该过程对顶点$u$的许多不同类型的相关顶点进行采样得到邻居$\mathcal N_S(u)$。邻居中不仅包含直接相连的顶点，还包括那些根据采样策略$S$得到的不相邻的、结构类似的顶点。

   因此采样策略$S$决定了顶点$u$的邻居集合。为公平比较不同的采样策略，这里限制每个顶点的邻居集合大小不超过$k$。

   有两种极端的采样策略：

   - 广度优先采样策略 `Breadth-first Sampling:BFS` ：顶点$u$的邻居$\mathcal N_S(u)$来自于和顶点$u$直接相连的那些顶点。

     如下图中$k=3$时，顶点$u$的邻居为$\mathcal N_S(u) = \{s_1,s_2,s_3\}$。

   - 深度优先采样策略 `Depth-first Sampling:DFS`：顶点$u$的邻居$\mathcal N_S(u)$来自于以$u$为源点的随机游走序列。

     如下图中$k=3$时， 顶点$u$的邻居为$\mathcal N_S(u) = \{s_4,s_5,s_6\}$。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/HdbMEAGqXPA4.png?imageslim">
   </p>
   

3. 网络中的顶点有两种相似性：同质性 `homophily`、结构对等性 `structural equivalence`。

   - 同质相似性：高度相连并且属于同一个社区的顶点必须紧密的 `embed`在一起。

     如上图中的结点$s_1$和$u$高度相连且属于同一个社区，因此它们是同质相似的，因此它们的 `embed` 必须有很高的相似度。

   - 结构对等相似性：在网络中具有相似结构角色`structural role` 的顶点必须紧密的 `embed` 在一起。

     如上图中的结点$s_6$和$u$都作为对应社区的中心结点，它们是结构对等相似的，因此它们的 `embed` 必须有很高的相似度。

   与同质性不同，结构对等性并不强调连通性，网络中相隔很远的两个顶点仍然可能具有相同的结构角色。

   现实世界中这两个概念不是互斥的，网络通常同时表现出两种行为：某些结点表现出同质性，另一些结点表现出结构对等性。

4. 广度优先搜索 `BFS` 和深度优先搜索 `DFS`在探索同质性和结构对等性起着关键作用：

   - 通过`BFS` 策略采样的邻居会导致产出的顶点表达具有结构对等性。

     我们注意到，通过准确的刻画局部邻域就可以确定结构对等性。例如，仅仅通过观察每个顶点的直接邻居就可以推断出该顶点的网络角色（如 `bridge/hub`），从而得到网络结构对等性。

     - 通过对搜索限制在源点附近的邻居顶点，`BFS` 能够得到每个顶点邻域的微观视图，并且导致产出的顶点表达具有结构对等性。

     - 在 `BFS` 中，直连的邻居顶点需要被重复采样很多次。这一点很关键，因为它降低了直连邻居顶点分布的方差。

       > 解释：如果仅采样一次，则每次采样结果中邻居顶点分布可能差异很大。

     - 对于任何给定的$k$，对每个顶点 `BFS` 只会探索该顶点附近的一个很小区域。

   - 通过`DFS` 策略采样的邻居会导致产出的顶点表达具有同质性。

     和`BFS` 相反，对任何给定的$k$，对每个顶点 `DFS` 能够探索该顶点附近一个很大的区域，因为 `DFS` 可以探索远离源点的区域。所以`DFS` 策略采样的邻居能够描绘邻域的宏观视图，而这对于基于同质性的社区推断至关重要。

     但是 `DFS` 存在两个问题：

     - 不仅要推断网络中那些结点之间存在依赖关系，还需要刻画这些依赖关系的准确特性。由于邻居规模不超过$k$，而需要探索的区域太大，因此很难做到这一点，这就导致采样的结果方差很大：即每次采样结果中邻居顶点的分布差异很大。
     - 探索更深的顶点导致更复杂的依赖关系，因为采样的顶点可能距离源点很远因此和源点相关性不大，因此不具代表性。

5. 基于以上观察`node2vec` 设计了一种灵活的邻居采样策略，该策略使得我们能够平滑的在 `BFS` 和 `DFS` 之间采样。

   给定源点$u$我们采样一个长度为$l$的随机游走序列，序列起始顶点为$c_0 = u$，$c_i$的采样策略为：

   其中$\pi_{v,x}$为非归一化的、从顶点$v$转移到顶点$x$的概率，$Z$为归一化常数。

   最简单的采样策略为：根据边的权重来采样下一个顶点，即$\pi_{v,x} = w_{v,x}$。如果是无向图则有$w_{v,x} = 1$。这种采样策略的缺点是没有考虑网络结构。

   考虑到 `BFS` 和 `DFS` 分别对应于结构对等性和同质性这两种极端采样方式，我们的采样策略必须考虑以下事实：结构对等性和同质性不是互斥的，现实世界网络通常会同时存在这两者。

   我们通过超参数$p$和$q$来定义了一个二阶随机游走过程来采样。考虑从顶点$t$转移到顶点$v$并且当前停留在顶点$v$，限制决定下一个采样的顶点$x$。

   定义未归一化的概率$\pi_{v,x} = \alpha_{p,q}(t,x)\times w_{v,x}$，其中：

   其中$d_{t,x}$表示顶点$t$和$x$之间的最短路径，它必须是在$\{0,1,2\}$之间，即：$x$的取值范围是有限的，必须在顶点$t$本身、顶点$t$的一阶邻居、顶点$t$的二阶邻居之间选择。

   通过设置$\pi_{v,x}$为前一个结点$t$的函数，随机游走过程成为一个二阶马尔科夫链。

   超参数$p$和$q$控制了随机游走的方向和速度，它们允许我们的搜索过程在 `BFS` 和 `DFS` 之间插值，从而反应结点的不同类型的相似性。

   - 返回参数 `Return Parameter`$p$：参数$p$控制了重新访问上一步已访问顶点的概率。

     - 一个较大的值$p \gt \max(q,1)$将使得接下来访问的顶点$x$不大可能是上一步已访问的顶点$t$（除非顶点$v$除了$t$之外再也没有其它邻居）。

       这种策略鼓励适当的进行探索新结点，并避免采样过程中的冗余`2-hop` （即：经过两步转移又回到了结点本身）。

     - 一个较小的值$p\lt \max(q,1)$将使得接下来访问的顶点$x$很可能是上一步已访问的顶点$t$。

       这种策略使得随机游走序列尽可能地留在源点$u$的局部区域。

   - 内外参数 `In-out parameter`$q$：参数$q$控制了探索的方向是向内搜索还是向外搜索。

     - 如果$q \gt 1$则$x$倾向于向$t$靠拢。这种策略将采样局限在一个很小的范围，从而获得网络的一个局部视图，类似于 `BFS` 的行为。
     - 如果$q\lt 1$则$x$倾向于远离$t$。这种策略将鼓励采样向外拓展，从而获得网络的一个宏观视图，类似于 `DFS` 的行为。

     但是这种策略和`BFS、DFS` 本质上是不同的：我们的策略采样的结点与源点的距离并不是严格相等的（`BFS`）、也不是严格递增的 （`DFS` ）。另外我们的策略易于预处理，且采样效率很高。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/U4YqDMwjty8O.png?imageslim">
   </p>
   

6. 与单纯的`BFS、DFS` 策略相比，我们的二阶马尔科夫随机游走策略的计算效率和空间效率较高。

   - 空间复杂度：存储所有结点的邻居的空间复杂度为$O(|E|)$。另外对于二阶随机游走过程，缓存每个顶点的邻居之间的关联是有益的，其空间复杂度为$O(\alpha^2|V|)$，其中$\alpha$为所有顶点的平均度`degree` 且相对于$|V|$通常很小。

     > 单个顶点的邻居之间的关联的复杂度为$O(\alpha \times \alpha)$，考虑所有顶点则空间复杂度为$O(\alpha^2|V|)$

   - 时间复杂度：由于我们的策略采样的结点与源点的距离并不是严格相等的、也不是严格递增的，因此不同源点采样的序列之间可以复用从而提高了采样效率。

     由于随机游走的马尔可夫性，通过一次性生成长度为$l\gt k$的随机游走序列，我们可以一次性为$l-k$个顶点各自生成长度为$k$的随机游走序列。因此每条随机游走序列的平均算法复杂度为$O\left(\frac{l}{(l-k)}\right)$，每个采样点的平均算法复杂度为$O\left(\frac{l}{k(l-k)}\right)$。

     如下图中我们采样了一条长度$l=6$的随机游走序列$\{u,s_4,s_5,s_6,s_8,s_9\}$，当$k=3$时这会产生三组邻居：

     注意：这种采样重用机制会给整个采用过程引入某些 `bias`，但是我们观察到这种采样方式能够极大提升采样效率。这就是效果和效率之间的折衷。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/MNw72zBJGTBS.png?imageslim">
     </p>
     

7. `Node2Vec` 算法主要包含两个过程：`Learn Feature` 过程、`node2vecWalk` 过程。

   `Learn Feature` 过程：

   - 输入：

     - 图$G=(V,E,W)$
     - 维度$d$
     - 每个源顶点开始的随机游走序列数量$r$
     - 每个随机游走序列长度$l$
     - 上下文尺寸$k$
     - 参数$p,q$

   - 输出：每个顶点的低维`representation`

   - 步骤：

     - 预处理权重$\pi = \text{PreprocessModifiedWeights(G,p,q)}$，重新构造新的图$G^\prime = (V,E,\pi)$

       > 一个问题，这里预处理是如何做的？论文未说明，得看代码。

     - 初始化列表$\text{walks}$为空：$\text{walks}=[]$

     - 迭代$r$次，对每个顶点生成以该顶点为起点、长度为$l$的$r$条随机游走序列。迭代过程为：$\text{iter} = 1,2,\cdots,r$：

       对每个顶点$u\in V$执行：

     - 执行随机梯度下降$\text{StochasticGradientDescent(k,d,walks)}$。

     - 返回求解到的每个顶点的 `representation`

   `node2vecWalk` 过程：

   - 输入：

     - 修改后的图$G^\prime(V,E,\pi)$
     - 开始顶点$u$
     - 随机游走序列长度$l$

   - 输出：长度为$l$的一条随机游走序列

   - 步骤：

     - 初始化列表$\text{walk} = [u,]$

     - 迭代$\text{iter2} = 1,2,\cdots,l$，迭代过程为：

       - 获取当前顶点$v = \text{walk}[-1]$

       - 获取当前顶点的邻居顶点$b_v = \text{GetNeighbors}(v,G^\prime)$

         > 该邻居理论上就是$v$的前一个顶点$\text{walk}[-2]$？论文未说明，得看代码

       - 采样下一个顶点$s= \text{AliasSample}(b_v,\pi)$

       - 将$s$添加到序列：$\text{walk.append(s)}$

     - 返回列表$\text{walk}$

8. `Node2Vec` 算法的`node2vecWalk` 过程中，由于源点$u$的选择导致引入了隐式的 `bias`。由于我们需要学习所有顶点的 `representation`，因此我们通过模拟从每个顶点开始的、长度固定为$l$的$r$个随机游走序列来抵消该 `bias` 。

   另外在 `node2vecWalk` 过程中，我们根据转移概率$\pi$来采样。由于二阶马尔可夫转移概率$\pi_{v,x}$可以提前计算好，因此可以通过 `AliasTable` 技术在$O(1)$时间内高效采样。

9. `Node2Vec` 算法的三个计算阶段：预处理从而计算转移概率、生成随机游走序列、使用随机梯度下降优化目标函数。

   这三个计算阶段依次执行，且每个阶段可以并行且异步执行，从而使得`Node2Vec`算法整体有很好的可扩展性。

10. 尽管搜索模型的超参数，如$p,q$需要额外的开销，但是正如实验表明：`node2vec` 是半监督的。因此可以通过少量的标记数据来有效学习这些参数。

### 6.2 边的representation

1. 有时候我们会对顶点`pair` 对感兴趣，如链接预测任务中，我们需要预测给定的两个顶点之间是否存在链接。

   由于我们的随机游走序列是基于网络的连接结构来生成的，因此可以将顶点的 `representation` 扩展到顶点对。

2. 给定两个顶点$u,v$，我们在它们的`representation`$\mathbf{\vec w}_u,\mathbf{\vec w}_v$上定义一个二元操作$o$从而生成边的`representation`$\mathbf{\vec g}_{u,v} = g(u,v) \in \mathbb R^{d^\prime}$，其中$g: V\times V\rightarrow \mathbb R^{d^\prime}$，$d^\prime$为顶点对$(u,v)$的 `representation` 的维度。

   我们希望二元操作符$o$在所有顶点对之间都有定义，而不仅仅是只有在存在边的顶点对之间有定义，因为预测时顶点对之间可能不存在边。

   因此这里选择了一些二元操作符，且令$d^\prime = d$：

   - 均值操作符：

    $\oplus: \mathbf{\vec g}_{u,v}= \mathbf{\vec w}_u\oplus\mathbf{\vec w}_v = \frac{\mathbf{\vec w}_u+\mathbf{\vec w}_v}{2}$

   - `Hadamard` 操作符：

    $\odot: \mathbf{\vec g}_{u,v}= \mathbf{\vec w}_u\odot\mathbf{\vec w}_v = (w_{u,1}\times w_{v,1},\cdots,w_{u,d}\times w_{v,d})^T$

   - `L1` 操作符：

    $||\cdot||_1: \mathbf{\vec g}_{u,v}= ||\mathbf{\vec w}_u,\mathbf{\vec w}_v||_1 = (|w_{u,1}- w_{v,1}|,\cdots,|w_{u,d}- w_{v,d}|)^T$

   - `L2` 操作符：

    $||\cdot||_2: \mathbf{\vec g}_{u,v}= ||\mathbf{\vec w}_u,\mathbf{\vec w}_v||_2 = (|w_{u,1}- w_{v,1}|^2,\cdots,|w_{u,d}- w_{v,d}|^2)^T$

     .

### 6.3 实验

#### 6.3.1 人物关系可视化

1. 我们用一个网络来描述小说《悲惨世界》中的角色，边代表角色之间是否共同出现过。网络共有 `77` 个顶点和 `254` 条边。

   我们设置维度$d=16$并利用 `node2vec` 学习每个顶点的`representation` 向量，然后通过`k-means` 算法对这些向量聚类。最后我们将`representation` 向量及其聚类结果在二维空间可视化，相同的颜色代表相同的聚类类别，顶点大小代表顶点的`degree` 。

   - 上半图给出了$p=1,q=0.5$的结果，可以看到`node2vec`挖掘出小说中的社区`community`：有密切交互关系的人的相似度很高。即`representation` 结果和同质性有关。

   - 下半图给出了$p=1,q=2$的结果，可以看到 `node2vec` 挖掘出小说中的角色：角色相同的人的相似度很高。即`representation`结果和结构对等性有关。

     如：`node2vec` 将蓝色顶点`embed`在一起，这些顶点代表了小说不同场景之间切换的桥梁`bridge` ；黄色顶点代表小说中的边缘角色。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/mBmN9XBYRfNF.png?imageslim">
   </p>
   

#### 6.3.2 多标签分类任务

1. 多标签分类任务中，每个顶点属于一个或者多个标签。训练集上每个顶点的所有标签是已知的，我们需要预测测试集每个顶点的所有标签。当标签集合$\mathbb L$很大时，多标签分类任务非常有挑战性。

2. 数据集：

   - `BlogCatalog`：该数据集包含 `BlogCatalog` 网站上博客作者之间的社交关系，标签代表通过元数据推断出来的博客兴趣。网络包含 `10312` 个顶点、`333983` 条边、`39`种不同标签。
   - `Protein-Protein Interactions:PPI`：该数据集包含蛋白质和蛋白质之间的关联，标签代表基因组。网络包含 `3890`个顶点，`76584`条边，`50`种不同标签。
   - 维基百科：该数据集包含维基百科`dump` 文件的前一百万字节中的单词共现，标签代表单词的词性 `Part-of-Speech:POS` （由 `Standford POS-Tagger` 生成）。网络包含 `4777` 个结点，`184812` 条边，`40` 种不同标签。

   所有这些网络都同时呈现同质性和结构对等性。如：

   - 博客作者的社交网络呈现出很强的同质性，但是可能还有一些 “熟悉的陌生人”：博客作者之间没有关联但是兴趣相同。因此它们在结构上是等效的顶点。
   - 蛋白质-蛋白质的相互作用中，当蛋白质和邻近蛋白质功能互补时，它们呈现出结构对等性；当相邻蛋白质功能相似时，它们呈现出同质性。
   - 在单词共现网络中，当相邻的单词具有相同 `POS` 标签时，它们呈现出同质性；当相邻单词呈现某种语法模式，如 `限定词+名词、标点符号 + 名词` ，它们呈现结构对等性。

3. 对比模型：

   - 谱聚类：这是一种矩阵分解方法，它对图$G$的归一化拉普拉斯矩阵进行分解，最后选取 `top d` 个特征向量作为顶点的表达。
   - `DeepWalk`：它可以视为 `node2vec`中$p=1,q=1$的特例。
   - `LINE`：它可以视为两阶段的特征学习：
     - 第一阶段通过 `BFS` 从直接邻居中学到前面$d/2$维的表达。
     - 第二阶段通过`BFD` 从二阶邻居中学到剩下$d/2$维的表达。

   我们排除了那些已被证明不如 `DeepWalk`的其它矩阵分解方法。我们还排除了 `GraRep` 方法，该方法虽然扩展了 `LINE` 算法但是无法有效的推广到大型`Graph` 。

4. 参数配置：

   - 之前的评估方式仅仅使用同样的`Graph` 作为训练集，而并没有考虑不同算法的采样规模。这里为每个算法执行相同次数的采样。

     令$\mathcal K$为总的采样次数，因此对于 `node2vec` 需要满足$\mathcal K = r \times l\times |V|$。

   - 所有方法都使用 `SGD` 进行优化，但是这里有两个改动：

     - `DeepWalk` 使用分层`softmax`，计算量太大，这里我们切换到负采样。

     - `Node2Vec` 和 `DeepWalk` 都用一个参数$k$来配置上下文邻居顶点的数量，数量越大则需要进行迭代的`step` 越多；对于`LINE` 该参数为`1`，因此 `LINE` 每个`epoch` 很快结束，因此我们训练`LINE`$k$个 `epoch` 。与此对比，`Node2Vec/DeepWalk` 训练一个 `epoch` 。

       > 这里需要看 LINE 源码

   - 所有模型的参数值为：$d=128,r=10,l=80,k=10$

   - 每组实验重复`10`次，使用`10` 折交叉验证搜索超参数$p,q\in \{0.25,0.5,1,2,4\}$。

5. 每个模型输出的顶点 `representation` 输入到一个$L_2$正则化的`one-vs-rest` 逻辑回归分类器中，采用`Macro-F1` 作为评估指标。`Micro-F1` 和准确率的趋势与 `Macro-F1` 类似，因此并未给出。

   数据集被拆分为 `50%` 训练集和 `50%` 测试集，随机拆分十次取其平均结果。

   结论：

   - `node2vec` 探索邻居时的灵活性使得它超越了其它算法。

     - 在 `BlogCatalog` 中，可以设置较低的$p,q$值来较好的融合同质性的结构对等性。

       `LINE` 的效果低于预期，这可能是因为它无法重用采样点，而基于随机游走的策略可以重用采样点。

     - 在 `PPI` 中，最佳策略$p=4,q=1$的表现和 `DeepWalk` 的$p=1,q=1$相差无几。

       `node2vec` 通过较大的$p$值来避免重复访问刚才已访问的顶点来获取一定的性能提升。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/9R9i1GRpLJWX.png?imageslim">
   </p>
   

6. 为进一步分析效果，我们将`train-test`拆分比例从 `10%~90%` ，参数$p,q$如前所述。结果表明：

   - 所有方法都明显超越了谱聚类。
   - `node2vec` 始终超越了 `LINE` ，并且最差情况也等价于`DeepWalk` 、最好情况相比 `DeepWalk` 有巨大提升。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/fD5I9GUVlxad.png?imageslim">
   </p>
   

7. `node2vec` 涉及很多超参数，我们在 `BlogCatalog` 数据集上评估这些超参数的影响。

   其中：训练集、测试集拆分比例为 `50%~50%`，除了待评估的参数外其它参数均为默认值（$p,q$的默认值均为 `1` ）。

   结论：

   - 随着$p,q$的减小 `node2vec` 性能将有所提高，这是因为更小的$p,q$将达到预期的同质性和结构对等性。

     低$q$虽然鼓励向外探索，但是由于 低$p$的平衡作用可以确保游走里源点不会太远。

   - 提高维度$d$可以提升性能，但是一旦维度达到 `100` 左右，性能提升将趋于饱和。

   - 增加源点的游走序列数量$\gamma$可以提升性能，增加序列的长度$l$也可以提升性能。

     这两个参数表明更大的预算$\mathcal K$，因此它们对于性能有较大的影响。

   - 上下文大小$k$以优化时间的增加为代价来提升性能，但是性能提升不明显。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/evjvQ5vsiIQ1.png?imageslim">
   </p>
   

8. 真实世界中我们因为各种原因无法获得`Grap` 的完整的、准确的信息，因此数据是有噪音的。这里我们针对 `BlogCatalog`数据集进行摄动研究。

   - 第一种情况：我们分析不同比例的边的缺失对于性能的影响。如上半图所示，随着边的缺失比例上升网络性能大致呈现线性下降，但是下降的斜率较小。
   - 第二种情况：我们分析增加不同比例的随机边（噪音）对于性能的影响。如下半图所示，与缺失边相比这种情况的下降速度稍快，但是斜率趋于放缓。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/aoEpf7aiH4XN.png?imageslim">
   </p>
   

9. 为测试可扩展性我们用 `node2vec` 学习 `Erdos-Renyi` 网络的结点表示。所有参数为默认值，网络规模从 `100` 到 一百万，每个结点的平均 `degreee` 为 `10` 。

   可以看到：`node2vec` 随着结点数量的增加，其收敛时间基本呈线性关系，在不到四个小时即可生成一百万顶点的`representation` 。

   在 `node2vec` 的训练过程我们采用了很多优化技巧，如随机游走过程中采用采样点的重用技巧、`Alias Table` 技巧；在优化时采用负采样和异步随机梯度下降。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/fb0bgI0otnMp.png?imageslim">
   </p>
   

#### 6.3.3 连接预测

1. 在连接预测任务中，我们给定一个删除了一定比例边的网络，希望模型能够预测这些被删除的边。

   数据集的生成方式为：

   - 网络中随机删除 `50%` 的边，剩下的所有边的顶点`pair` 对作为正样本。
   - 随机选择网络中的 `n` 对不存在边的顶点 `pair` 对作为负样本。

   由于暂时还没有针对连接预测的特征学习算法，因此我们将`node2vec` 和一些流行的启发式方法进行对比。这些启发式方法通过评估顶点$u$的邻居集合$\mathcal U(u)$和顶点$v$的邻居集合$\mathcal U(v)$的某种得分，然后根据该得分来判断$u,v$之间是否存在边。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/aioP8Jqn74GM.png?imageslim">
   </p>
   

2. 数据集：

   - `FaceBook` 数据集：包含`FaceBook`用户之间的社交关系，顶点代表用户、边代表好友关系。网络一共包含 `4039` 个顶点和 `88234` 条边。
   - `Protein-Protein Interactions：PPI` 数据集：包含蛋白质和蛋白质之间的关联。网络一共包含 `19706`个顶点、`390633`条边。
   - `arXiv ASTRO-PH` 数据集：包含 `arXiv` 网站上发表论文的作者之间的关联，顶点代表作者、边代表两个作者共同参与同一篇论文写作。网络一共包含 `18722`个顶点、`198110` 条边。

3. 我们经过超参数优化选择最佳的$p,q$，优化过程这里不做讨论。下图给出了`node2vec` 的不同`operator`，以及不同启发式算法的结果，指标为 `auc` 。

   图中`a` 表示均值算子，`b` 表示 `Hadamard`积算子，`c` 表示 `WeightedL1` 算子，`d` 表示 `WeightedL2` 算子。

   结论：

   - 通过`feature learning` 学习定点对特征（如`node2vec,LINE,DeepWalk,谱聚类` 等等）的效果明显优于启发式方法。
   - 在所有特征学习的算法中，`node2vec` 的效果最佳。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/zUAa1FIbVB5A.png?imageslim">
   </p>
   

## 七、WALKLETS

1. 社交网络本质都是分层的，每个人（顶点）都属于多个社区，这些社区范围从小型社区（如家庭、朋友）、中型社区（如学校、企业）到大型社区（如民族、国家），代表了不同尺度 `scale` 的关系。

   随着关系尺度的变化，网络的拓扑结构也发生变化。如下图所示：

   - 当考虑家庭、朋友关系这一尺度时只有黄色部分构成一个社区。
   - 当考虑学校、企业关系这一尺度时只有蓝色部分（包括黄色部分）构成一个社区。
   - 当考虑民族、国家关系这一尺度时所有顶点都构成一个社区。

   在这个过程中尺度扮演者关键的角色，它决定了社区的规模以及顶点归属到哪些社区。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/gKBgGjKaquMW.png?imageslim">
   </p>
   

2. 在网络`representation` 学习的任务中，大多数方法都是一刀切：每个顶点得到一个融合了所有尺度的`representation` ，无法明确的捕获网络内结点之间的多尺度关系，因此无法区分网络在各个尺度上的差异。

   `GraRep` 方法显式的建模多尺度关系，但是计算复杂度太高从而无法扩展到真实世界的大型网络。

   论文 `《Don’t Walk, Skip! Online Learning of Multi-scale Network Embeddings》` 提出了 `WALKLETS` 模型，该模型是一种在线的图`reprensentation`学习方法，可以捕获网络顶点之间的多尺度关系，并且可扩展性强，支持扩展到百万顶点的大型网络。

   `WALKLETS` 将每个顶点映射到低维`reprensentation` 向量，该向量捕获了顶点所属社区的潜在层次结构。在预测时可以用单个尺度 `representationi` 或者组合多个尺度 `representation` 来提供顶点更全面的社区关系。

   下图来自于 `Cora` 引用网络的一个子网络的二维可视化，中心的红色顶点表示源点，顶点颜色代表该顶点和源点的距离：距离越近颜色越红，距离越远颜色越蓝。

   - 左图给出了细粒度 `representation` 相似度的结果。在这一尺度下，只有源点的直系邻居才是相似的。
   - 右图给出了粗粒度 `representation` 相似度的结果。在这一尺度下，源点附近的很多顶点都是相似的。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/jmWham1Jf31s.png?imageslim">
   </p>
   

3. 我们受到一个事实的启发：通过一个结点可能同时隶属于不同层次的圈子，如家庭、学校、公司等等。针对这些不同的社区进行建模和预测不仅对于了解网络结构至关重要，而且还具有重要的商业价值，如更有效的广告定向。

### 7.1 模型

1. 给定图$G=(V,E)$，其中：

   -$V=\{v_1,\cdots,v_n\}$为图的顶点集合，$v_i$表示图的一个顶点。
   -$E=\{e_{i,j}\}$为图的边集合，$e_{i,j}$表示顶点$v_i,v_j$之间的边，其权重为$w_{i,j}$。权重衡量了边的重要性。

   定义邻接矩阵$\mathbf S$为：

  $S_{i,j} = \begin{cases}w_{i,j},&\text{if exist edge from $v_i$ to $v_j$} \\0,&\text{else}\end{cases}$

   这里假设所有的权重都是正的。

   定义度矩阵`degree matrix`$\mathbf D$为一个对角矩阵：

  $D_{i,j} = \begin{cases}\sum_{p\ne i, 1\le p\le n}S_{i,p},&\text{if $i = j$}\\0,&\text{if $i \ne j$}\end{cases}$

   假设顶点$v_i$转移到顶点$v_j$的概率正比于$S_{i,j}$，则定义转移概率矩阵 `probability transition matrix`：$\mathbf A = \mathbf D^{-1}\mathbf S$。其中$A_{i,j}$定义了从顶点$v_i$经过一步转移到顶点$v_j$的概率，因此也称为一阶转移概率矩阵。

   多尺度 `representation` 学习任务需要学习$K$个 `representation`$\mathbf X_1,\cdots,\mathbf X_K$，其中$\mathbf X_k\in \mathbb R^{|V|\times d}$捕获了网络在尺度$k$的视图 `view` 。从直观上看，每个 `representation` 都编码了视角网络在不同视图下的相似性，对应于不同尺度下的潜在社区成员关系。

2. `DeepWalk` 采用截断的随机游走序列来对共现顶点`pair` 对建模，其目标是估计顶点$v_i$和它邻居结点共同出现的可能性。根据论文 `《NeuralWord Embedding as Implicit Matrix Factorization》` ， 它等价于矩阵分解。即：基于层次`Softmax` 的 `DeepWalk`模型的优化目标对应于矩阵$\mathbf M$的分解，其中：

   其中：

   -$\mathbf{\vec e}_i$是一个`one-hot` 向量，其第$i$个元素为`1` 其余元素为零。
   -$[\cdot]_j$表示向量的第$j$个元素。

   由于$\mathbf A^k$的不同幂次$k=1,2,\cdots,K$代表了不同的尺度，可以看到 `DeepWalk` 已经隐式的建模了从尺度`1` 到尺度 `K` 的多尺度依赖关系，这表明`DeepWalk` 学到的表达能够捕获社交网络中的短距离依赖性和长距离依赖性。尽管如此，`DeepWalk` 捕获的多尺度`representation` 仍然有以下缺陷：

   - 无法保证`representation` 的多尺度：`DeepWalk` 不保证能够捕获多尺度的表达，因为 `DeepWalk` 的目标函数并没有明确的保留多个尺度。

     考虑一条长度为$L$的随机游走序列，从该序列获得的一阶关系顶点对有$L-1$个，从该序列获得的$k$阶关系顶点对有$(L-1)/k$对。因此`DeepWalk` 获得的$k$阶关系顶点对数量是一阶关系顶点对数量的$1/k ,k \gt 1$，这使得`DeepWalk` 学到的表达倾向于捕获小尺度关系。

     如果下游任务需要网络的大尺度特征（如社交网络的用户年龄预测），这种小尺度倾向性是一个致命弱点。

   - 仅保留全局`representation`：`DeepWalk` 得到的是一个覆盖网络所有尺度的全局表达，因此无法访问不同尺度的表达。

     一个理想的`representation` 形式是：依次给出从小尺度到大尺度等各个不同尺度下的表达。这样下游的不同任务可以各自选择对自己最有利的尺度对应的表达来完成任务。

3. `WALKLETS` 扩展自`DeepWalk` ，但是显式的对多尺度依赖性进行建模。

   `WALKLETS` 也是基于截断的随机游走序列对网络进行建模，但是它对采样过程进行修改：在随机游走序列上跳过$k-1$个顶点来显式构建$k$阶关系。

   如下图所示：

   - 左图为`DeepWalk` 的采样过程：给定一条截断的随机游走序列，我们将所有的$k=1,2,\cdots,K$阶关系顶点`pair` 对加入语料库来训练。

   - 右图为 `WALKLETS` 的采样过程：给定一条截断的随机游走序列，我们采样了$K$个语料库：

     -$\mathbf A^1$语料库：所有一阶关系顶点`pair` 对加入该语料库。
     -$\mathbf A^2$语料库：所有二阶关系（跳过一个顶点）顶点`pair` 对加入该语料库。
     - ...
     -$\mathbf A^K$语料库：所有$K$阶关系（跳过$K-1$个顶点）顶点`pair` 对加入该语料库。

     不同的语料库独立训练，分别得到不同尺度下的顶点表达。

     本质上第$k$个语料库的优化目标对应于矩阵$M_{i,j} = \log [ \mathbf A^k]_{i,j}$的矩阵分解。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/YHnXxwVksWra.png?imageslim">
   </p>
   

4. `WALKLETS` 除了采样策略与`DeepWalk` 不同之外，优化方式、搜索策略都与 `DeepWalk` 相同。

   - `WALKLETS` 和 `DeepWalk` 都使用随机梯度下降 `SGD` 来优化目标函数。
   - `WALKLETS` 和 `DeepWalk` 通常应用于无权图，但是可以通过将梯度乘以边的权重从而扩展到加权图。也可以通过`边采样` 技术从而扩展到加权图。
   - `WALKLETS` 和 `DeepWalk` 都使用深度优先搜索策略。相比于宽度优先搜索，深度优先搜索可以编码更长距离的信息，从而能够学得更高阶的`representation` 。
     - 也可以引入`node2vec` 的有偏随机游走策略来扩展`WALKLETS` 。
     - 也可以直接计算$\mathbf A^k$从而根据$k$阶转移概率矩阵来采样生成$k$阶顶点`pair` 对。但是这种方式只适合小型网络，因为大型网络计算$\mathbf A^k$的算法复杂度太高。

### 7.2 实验

#### 7.2.1 可视化

1. 数据集为 `Cora` 数据集，该数据集包含来自七个类别的 `2708` 篇机器学习论文。

   - 论文之间的链接代表引用关系，共有 `5429`个链接。

   - 从标题、摘要中生成短文本作为文档，并经过停用词处理、低频词处理（过滤文档词频低于 10个的单词），并将每个单词转化为 `one-hot` 向量。

     最终每篇文档映射为一个 `1433` 维的`binary` 向量，每一位为`0/1` 表示对应的单词是否存在。

2. 我们首先以顶点$v_{35}$作为源点，观察其它所有顶点和源点以`representation` 的 `cosin` 相似度作为距离的分布。

   可以看到：随着尺度的扩大（从$\mathbf A^1$到$\mathbf A^5$），一个社区逐渐形成。这说明更大尺度的`representation` 可以更好的捕获网络的底层结构特征，从而有助于解决下游任务。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/SI1nU1Id0sGO.png?imageslim">
   </p>
   

3. 我们在原始图（经过 `t-SNE` 二维可视化）中展示所有顶点到源点的距离热力图，距离越近颜色越红、距离越远颜色越蓝。

   这也显式了随着尺度的扩大，模型捕捉到一个较大社区的过程。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/feTwrEsP3wKW.png?imageslim">
   </p>
   

#### 7.2.2 多标签分类

1. 数据集：

   - `BlogCatalog`：该数据集包含 `BlogCatalog` 网站上博客作者之间的社交关系，标签代表通过元数据推断出来的博客兴趣。网络包含 `10312` 个顶点、`333983` 条边、`39`种不同标签。
   - `DBLP Network`：该数据集包含学者之间的论文合作关系。每个顶点代表一个作者，边代表一个作者对另一位作者的引用次数。网络包含 `29199` 个顶点、`133664` 条边、`4` 种不同标签。
   - `Flickr` 数据集：`Flickr`网站用户之间的关系网络。标签代表用户的兴趣组，如“黑白照片”。网络包含 `80513` 个顶点、`58999882` 条边、`195` 种不同标签。
   - `YouTube` 数据集：`YouTube` 网站用户之间的社交网络。标签代表用户的视频兴趣组，如“动漫、摔跤”。网络包含 `1138499` 个顶点、`2990443` 条边、`47` 种不同标签。

2. 基准模型：

   - `DeepWalk` ：使用截断的随机游走序列来学习顶点的表达，学到的`representation` 捕获了多个尺度的社区成员关系的线性组合。

   - `LINE`：类似于 `DeepWalk`，但是这里我们仅仅考虑一阶邻近度，即只考虑$\mathbf A^1$。

   - `GraRep`：通过显式的计算$\mathbf A^1,\cdots,\mathbf A^K$并执行 `SVD` 分解来产生多个不同尺度的`representation` 。

     本质上该方法和 `WALKLET` 是类似的，但是它不是在线学习方法，且无法扩展到大规模网络。

3. 网络参数：

   - 我们使用随机梯度下降法求解最优化目标，并且使用$C=1$的$L_2$正则化，默认的初始化学习率为 `0.025` 。

   - 每个顶点的游走序列数量为 `1000`，每个序列的长度为 `11`，维度$d$为 `128` 。

   - 对于 `WALKLETS` 我们分别考虑尺度为 `1,2,3` 的 `representation`。

     同时我们也会考虑多个尺度表达的融合，此时我们分别将每个尺度的表达向量拼接起来，然后使用 `PCA`降到 `128` 维。通过这种方式我们和其它方法进行可解释的比较。

4. 我们随机抽取$T_f$比例的标记样本作为训练集，剩下的顶点作为测试集，采用基于 `one vs rest` 的逻辑回归来执行多标签分类任务。

   每种情况随机重复执行十次，取平均的 `MICRO-F1` 作为评估指标。这里不报告准确率以及 `MACRO-F1` 得分，因为这些指标的趋势都相同。实验结果如下：

   - `BlogCatalog` 网络：使用$\mathbf A^2$的 `WALKLETS` 超越了所有其它模型。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/Sn4FXKXEgYuD.png?imageslim">
     </p>
     

   - `DBLP` 网络：使用$\mathbf A^3$的 `WALKLETS` 超越了除 `GraRep` 之外的所有其它模型。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/MchIe5XQUPIL.png?imageslim">
   </p>
   

   - `Flickr`网络：使用$\mathbf A^2$的 `WALKLETS` 超越了所有其它模型。

     `GraRep` 无法处理该数据集，因为内存不足。

      <p align="center">
         <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/SfDYnt1XOzPp.png?imageslim">
      </p>
      

   - `Youtube`网络： 联合使用$\mathbf A^2, \mathbf A^3$的 `WALKLETS` 超越了所有其它模型。

     `GraRep` 无法处理该数据集，因为内存不足。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/RBddv3Cxdp9Y.png?imageslim">
   </p>
   

5. 这些实验表明：没有哪个固定尺度的表达能够胜任所有任务，因此我们需要针对不同任务选取最合适的尺度表达。

   作者推测这些分类任务表现出层次结构，而 `WALKLETS` 学到的不同尺度的表达可以充分利用网络的层次结构来匹配对应的任务。

6. 我们继续考察 `WALKLETS` 通过采样来逼近$\mathbf A^k$的有效性。

   给定图$G$，我们首先显式的计算$k$阶转移矩阵$\mathbf A^k$，然后通过`WALKLETS` 的随机游走过程来预估$\mathbf A^k_{walk}$，并计算这两个矩阵的差异：

  $\mathbf M_{err} = |\mathbf A^k - \mathbf A^k_{walk}|$

   我们在 `DBLP,BlogCatalog` 数据集上进行评估，游走参数如前所述。我们分别给出平均误差和标准差。

   可以看到在各种尺度下，误差的均值和标准差都很低，这表明通过随机游走序列能够得到转移矩阵的一个良好近似。并且通过增加随机游走序列的数量，这种误差会进一步降低。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/AJJn33SzGw6d.png?imageslim">
   </p>
   

## 八、SDNE

1. 网络 `representation` 学习有以下几个主要挑战：

   - 高度非线性 `non-linear`：网络的底层结构是高度非线性的，如何设计模型来捕获这种高度非线性相当困难。
   - 保留结构 `structure-preserving`：如何在低维空间种保留原始网络的全局结构和局部结构也是一个难点。
   - 网络稀疏性 `sparsity` ：大多数现实世界的网络非常稀疏，仅利用已观察到的有限链接不足以获得效果较好的 `representation` 。

2. 高度非线性：过去的几十年提出了很多基于浅层模型的网络`embedding` 方法，如 `IsoMap, Laplacian Eigenmaps(LE), LINE` 。由于浅层模型的表达能力有限，所以这些方法很难捕获高度非线性的网络结构，因此得到的是次优（非最优）的网络`representation` 。

   尽管可以采用核技巧来捕获非线性，但是核技巧本身也是浅层的。

3. 保留结构：有一些方法，如 `LINE, GraRep,WALKLETS` 尝试分别使用一阶邻近度和高阶邻近度来保留局部结构和全局结构。其做法是分别学习一阶`representation` 和高阶 `representation`，然后简单的将二者拼接在一起。

   与在一个统一模型中同时建模局部网络结构和全局网络结构相比，这种方法不是最优的。

4. 论文 `《Structural Deep Network Embedding》` 提出了 `SDNE` 模型。

   - 模型利用多个非线性层来捕获非线性的网络结构。这些非线性层的组合将原始数据映射到高度非线性的潜在低维空间中，从而能够捕获到网络结构的高度非线性。

   - 一阶邻近度是直接相连的两个顶点之间的局部的、成对的相似性，它刻画了网络的局部结构。但是由于网络的稀疏性，很多真实存在的链接由于未被观察到所以缺失，这导致仅依赖一阶邻近度不足以描述整个网络。二阶邻近度表示顶点邻域结构的相似性，它刻画了网络的全局结构。

     通过一阶邻近度和二阶邻近度，我们可以很好的刻画网络本地结构和网络全局结构。而 `SDNE` 模型就利用一阶邻近度和二阶邻近度来保留网络结构。其中使用无监督学习来利用二阶邻近度从而捕获全局网络结构，使用监督学习来利用一阶邻近度从而捕获局部网络结构。

     通过在一个优化目标中同时优化这两者，`SDNE` 既可以保留局部网络结构，又可以保留全局网络结构。

     同时，由于网络二阶邻近度非零的顶点对比一阶邻近度数量多得多，因此采用二阶邻近度可以提供更多的网络结构信息，这有助于解决网络稀疏性问题。

     如下图所示，二阶邻近度顶点对的数量与一阶邻近度顶点对数量对比：

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/WucvGWwyhoBr.png?imageslim">
     </p>
     

### 8.1 模型

1. 定义图$G = (V,E)$，其中$V=\{v_1,\cdots,v_n\}$为顶点集合；$E=\{e_{i,j}\}$为边的集合，$e_{i,j}$表示顶点$v_i,v_j$之间的边，它带有一个权重$s_{i,j}\ge 0$。

   - 如果$v_i,v_j$之间不存在边，则$s_{i,j} = 0$。
   - 对于无权图有$s_{i,j}\in \{0,1\}$，对于带权图有$s_{i,j}\in \mathbb R^+$，其中$\mathbb R^+$表示所有的正实数，这里不考虑负权重。

2. 一阶邻近度 `First-Order Proximity` 刻画了成对顶点之间的相似性。

   对于顶点对$(v_i,v_j)$，其一阶邻近度就是$s_{i,j}$：

  $\text{FOP}(v_i,v_j) = s_{i,j}$

   网络嵌入必须保持一阶邻近度，这意味着如果两个顶点是直接相连的则它们总是相似的。如：如果一篇论文引用了另一篇论文，则它们应该包含一些共同主题。

   但是现实世界的网络通常非常稀疏，观察到的链接仅占很小的比例，许多彼此相似的顶点之间没有任何直接链接，因此仅仅捕获一些邻近度是不够的。

3. 二阶邻近度 `Second-Order Proximity`刻画了一对顶点的邻居之间的相似度。

   令顶点$v_i$与所有其它顶点的一阶邻近度为$\mathcal N_i=\{s_{i,1},\cdots,s_{i,|V|}\}$，因此顶点$v_i$和$v_j$的二阶邻近度由$\mathcal N_i,\mathcal N_j$的相似度来定义：

  $\text{SOP}(v_i,v_j) = \frac{\mathbf{\vec s}_i\cdot \mathbf{\vec s}_j}{||\mathbf{\vec s}_i||_2\times ||\mathbf{\vec s}_i||_2}$

   其中$\mathbf{\vec s}_i =( s_{i,1},\cdots,s_{i,|V|})^T,\mathbf{\vec s}_j =( s_{j,1},\cdots,s_{j,|V|})^T$，$||\cdot||_2$为向量长度。

   二阶邻近度假设：若两个顶点的共同邻居越多，则它们越相似。该假设在很多领域是合理的。如：在 `NLP` 中，如果两个单词的上下文越相似，则这两个单词越相似；在社交网络中，如果两个用户的共同好友越多，则他们是好友的可能性越大。

   已经证明二阶邻近度是刻画顶点相似性的一个良好指标，即使这两个顶点之间不存在直接相连的边。因此通过引入二阶邻近度可以更好的描述网络全局结构，并且缓解网络稀疏性问题。

4. 网络表示任务的目标是对每个顶点$v\in V$，学习一个低维映射函数$f: V \rightarrow \mathbb R^d$，其中$d \ll |V|$，最终得到顶点$v$的低维表达$\mathbf{\vec w}_v \in \mathbb R^d$。

   在映射的过程中还需要保持网络结构，即：同时保留网络的一阶邻近度和二阶邻近度。

5. 给定图$G=(V,E)$另其邻接矩阵为$\mathbf S$，其第$i$行$\mathbf{\vec s}_i =( s_{i,1},\cdots,s_{i,|V|})^T$刻画了顶点$v_i$的邻域结构，因此矩阵$\mathbf S$刻画了所有顶点的邻域结构。

   `SDNE` 基于$\mathbf S$和深度自编码器来保留二阶邻近度。深度自编码器首先通过编码器将输入数据映射到 `representation` 空间，然后通过解码器将 `representation` 数据映射回原始空间，在这个过程中保持重构误差最小化。

   对于顶点$i$，假设原始输入数据为$\mathbf{\vec x}_i$。由于我们将邻接矩阵$\mathbf S$作为自编码器的输入，因此有$\mathbf{\vec x}_i = \mathbf{\vec s}_i$。

   - 假设编码器为$K$层，则编码过程为：

     其中$\sigma(\cdot)$为非线性激活函数，$\{\mathbf W^{(k)},\mathbf{\vec b}^{(k)}\}_{k=1,\cdots,K}$为编码器参数。

   - 假设解码器为$K + 1$层，则解码过程为：

     其中 ：

     -$\{\hat{\mathbf W}^{(k)},\hat{\mathbf{\vec b}}^{(k)}\}_{k=0,\cdots,K}$为解码器参数。
     - 解码器多出来的一层是$\mathbf{\vec y}_i^{(K)} \rightarrow \hat{\mathbf{\vec y}_i^{(K)}}$。
     - 解码器层数也可以是任意层，不一定和编码器层数相关。

   - 最终自编码器的损失函数为重构误差：

    $\mathcal L = \sum_{i=1}^{|V|}||\hat{\mathbf{\vec x}}_i - \mathbf{\vec x}_i||_2^2$

     尽管最小化$\mathbf S$的重构误差并未明确保留样本之间的邻域结构，但是基于重构误差最小化准则可以捕捉到数据流形从而保留顶点的二阶邻近度。最终重构过程使得具有相似邻域结构的顶点具有相似的潜在 `representation`。

6. 在网络中我们虽然能够观察到部分链接，但是很多实际中存在的链接由于无法观察而缺失。这意味着：虽然顶点之间存在链接确实表明它们之间的相似，但是顶点之间不存在链接不一定表明它们不相似。

   此外，由于网络的稀疏性$\mathbf S$中非零元素远远少于零元素的数量，如果直接使用$\mathbf S$作为自编码器的输入，则自编码器更倾向于重建零元素。而这不是我们期望的行为。

   为解决该问题，我们在重构中对非零元素赋予比零元素更大的误差，因此调整目标函数为：

  $\mathcal L_{2nd} = \sum_{i=1}^{|V|}||\left(\hat{\mathbf{\vec x}}_i - \mathbf{\vec x}_i\right)\odot \mathbf{\vec c}_i||_2^2= ||(\hat{\mathbf X} - \mathbf X)\odot \mathbf C||_F^2$

   其中$\odot$表示 `Hadamard`积，$\mathbf{\vec c}_i$是$\mathbf{\vec x}_i$的误差权重向量：

   即：

   - 若顶点$v_i,v_j$存在链接（即$S_{i,j} \ne 0$），则误差权重为$\beta$，其中$\beta \gt 1$为超参数。
   - 若顶点$v_i,v_j$不存在链接（即$S_{i,j} = 0$），则误差权重为 `1` 。

7. 我们利用无监督部分重构顶点之间的二阶邻近度从而保留网络的全局网络结构。

   但是我们不仅要保留网络的全局结构，还需要保留网络的局部结构。

   一阶邻近度可以表示网络的局部结构，因此可以作为监督信息从而约束顶点的 `representation` 。因此 `SDNE` 设计了监督部分来保留局部网络结构，监督部分的损失函数为：

  $\mathcal L_{1st} = \sum_{i,j} s_{i,j} ||\mathbf{\vec y}_i^{(K)} - \mathbf{\vec y}_j^{(K)}||_2^2= \sum_{i,j} s_{i,j} ||\mathbf{\vec y}_i - \mathbf{\vec y}_j ||_2^2$

   其中：

   -$\mathbf{\vec y}_i = \mathbf{\vec y}_i^{(K)}$为顶点$v_i$的低维 `representation`，$\mathbf{\vec y}_j = \mathbf{\vec y}_j^{(K)}$为顶点$v_j$的低维 `representation`。
   -$s_{i,j}$为顶点$v_i,v_j$之间的一阶邻近度。

   该损失函数参考了 `Laplacian Eigenmaps` 的思想：相似顶点在嵌入空间中映射越远则损失越大。

   >$s_{i,j}$越大则顶点$v_i,v_j$越相似，此时它们距离$||\mathbf{\vec y}_i - \mathbf{\vec y}_j ||_2^2$应该越小。

8. 为同时保留一阶邻近度和二阶邻近度，`SDNE` 提出了一个半监督学习模型，其目标函数为：

   其中：

   -$\mathbf X = \mathbf S$为网络的邻接矩阵，它作为网络的输入。

   -$\hat {\mathbf X}$为深度自编码器对网络输入的重构结构

   -$\mathbf C$为误差权重，当$S_{i,j} = 0$时$C_{i,j} = 1$；当$S_{i,j} \ne 0$时$C_{i,j} = \beta$。

   -$\mathcal L_{reg}$为$L_2$正则化项，用于防止过拟合：

    $\mathcal L_{reg} = \frac 12 \sum_{k=1}^K ||\mathbf W^{(k)}||_F^2 + \frac 12 \sum_{k=0}^K ||\hat{\mathbf W}^{(k)}||_F^2$

   -$\alpha,\nu$为系数，用于平衡无监督损失、监督损失、正则化项。

   `SDNE` 的整体架构如下所示，模型采用多层神经网络将输入数据映射到高度非线性的潜在空间来捕获高度非线性的网络结构。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/Wsn1FOn31rXE.png?imageslim">
   </p>
   

9. 由于模型的高度非线性，目标函数的最优化过程在参数空间中很容易陷入局部最优解。为了找到一个更好的参数空间，`SDNE`首先使用深度信念网络对参数进行预训练。

10. 对于网络加入的新顶点$v_k$，如果它和其它旧顶点相连，则我们可以得到邻接向量$\mathbf{\vec x} = \{s_{k,1},\cdots,s_{k,|V|}\}$，其中$s_{k,i}$表示顶点$v_k$和旧顶点$v_i$的链接权重。

    然后我们固定其它参数，将$\mathbf{\vec x}$加入模型来训练从而得到其 `representation`。

    如果$v_k$不与所有的旧顶点相连，则目前所有的已知方法都无法处理。此时我们需要求助其它辅助信息，如新顶点的内容特征。

11. `SDNE` 模型的训练复杂度为$O(|V|\times D \times d \times I)$，其中：$|V|$为顶点数量，$D$为网络所有顶点的平均 `degree`，$d$为神经网络所有层的最大维度，$I$为迭代数量。 这里$D$主要和$\mathcal L_{1st}$相关。

    参数$d$通常和 `representation` 的维度相关，与顶点数量无关；迭代数量$I$也独立于顶点数量；网络顶点的平均 `degree` 通常被视为一个常量，如：社交网络中平均每个人的好友数量是有限的。因此$d, I, D$都与顶点数量无关，模型训练的整体复杂度和顶点数量成线性关系。

### 8.2 实验

1. 数据集：

   - `ARXIV GR-QC` 数据集：该数据集包括 `arXiv` 上广义相对论`General Relativity` 和量子力学 `Quantum Cosmology` 领域的论文的作者关联信息。每个顶点代表一个作者，边代表两个作者共同撰写了论文。

     因为我们没有顶点类别信息，因此该数据集用于链接预测任务。

   - `20-NEWSGROUP` ：该数据集包含两万个新闻组文档，每篇文档都标记为20个类别之一。我们用单词的 `tf-idf` 向量表示文档，用余弦相似度表示文档之间的相似性。网络中每个顶点代表一篇文档，边代表文档之间的相似性。

     我们选择以下三类标签的文档来执行可视化任务：`comp.graphics,rec.sport.baseball,talk.politics.gums`。

   - `BLOGCATALOG,FLICKR,YOUTUBE`数据集：这些数据集用于多标签分类任务。

     - `BlogCatalog`：该数据集包含 `BlogCatalog` 网站上博客作者之间的社交关系，标签代表通过元数据推断出来的博客兴趣。网络包含 `39` 种不同标签。
     - `Flickr` 数据集：`Flickr`网站用户之间的关系网络。标签代表用户的兴趣组，如“黑白照片”。网络包含`195` 种不同标签。
     - `YouTube` 数据集：`YouTube`网站用户之间的社交网络。标签代表用户的视频兴趣组，如“动漫、摔跤”。网络包含 `47` 种不同标签。

   这些数据集分别代表了加权图/无权图、稀疏图/稠密图、小型图/大型图。总体统计如下表：

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/LIShkUQanvgY.png?imageslim">
   </p>
   

2. 基准模型：

   - `DeepWalk`：使用截断的随机游走和 `SkipGram` 模型来生成网络表达。
   - `LINE`：分别定义损失函数来保留一阶邻近度和二阶邻近度来求解一阶邻近度`representation` 和二阶邻近度 `representation`，然后将二者拼接一起作为 `representation`。
   - `GraRep`：扩展到高阶邻近度并基于 `SVD` 分解来求解模型。它也是拼接了一阶邻近度 `representation` 和高阶邻近度 `representation` 作为最终的表达。
   - `Laplacian Eigenmaps`：通过分解邻接矩阵的拉普拉斯矩阵来生成网络表达，它仅仅保留了网络的一阶邻近度。
   - `Common Neighbor`：它仅使用共同邻居的数量来衡量顶点之间的相似性。该方法仅在链接预测任务中作为基准方法。

3. 我们分别执行了顶点重建、链接预测、多标签分类和顶点可视化任务。

   - 对于顶点重建和链接预测任务，我们采用 `precision@k` 和 `Mean Average Precision:MAP` 指标。

     - `precision@k` 指标：

      $\text{precision}@k(v_i) = \frac{|\{v_j\mid v_i,v_j \in V, \text{index}(v_j) \le k,\Delta_{v_i}(v_j) = 1\}|}{k}$

       其中：$V$为顶点集合；$\text{index}({v_j})$为预测结果中顶点$v_j$在所有顶点预测结果中排名，得分越高排名越靠前；$\Delta_{v_i}(v_j) = 1$表示顶点$v_i$和$v_j$中确实存在边。

       该指标的物理意义为：预测结果的 `top k` 顶点中，和顶点$v_i$真实存在边的顶点比例。

       该指标仅仅考察单个顶点，无法评估所有顶点的效果。

     - `MAP` 指标：

       其中$\text{AP}@k(v_i)$刻画了单个顶点在`top1,... top K` 的 `precision`，而$\text{MAP} @k$进一步评估了所有顶点在所有 `top1,... top K` 上的`precision` 。

   - 对于多标签分类任务，我们使用 `Micro-F1` 和 `Macro-F1` 指标。

4. 模型参数：

   - 对于 `LINE` 模型，随机梯度下降的 `batch-size=1`，初始化学习率为 `0.025`，负采样比例为 `5` ，总采样的样本数为 `100` 亿。

     最终将一阶邻近度表达和二阶邻近度表达拼接形成拼接向量，并使用$L_2$归一化。

   - 对于 `DeepWalk` 模型，共现窗口大小设置为 `10`，每个顶点开始的游走序列数量为 `40`，每个游走序列长度为 `40` 。

   - 对于 `GraRep`，最高转移矩阵阶数为 `5` 。

   - 对于 `SDNE`，深度自编码器的层数随着不同数据集而不同，层数和各层维度见下表。如果使用更深的模型，则由于优化困难导致性能几乎不变甚至更差。

     另外 `SDNE` 通过验证集上执行网格搜索来选择最佳超参数$\alpha,\beta,\nu$。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/RXg7soarntem.png?imageslim">
     </p>
     

#### 8.2.1 网络重建任务

1. 进行网络重建任务是为了验证：良好的网络`embedding` 模型应该确保学到的 `embedding` 能够保留原始的网络结构。

   验证方式：给定一个网络，通过模型来学习网络的`representation`，然后基于 `representation` 来重建网络并评估误差。

   注意：这里的误差是训练误差，而不是测试误差。因为网络的链接用于生成矩阵$\mathbf S$作为模型输入，所以所有的链接是已知的。

   由于所有链接是已知的，因此可以作为 `label`；而得到 `representation` 之后可以得到每个顶点的 `top k` 相似顶点。基于二者我们可以计算 `precision@k` 和 `MAP`指标作为训练误差。

   我们使用 `ARXIV GR-QC` 和 `BLOGCATALOG`数据集来验证，结果如下。可以看到：

   - `SDNE` 在两个数据集上 `MAP` 指标始终超越其它模型。
   - 当$k$增加时，`SDNE` 在两个数据集上 `precision@k` 始终最高。
   - `SDNE` 和 `LINE` 均优于 `LE`，这表明引入二阶邻近度可以更好的保留网络结构。
   - 尽管 `SDNE` 和 `LINE` 都利用了一阶邻近度和二阶邻近度来保留网络结构，但是 `SDNE` 效果超越了 `LINE`，原因可能有两个：
     - `LINE` 采用浅层结构，因此难以捕获底层网络的高度非线性结构。
     - `LINE` 直接将一阶邻近度的表达和二阶邻近度的表达拼接在一起，这比 `SDNE` 直接联合优化一阶邻近度和二阶邻近度要差。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/OE1YWChKcVlo.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/6h3VdLfVJloA.png?imageslim">
   </p>
   

#### 8.2.2 多标签分类任务

1. 对于 `BLOGCATALOG` 我们随机抽取 `10%` 到 `90%` 的顶点作为训练集，剩余顶点作为测试集；对于 `FLICKR,YOUTUBE` 我们随机抽取 `1%` 到 `10%` 的顶点作为训练集，剩余顶点作为测试集。另外我们删除 `YOUTUBE`中没有任何标签的顶点。

   我们随即重复此过程五次并报告平均的 `Micro-F1` 和 `Macro-F1` 指标。

   `BLOGCATALOG` 结果如下：

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/tiYgD0AQg7Ta.png?imageslim">
   </p>
   

   `FLICKR` 结果如下：

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/LF92uWBiFKdW.png?imageslim">
   </p>
   

   `YOUTUBE` 结果如下：

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/4c2bs3FHK5DW.png?imageslim">
   </p>
   

   可以看到：

   - `SDNE` 的效果始终超越其它模型，证明`SDNE` 可以推广到多标签分类任务中。
   - 在 `BLOGCATALOG` 数据集中，当只有 `10%` 的训练数据集时 `SDNE` 比其它模型有显著提升。这表明 `SDNE`相比于基准模型，它对于数据稀疏性鲁棒性更好。
   - 大多数情况下 `DeepWalk` 效果最差。这有两个原因：
     - `DeepWalk` 没有明确的目标来捕获网络结构。
     - `DeepWalk` 使用随机游走来产生顶点的邻居。由于随机性这会引入很多噪音，尤其对于 `degree` 很高的顶点。

#### 8.2.3 链接预测

1. 在链接预测任务中，我们随机隐藏了一部分已有链接，然后用剩下的网络来训练模型。训练完成之后我们获得每个顶点的表达，然后基于顶点表达预测未观察到的链接。

   和网络重建任务不同，链接预测任务预测的是未观察到的链接，而不是重建观察到的链接。因此链接预测任务可以评估不同模型的预测能力。

2. 在连接预测任务中，我们增加了 `Common Neighbor` 基准方法，该方法已被证明是进行链接预测的有效方法。

3. 这里使用 `ARXIV GR-QC` 数据集。首先随机隐藏 `15%` 的链接（大约 `4000` 个链接），并使用 `precision@k` 作为评估指标。

   当$k$从 `2` 逐渐增加到 `10000`时，实验结果如下。

   结论：

   - 当$k$增加时，`SDNE` 方法始终由于其它方法。这证明了 `SDNE`学到的`representation` 对于未观察到的链接具有很好的预测能力。

   - 当$k=1000$时，`SDNE` 的`precision` 仍然高于 `0.9`，而其它方法的 `precision` 掉到 `0.8` 以下。

     这表明 `SDNE` 对于排名靠前的链接预测的比较准。对于某些实际应用，如推荐和信息检索，这种优势非常重要。因为这些应用更关心排名靠前的结果。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/egygpJGXld3Y.png?imageslim">
   </p>
   

4. 我们随机删除原始网络的一部分链接从而产生更稀疏的网络，然后重复上述步骤来执行链接预测，从而评估稀疏网络的链接预测能力。

   结论：

   - 当网络更稀疏时，`LE` 和 `SDNE/LINE` 模型之间的差距增大。这表明：二阶邻近度的引入使得学到的表达对于稀疏网络鲁棒性更好。
   - 当删除 `80%` 链接时，`SDNE` 仍然比所有其它方法好。这表明：`SDNE` 在处理稀疏网络时能力更强。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/uxhNPGvKkrG0.png?imageslim">
   </p>
   

#### 8.2.4 可视化任务

1. 我们将 `20-NEWSGROUP` 学到的网络表达通过 `t-SNE` 进行可视化。每篇文档都被映射到二维空间的一个点，不同类别的文档采用不同颜色：`rec.sport.baseball` 为蓝色、`comp.graphics` 为红色、`talk.politics.guns` 为绿色。可视化结果如下图所示。

   结论：

   - `LE` 和 `DeepWalk` 的效果较差，因为不同类别的顶点彼此混合。
   - `LINE` 虽然不同类别形成了簇，但是中间部分不同类别的文档依然彼此混合。
   - `GraRep` 效果更好，但是簇的边界不是很清晰。
   - `SND` 在簇的分组以及簇的边界上表现最佳。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/3nq8MNJkhuhr.png?imageslim">
   </p>
   

   另外我们也评估了可视化的`LK` 散度指标，该指标越低越好。从下表可见，`SDNE` 效果最好。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/iJbf9P8XkuGE.png?imageslim">
   </p>
   

#### 8.2.5 参数探索

1. 我们在 `ARXIV-GRQC` 数据集上研究不同超参数的影响。

2. 维度$d$探索：

   - 当维度增加时效果先提升。这是因为更大的维度可以容纳更多的有效信息。
   - 继续增加维度将导致效果下降。这是因为太大的维度会引入更多的噪音。
   - 总体而言维度大小很重要，但 `SDNE`对此参数不是特别敏感。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/HAHcnatyGUKt.png?imageslim">
   </p>
   

3. 参数$\alpha$探索 ：参数$\alpha$用于平衡一阶邻近度和二阶邻近度的损失权重。

   - 当$\alpha = 0$时，模型性能完全取决于二阶邻近度；当$\alpha$越大，模型性能越趋近于一阶邻近度。
   - 当$\alpha = 0.1$或者$\alpha = 0.2$时模型性能最佳。这表明：一阶邻近度和二阶邻近度对于刻画网络结构都是必不可少的。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/UvMQzKobT98j.png?imageslim">
   </p>
   

4. 参数$\beta$探索：参数$\beta$控制了$\mathbf S$中非零元素的重建，$\beta$越大则模型越倾向于重建非零元素。

   - 当$\beta = 1$时效果不佳。因为$\beta = 1$表示模型重构$\mathbf S$时认为零元素和非零元素都是同样重要的。

     如前所述，虽然两个结点之间没有链接不一定表示两个结点不相似，但是两个结点之间存在链接一定表明两个结点的相似性。因此非零元素应该比零元素更加重要。所以$\beta = 1$在零元素的重构中引入大量噪声，从而降低模型性能。

   - 当$\beta$太大时性能也较差。因为$\beta$很大表示模型在重构中几乎忽略了$\mathbf S$的零元素，模型倾向于认为每一对顶点之间都存在相似性。事实上$\mathbf S$之间的很多零元素都表明顶点之间的不相似。所以$\beta$太大将忽略这些不相似，从而减低性能。

   - 这些实验表明：我们应该更多的关注$\mathbf S$中非零元素的重构误差，但是也不能完全忽略$\mathbf S$中零元素的重构误差。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/4VJWyseBpCK1.png?imageslim">
   </p>
   

## 九、CANE

1. 一个顶点在和不同的邻居顶点交互时，通常表现出不同的形象`aspect` 。例如：

   - 一个学者可以和不同的合作者`partner` 就不同的研究方向进行合作。

     如下图所示：红色、蓝色、绿色字体分别表述左侧学者、右侧学者以及所有学者都关注的研究方向。

   - 一个自媒体作者可以和不同的朋友就不同的兴趣进行分享。

   - 一个网页可以可以因为不同的目的而链接到不同的其它网页。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/xLrjfQFlR4lA.png?imageslim">
   </p>
   

2. 目前的大多数 `GraphEmbedding` 方法忽略了顶点交互过程中每个顶点的各种角色，仅为每个顶点分配一个统一的向量，这带来两个问题：

   - 它们无法处理顶点针对不同的邻居交互呈现不同`aspect` 的问题。

   - 它们往往强迫顶点的不同邻居之间的 `embedding` 也是彼此靠近的，然而事实并非总是如此。

     如上图所示：左侧学者和中间学者的距离较近，右侧学者和中间学者的距离也较近。但是，事实上左侧学者和右侧学者的距离是较远的，因为二者之间共同关注的主题很少。而传统的模型认为它们是彼此接近的，仅仅因为它们都和中间顶点相连。

   这使得顶点的 `embedding` 没有区分度，无法区分顶点的链接是刻画哪个 `aspect`。

   为解决该问题，论文 `《CANE: Context-Aware Network Embedding for Relation Modeling》` 提出了 `Context-Aware Network Embedding:CANE` 框架来精确建模顶点之间的关系。

   `CANE` 假设一个顶点在和不同的邻居顶点交互时表现出不同的角色从而产生不同的 `embedding`。

   具体而言，`CANE` 考虑每个顶点包含的丰富的外部信息，如：文本、标签以及其它元数据。传统的`Graph embedding` 模型忽略了这些上下文信息，因此每个顶点都是静态的`embedding` 向量。`CANE` 根据和顶点交互的不同邻居为顶点动态分配一个`embedding` 向量，这被称作 `context-aware embedding` 上下文感知向量。`CANE` 通过`attentioin` 机制学习顶点的上下文感知向量，从而精确的建模顶点之间的语义关系。

3. 本文仅考虑外部信息为文本的文本信息网络，但是 `CANE` 可以轻松扩展到其它类型的信息网络。

### 9.1 模型

1. 给定信息网络$G=(V,E,T)$，其中$V$为顶点集合、$E \sube V\times V$为边的集合、$T$为顶点的文本信息集合。边$e_{u,v} \in E$表示顶点$(u,v)$的关系，并且它关联权重$w_{u,v}$。

   每个顶点$v\in V$的文本信息为一个单词序列$\mathbb S_v=\{w_1,w_2,\cdots,w_{n_v}\}$，其中$n_v = |\mathbb S_v|$，$w_i$为词汇表中的单词。

   网络 `Representation learning` 学习的任务是：根据网络结构和关联的文本信息为每个顶点$v\in V$学到一个低维 `embedding` 向量$\mathbf{\vec v}\in \mathbb R^d$，其中$d\ll |V|$。

   有两种类型的 `embedding` 向量：

   - 上下文无关 `embedding`：学到的 `embedding` 向量是上下文无关的。即：每个顶点学到的 `embedding` 向量是唯一的，不会因为上下文信息（即与顶点交互的邻近顶点）的改变而改变。
   - 上下文感知`embedding`：学到的 `embedding` 向量随着上下文的不同而不同。

2. 为了充分利用网络的结构信息和文本信息，`CANE` 提出两种类型的 `embedding`：

   - 结构 `embedding` ：捕获网络的结构信息。
   - 文本 `embedding` ：捕获顶点的文本信息。

   给定顶点$v$，结构 `embedding`用$\mathbf{\vec v}^s$来表示，文本 `embedding` 用$\mathbf{\vec v}^t$来表示。顶点$v$的 `embedding` 可以通过简单的拼接这些 `embedding` 来实现：$\mathbf{\vec v} = \mathbf{\vec v}^s \oplus \mathbf{\vec v}^t$，其中$\oplus$表示向量拼接操作。

   文本 `embedding` 可以是上下文相关的，也可以是上下文无关的。当文本 `embedding` 是上下文相关时，顶点的整体 `embedding` 也是上下文相关的。

3. `CANE` 的目标函数为：

  $\mathcal L = \sum_{e\in E} L(e)$

   其中$L(e) = L_s(e) + L_t(e)$由两部分组成：$L_s(e)$表示基于结构的目标函数，$L_t(e)$表示基于文本的目标函数。

   - 基于结构的目标函数：假设网络是有向的（无向边可以视为两个方向相反、权重相等的有向边），基于结构的目标函数旨在通过结构`embedding` 来最大化有向边的对数似然：

    $L_s(e) = w_{u,v}\times \log p(\mathbf{\vec v}^s \mid \mathbf{\vec u}^s)$

     和 `LINE` 一样，我们定义已知顶点$u$的条件下存在边$(u,v)$的概率为：

    $p(\mathbf{\vec v}^s \mid \mathbf{\vec u}^s) = \frac{\exp(\mathbf{\vec u}^s \cdot \mathbf{\vec v}^s)}{\sum_{z\in V} \exp(\mathbf{\vec u}^s \cdot \mathbf{\vec z}^s)}$

   - 基于文本的目标函数：现实世界网络中的顶点通常会伴随关联的文本信息，因此我们可以利用这些文本信息来学习基于文本的顶点 `embedding` 。

     基于文本的目标函数$L_t(e)$有多种形式。为了和$L_s(e)$保持一致，我们定义$L_t(e)$为：

     其中：

     -$\alpha,\beta,\gamma$控制了对应部分的权重

     - 条件概率$p(\mathbf{\vec v}^t\mid \mathbf{\vec u}^t),p(\mathbf{\vec v}^t\mid \mathbf{\vec u}^s),p(\mathbf{\vec v}^s\mid \mathbf{\vec u}^t)$将两种类型的顶点 `embedding` 映射到相同的表达空间中，其计算公式也采用类似$p(\mathbf{\vec v}^s \mid \mathbf{\vec u}^s)$的 `softmax` 。

       考虑到结构`embedding` 和文本 `embedding` 的特定，理论上不需要强制将它们映射到同一个表达空间。

4. 网络的结构 `embedding` 的学习和传统网络 `embedding` 相同，但是网络的文本 `embedding` 直接从顶点的关联文本中学习。

   我们可以用上下文无关的方式学习文本 `embedding` ，也可以用上下文相关的方式学习文本 `embedding` 。

#### 9.1.1 上下文无关文本 embedding

1. 有很多神经网络模型可以从单词序列中获取文本 `embedding`，包括 `CNN,RNN` 等。论文中作者研究了多个模型，包括 `CNN` 、双向 `RNN`、`GRU` ，最终选择了效果最好的 `CNN`，因为 `CNN` 能够捕获单词之间的语义依赖性。

   `CNN` 将顶点的单词序列作为输入，通过三层神经网络获取顶点的文本 `embedding`。这三层依次为 `looking-up` 层、卷积层、池化层。

   - `looking-up` 层：给定单词序列$\mathbb S= \{w_1,\cdots,w_n\}$，`looking-up` 层将单词$w_i$转换成对应的词向量$\mathbf{\vec w}_i \in \mathbb R^{d^\prime}$，最终得到输入对应的`word embedding` 序列：

     其中$d^\prime$为词向量的维度。

   - 卷积层：卷积层提取`word embedding` 序列$\mathbf S$的局部特征。

     卷积层在一个长度为$l$的滑动窗口上使用卷积核$\mathbf C\in \mathbb R^{d\times (l\times d^\prime)}$对$\mathbf S$执行卷积操作：

    $\mathbf{\vec x}_i = \mathbf C *\mathbf S_{i:i+l-1} + \mathbf{\vec b}$

     其中$\mathbf S_{i:i+l-1}$表示 `word embedding` 序列的第$i$个滑动窗口内的$l$个词向量，$*$表示卷积，$\mathbf{\vec b} \in \mathbb R^d$为偏置向量。

     注意：这里对句子边缘添加了零填充词向量。

   - 最大池化层：为获取文本`embedding`$\mathbf{\vec v}^t$，我们采用了一个最大池化层以及一个非线性映射：

    $r_i = \tanh(\max(x^i_0,x^i_1,\cdots,x^i_n))$

     其中$x_j^i$表示第$j$个向量的第$i$个分量。

     最终得到文本 `embedding` 向量为：$\mathbf{\vec v}^t = (r_1,\cdots,r_d)^T$。

   这种方式得到的$\mathbf{\vec v}^t$和其它顶点无关，因此称之为上下文无关的文本 `embedding` 。

#### 9.1.2 上下文相关文本embedding

1. 如前所述，我们假设顶点和不同的邻居顶点交互时扮演不同的角色。即：每个顶点都应该针对不同的目标顶点产生不同的焦点。这将产生上下文相关的文本`embedding` 。

   为实现该目的，论文提出了 `mutual attention` 来获取上下文相关的文本`embedding`。`mutual attention` 机制使得 `CNN` 池化层能够感知到链接对端的顶点，从而使得链接对端顶点的文本信息能够直接影响当前顶点的文本`embedding` 。

2. 给定一条边$e_{u,v}$以及顶点$u$的文本序列$\mathbb S_u$和顶点$v$的文本序列$\mathbb S_v$，假设从顶点$u$卷积层得到的句子为$\mathbf P\in \mathbb R^{d\times m}$、从顶点$v$卷积层得到的句子为$\mathbf Q \in \mathbb R^{d\times n}$，其中$m,n$分别代表两个序列的长度。

   定义注意力矩阵 `attentive matrix` 为$\mathbf A\in \mathbb R^{d\times d}$，则我们得到相关性矩阵 `correlation matrix` 为：

  $\mathbf F = \tanh(\mathbf P^T\mathbf A\mathbf Q) \in \mathbb R^{m\times n}$

   其中$F_{i,j}$表示两个隐向量$\mathbf{\vec p}_i,\mathbf{\vec q}_j$的成对相关性得分`pair-wise correlation score` 。

   - 我们沿着$\mathbf F$的行、列分别进行池化操作，从而得到重要性向量 `importance vector`，分别称作行池化 `row pooling` 、列池化 `column pooling` 。根据经验，均值池化的效果优于最大池化，因此有:

     其中：

     -$\mathbf{\vec g}^p = (g_1^p,\cdots,g_m^p)^T \in \mathbb R^n$表示行池化向量，它就是$\mathbf P$的重要性向量。
     -$\mathbf{\vec g}^q = (g_1^q,\cdots,g_n^q)^T\in \mathbb R^m$表示列池化向量，它就是$\mathbf Q$的重要性向量。

   - 然后我们将重要性向量转化为注意力向量：

     最终顶点$u$和$v$的上下文相关文本 `embedding`分别为：

   - 给定边$(u,v)$中顶点$u$和$v$的结构 `embedding`，则最终得到顶点$u$和$v$的上下文相关 `embedding` 为：

     其中$\oplus$表示向量拼接操作。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/YMBGfuLqT3my.png?imageslim">
   </p>
   

#### 9.1.3 最优化

1. `CANE` 的目标函数为：

  $\mathcal L = \sum_{e\in E} L(e) = \sum_{e\in E}\left( L_s(e)+\alpha \times L_{tt}(e) + \beta\times L_{ts}(e) + \gamma \times L_{st}(e)\right)$

   这里存在多个条件概率，且这些条件概率都是以 `softmax` 函数的形式，计算复杂度太高。`CANE` 采用负采样来降低计算复杂度，将目标函数转化为以下形式：

  $\log \sigma(\mathbf{\vec u }\cdot \mathbf{\vec v}) + \lambda \mathbb E_{z\sim P(v)}[\log \sigma(-\mathbf{\vec u}\cdot \mathbf{\vec z})]$

   其中$\lambda$为负采样的样本数，$\sigma(\cdot)$为 `sigmoid`函数，$P(v)\propto d_v^{3/4}$为顶点的概率分布，$d_v$为顶点的`degree` 。

2. `CANE` 采用 `Adam` 优化器来优化目标函数。

3. 对于新顶点，`CANE` 可以通过训练好的 `CNN` 来产生文本`embedding` 从而实现 `zero-shot` 。

### 9.2 实验

1. 数据集：

   - `Cora` ：一个典型的论文引用网络数据集。在过滤掉没有文本信息的论文之后，网络中包含 `2277` 篇机器学习论文，涉及 `7` 个类别。
   - `HepTh`：另一个论文引用网络数据集。在过滤掉没有文本信息的论文之后，网络中包含 `1038` 篇高能物理方面的论文。
   - `Zhihu`：一个大型的在线问答网络，用户可以相互关注并在网站上回答问题。我们随机抽取了`1万` 名活跃用户，并使用他们关注主题的描述作为文本信息。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/YnT0BL2U4Fj5.png?imageslim">
   </p>
   

2. 基准模型：

   - 仅考虑网络结构的基准模型：
     - `Mixed Membership Stochastic Blockmodel:MMB`: 是关系数据的一个传统图模型，它允许每个顶点在形成边的时候随机选择一个不同的`topic`。
     - `DeepWalk`：使用截断的随机游走将图结构转化为线性结构，然后使用层次 `softmax`的 `SkipGram` 模型处理序列。
     - `LINE`：分别定义损失函数来保留一阶邻近度和二阶邻近度来求解一阶邻近度`representation` 和二阶邻近度 `representation`，然后将二者拼接一起作为 `representation` 。
     - `Node2Vec`：通过一个有偏随机游走过程`biased random walk procedure` 来将图结构转化为线性结构，然后使用层次 `softmax` 的 `SkipGram` 模型处理序列。
   - 考虑结构和文本的模型：
     - `Naive Combination`：用两个模型分别计算结构 `embedding` 和 文本 `embedding` ，然后简单的将结构 `embedding` 和 文本`embedding` 拼接起来。
     - `TADW`：采用矩阵分解的方式将顶点的文本特征融合到网络 `embedding` 中。
     - `CENE`：将文本内容视为一种特殊的顶点来利用结构和文本信息，并优化异构链接的概率。

3. 评估指标：链接预测任务： `AUC` 指标；顶点分类任务：分类准确率。

4. 模型超参数：为公平起见，所有模型的 `embedding` 维度都是 `200` 维。

   - `LINE` 模型中，负采样数设置为 `5`；一阶`embedding` 和 二阶 `embedding` 维度都是 `100` 维，使得拼接后的向量有 `200` 维。
   - `node2vec` 模型中，我们使用网格搜索并选择最佳的超参数。
   - 在 `CANE` 中，我们通过超参数搜索选择最佳的$\alpha,\beta,\gamma$。并且为了加速训练过程，我们选择负采样数$\lambda = 1$。
   - 为了说明文本 `embedding` 以及注意力机制的效果，我们设计了三个版本：
     - `CANE with text only`：仅仅包含文本 `embedding` （包含 `attention`）
     - `CANE without attention`：包含结构 `embedding` 以及上下文无关的文本 `embedding`
     - `CANE`：完整的 `CANE` 模型

#### 9.2.1 链接预测任务

1. 我们分别移除了 `Cora,HepTh,Zhihu` 等网络不同比例的边，然后评估预测的 `AUC` 指标。

   注意到：当仅保留 `5%` 的边来训练时，大多数顶点是没有链接的，此时所有方法的效果都较差。因此我们不考虑 `5%` 以下比例的情况。

   - `Cora` 的结果如下，其中$\alpha = 1.0,\beta = 0.3,\gamma = 0.3$。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/Lht1qFMcN4Ac.png?imageslim">
     </p>
     

   - `HepTh` 的结果如下，其中$\alpha = 0.7,\beta = 0.2,\gamma = 0.2$。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/m2yiv4dkO0PQ.png?imageslim">
     </p>
     

   - 知乎的结果如下，其中$\alpha = 1.0,\beta = 0.3,\gamma = 0.3$。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/JR79kdXq5I72.png?imageslim">
     </p>
     

2. 从链接预测任务的实验结果可以看到：

   - `CANE` 在所有训练集、所有训练比例都取得最好的效果，这表明 `CANE`在链接预测任务中的有效性，验证了 `CANE` 具备对顶点之间关系进行精确建模的能力。
   - 在不同的训练比例下，`CENE` 和 `TADW` 的效果不稳定。
     - `CENE` 在训练比例较小时效果比 `TADW` 较差，因为相比 `TADW` ，`CENE` 使用了更多的参数（如卷积核和`word embedding`），因此 `CENE`需要更多的训练数据。
     - `CANE` 在各种情况下，效果都很稳定。
   - 通过引入注意力机制，学到的上下文相关`embedding` 比上下文无关`embedding` 效果更好。这验证了我们的假设：顶点和其它顶点交互时扮演了不同的角色。这有利于链接预测任务。

#### 9.2.2 顶点分类任务

1. 网络分析任务（如顶点分类和聚类）需要得到该顶点的整体 `embedding`，而不是顶点的很多个上下文感知`embedding`。为了得到整体`embedding`，我们简单的将每个顶点的所有上下文感知`embedding` 取平均：

  $\mathbf{\vec u} = \frac 1N \sum_{(u,v)\;\text{or}\; (v,u)\in E} \mathbf{\vec u}_{v}$

   其中$N$为顶点$u$存在链接的顶点的数量。

2. 我们在 `Cora` 数据集上执行顶点分类任务，采用 `2-fold` 交叉验证并报告平均准确率。

   结论：

   - `CANE` 可以取得最先进的 `CENE` 相当的性能。这表明：学到的上下文相关`embedding` 可以通过简单的取平均操作转化为高质量的上下文无关`embedding`，并进一步应用于其它网络分析任务。
   - 通过引入注意力机制，`CANE` 的性能比没有注意力的版本得到提升。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/qQGLxs3uYacD.png?imageslim">
   </p>
   

#### 9.2.3可视化

1. 为了说明 `mutual attention` 机制从文本信息中选择有意义特征的重要性，我们可视化了两个顶点的热力度。图中，每个单词都带有各种背景色，颜色越深该单词的权重越大。每个单词的权重根据注意力权重来计算，计算方式为：

   - 首先，对于每对顶点`pair`，我们获取每个卷积窗口的注意力权重：

    $a_i^p = \frac{\exp(g_i^p)}{\sum_{j=1}^m \exp(g_j^p)}$

   - 然后，在这个窗口内我们为每个单词分配注意力权重。

   - 最后，考虑到一个单词可能会出现在多个窗口内，我们将该单词的多个权重相加，得到单词的注意力权重。

   如下图所示，我们选择 `Cora` 数据集存在引用关系的三篇论文 `A,B,C` 。可以看到：尽管论文 `B,C` 和论文 `A` 存在引用关系，但是论文 `B` 和论文 `C` 关注的是论文 `A` 的不同部分： `Edge #1` 在论文 `A` 上重点关注`reinforcement learning`；`Edge #2` 在论文 `A` 上重点关注 `machine learning, supervised learning algorithms,complex stochastic models` 。

   另外，论文`A` 中的所有这些关键元素都可以在论文 `B` 和论文 `C` 中找到对应的单词。

   这些挖掘出的顶点之间显著的相关性，反应了 `mutual attention` 机制的有效性，同时也表明 `CANE` 精确建模关系的能力。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/M10a4nPXT0b1.png?imageslim">
   </p>
   

## 十、EOE

1. 大数据时代有大量的相关信息可用，这些信息可以融合成一个耦合的异构网络，其中每种类型的信息可以表示为单个同构子网络。

   这里我们定义耦合异构网络为：两个不同类型但是相互关联的子网络组成的网络。这些子网络通过子网络之间的链接来相互连接。

   例如：

   - 论文引用网络中的`author` 和 `word`：作者可以通过作者之间的交互来链接，单词可以通过单词共现来链接，作者可以通过他们文章的单词来链接。
   - 社交网络中的 `user` 和 `word`：类似于论文引用网络中的 `author` 和 `word` 。
   - 观影网络中的 `customer` 和 `movie`：观众可以通过观众之间的关系来链接，电影可以通过共同的演员或者导演来链接，观众可以通过他们观看的电影来链接。
   - 基因和化合物：基因可以通过基因之间的相互作用来链接，化合物可以通过具有相同的基团关系来链接，基因可以通过 `binding` 关系和化合物链接。

   为直观说明这个概念，下图实现了`author`和 `word` 网络的例子。其中作者通过 `co-authorship` 关系来链接，单词通过标题中的共现关系来链接。我们从 `DBLP` 数据集进行采样，采样的 `author` 包含 `2000 ~2003` 年在两个数据挖掘会议 `KDD,IDCM`以及两个数据库会议 `SIGMOD,VLDB` 上发表论文的作者。`author` 和 `word` 之间链接用黑线表示。为了更清晰的展示，我们忽略了`author` 网络内部的边以及 `word` 网络内部的边，并且绘制的顶点大小和它的 `degree` 成正比。

   可以看到：

   - `author` 形成了两个聚类，`word` 也形成了两个聚类。这些聚类可以通过社团检测算法来生成。

   - 数据挖掘专家到数据挖掘领域单词之间的边，要比数据挖掘专家到数据库领域单词的边更多。

     数据库专家到数据库领域单词之间的边，要比数据库专家到数据挖掘领域单词之变的边更多。

   这说明：作者和单词之间的链接可以在存在作者之间链接的基础上，额外提供补充信息。这是因为相同领域的作者更有可能在其领域内的单词上存在链接，这使得仅仅从作者之间的链接学到的`embedding` 更加全面和准确。

   当作者之间缺乏链接时（如冷启动时），这种补充信息尤为重要。

   学习单词的`embedding` 的情况也是类似的。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/y3GGFdJFs5J2.png?imageslim">
   </p>
   

2. 目前存在一些分别作用于`author` 网络或者`word` 网络的 `embedding` 方法，但是从单个网络的 `embedding` 方法扩展到用于耦合异构网络的 `embedding` 并不容易。主要挑战在于两个不同网络的异构性，这将导致两个异构的潜在空间`latent space`，而这两个空间的特征不能直接匹配。

   为解决该问题，论文 `《Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks》` 提出了`Embedding of Embedding : EOE` 方法，该方法通过引入一个调和嵌入矩阵 `harmonious embedding matrix` 从而将`embedding` 从一个潜在空间进一步嵌入到另一个潜在空间。

   作为一种 `embedding` 方法，`EOE` 使得存在链接的顶点在潜在空间中尽可能接近。但是和现有 `embedding` 方法相比，`EOE`的关键区别在于：在 `EOE` 方法中存在两个子网络、三种类型的链接、两个潜在空间。此外 `EOE` 必须同时学习两个子网络的潜在特征，因为任一侧特征都可以通过网络间的链接向另一侧子网络提供补充信息。

3. `EOE` 模型存在三种类型的参数：两个子网络的 `embedding` 参数以及调和嵌入矩阵。为了优化目标函数，`EOE` 提出了一种交替优化算法，其中每次仅针对其中某一种类型的参数进行优化。

   通过这种交替优化的方式，`EOE` 可以用一系列简单的优化代替对三种参数的比较困难的联合优化。

4. `DeepWalk` 的随机游走可能会更跨多个社区，这违背了保持网络结构的目标。

### 10.1 模型

1. 耦合异构网络：一个耦合异构网络是由通过网络间链接相连的两个不同、但是相关的子网络组成。“不同”指的是两个子网络的顶点是不同类型的，“相关” 指的是两个子网络的顶点存在某种联系。

   定义两个子网络分别为：$G_u(U,E_u,W_u),G_v(V,E_v,W_v)$，其中$U,V$分别表示两个子网络的顶点， ，$E_u,E_v$分别为两个子网络的边，$W_u,W_v$分别为两个子网络的边的权重。

   定义耦合异构网络为：$G_{u,v}(G_u,E_{u,v},W_{u,v},G_v)$，其中$E_{u,v}$为子网之间的边，$W_{u,v}$为对应的权重。

   所有的边可以是带权重的、无权重的，有向的、无向的。

2. `EOE` 假设存在链接的一对顶点在潜在空间中是相邻的。

   - 在子网络$G_u(U,E_u,W_u)$中，设两个顶点$i,j$的`embedding` 向量分别为$\mathbf{\vec u}_i,\mathbf{\vec u}_j$，则它们在潜在空间的相似度定义为：

    $p(\mathbf{\vec u}_i,\mathbf{\vec u}_j) = \frac{1}{1+ \exp(- \mathbf{\vec u}_i\cdot \mathbf{\vec u}_j)}$

   - 在子网络$G_v(V,E_v,W_v)$中，设两个顶点$k,l$的`embedding` 向量分别为$\mathbf{\vec v}_k,\mathbf{\vec v}_l$，则它们在潜在空间的相似度定义为：

    $p(\mathbf{\vec v}_k,\mathbf{\vec v}_l) = \frac{1}{1+ \exp(- \mathbf{\vec v}_k\cdot \mathbf{\vec v}_l)}$

   - 在异构网络$G_{u,v}(G_u,E_{u,v},W_{u,v},G_v)$中，子网络$G_u$的顶点$i$和子网络$G_v$的顶点$k$的相似度不能使用上式的定义，因为顶点$i$的 `embedding` 向量$\mathbf{\vec u}_i$和顶点$k$的 `embedding` 向量$\mathbf{\vec v}_k$来自于不同的潜在空间。

     为了调和两个潜在空间的异构性，我们引入一个调和嵌入矩阵$\mathbf M \in \mathbb R^{d_u\times d_v}$，其中$d_u$和$d_v$分别为$U$和$V$的潜在特征的维度。这个调和嵌入矩阵将一个潜在空间进一步嵌入到另一个潜在空间。

     因此子网络$G_u$的顶点$i$和子网络$G_v$的顶点$k$的相似度定义为：

    $p(\mathbf{\vec u}_i,\mathbf{\vec v}_k) = \frac{1}{1+ \exp(- (\mathbf M^T\mathbf{\vec u}_i)\cdot \mathbf{\vec v}_k)}$

3. `EOE` 模型不仅使得存在链接的顶点彼此相似，还使得不存在链接的顶点彼此不相似。后者同样重要，因为它将保留哪些顶点之间不大可能交互的信息，这也是网络结构信息的一部分。

   因此 `EOE` 将这两个规则（存在链接的顶点彼此相似、不存在链接的顶点彼此不相似）转化为一个最优化问题：最大似然存在链接的顶点`pair` 对，以及最小似然不存在链接的顶点 `pair` 对。

   - 最大似然存在链接的顶点`pair` 对，这一部分的损失函数为：

    $L(U) = \sum_{(i,j) \in E_u} (w_u)_{i,j} \times f(p(\mathbf{\vec u}_i,\mathbf{\vec u}_j))$

     其中$(w_u)_{i,j}$为顶点$u_i,u_j$之间的边的权重，$f(\cdot)$为自定义的损失函数。

   - 最小似然不存在链接的顶点 `pair`对，这一部分的损失函数为：

    $L(U) = \sum_{(h,k)\ne E_u}f( 1- p(\mathbf{\vec u}_h,\mathbf{\vec u}_k))$

     当网络规模太多时，在所有不存在链接的顶点`pair` 对之间计算损失函数是不现实的，因此我们对不存在边的顶点 `pair` 对进行采样。

   通常损失函数$f(\cdot)$应该是单调递减的：概率越大损失越小，概率为 `1` 时损失为零。且损失函数应该是凸的、连续可微函数，从而方便优化。通常我们采用负的对数似然，因此有：

  $L(U) = - \sum_{(i,j) \in E_u} (w_u)_{i,j} \times \log p(\mathbf{\vec u}_i,\mathbf{\vec u}_j) - \sum_{(h,k)\ne E_u}\log( 1- p(\mathbf{\vec u}_h,\mathbf{\vec u}_k))$

   对图$G_u,G_v$以及两个子图之间的链接都采用这种方式的损失函数，再结合$L_1,L_2$正则化，则总的损失函数为：

   其中$\lambda,\beta,\eta \in \mathbb R$为正则化系数，$N_u,N_v$为$G_u,G_v$对应的顶点数。

   调和嵌入矩阵的$L_1$正则化用于执行特征选择来调和两个潜在空间，因为这会引入稀疏性。

### 10.2 交替优化

1. 最小化$\mathcal L(U,V,M)$是一个凸优化问题。根据定义有：

   其中$d_u,d_v$为各自潜在空间的维度。

   其中$m_{i,j}$为$\mathbf M$为第$i$行第$j$列，$\text{sign}(x)$定义为：

  $\mathcal L(U,M,V)$对于$m_{i,j} = 0$不可微的，但是考虑到$m_{i,j}$在实际中很少会刚好为 0，因此如果$m_{i,j}$在梯度下降过程中会穿越零点，则我们将其设置为零。这称作 `lazy update`，因此将鼓励$\mathbf M$成为稀疏矩阵。

2. `EOE` 提出了一种基于梯度的交替优化算法，其中损失函数每次优化一种类型的变量直至收敛。这种交替优化算法用一系列更容易的优化来代替对三个变量的联合的、困难的优化。

   所谓交替优化，即直到当前类型的变量收敛时才进行下一个类型的变量的优化。这是因为每个类型的变量都影响其它两个类型的变量。如果当前类型的变量未能够优化，则它可能会带来负面影响。

   一个直观的例子，在优化`author` 和 `word`耦合异构网络时，假设作者 `A` 和作者 `B` 存在链接、作者 `A` 和单词$w_a$存在链接、作者 `B` 和单词$w_b$存在链接、单词$w_a$和单词$w_b$存在链接。我们首先优化`word` 子网络、再优化 `author` 子网络：

   - 假设在切换到 `author` 子网络之前，`word` 网络没有优化好，导致两个共现的单词$w_a,w_b$在 `word` 潜在空间中并没有相邻。
   - 在切换到 `author` 子网络之后，由于作者 `A` 和 `B` 之间存在链接，则二者在`author` 潜在空间中应该时相邻的。但是由于单词$w_a$和单词$w_b$在 `word` 潜在空间中是不相邻的，则导致作者 `A` 和 `B` 在 `author` 潜在空间中被分开。

3. `EOE` 交替优化算法：

   - 输入：

     - 耦合异构图$G_{u,v}(G_u,E_{u,v},W_{u,v},G_v)$，维度$d_u,d_v$
     - 正则化系数$\lambda,\beta,\eta$

   - 输出：顶点$U,V$的 `embedding` 向量，调和嵌入矩阵$\mathbf M$

   - 算法步骤：

     - 随机初始化顶点$U,V$的 `embedding` 参数，以及矩阵$\mathbf M$

     - 预训练顶点$U$，循环迭代直到算法收敛。迭代步骤为：

       - 对$U$里的所有顶点计算梯度$\nabla_{\mathbf{\vec u}_i}\mathcal L(U)=\frac{\partial \mathcal L(U)}{\partial \mathbf{\vec u}_i}$，其中$\mathcal L(U)$为仅仅考虑子图$G_u$的损失函数。

       - 线性搜索找到学习率$\eta_u$。

       - 更新参数：

          

        $\mathbf{\vec u}_i^{

         } = \mathbf{\vec u}_i^{

         } - \eta_u \nabla_{\mathbf{\vec u}_i}\mathcal L(U)$，其中$p$为迭代次数。

     - 预训练顶点$V$，循环迭代直到算法收敛。迭代步骤为：

       - 对$V$里的所有顶点计算梯度$\nabla_{\mathbf{\vec v}_i}\mathcal L(V)=\frac{\partial \mathcal L(V)}{\partial \mathbf{\vec v}_i}$，其中$\mathcal L(V)$为仅仅考虑子图$G_v$的损失函数。
       - 线性搜索找到学习率$\eta_v$。
       - 更新参数：$\mathbf{\vec v}_i^{} = \mathbf{\vec v}_i^{} - \eta_v \nabla_{\mathbf{\vec v}_i}\mathcal L(V)$，其中$p$为迭代次数。

     - 循环直到收敛，迭代步骤为：

       - 固定$U$和$V$的 `embedding` 参数，利用梯度下降算法优化$\mathbf M$
       - 固定$V$的 `embedding` 参数和$\mathbf M$，利用梯度下降算法优化$U$的 `embedding` 参数
       - 固定$U$的 `embedding` 参数和$\mathbf M$， 利用梯度下降算法优化$V$的 `embedding` 参数

     - 最终返回顶点$U,V$的 `embedding` 向量以及调和嵌入矩阵$\mathbf M$

4. 在 `EOE` 交替优化算法中，我们对$G_u$和$G_v$分别执行预训练来学习顶点$U$和$V$的潜在特征，这是为了缓解在优化一种类型的顶点时，另一种类型的顶点带来负面影响。

5. 在 `EOE` 交替优化算法中，我们采用 `backtracking line search` 方法为每次迭代选择一个合适的学习率。

6. `EOE` 交替优化算法的收敛条件为：当前迭代和最近一次迭代之间的损失函数差距小于一个阈值，如 `1e-5` 。

7. `EOE` 交替优化算法的复杂度和计算顶点`embedding` 梯度、计算调和矩阵梯度的复杂度成比例。这些梯度的计算复杂度都是$O(nd_u\times d_v)$，其中$n$为存在链接的顶点`pair` 对的数量，因此总的算法复杂度为：$O(nd_u\times d_v\times I)$，其中$I$为迭代次数。

   因此 `EOE` 交替优化算法可以在多项式时间内求解出结果，所以它也适合大型网络。

### 10.3实验

1. `EOE` 方法适合各种类型的图（带权图、无权图，有向图、无向图），它仅仅保留一阶邻近度。

2. 基准模型：

   - 谱聚类：使用归一化的拉普拉斯矩阵的 `top d` 个特征向量作为 `embedding` 向量。

   - `DeepWalk`：通过截断的随机游走和 `SGNS` 来学习顶点的 `embedding` 向量。

     由于 `DeepWalk` 仅适用于未加权的边，因此所有边的权重设置为 1 。

   - `LINE`：分别定义损失函数来保留一阶邻近度和二阶邻近度来求解一阶邻近度`representation` 和二阶邻近度 `representation`，然后将二者拼接一起作为 `representation`。

     `LINE` 有两个变体：`LINE-1st` 仅保留一阶邻近度，`LINE-2nd` 仅保留二阶邻近度。由于如何合理的结合 `LINE-1st` 和 `LINE-2nd` 尚不清楚，因此我们不和 `LINE(1st+2nd)`进行比较。

#### 10.3.1 可视化

1. 数据集：`DBLP` 数据集：从四个研究研究领域中的热门会议中得到的论文网络数据，包含的会议为：数据库领域的 `SIGMOD,VLDB,ICDE,EDBT,PODS`，数据挖掘领域的 `KDD,ICDM,SDM,PAKDD`， 机器学习领域的 `ICML,NIPS,AAAI,IJCAI,ECML` ，信息检索领域的 `SIGIR,WSDM,WWW,CIKM,ECIR`。

   我们选择了 `2000` 到 `2009` 年发表的论文，过滤掉少于三位作者的论文，并过滤掉停用词，如`where,how` 。过滤后的作者网络`author network` 包含 `4941`位作者、`17372` 条链接，单词网络 `word network` 有`6615`个单词、`78217`条链接，作者网络到单词网络的链接有 `92899` 条。

2. 所有基准模型都分别为作者和单词学习`embedding`，而`EOE` 通过联合推断来学习这些 `embedding` 。

   所有方法学到的`embedding` 向量的维度均为 `128` 维，我们采用 `t-SNE` 将这些向量映射到二维空间，如下图所示。其中：绿色为数据库领域、浅蓝色为信息检索领域、深蓝色为数据挖掘领域、红色为机器学习领域。

   > 下图的第二排第一图应该为 SpectralClustering Word Network，这个是作者原图的标示错误。

   `author` 子网络和 `word` 子网络应该显示四个聚类的混合，因为所选的四个研究领域密切相关。

   - 谱聚类的结果不符合预期，因为谱聚类的学习目标不是保留网络结构。

   - 尽管 `DeepWalk` 和 `LINE` 在某种程度上表现更好，但是它们无法同时展示四个不同的聚类。

     `DeepWalk` 的问题可能在于：随机游走跨越了多个研究领域，结果导致来自不同领域的数据点被分布在一起。

     `LINE` 也有类似的问题，因为作者可能具有来自不同领域的其它作者的链接。

   - `EOE` 的结果最符合预期，因为 `EOE`可以通过补充的文本信息来克服上述问题。

     具体而言，即使某些作者和很多不同邻域的其它作者存在链接，作者的论文中的单词也能够提供该作者研究领域的补充信息。

     单词网络的情况也类似。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/yD2DV7fy2xHb.png?imageslim">
   </p>
   

#### 10.3.2 链接预测

1. 链接预测问题是指：通过顶点之间的相似性来预测顶点之间是否存在链接。我们实验了两种场景：未来链接预测 `future link prediction`、缺失链接预测 `missing link prediction`。

   - 未来链接预测：两个顶点之间目前不存在链接，但是将来存在链接。
   - 缺失链接预测：两个顶点之间本来存在链接，但是未被观测到。

2. 数据集：

   - `DBLP co-authorship` 数据集：从四个研究研究领域中的热门会议中得到的论文网络数据，包含的会议为：数据库领域的 `SIGMOD,VLDB,ICDE,EDBT,PODS`，数据挖掘领域的 `KDD,ICDM,SDM,PAKDD`， 机器学习领域的 `ICML,NIPS,AAAI,IJCAI,ECML` ，信息检索领域的 `SIGIR,WSDM,WWW,CIKM,ECIR` 。

     我们使用前述的相同的预处理策略：过滤掉少于三位作者的论文，并过滤掉停用词。

   - `DBLP` 论文引用数据集：数据源同上所述，但是这里考虑的是论文引用关系，而不是作者的共同发表关系。我们采用 `2000~2009` 年的论文，过滤掉该时间段内未发表的参考文献，并且过滤掉词频小于`5` 的单词。

     最终论文引文子网络包含 `6904` 篇论文、`19801` 个引用关系（链接），单词子网络包含 `8992` 个单词、`118518` 条链接，论文和单词之间有 `59471` 条链接。

   - `SLAP` 数据集：数据包含基因以及化合物之间的关系。基因子网络包含基因和基因之间的相互作用，化合物子网络包含化合物之间是否具有相同的化学基团，两个子网络之间包含基因和化合物之间的作用。

     最终化合物子网络有 `883` 个顶点、`70746` 个链接，基因子网络有 `2472`个基因、`4396` 条链接，基因和化合物之间存在 `1134` 条链接。

   - `BlogCatalog` 数据集：包含博客作者之间关系的数据集。我们选择四种热门类型（`Art,Computers,Music,Photography` 艺术、计算机、引用、摄影）的用户，这包含 `5009` 个用户，用户之间存在 `28406` 条链接。

     我们基于这些用户的博客来构建单词子网络，词频小于 8 的单词被过滤掉。我们采用窗口大小为 5 的滑动窗口来生成单词共现关系。最终得到的单词子网络包含 `9247` 个单词、`915655` 条链接。

     用户和单词之间存在 `350434` 条链接。

3. 未来链接预测：我们采用 `DBLP` 的 `2000`到 `2009` 年发表的论文作为训练集，然后预测 `2010` 年、`2010~2011`年、`2011~2012`年、`2010~2012` 年这四个时期的链接。在这四个时期，我们除了直接得到正样本(共同发表论文的作者) 之外，我们还通过计算来间接得到负样本：通过随机生成数量与正样本相同、没有共同发表论文的作者`pair` 对。通过负样本来评估模型检测负样本的能力。

   我们通过以下概率来预测新的`co-authoreship` 相似性：

  $p(\mathbf{\vec u}_i,\mathbf{\vec u}_j) = \frac{1}{1+\exp(-\mathbf{\vec u}_i\cdot\mathbf{\vec u}_j)}$

   其中$\mathbf{\vec u}_i,\mathbf{\vec u}_j$为两个顶点的 `embedding` 向量。

   我们使用 `AUC` 来评估二类分类器的预测能力，结果如下。结果表明：

   - 所有的神经网络模型都优于谱聚类，这表明神经网络`embedding` 对于学习顶点潜在特征的有效性。
   - `EOE` 方法在四个时间都优于其它模型，这表明 `EOE` 方法捕捉的信息更全面、准确。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/YHCfOFLc1FWA.png?imageslim">
   </p>
   

   为了研究 `EOE` 性能优异的原因，我们研究这些方法如何针对测试样本进行预测。我们给出两个具有代表性的测试样本。这两个测试样本是发生在 `CVPR'10` 和 `SIGIR'10` 会议的两个`co-authorship` 关系。

   - 所有的基准方法都低估这两个未来链接发生的概率。
   - `EOE` 通过作者之间共享相似的研究方向的信息（通过`common word` 给出）给出了最佳的预测。
   - `LINE(2nd)` 给出的预测结果被排除，因为它对所有测试样本（包括不存在 `co-authorship` 的作者 `pair` 对）预测的概率都接近 `100%`。我们不知道原因，并且其背后的原因也不在本文讨论范围之内。但是我们知道 `LINE(2nd)` 的学习目标是保留二阶邻近度，而推断新的链接是预测一阶邻近度，二者不匹配。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/Epq9VbKuMtx7.png?imageslim">
   </p>
   

4. 缺失链接预测：我们在论文引用网络、社交网络、基因交互网络上执行缺失链接预测。我们开展了10次实验，分别对训练集中的链接保留比例从 `0%~90%` 。其中， `0%`的保留比例即丢弃所有链接，这将导致冷启动问题。

   - 目前任何基准模型都无法解决冷启动问题，因为所有基准模型都依赖于已有的链接来学习潜在特征，而 `EOE` 可以通过补充信息来解决冷启动问题。
   - 在所有比例的情况下，`EOE` 仍然优于基准方法，这说明了`EOE` 针对耦合异构网络共同学习 `embedding` 的优势。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/M3IVNPX8v6L7.png?imageslim">
   </p>
   

#### 10.3.3 多类分类

1. 多类分类任务需要预测每个样本的类别，其中类别取值有多个，每个样本仅属于其中的某一个类别。

2. 数据集：`DBLP` 论文引用数据集：数据源同上所述，但是这里考虑的是论文引用关系，而不是作者的共同发表关系。我们采用 `2000~2009` 年的论文，过滤掉该时间段内未发表的参考文献，并且过滤掉词频小于`5` 的单词。论文的标签是改论文所述的领域，为数据库、数据挖掘、机器学习、信息检索这四个领域之一。

   最终论文引文子网络包含 `6904` 篇论文、`19801` 个引用关系（链接），单词子网络包含 `8992` 个单词、`118518` 条链接，论文和单词之间有 `59471` 条链接。

3. 我们开展了10次实验，分别对训练集中的链接保留比例从 `10%~100%` 。一旦学到潜在特征，我们使用具有线性核的 `SVM` 来执行分类任务。

   我们采用 `10-fold` 交叉验证的平均准确率作为评估指标，结果如下：

   - `EOE` 在所有比例的情况下，始终优于其它基准模型。
   - 当链接比例越少（如 `10%~20%`），`EOE` 相比于基准模型的提升越明显。这是因为论文的摘要中包含了大量的关于论文研究领域的信息，`EOE` 可以很好的利用这些信息。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/ImKHwGIa2yMl.png?imageslim">
   </p>
   

#### 10.3.4 多标签分类

1. 多类分类任务需要预测每个样本的类别，其中类别取值有多个，每个样本可以属于多个类别。

2. 数据集：

   - `DBLP co-authorship` 数据集：从四个研究研究领域中的热门会议中得到的论文网络数据。每个作者有多个标签，因为一个作者可以是多个领域的专家。
   - `BlogCatalog` 数据集：包含博客作者之间关系的数据集。每个博客作者可以注册他们的博客为多个类别。

3. 我们开展了5次实验，分别对训练集中的链接保留比例从 `20%~100%` 。一旦学到潜在特征，我们使用具有线性核的二元分类 `SVM` 来执行多标签分类任务。

   我们采用 `10-fold` 交叉验证的`Micro-F1`和 `Macro-F1` 作为评估指标，结果如下：

   - `EOE` 在所有任务中都超越了基准模型。
   - 当链接比例越少（如 `20%`），`EOE`相比于基准模型的提升越明显。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/BXUtgICb8b8i.png?imageslim">
   </p>
   

4. 从可视化任务、链接预测任务、多类分类任务、多标签分类任务可以看到：多个子网络之间的链接提供的额外补充信息是有益的，因为它可以使得学到的潜在特征更加准确、全面，这在解决冷启动问题时尤为重要。

#### 10.3.5 参数探索

1. `EOE` 模型有两种主要的超参数：正则项的系数、潜在特征维度。在所有之前的实验中，这两个超参数都固定为常数，正则化系数设置为 `1.0`，潜在特征维度为 `128` 。

   这里我们将正则化系数分别设置为 `0.1,0.5,1.0,2.0,5.0,10.0` 、将潜在特征维度分别设置为 `32,64,128,256,512`，然后观察这些超参数的影响。

   注意：研究潜在特征维度时，正则化系数固定为 `1.0`；研究正则化系数时，潜在特征维度固定为 `128` 。我们的训练数据集为 `DBLP co-authorship` 数据集。

   - 潜在特征维度：可以看到 `EOE` 对于潜在特征维度不是特别敏感，只要该值不是太小。最佳性能出现在潜在特征维度为 `128` 时。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/5rCKh3ipauMf.png?imageslim">
     </p>
     

   - 正则化系数：可以看到 `EOE` 对于正则化系数更为敏感，当正则化系数为 `1.0` 时模型效果最好。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/TUvlCGFx8lRt.png?imageslim">
     </p>
     

## 十一、metapath2vec

1. 目前为止绝大多数`embedding` 方法都集中在异质网络的表示学习`representation learning` 上，即网络的顶点类型是单一的，边的类型也是单一的。但是大量的社交网络以及其它信息网络本质上是异质`heterogeneous`的，网络包含多种顶点类型，也包含多种边的类型。因此异质网络表示学习的挑战在于：网络中存在多种类型的顶点和边。

   传统的 `embedding` 模型将不同类型的顶点和边采用相同的处理方式，这将导致为异质顶点生成没有类型区分的表示。因此这些异质网络是专门为同质`homogeneous`网络设计的 `embedding` 模型无法解决的。

   论文 `《metapath2vec: Scalable Representation Learning for Heterogeneous Networks》` 首先形式化异质网络的表示学习问题，然后提出了 `metapath2vec` 框架，及其扩展的 `metapath2vec ++` 框架来解决异质网络的表示学习问题。

2. 异质网络表示学习的目的是同时学习多种类型顶点的低维 `embedding` 。`metapath2vec` 框架基于`meta-path` 的随机游走从而构造顶点的异质邻域，然后利用异质 `skip-gram` 模型来执行顶点 `embedding` 。`metapath2vec` 的目标是最大化的保留给定异质网络的结构关系和语义关系。

   而 `metapath2vec ++` 在 `metapath2vec`的基础上使用了一种基于异质负采样的方法，称作 `metapath2vec ++` ，该方法可以有效并且准确的预测顶点的异质邻域。

   大量实验表明，`metapath2vec` 和 `metapath2vec ++` 不仅能够超越各种异质网络挖掘任务的 `SOA` 的 `embedding` 模型，还能够识别不同顶点之间的结构相关性和语义相关性。

3. `metapath2vec` 和 `metapath2vec ++` 不同于传统的网络 `embedding` 模型，后者专注于同质网络。

   `metapath2vec` 和 `metapath2vec++` 在某些方面也不同于 `Predictive Text Embedding:PTE` 模型。

   - 首先 `PTE` 是一种半监督学习模型，其中包含文本数据的标签信息。
   - 其次，`PTE` 中的异质性来自于文本网络，网络中存在了单词到单词的链接、单词到它所属文档的链接、单词及其 `label` 的链接。 因此本质上 `PTE` 的原始输入为单词，输出是每个单词的 `embedding` ，而不是多种类型的输入。

### 11.1 模型

#### 11.1.1 问题定义

1. 论文对异质网络的表示学习进行了形式化。定义一个异质网络是一个图$G=(V,E,T)$，其中每个顶点$v$和每条边$e$分别关联一个映射函数：

   其中$T_V$表示顶点类型，$T_E$表示边的类型，且满足$|T_V| + |T_E| \gt 2$。

   例如可以用作者 `author:A`、论文 `paper:P`、会议 `venue:V`、组织 `organization:O` 作为顶点类型来表示下图中的学术网络。其中边可以表示`coauthor` 合作者关系 `A-A`、`publish` 发表关系 `A-P,P-V`、`affiliation`附属关系 `O-A` ，`reference` 引用关系 `P-P` 。黄色虚线表示 `coauthor` 关系，红色虚线表示引用关系。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/76sBVO9OUy9h.png?imageslim">
   </p>
   

2. 异质网络表示学习问题`Heterogeneous Network Representation Learning`：给定一个异质网络$G$，任务的目标是学习一个$d$维的潜在表示$\mathbf X\in \mathbb R^{d\times |V|}$，其中$d\ll |V|$，使得该表示能够捕获网络的结构关系以及语义关系。任务的输出是一个低维矩阵$\mathbf X$，其第$v$列对应一个$d$维向量$\mathbf{\vec x}_v$，即顶点$v$的 `embedding` 。

   - 尽管$V$中存在不同类型的顶点，但是各种类型顶点的 `embedding` 都被映射到相同的潜在空间中。
   - 学到的顶点 `embedding` 可以应用于各种异质网络挖掘任务，如顶点分类、聚类、相似性查找等。
   - 问题的挑战来自于网络异质性。网络`embedding` 模型的基本假设是：在`embedding` 空间保持顶点及其邻居（上下文）之间的邻近性。但是在异质网络中，如何定义和建模 “顶点 - 邻居” 的概念？另外模型如何优化，其中该模型有效维护多种类型顶点和边的结构关系和语义关系？

#### 11.1.2 metapath2vec

1. 论文提出了一个通用的异质网络`embedding` 学习框架 `metapath2vec`，其目标是：在考虑多种类型的顶点和边的情况下，最大化网络的概率。

2. 首先考虑 `word2vec` 模型。给定网络$G=(V,E)$，模型的目标是根据局部结构来最大化网络概率：

  $\arg\max_{\theta}\prod_{v\in V}\prod_{c\in \mathcal N(v)}p(c\mid v;\theta)$

   其中$\mathcal N(v)$为网络$G$中顶点$v$的邻域，它可以由不同的定义方式，如$v$的直接相连的顶点集合；$p(c\mid v;\theta)$定义了给定顶点$v$的条件下，上下文$c$出现的条件概率。

   为了对顶点的异质邻域建模，`metapath2vec` 引入了异质 `skip-gram` 模型 `Heterogeneous Skip-Gram`。为了将异质网络结构融合到异质 `skip-gram` 模型中，`metapath2vec` 引入了基于 `meta-path` 的随机游走。

3. 异质 `skip-gram` 模型：在 `metapath2vec`中，给定异质网络$G=(V,E,T)$，其中$T_V$表示顶点类型，$T_E$表示边的类型，$|T_V|\gt 1$。我们通过最大化给定顶点$v$的条件下，异质上下文$\mathcal N_t(v),t\in T_V$的条件概率，使得 `skip-gram` 能够学习异质网络$G$的有效顶点表示：

  $\arg\max_{\theta}\sum_{v\in V}\sum_{t\in T_V}\sum_{c_t\in \mathcal N_t(v)}\log p(c_t\mid v;\theta)$

   其中：

   -$\mathcal N_t(v)$表示顶点$v$的第$t$种类型的邻域顶点。考虑下图的学术网络，其中`author` 顶点$a_4$的邻域可以为：`author` 邻域$a_2,a_3,a_5$；`venue` 邻域$\{\text{ACL},\text{KDD}\}$，`organization` 邻域$\{\text{CMU},\text{MIT}\}$；`paper` 邻域$\{p_2,p_3\}$。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/3o4IDJyx6VCq.png?imageslim">
     </p>
     

   -$p(c_t\mid v;\theta)$通常由一个 `softmax` 函数定义：

    $p(c_t\mid v;\theta) = \frac{\exp(\mathbf{\vec x}_{c_t}\cdot \mathbf{\vec x}_v)}{\sum_{u\in V}\exp(\mathbf{\vec x}_{u}\cdot \mathbf{\vec x}_v)}$

     其中$\mathbf{\vec x}_v$为$\mathbf X$的第$v$列，是顶点$v$的 `embedding` 向量。

   为了有效优化目标函数，`Milkolov` 等人引入了负采样，它从网络种随机采样相对较小的一组顶点来代替 `softmax` 。`metapath2vec` 也采用了相同的负采样技术。给定一个负采样大小$M$，则定义：

  $\log p(c_t\mid v;\theta) = \log \sigma(\mathbf{\vec x}_{c_t}\cdot \mathbf{\vec x}_v) + \sum_{m=1}^M\mathbb E_{u^{(m)} \sim P(u)}[\log \sigma(-\mathbf{\vec x}_{u^{(m)}}\cdot \mathbf{\vec x}_v)]$

   其中：

   -$\sigma(x) = \frac{1}{1 + \exp(-x)}$
   -$P(u)$为预定义的顶点分布函数，根据该分布来采样$M$个负采样顶点$u^{(m)}$。`metapath2vec` 无视顶点类型来构建顶点的频率分布。

4. 基于 `meta-path` 的随机游走：如何有效的将网络结构转化为 `skip-gram`？在 `DeepWalk` 和 `node2vec` 中，这是通过将 `random walker` 在网络上遍历的顶点路径结合一个邻域函数来实现的。

   我们也可以将 `random walker` 放到异质网络中，从而生成多种类型顶点的路径。在随机游走的第$i$步，我们定义转移概率$p(v^{}\mid v^{*})$为：在顶点$v^{\*}$的邻域上的归一化概率分布，其中忽略顶点类型。然后将生成的游走序列作为 `node2vec` 和 `DeepWalk` 的输入。**

   **但是， `Sun` 等人证明了：异质随机游走偏向于高度可见的顶点类型（即那些在路径中频繁出现的顶点类型），以及中心顶点（即那些出现频繁出现在关键路径中的顶点）。有鉴于此，`metapath2vec` 设计了基于 `meta-path` 的随机游走，从而生成能够捕获不同类型顶点之间的语义相关性和结构相关性的游走序列，从而有助于我们将异质网络结构转化为异质 `skip-gram`。**

   **形式化的，一个 `meta-path` 范式$\mathcal P$定义为一个路径：**

   **$V_1 \xrightarrow{R_1}V_2\xrightarrow{R_2}\cdots V_t\xrightarrow{R_t} V_{t+1}\xrightarrow{R_{t+1}}\cdots\xrightarrow{R_{l-1}} V_l$**

   **其中$R=R_1\circ R_2\circ \cdots\circ R_{l-1}$定义顶点类型$V_1,\cdots V_l$之间的一个组合关系。以下图为例：`meta-path:APA` 代表两个作者 `A` 在同一论文 `P` 上的 `co-author` 关系；`meta-path:APVPA` 代表两个作者 `A` 在同一个会议 `V` 的不同论文 `P` 上的 `co-venue` 关系。**

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/h2daKNp84mut.png?imageslim">
   </p>
   

   **先前的工作表明：异质信息网络中的许多数据挖掘任务都可以通过 `meta-path` 建模受益。这里我们展示如何利用 `meta-path` 来指导异质 `random walker` 。**

   **给定一个异质网络$G=(V,E,T)$以及一个 `meta-path` 范式$\mathcal P$，在随机游走的第$i$步的转移概率定义为：**

   **其中：$v_t^{\*} \in V_t$表示第$i$步的顶点；$\mathcal N_{t+1}(v_t^{\*})$表示顶点$v_t^{\*}$的、类型为$V_{t+1}$的邻居顶点。**

   - 由于$v^{} \in V_{t+1}$，这意味着 `random walker` 必须根据预定义的 `meta-path`$\mathcal P$来游走。

   - 通常`meta-path` 都是对称的，如$V_1= V_l$，因此有：

     $p(v^{}\mid v_l^{\*}) = p(v^{}\mid v_1^{\*})$**

   - 基于 `meta-path` 的随机游走策略可以确保不同类型顶点之间的语义关系可以正确的合并到 `skip-gram`中。

     如下图所示，假设上一个顶点为 `CMU`，当前顶点为$a_4$。在传统的随机游走过程中，下一个顶点可能是$a_4$周围的顶点$\{a_2,a_3,a_5,p_2,p_3,\text{CMU}\}$。但是在`meta-path` 的范式 `OAPVPAO` 下，下一个顶点倾向于论文类型的顶点$\{p_2,p_3\}$，从而保持路径的语义。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/h2daKNp84mut.png?imageslim">
     </p>
     
     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/UpybrqGgo79R.png?imageslim">
     </p>
     

#### 11.1.3 metapath2vec ++

1. `metapath2vec` 在创建顶点$v$的邻域$\mathcal N_t(v)$时，根据上下文顶点的类型来区分顶点$v$的上下文顶点。但是它忽略了 `softmax` 函数中的顶点类型信息。即：在给定顶点$v$的条件下，为了推断来自邻域$\mathcal N_t(v)$中的上下文$c_t$的顶点类型，`metapath2vec` 实际上采用了所有类型的负样本，包括相同类型$t$的负样本，以及其它类型的负样本。

   论文提出的 `metapath2vec ++` 框架采用异质负采样策略 `heterogeneous negative sampling` 。该框架中，`softmax` 根据上下文顶点$c_t$的类型进行归一化，即$p(c_t\mid v;\theta)$根据指定的顶点类型$t$来调整：

   $p(c_t\mid v;\theta) = \frac{\exp(\mathbf{\vec x}_{c_t}\cdot \mathbf{\vec x}_{v})}{\sum_{u_t\in V_t}\exp(\mathbf{\vec x}_{u_t}\cdot\mathbf{\vec x}_v)}$

   其中$V_t$为类型为$t$的顶点集合。

2. `metapath2vec ++` 会为 `skip-gram` 模型输出层中的每种邻域类型定义一组 `softmax` 分布。

   在 `metapath2vec、node2vec、DeepWalk` 中，输出`softmax` 分布的维度等于网络的顶点数量$|V|$。但是在 `metapath2vec ++` 中，输出 `softmax` 分布的维度由类型为$t$的顶点数量$|V_t|$决定。

   如下图所示给定顶点$a_4$，`metapath2vec ++` 输出四组`softmax` 分布，每一组分布对应于不同的邻域顶点类型：会议`V`、作者 `A`、组织 `O`、论文 `P` 。$V_t$给出了类型为$t$的顶点集合，$V=V_V\cup V_A\cup V_O\cup V_P$。$k_t$给出了顶点邻域中类型为$t$的邻居顶点数量，$k=k_V + k_A + k_O + k_P$。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/u1ewWH72HMi2.png?imageslim">
   </p>
   

3. 受到 `PTE` 的启发，`metapath2vec` 在负采样过程中同样可以根据邻居顶点$c_t$的类型进行调整，因此有目标函数：

   $\mathcal L = \log \sigma(\mathbf{\vec x}_{c_t}\cdot \mathbf{\vec x}_v) + \sum_{m=1}^M\mathbb E_{u_t^{(m)}\in P_t(u_t)}[\log \sigma(-\mathbf{\vec x}_{u_t^{(m)}}\cdot \mathbf{\vec x}_v)]$

   其中$P_t(u_t)$为类型为$t$的顶点的分布函数。

   目标函数的梯度为：

   其中$\mathbb I_{c_t}[u_t^{(m)}]$为一个示性函数，用于指示$u_t^{(m)}$是否为邻域上下文顶点$c_t$，并且当$m=0$时$u_t^{(m)} = c_t$。

   最终可以通过随机梯度下降算法来对模型进行优化求解。

4. `metapath2vec++` 算法：

   - 输入：

     - 异构网络$G = (V,E,T)$
     - 一个 `meta-path` 范式$\mathcal P$
     - 每个顶点开始的游走序列的数量$w$
     - 每条游走序列的长度$l$
     - `embedding` 维度$d$
     - 邻域大小$k$（即窗口大小）

   - 输出：顶点的 `embedding` 矩阵$\mathbf X\in \mathbb R^{d\times |V|}$

   - 算法步骤：

     - 初始化$\mathbf X$

     - 迭代$i=1,2,\cdots,w$，迭代步骤为：

       遍历图$V$中的每个顶点$v$，执行：

     - 返回$\mathbf X$

5. `MetaPathRandomWalk` 算法：

   - 输入：

     - 异构网络$G = (V,E,T)$
     - 一个 `meta-path` 范式$\mathcal P$
     - 当前顶点$v$
     - 每条游走序列的长度$l$

   - 输出：一条 `meta-path` 随机游走序列

   - 算法步骤：

     - 初始化：$\text{MP}[1] = v$

     - 迭代$i=1,2,\cdots,l-1$，迭代过程为：

       根据$p(v^{}\mid v_t^{\*},\mathcal P)$采样顶点$u$，并记录$\text{MP}[i+1] = u$*

     - 返回$\text{MP}$

6. `HeterogeneousSkipGram` 算法：

   - 输入：

     - 当前的$\mathbf X$
     - 邻域大小$k$
     - 随机游走序列$\text{MP}$，以及它的长度$l$

   - 输出：更新后的$\mathbf X$

   - 算法步骤：

     - 迭代$i=1,2,\cdots,l$，迭代步骤为：

       取出当前顶点$v=\text{MP}[i]$, 对左侧的$k$个顶点、右侧的$k$个顶点共计$2k$个顶点进行更新：

       其中$\eta$为学习率。

7. 下图给出了异质学术网络`heterogeneous academic network` ，以及学习该网络`embedding` 的 `metapath2vec/metapath2vec ++` 的 `skip-gram` 架构。

   - 图 `a` 的黄色虚线表示 `co-author`关系，红色虚线表示论文引用关系。
   - 图 `b` 表示 `metapath2vec` 在预测$a_4$的上下文时，使用的 `skip-gram` 架构。其中$|V|=12$为顶点数量，$a_4$的邻居顶点包括$\{\text{CMU},a_2,a_3,a_5,p_2,p_3,\text{ACL},\text{KDD}\}$，窗口大小$k=8$。
   - 图 `c` 表示 `metapath2vec ++` 在预测$a_4$的上下文时，使用的 `skip-gram` 架构。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/lCtFH1alxafU.png?imageslim">
   </p>
   

### 11.2 实验

1. 这里我们展示了 `metapath2vec/metapath2vec ++` 用于异质网络表示学习的效果和效率。我们评估了经典的三种异质网络挖掘任务，包括：顶点分类问题、顶点聚类问题、顶点相似性查找问题。

   另外，我们还通过 `tensorflow` 的 `embedding` 可视化来观察异质学术网络中学到的顶点 `embedding` 。

2. 数据集：我们使用以下两个异质网络，它们都是公开的：

   - `AMiner Computer Science(CS) dataset`：包含 `2016` 年之前j举行的 `3883` 个计算机科学`venue`（包含会议 `conference` 和期刊 `journal`）的 `9323739` 名计算机科学家，以及 `3194405` 篇论文。

     我们构建了一个异质网络，该网络包含三种类型的顶点：作者 `A`、论文 `P`、`venue:V` 。

   - `Database and Information Systems(DBIS) dataset`：包含 `464` 个会议，以及其中 `top-5000`个作者，以及对应的 `72902` 篇论文。

     我们也构建了一个异质网络，该网络包含四种类型的顶点：作者 `A`、论文 `P`、`venue:V` 。

3. 基准模型：我们将 `metapath2vec/metapath2vec ++` 和以下几种 `embedding` 方法比较：

   - `DeepWalk/node2vec` ：在相同的随机游走序列输入下，我们发现 `DeepWalk` 的`hierarchical softmax` 和$p=1,q=1$的 `node2vec` 的负采样之间并未产生显著差异，因此我们这里采用$p=1,q=1$的 `node2vec`。
   - `LINE`：我们使用同时采用了`1` 阶邻近度和 `2` 阶邻近度的 `LINE` 模型。
   - `PTE`：我们构建了三个异质二部图 `A-A, A-V, V-V` ，并将其作为无监督 `embedding` 学习方法的约束。
   - 谱聚类`Spectral Clustering`/图的因子分解 `Graph Factorization`：因为早前的研究表明 `DeepWalk` 和 `LINE` 的性能超越了它们，因此这里并不考虑这两种方法。

4. 参数配置：对于所有模型，我们使用以下相同的参数：

   - 每个顶点开始的随机游走序列数量$w=1000$

   - 每个随机游走序列长度$l=100$

   - 隐向量维度$d=128$。注意：对于 `LINE` 模型，其一阶`embedding` 和 二阶 `embedding` 的维度都是 `128` 。

   - 邻域尺度$k=7$

   - 负采样大小$M=5$

   - 对于 `metapath2vec/metapath2vec ++`，我们还需要指定 `meta-path` 范式来指导随机游走的生成。我们调查了大多数基于 `meta-path` 的工作，发现异质学术网络中最常用和有效的`meta-path` 范式是 `APA` 和 `APVPA`。

     我们的实验表明：`meta-path` 范式 `APVPA` 产生的顶点 `embedding` 可以泛化到各种异质学术网络的挖掘任务中。

   最后，在参数敏感性实验中，我们对参数中的一个进行变化，同时固定其它参数来观察 `metapath2vec/metapath2vec ++`的效果。

#### 11.2.1 顶点分类

1. 我们使用第三方的 `label` 来确定每个顶点的类别。

   - 首先将 `Google Scholar` 中的八个会议的研究类别与 `AMiner` 数据中的会议进行匹配：

     1

     ```
     Computational Linguistics, Computer Graphics, Computer Networks & Wireless Communication, Computer Vision & Pattern Recognition, Computing Systems, Databases & Information Systems, Human Computer Interaction, Theoretical Computer Science
     ```

     每种类别`20` 个会议。在这 `160` 个会议中，有 `133` 个已经成功匹配并进行相应的标记。

   - 然后，对于这 `133` 个会议的论文的每位作者，将他/她分配给他/她大部分论文的类别，如果论文类别分布均匀则随机选择一个类别。最终有 `246678` 位作者标记了研究类别。

2. 我们从完整的 `AMiner` 数据集中学到顶点的 `embedding` ，然后将上述顶点标记以及对应的顶点 `embedding` 一起作为逻辑回归分类器的输入。

   我们将训练集规模从标记样本的 `5%` 逐渐增加到 `90%` ，并使用剩下顶点进行测试。我们将每个实验重复十轮，并报告平均的 `macro-F1` 和 `micro-F1` 。

   下表给出了 `Venue` 类型顶点的分类结果。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/VIqXBeRgQW91.png?imageslim">
   </p>
   

   下表给出了 `Author` 类型顶点的分类结果：

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/JnKKnzDg5Beo.png?imageslim">
   </p>
   

   结论：

   - `metapath2ve/metapath2vec ++`模型始终一致且明显的优于所有其它基准方法。
   - 在预测 `venue` 类别顶点时，由于训练数据量比 `author` 类别顶点少得多，因此 `metapath2vec/metapath2vec ++`的优势特别明显。

3. 我们接下来对 `metapath2vec ++` 的几个通用参数进行超参数敏感性分析。我们变化其中的一个超参数，然后固定其它的超参数。

   结论：

   - 从图 `a` 和 `b` 可以看到，参数$w$和$l$对于 `author`顶点的分类性能是正相关的，其中$w=1000$、$l=100$左右，`author` 顶点分类性能提到到峰值附近。

     但是令人惊讶的是，$w,l$对于 `venue` 顶点的分类性能无关。

   - 从图 `c` 和 `d` 可以看到，`embedding`尺寸$d$和邻域大小$k$与 `venue` 顶点分类性能无关。而$k$对于 `author`顶点分类性能至关重要，下降的曲线表明较小的邻域大小对于 `author` 顶点能够产生更好的 `embedding` 表示。

     这和同质图完全不同。在同质图中，邻域大小$k$通常对顶点分类显示出积极影响，而这里是消极影响。

   - 这些通用的超参数在异质网络中表示出和同质网络中不同的效果，这表明对于异质网络表示学习需要采用不同的思路和解决方案。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/voC96WXNyQxm.png?imageslim">
   </p>
   

#### 11.2.2 顶点聚类

1. 我们采用与上述分类任务中使用的相同的八个类别的 `author` 顶点和 `venue` 顶点，我们针对这些顶点的 `embedding` 进行聚类。这里我们使用 `k-means` 聚类算法，并通过归一化的互信息 `NMI` 来评估聚类效果。

   所有聚类实验均进行十次，并报告平均性能，结果如下表所示。结论：

   - 总体而言，`metapath2vec` 和 `metapath2vec ++` 优于其它所有方法。
   - 从 `venue` 顶点聚合结果可以看到，大多数方法的`NMI` 都较高，因此该任务相对容易。而 `author` 顶点聚类指标都偏低，因此任务更难。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/94AV4CzhOfEn.png?imageslim">
   </p>
   

2. `NMI` 指标：给定随机变量$X,Y$，则归一化的互信息定义为：

   $\text{NMI}(X;Y) = \frac{I(X;Y)}{H(X)} = \frac{H(X) - H(X\mid Y)}{H(X)} = \frac{\sum_{x}\sum_y p(x,y)\log \frac{p(x,y)}{p(x)\times p(y)}}{-\sum_x p(x)\log x}$

3. 和分类实验的步骤相同，我们研究了聚类任务中 `metapath2vec ++` 的参数敏感性，衡量指标为 `NMI` 。

   - 从图 `a` 和 `b` 可以看到，`author` 顶点和 `venue` 顶点在$l=100, w=800\sim 1000$时可以在效果和计算效率之间取得平衡。

   - 从图 `c` 和 `d` 可以看到，对于 `author` 顶点，$d$和$k$与聚类性能呈负相关；而对于`venue` 顶点，$d$也与聚类性能负相关，但是$k$增加时聚类 `NMI` 先增加后减小。

     对于 `author` 顶点和 `venue` 顶点，当$d=128,k=7$时，生成的 `embedding` 可以得到较好的聚类结果。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/lFxgpfcoCqMj.png?imageslim">
   </p>
   

#### 11.2.3 相似度查找

1. 我们从 `AMiner` 数据集选择 `16` 个不同领域的顶级 `CS` 会议，然后通过余弦相似度选择这些会议顶点的 `top 10` 相似度结果。

   结论：

   - 对于`query` 顶点 `ACL`，`metapath2vec++` 返回的`venue` 具有相同的主题 `NLP` ，如 `EMNLP(1st)`、`NAACL(2nd)`、`Computational Linguistics(3rd)`、`CoNLL(4th)`、`COLING(5th)` 等等。

     其它领域的会议的 `query` 也有类似的结果。

   - 大多数情况下，`top3` 结果涵盖了和 `query` 会议声望类似的`venue`。例如`theory`理论领域的 `STOC` 和 `FOCS`、`system`系统领域的 `OSDI` 和 `SOSP`、`architecture` 架构领域的`HPCA` 和 `ISCA`、`security` 安全领域的 `CCS` 和 `S&P`、`human-computer interaction` 人机交互领域的 `CSCW` 和 `CHI`、`NLP` 领域的 `EMNLP` 和 `ACL`、`machine learning` 机器学习领域的 `ICML` 和 `NIPS`、`web` 领域的 `WSDM` 和 `WWW`、`artificial intelligence` 人工智能领域的 `AAAI` 和 `UCAI`、`database` 数据库领域的 `PVLDB` 和 `SIGMOD` 等等。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/iedueorzDnjk.png?imageslim">
   </p>
   

2. 类似的，我们从 `DBIS` 数据中选择另外的 `5` 个 `CS` 会议，然后通过余弦相似度选择这些会议顶点的 `top 10` 相似度结果。结论也类似。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/nX6yej1chB6y.png?imageslim">
   </p>
   

3. 我们从 `DBIS` 数据中选择一个会议顶点、一个作者顶点，然后比较不同 `embedding`方法找出的 `top-5` 相似度的结果。结果如下表所示，其中 `metapath2vec ++` 能够针对不同类型的 `query` 顶点获得最佳的 `top-5` 相似顶点。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/n7O7mm0EWhG2.png?imageslim">
   </p>
   

#### 11.2.4 可视化

1. 我们使用 `tensorflow embedding` `2` 维 `PCA` 来进一步可视化模型学到的顶点 `embedding` 。我们从 `AMiner` 数据集选择 `16` 个不同领域的顶级 `CS` 会议，以及对应的顶级作者进行可视化。

   - 从图 `d` 可以看到，`metapath2vec++` 能自动组织这两种类型的顶点，并隐式学习它们的内部关系。这种内部关系可以通过连接每对顶点的箭头的方向和距离表示，如 `J.Dean --> OSDI`，`C.D.Manning --> ACL`，`R.E.Tarjan --> FOCS`，`M.I.Jordan --> NIPS` 等等。

     这两类顶点位于两个独立的“列”中，很显然图 `a` 和 `b` 的 `embedding`无法达到同样的效果。

   - 从图 `c` 可以看到，`metapath2vec`能够将每对 “作者-会议” `pair` 对进行紧密的分组，而不是将两种类型的顶点分类两列。如 `R.E.Tarjan + FOCS`、`H.Jensen + SIGGRAPH`、`H.Ishli + CHI`、`R.Agrawal + SIGMOD` 等等。 常规的 `embedding` 方法也无法达到这种效果。

   - `metapath2vec/metapath2vec++`都可以将来自相似领域的顶点放在一起、不同领域的顶点相距较远。这种分组不仅可以通过会议顶点来反映，还可以通过作者顶点来反映。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/dUI9p25dwmKX.png?imageslim">
   </p>
      

2. 下图通过 `t-SNE` 可视化了`metapath2vec++` 学到的`AMiner` 数据集不同领域的顶级 `CS` 会议，一共`48` 个会议、`16`个领域，每个领域 `3` 个会议。

   - 可以看到：来自同一个领域的会议彼此聚在一起，并且不同组之间的间隔比较明显。这进一步验证了 `metapath2vec++` 的 `embedding` 能力。
   - 另外，异质 `embedding` 能够揭示不同领域的顶点之间的相似性。如，右下角的 `Core CS` 领域的几个簇、右上角的 `Big AI` 领域的几个簇。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/EQYgLVe6k7X7.png?imageslim">
   </p>
   

#### 11.2.5 可扩展性

1. `metapath2vec/metapah2vec ++` 可以使用和 `word2vec/node2vec` 中相同的机制来并行化。我们对 `AMiner` 数据集进行实验，实验在 `Quad 12(48) core 2.3 GHz Intel Xeon CPUs E7-4850` 上进行。我们实现不同的线程数 `{1,2,4,8,16,24,32,40}` ，每个线程使用一个 `CPU core` 。

   下面给出了`metapath2vec/metapath2vec++` 的加速比。最佳加速比由虚线$y=x$表示。我们的这两种方法都可以达到能够接受的亚线性加速比，它们都接近于虚线。

   具体而言，在使用 `16` 个 `core` 时，它们可以实现 `11-12` 倍的加速比；使用 `40` 个 `core` 时，它们可以实现 `24-32` 倍加速比。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/GRPBX28WbDvI.png?imageslim">
   </p>
   

2. 在使用 `400` 个 `core` 时，`metapath2vec ++` 的学习过程只需要 `9` 分组即可训练整个 `AMS CS` 网络的 `embedding` ，该网络由 `900` 万以上的作者、`3300` 多个 `venue`、`300` 万篇论文组成。

   总体而言，对于具有数百万个顶点的大规模异质网络，`metapath2vec/metapath2vec++` 是有效的且 `scalable` 的。

#### 11.2.6 未来方向

1. `metapath2vec/metapath2vec++` 模型和 `DeepWalk/node2vec` 一样，在对网络采样生成大量随机游走路径时，面临大量中间临时输出数据的挑战。因此识别和优化采样空间是一个重要方向。
2. 和所有基于 `meta-path` 异质网络挖掘方法一样，可以通过自动学习有意义的 `meta-path` 来进一步改善 `metapath2vec/metapath2vec++` 。

## 十二、GraphGAN

1. 目前的`Graph Rresentation Learning`图表示学习方法可以分为两类：

   - 用于学习图的 `underlying connectivity distribution` 底层连接分布的生成模型。

     类似于经典的生成模型（如混合高斯模型、`LDA` 模型），生成式图表示学习学习模型假设对每个顶点$v_c$，都存在一个潜在的真实连接分布$p_{true}(v\mid v_c)$。这个真实的连接分布表明了$v_c$和图中其它顶点之间的连接性偏好。因此可以将图中存在的边视为从这些条件分布中生成的观测样本，并且生成模型可以通过最大化图中边的概率来学习顶点的 `embedding` 。

     如 `DeepWalk` 使用随机游走来为每个顶点采样上下文顶点，然后最大化给定顶点观测到的上下文顶点的对数似然。`Node2vec` 通过提出一个有偏的随机游走过程进一步拓展了这个思想，当为给定顶点选择上下文顶点时，这会提供更大的灵活性。

   - 用于预测一对顶点`pair` 对之间存在边的可能性的判别模型。

     与生成模型不同，判别模型不再将边视为从潜在的条件分布中产生，而是学习用于直接预测边的存在性的分类器。

     通常判别模型联合一对顶点 `pair`对$v_i,v_j$作为特征，然后预测这对顶点之间存在边的概率$p(e_{i,j}\mid (v_i,v_j))$。

     如 `SDNE` 使用高维稀疏的顶点邻接向量作为每个顶点的原始特征，并通过应用自编码器，基于边是否存在的监督信息来提取顶点的 `embedding` 。`PPNE` 通过在正样本（连接的顶点`pair` 对）、负样本（断开的顶点 `pair` 对）上进行监督学习，直接学习顶点 `embedding`，并在学习过程中保留顶点的固有属性。

   受 `GAN` 启发，论文 `《GraphGAN: Graph Representation Learning with Generative Adversarial Nets》` 提出了 `GraphGAN` 模型，一种结合了上述两种方法的新的图表示学习框架。

   具体而言，`GraphGAN` 在学习过程中训练两个模型：

   - 生成器$G(v\mid v_c)$：它试图尽可能地拟合底层的真实连接分布$p_{true}(v\mid v_c)$，并生成与顶点$v_c$最可能连接的顶点，即 `fake`顶点。
   - 判别器$D(v,v_c)$：它试图区分`well-connected` 顶点对与 `ill-connected` 顶点对，并计算顶点$v$和$v_c$之间存在边的概率。

   生成器$G$和判别器$D$在 `min-max game` 中分别扮演两个角色：

   - 生成器$G$在判别器$D$提供的指导下生成最难区分的 `fake` 顶点。
   - 判别器$D$试图区分真实顶点和 `fake` 顶点，从而避免被生成器欺骗。

   通过在`game` 中相互竞争，最终它们都提高了自己的能力，直到生成器$G$和$p_{true}(v\mid v_c)$无法区分为止。

2. 传统 `softmax` 及其变体不适于生成器，原因有两个：

   - 对给定的顶点$v_c$， `softmax` 对所有其它顶点一视同仁，完全没有考虑到图的结构信息。
   - `softmax` 的计算涉及到图中的所有顶点，这既低效又耗时。

   为克服这些限制， `GraphGAN` 提出了一种新的图 `softmax` ，称作 `Graph Softmax`。论文证明了 `Graph Softmax`满足归一性`normalization`、结构感知`graph structure awareness`、计算高效`computational efficiency` 等优点。

   另外，`GraphGAN` 对生成器提出了一种基于随机游走的生成策略，该策略与 `Graph Softmax` 的定义一致，从而大大降低了计算复杂度。

3. 论文在五个真实世界的图数据集上执行三个任务（链接预测、顶点分类、推荐），实验结果表明 `GraphGAN` 在 `SOA` 基准方法上取得相当大的提升。这归功于统一的对抗学习框架，以及捕获了图结构信息的 `Graph Softmax` 。

### 12.1 模型

1. 设图$\mathcal G = (\mathcal V,\mathcal E)$，其中$\mathcal V = \{v_1,\cdots,v_{|\mathcal V|}\}$为顶点集合，$\mathcal E = \{e_{i,j}\}_{i,j=1}^{|\mathcal V|}$为边集合。

   给定顶点$v_c$，定义$\mathcal N(v_c)$为$v_c$直接相连的顶点集合，定义$v_c$的底层真实连接分别为$p_{true}(v\mid v_c)$。

   - $p_{true}(v\mid v_c)$反映了$v_c$和所有其它顶点的连接分布。
   - $\mathcal N(v_c)$可以视为从$p(v\mid v_c)$中采样到的观测样本。

2. 给定图$\mathcal G$，`GraphGAN` 的目标是学习两个方法：

   - 生成器 `generator`$G(v\mid v_c;\theta_G)$：它的目标是逼近$p_{true}(v\mid v_c)$，然后针对$v_c$生成最有可能和它连接的顶点。更准确的说，是从$\mathcal V$中选择和$v_c$最有可能连接的顶点。
   - 判别器 `discriminator`$D(v,v_c;\theta_D)$：它的目标是判断顶点 `pair` 对$(v,v_c)$之间的连接性。它输出一个标量，代表顶点$v$和$v_c$之间存在边的概率。

   生成器$G$和判别器$D$扮演两种角色：

   - 生成器$G$试图完美拟合$p_{true}(v\mid v_c)$，然后生成`fake` 顶点。这些生成的顶点和$v_c$真实连接的邻居顶点相似，从而欺骗判别器。
   - 判别器$D$试图检测这些顶点是否是$v_c$的真实相连的顶点，还是由生成器$G$生成的 `fake` 顶点。

   形式化的，生成器$G$和判别器$D$正在参与具有以下值函数$V(G,D)$的双人 `min-max` 游戏：

   $\min_{\theta_G}\max_{\theta_D}V(G,D) = \sum_{c=1}^{|\mathcal V|}\left(\mathbb E_{v\in p_{true}(\cdot\mid v_c)}[\log D(v,v_c;\theta_D)]+\mathbb E_{v\sim G(\cdot\mid v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]\right)$

   其中生成器和判别器的参数可以通过交替最大化和最小化值函数$V(G,D)$来学习。

3. `GraphGAN` 整体框架如下图所示。每次迭代过程中，我们使用来自$p_{true}(\cdot\mid v_c)$的正样本（绿色顶点）和来自生成器$G(\cdot\mid v_c;\theta_G)$的负样本（带蓝色条纹的顶点）来训练判别器$D$，并在$D$的指导下使用策略梯度更新生成器$G$。

   $G$和$D$之间的竞争趋势它们都改进自己的参数，直到$G$与$p_{true}(\cdot\mid v_c)$无法区分。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/AKQTzOu6IRqu.png?imageslim">
   </p>
   

#### 12.1.1 判别器

1. 给定从真实连接而来的正样本，以及从生成器而来的负样本，判别器$D$的目标是最大对数似然。

   如果判别器$D$是可微的，则可以通过针对$\theta_D$的梯度上升来解决。

2. 在 `GraphGAN` 中，我们简单的定义判别器$D$为两个输入顶点的 `embedding` 内积的 `sigmoid` 函数：

   $D(v,v_c) = \sigma(\mathbf{\vec d}_v\cdot \mathbf{\vec d}_{v_c}) = \frac{1}{1+\exp(-\mathbf{\vec d}_v\cdot \mathbf{\vec d}_{v_c})}$

   其中$\mathbf{\vec d}_v,\mathbf{\vec d}_{v_c}\in \mathbb R^k$分别是判别器$D$中顶点$v$和$v_c$的$k$维 `representation` 向量，$\theta_D=\{\mathbf{\vec d}_1,\cdots,\mathbf{\vec d}_{|V|}\}$。

   注意到这里仅包含$v$和$v_c$，这意味着给定一对样本$(v,v_c)$，我们仅仅需要通过对应的梯度上升来更新$\mathbf{\vec d}_v$和$\mathbf{\vec d}_{v_c}$：

   既：没必要更新全网顶点的 `embedding`。

3. 理论上 `SDNE` 等任何判别式模型都可以用作判别器$D$，论文将如何选择合适的判别器作为未来的研究。

#### 12.1.2 生成器

1. 和判别器相比，生成器$G$的目标是：使得判别器$D$区分$G$生成的`fake` 顶点的概率最小化。

   即：生成器通过调整参数$\theta_G$，从而增加其生成的 `fake` 顶点在判别器$D$中的得分。

2. 由于生成器采样的顶点$v$是离散的，因此遵循 `Schulman` 等人的做法，论文建议通过策略梯度`policy gradient`计算$V(G,D)$对$\theta_G$的梯度：

   $N$为生成器采样的顶点数量。

   其中倒数第二行是因为$G \nabla_{\theta_G} \log G = G\times \frac 1G\nabla_{\theta_G} G$。

   梯度$\nabla_{\theta_G}V(G,D)$的物理意义为$\nabla_{\theta_G}\log G(v\mid v_c;\theta_G)$的加权和，权重为$\log(1-D(v,v_c;\theta_D))$。

   - 当$v$更容易被判别器$D$识别时，$D(v,v_c;\theta_D)$得分较小，则加权的权重较大。
   - 当$v$更不容易被判别器$D$识别时，$D(v,v_c;\theta_D)$得分较大，则加权的权重较小。

   这表明：当我们对$G$应用梯度下降时，更容易被判别器$D$区分的`fake`顶点将“拉扯”生成器，使其得到一个较大的更新。

3. 生成器$G$最简单直接的实现方式是通过 `softmax` 函数来实现：

   $G(v\mid v_c) = \frac{\exp(\mathbf{\vec g}_v\cdot \mathbf{\vec g}_{v_c})}{\sum_{v\ne v_c}\exp(\mathbf{\vec g}_v\cdot \mathbf{\vec g}_{v_c})}$

   其中$\mathbf{\vec g}_v,\mathbf{\vec g}_{v_c} \in \mathbb R^k$为分别是生成器$G$中顶点$v$和$v_c$的$k$维 `representation` 向量，$\theta_G=\{\mathbf{\vec g}_1,\cdots,\mathbf{\vec g}_{|V|}\}$。

4. 为了在每轮迭代中更新$\theta_G$，我们首先基于$G(v\mid v_c)$计算近似的连接分布，然后根据该分布随机采样一组$(v,v_c)$，然后通过随机梯度下降法来更新$\theta_G$。

#### 12.1.3 Graph Softmax

1. 在生成器中应用 `softmax` 有两个限制：

   - 计算$G(v\mid v_c)$的 `softmax` 函数时，包含图中的所有顶点的`embedding` 。这意味着对于每个生成的样本$v$，我们需要计算梯度$\nabla_{\theta_G}\log G(v\mid v_c;\theta_G)$，然后更新所有顶点。

     这种方式计算效率太低，尤其是对于具有数百万个顶点的真实世界的大型`Graph` 。

   - 图的结构信息编码了顶点之间丰富的邻近信息，但是 `softmax` 函数完全未考虑图的结构信息，因为它不加区分的对待所有顶点。

   最近 `hierarchical softmax` 和负采样技术是 `softmax` 的流行替代方案。尽管这些方法可以在一定程度上减少计算量，但是它们也没有考虑图的结构信息，因此应用于图的表示学习时也无法获得令人满意的性能。

2. 未解决`softmax` 的问题，`GraphGAN` 提出了 `Graph Softmax`，其关键思想是：重新对生成器$G$定义一个满足下面三个属性的理想的`softmax` 方法：

   - `normalized` 归一化：生成器$G$应该是一个有效的概率分布。即：$\sum_{v\ne v_c} G(v\mid v_c;\theta_G) = 1$。
   - `graph-structure-aware` 结构感知：生成器$G$应该利用图的结构信息。即：顶点之间的最短距离越小，则它们连接的概率越大；顶点之间的最短距离越大，则它们连接的概率越小。
   - 计算高效 `computationally efficient`：和计算全量 `softmax`不同，计算$G(v\mid v_c;\theta_G)$应该仅仅包含图中的一小部分顶点。

3. 为计算$G(\cdot\mid v_c;\theta_G)$，我们首先从顶点$v_c$开始对原始图$G$进行广度优先搜索 `BFS` ，这得到一棵以$v_c$为根的 `BFS` 树$T_c$。

   给定$T_c$我们将$\mathcal N_c(v)$表示为$T_c$中$v$的邻居的集合，即：和$v$直连的顶点，包括$v$的父顶点和所有子顶点。注意：$\mathcal N_c(v)$既依赖于顶点$v$，又依赖于顶点$v_c$。如果根顶点$v_c$不同，则`BFS` 树$T_c$也不同，则$v$的邻居集合也不同。

   对于给定的顶点$v$及其邻居顶点$v_i\in \mathcal N_c(v)$，我们定义给定$v$的条件下$v_i$的概率为：

   $p_c(v_i\mid v) = \frac{\exp(\mathbf{\vec g}_{v_i}\cdot \mathbf{\vec g}_v)}{\sum_{v_j\in \mathcal N_c(v)}\exp(\mathbf{\vec g}_{v_j}\cdot \mathbf{\vec g}_v)}$

   这其实是定义在$\mathcal N_c(v)$上的一个 `softmax` 函数。

   注意到$T_c$中的每个顶点$v$可以从根节点$v_c$通过唯一的一条路径达到，定义该路径为$P_{v_c\rightarrow v} = (v_{r_0},v_{r_1},\cdots,v_{r_m})$，其中$v_{r_0} = v_c, v_{r_m} = v$。则 `Graph Softmax` 定义为：

   $G(v\mid v_c;\theta_G) = \left(\prod_{j=1}^m p_c(v_{r_j}\mid v_{r_{j-1}})\right)\times p_c(v_{r_{m-1}}\mid v_{r_m})$

   其中$p_c(\cdot\mid \cdot)$由上式定义。

4. 可以证明这种定义的 `Graph Softmax` 满足归一性、结构感知、计算高效等性质。

   - 定理一：`Graph Softmax` 满足$\sum_{v\ne v_c} G(v\mid v_c;\theta_G) = 1$。证明见原始论文。

   - 定理二：在原始图中如果$v$和$v_c$的最短路径增加，则$G(v\mid v_c;\theta_G)$下降。

     证明：由于$T_c$是通过广度优先 `BFS` 得到的，因此路径$P_{v_c\rightarrow v}$同时也定义了一条从$v_c$到$v$的最短路径，该路径长度为$m$。随着最短路径增加，则$G(v\mid v_c;\theta_G)$中的概率乘积越多，因此$G$越小。

   - 定理三：计算$G(v\mid v_c;\theta_G)$依赖于$O(d\times\log |\mathcal V|)$个顶点，其中$d$是所有顶点的平均 `degree`，$|\mathcal V|$为所有顶点的大小。

     证明：计算$G(v\mid v_c;\theta_G)$依赖于两种类型的顶点：

     - 在路径$P_{v_c\rightarrow v}$上的顶点。路径的最长长度为$\log |\mathcal V|$，它是 `BFS` 树的深度。
     - 路径上顶点直接相连的顶点。路径上每个顶点平均有$d$个邻居顶点相连。

5. 生成器$G$生成 `fake` 顶点的一个简单方法是：为所有的$v\ne v_c$顶点计算$G(v\mid v_c;\theta_G)$，然后根据这个概率按比例执行随机采样。

   这里我们提出一种在线的顶点生成方法，该方法计算效率更高并且与 `Graph Softmax` 的定义一致：根据转移概率$p_c(v_i\mid v)$在$T_c$中执行一个从根节点$v_c$开始的随机游走。在随机游走过程中，如果$G$首次需要访问$v$的父顶点，则选择$v$作为生成的顶点。

   下图给出了 `fake` 顶点的在线生成过程，以及 `Graph Softmax` 计算过程。蓝色数字表示概率$p_c(v_i\mid v)$，蓝色实线箭头表示随机游走的方向，蓝色条纹顶点是被采样的顶点。最后一幅图的带颜色顶点（绿色、橙色、蓝色、蓝色带条纹）是需要更新参数的顶点。

   - 在每个随机游走步通过概率$p_c(v_i\mid v_{curr})$随机选择当前顶点$v_{curr}$的一个邻居顶点（标记为蓝色）。
   - 一旦$v_i = v_{pre}$，则将$v_{cur}$作为采样顶点并返回（标记为蓝色带条纹）。
   - 然后根据$\nabla_{\theta_G}V(G,D)$对路径$P_{v_c\rightarrow v_{curr}}$上的所有路径顶点、以及它们直接相连的顶点的参数进行更新。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/LJCSI341SnJp.png?imageslim">
   </p>
   

6. 生成器$G$在线生成算法：

   - 输入：
     - 图$G$
     - `BFS` 树$T_c$
     - 顶点的`embedding`$\{\mathbf{\vec g}_i\}_{i\in \mathcal V}$
   - 输出：生成的顶点$v_{gen}$
   - 算法步骤：
     - 初始化：$v_{pre} \leftarrow v_c, v_{cur}\leftarrow v_c$
     - $\text{while true do:}$
       - 根据概率$p_c(v_i\mid v_{cur})$随机选择一个顶点$v_i$
       - 如果$v_i = v_{pre}$，则$v_{gen}\leftarrow v_{cur}$，直接返回$v_{gen}$
       - 否则$v_{pre}\leftarrow v_{cur},v_{cur}\leftarrow v_i$

7. 生成器$G$在线生成算法最多执行$O(\log |\mathcal V|)$个迭代步，因为随机游走路径最迟会在到达叶结点时终止。类似于 `Graph Softmax`，在线生成方法的计算复杂度为$O(d\times\log|\mathcal V|)$，这大大低于离线生成方法的$O(|\mathcal V|\times d\log |\mathcal V|)$。

#### 12.1.4 GraphGan

1. `GraphGAN` 算法：

   - 输入：
     - `embedding` 维度$k$
     - 生成样本数$s$
     - 判别样本数$t$
   - 输出：
     - 生成器$G(v\mid v_c;\theta_G)$
     - 判别器$D(v,v_c;\theta_D)$
   - 算法步骤：
     - 初始化和预训练$G(v\mid v_c;\theta_G)$以及$D(v,v_c;\theta_D)$
     - 对每个顶点$v_c\in \mathcal V$，构建 `BFS`树$T_c$。
     - 迭代直到 `GraphGAN` 收敛，迭代步骤为：
       - $\text{for G-steps} :$
         - 根据生成器在线生成算法，生成器$G$对每个顶点$v_c$采样$s$个顶点
         - 根据$\nabla_{\theta_G} V(G,D)$更新$\theta_G$
       - $\text{for D-steps}:$
         - 对每个顶点$v_c$，从真实邻居顶点中采样$t$个正样本，从$G(v\mid v_c;\theta_G)$中采样$t$个负样本。
         - 根据$\nabla_{\theta_D} V(G,D)$来更新$\theta_D$
     - 返回$G(v\mid v_c;\theta_G),D(v,v_c;\theta_D)$

2. 由于构建每棵`BFS` 树的算法复杂度为$O(|\mathcal V| + |\mathcal E|)$，则构建所有 `BFS` 树的计算复杂度$O(|\mathcal V|\times (|\mathcal V| + |\mathcal E|)) = O(d\times |\mathcal V|^2)$。其中$d$为顶点的平均`degree` 。

   在`G-steps` ，每一行的算法复杂度都是$O(s|\mathcal V|\times d\log|\mathcal V|\times k)$；在 `D-steps`，第一行的算法复杂度为$O(t|\mathcal V|\times d\log |\mathcal V|\times k)$，第二行的算法复杂度为$O(t|\mathcal V|\times k)$。由于$k,s,t,d$都是常数，因此 `GraphGAN` 每次迭代的算法复杂度为$O(|\mathcal V|\times \log |\mathcal V|)$。

   因此总的算法复杂度为$O(|\mathcal V|^2)$，由构建 `BFS` 树的部分决定。

### 12.2 实验

1. 我们评估 `GraphGAN` 在一系列真实数据集上的性能，覆盖链接预测、顶点分类、推荐等三个任务。

2. 数据集：

   - `arXiv-AstroPh` 数据集：来自 `arXiv` 上天文物理领域的论文，包含了作者之间的合作关系。顶点表示作者，边表示`co-author` 关系。该图包含 `18772` 个顶点、`198110`条边。
   - `arXiv-GrQc` 数据集：来自 `arXiv`上广义相对论与量子宇宙学领域的论文，包含了作者之间的合作关系。顶点表示作者，边表示`co-author` 关系。该图包含 `5242` 个顶点、`14496` 条边。
   - `BlogCatalog` 数据集：来自 `BlogCatelog` 网站上给出的博客作者的社交关系网络。顶点标签表示通过博客作者提供的元数据推断出来的作者兴趣。该图包含 `10312` 个顶点、`333982` 条边、`39` 个不同的标签。
   - `Wikipedia` 数据集：来自维基百科，包含了英文维基百科 `dump` 文件的前$10^9$个字节中的单词共现网络。顶点的标签表示推断出来的单词词性`Part-of-Speech:POS` 。该图包含 `4777` 个顶点、`184812` 条边、`40` 个不同的标签。
   - `MovieLens-1M` 数据集：是一个二部图，来自 `MovieLens` 网站上的大约 `100万` 个评分（边），包含 `6040` 位用户和 `3706` 部电影。

3. 基准模型：

   - `DeepWalk`：通过随机游走和 `skip-gram` 来学习顶点 `embedding`。
   - `LINE`：保留了顶点的一阶邻近关系和二阶邻近关系。
   - `Node2Vec`：是 `DeepWalk` 的一个变种，通过一个`biased` 有偏的随机游走来学习顶点 `embedding` 。
   - `Struct2Vec`：捕获了图中顶点的结构信息。

4. 参数配置：

   - 所有方法的`embedding` 的维度$k=20$。

   - 所有基准模型的超参数都是默认值。

   - 在所有任务上，`GraphGAN` 的学习率都是 `0.001`，$s=20$，$t$为测试集的正样本数，然后分别执行 `G-steps` 和 `D-steps`30 次。这些超参数都是通过交叉验证来选取的。

     最终学到的顶点 `embedding` 为$\mathbf{\vec g}_i$。

5. 我们首先实验了图中连接分布的模式，即：边的存在概率如何随着图中最短路径的变化而变化。

   我们首先分别从 `arXiv-AstroPh` 和 `arXiv-GrQc` 数据集中随机抽取 `100 万`个顶点 `pair` 对。对于每个选定的顶点 `pair` 对，我们删除它们之间的连接（如果存在），然后计算它们之间的最短距离。我们计算所有可能的最短距离上边存在的可能性，如下图所示。

   - 显然，顶点 `pair` 对之间最短距离的增加会极大降低它们之间存在边的可能性。

   - 从对数概率曲线几乎为线性可以看出，顶点 `pair` 对之间的边存在概率和它们最短距离的倒数成指数关系。

     这进一步证明了 `Graph Softmax`捕获了真实世界 `Graph` 的结构信息。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/kwbY1LueP4HT.png?imageslim">
   </p>
   

6. 在链接预测任务中，我们的目标是预测两个给定顶点之间是否存在边。因此该任务显式了不同的图表示学习方法预测链接的能力。

   我们随机将原始图中 `10%` 的边作为测试集，并在图上删掉这些边，然后用所有的顶点和剩下的边来训练模型。训练后，我们根据所有顶点训练得到的 `embedding`向量，然后用逻辑回归来预测给定顶点 `pair` 对之间存在边的概率。

   测试集包含原始图中删掉的 `10%` 条边作为正样本，并随机选择未连接的相等数量的`pair` 对作为负样本。

   我们使用 `arXiv-AstroPh` 和 `arXiv-GrQc` 作为数据集，并在下表报告准确率和 `Macro-F1` 的结果。

   结论：

   - `LINE` 和 `struct2vec` 的性能在链接预测方面相对较差，因为它们不能完全捕获图中链接存在的模式。
   - `DeepWalk` 和 `node2vec` 的性能优于 `LINE` 和 `struct2vec`，这可能优于 `DeepWalk` 和 `node2vec` 都利用了基于随机游走的 `skip-gram` 模型，该模型在提取顶点之间邻近度方面表现更好。
   - `GraphGAN` 优于所有基准方法。与基准方法的单个模型训练相比，对抗训练为 `GraphGAN` 提供了更高的学习灵活性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/6B9LIrY8Oi1C.png?imageslim">
   </p>
   

7. 为直观了解 `GraphGAN` 学习的稳定性，我们进一步给出了生成器和判别器在 `arXiv-GrQc` 上的学习曲线。可以看到：

   - `GraphGAN` 中的 `mini-max game` 达到了平衡，其中生成器表现出色。
   - 判别器的性能首先增强，然后但逐渐降到 0.8以下。 注意，判别器不会降级到随机盲猜的水平，因为在实践中生成器仍然提供很多真实的负样本。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/NTWY9YOlQdI0.png?imageslim">
   </p>
   

8. 我们在 `BlogCatalog` 和 `Wikipedia` 数据集上执行顶点分类任务。我们在整个图上训练模型，然后将顶点 `embedding` 作为逻辑回归分类器的输入。其中训练集、测试集按照 `9:1` 的比例进行拆分。

   我们报告了测试集的准确率和 `Macro-F1`结果。

   可以看到：`GraphGAN` 性能在这两个数据集上都优于所有基准模型。这表明：尽管`GraphGAN` 设计用于建模顶点之间的连接性分布，但是它仍然可以有效的将顶点信息编码到顶点 `embedding` 中。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/v9VJUSX56BWt.png?imageslim">
   </p>
   

9. 我们使用 `Movielens-1M` 作为推荐数据集，我们的目标是对每个用户向他/她推荐一组尚未观看、但是可能被用户喜欢的电影。

   我们首先将所有的`4星` 和 `5星` 评级视为边，从而得到一个二部图。然后将原始图的 `10%` 边随机隐藏作为测试集，并为每个用户构建 `BFS` 树。

   注意：和之前的两个任务不同，在之前任务中，对于给定的顶点我们定义了它与所有其它顶点的连接性分布。但是推荐任务中，我们仅在一小部分顶点上定义连接性分布，如电影顶点（而不包括用户顶点）。因此我们在用户的 `BFS` 树中，对于除根顶点之外的所有用户顶点，我们将它们和位于当前 `BFS` 树中的电影顶点添加直连边来 `shortcut` 。

   在训练并获得用户和电影的`embedding` 之后，对于每个用户，我们基于`user embeding` 和电影 `embedding` 的相似度来挑选$K$个用户未观看的电影来作为推荐结果。我们给出测试集上的 `Precision@K` 和 `Recall@K` 指标。可以看到：`GraphGAN` 始终优于所有基准方法，并在两个指标上均有显著改进。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/ciDI0eqyx14Q.png?imageslim">
   </p>
   

## 十三、struc2vec

1. 几乎所有网络中的顶点都具有一个或者多个角色，这些角色很大程度上决定了顶点在网络中的功能。如：社交网络中的成员具有社交角色或者社会地位，而蛋白质网络中的蛋白质具有特定的功能。直觉上看，这些网络中的不同顶点可能具有相同的角色从而发挥相似的作用。如：公司社交网络中的实习生`intern` 角色，蛋白质网络中的催化剂`catalyst` 角色。因此，可以根据顶点在网络中的角色来将顶点划分为等效的类别`equivalent classe` 。

   通常识别顶点的角色需要利用顶点的特征或者边的特征，但是仅仅依靠网络结构来识别顶点角色，这更有挑战性。此时，顶点的角色仅仅取决于顶点之间的链接。

   确定顶点结构角色的常用方法是基于距离的方法或者基于递归的方法：

   - 基于距离的方法：根据每个顶点的邻域信息（如邻域大小，邻域形状）来计算每对顶点之间的距离（邻域越相似则距离越小），然后通过聚类、规则匹配等方式来确定顶点的等效类别。
   - 基于递归的方法：构造基于邻域的递归，然后不停迭代直到收敛，并采用最终迭代的结果来确定顶点的等效类别。如 `Weisfeiler-Lehman` 算法。

   最近一些网络顶点的`embedding` 学习方法非常成功，这些方法认为相邻的顶点具有相似的 `representation` 。因此具有结构相似、但是相距很远的两个顶点将不会有相似的 `embedding` 。如下图所示，顶点$u$和顶点$v$扮演相似的角色（具有相似的局部结构），但是它们在网络上相距甚远。由于它们的邻域不相交，因此一些最新的`embedding` 方法无法捕获它们的结构相似性。下图中，顶点$u$的度为 `5` 、连接到 `3` 个三角形并通过`2` 个顶点连接到剩余网络；顶点$v$的度为 `4`、连接到`2` 个三角形并通过`2` 个顶点连接到剩余网络。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/7kr4mSmyht3M.png?imageslim">
   </p>
   

   为什么这些`embedding` 方法（如 `DeepWalk/node2vec` 在分类任务中大获成功，但是在结构等效`structural equivalence` 任务中难以奏效？原因是大多数真实网络结构中，很多顶点的特征都表现出很强的同质性。如：两个具有相同政治偏好的博客更有可能被连接，而不是随机性的连接。网络中相邻的顶点更可能具有相同的特征，因此在 `embedding` 空间中趋于靠近；网络中相距较远的顶点可能具有迥异的特征，因此在`embedding` 空间中趋于分离。这

   种性质和顶点的局部结构无关，因此这些 `embedding` 方法无法捕获结构等效性。因此如果任务更依赖于结构等效性而不是同质性，则这些 `embedding` 方法容易失败。

   论文 `《struc2vec: Learning Node Representations from Structural Identity》` 提出了一个学习顶点结构等效性的无监督学习框架 `struc2vec`，它可以根据顶点的局部网络结构来识别`structural identity` 结构角色。

   `struc2vec` 的关键思想是：

   - 不依赖于顶点特征、边的特征、顶点的位置，仅仅依赖于顶点的局部结构来衡量顶点之间的结构相似性。我们也不需要网络是连通的，我们可以识别不同连通分量`connected componet` 中结构相似的顶点。
   - 对结构相似性进行分层 `hierarchy`，随着层次的提高结构相似越严格。在层次的底部，`degree` 相近的顶点之间是结构相似的；在层次的顶部，需要整个网络相似的顶点之间才是结构相似的。
   - 采用加权随机遍历一个 `multi-layer` 图来生成每个顶点的随机上下文。这个 `multi-layer` 是根据原始网络生成的，`multi-layer` 图中的边是根据原始图中顶点结构相似性得到。因此有相似上下文的两个顶点很可能具有相似的结构。

2. `DeepWalk` 使用随机游走从网络中生成顶点序列，并基于 `SkipGram` 学习顶点 `embedding` 。网络中接近的顶点将具有相似的上下文，因此具有相似的 `embedding`。

   `node2vec` 通过一个有偏的二阶随机游走模型来扩展了这个想法，从而在生成顶点上下文时提供了更大的灵活性。特别的，可以通过设计边的权重从而尝试捕获顶点的同质性和结构等效性。

   但是 `DeepWalk` 和 `node2vec` 等方法的一个基本缺陷是：结构上相似的顶点如何其距离大于 `SkipGram` 窗口的大小，则它们永远不会共享相同的上下文。

   `RolX` 是一种仅利用网络结构来明确标识顶点角色的方法。这种无监督方法基于枚举顶点的各种结构特征，然后找到联合特征空间的基向量`basis vector`，并为每个顶点赋予一个角色分布，这允许顶点具有多个混合的角色。在没有明确考虑顶点结构相似性的情况下，`RolX` 可能会遗漏结构相似的顶点`pair` 对。

### 13.1 模型

1. 考虑捕获网络中结构相似性的顶点`representation` 学习方法，它应该具有预期的特点：

   - 顶点之间结构越相似，则顶点的`embedding` 距离越近。因此如果两个顶点的局部网络结构相同，则它们应该具有相同的 `embedding` 。
   - `embedding` 的学习不依赖于任何顶点的特征、边的特征、顶点的 `label`、顶点的位置，仅仅依赖于顶点的局部网络结构。

   考虑这两个特点，我们提出了 `struc2vec`框架，该框架由四个部分组成：

   - 结构相似性度量：确定图中每对顶点之间不同大小邻域的结构相似性。这在结构相似性度量中引入了层次 `hierarchy` ，从而在不同层次水平上衡量结构相似性。
   - 加权 `multi-layer` 图：该图的每一层都包含原始图的所有顶点，每一层对应于`hierarchy` 结构相似性中的某个层级。层内顶点之间边的权重与结构相似度成反比。
   - 上下文生成：基于 `multi-layer`图上的有偏随机游走来生成顶点序列，然后为每个顶点生成上下文。这些随机游走序列更可能包含结构相似的顶点。
   - 学习`embedding`：采用一种技术（如 `SkipGram` ）从顶点序列中学到顶点的 `embedding` 。

   `struc2vec` 非常灵活，它不要求任何特定的结构相似性度量或者表示学习方法。

#### 13.1.1 结构相似性

1. `struc2vec` 需要确定两个顶点之间的结构相似性，其中不依赖于任何顶点的特征或者边的特征。

   另外，这种相似性应该是层次的 `hierarchical` ：随着邻域越来越大，捕获的结构相似性越来越精确。直观地，具有相同`degree` 的两个顶点在结构上相似。但是如果它们的邻居也具有相同的 `degree`，则它们在结构上会更相似。随着邻居的邻居也参与其中，则相似性会得到进一步提高，即越来越精确。

2. 设$G=(V,E)$为一个无向无权图，设$k^\*$为图的直径（顶点对之间距离的最大值）。定义$R_k(u)$为图中与顶点$u$距离为$k$的顶点的集合，$k\ge 0$。根据该定义，$R_1(u)$表示$u$的直接邻居顶点的集合，$R_k(u),k\gt 1$表示和$u$距离为$k$的环。

   定义$s(S)$为顶点集合$S\sub V$的有序`degree` 序列 `ordered degree sequence` 。

   通过比较顶点$u$和$v$的距离为$k$的环的有序`degree`序列，我们可以得到顶点$u$和$v$的相似性。根据不同的距离$k$，我们可以得到一个 `hierarchy` 层次相似性。

3. 定义$f_k(u,v)$为顶点$u$和$v$的$k$阶结构距离，我们递归定义为：

   其中$g(D_1,D_2) \ge 0$衡量了两个有序 `degree` 序列$D_1,D_2$的距离，$f_{-1}(\cdot,\cdot) = 0$。

   其物理意义为：顶点$u,v$的$k$阶距离等于它们的$k-1$阶距离加上它们的第$k$阶环之间的距离。

   根据定义有：

   - $f_k(u,v)$是随着$k$递增，并且只有当$u,v$各自都存在距离为$k$的邻域顶点时才有定义。
   - $f_k(u,v)$将比较顶点$u$和$v$各自距离为$1,2,\cdots,k$的环的有序 `degree` 序列。
   - 如果$u$和$v$的$k$阶邻域是 `isomorphic` 同构 的，则有$f_k(u,v) = 0$。

4. 考虑到两个有序 `degree` 序列$s(R_k(u))$和$s(R_k(v))$可能具有不同的长度，且它们的元素是位于$[0,|V| - 1]$之间的任意整数，则如何选择合适的距离函数是一个问题。

   论文采用 `Dynamic Time Warping:DTW`算法来衡量两个有序 `degree` 序列的相似性，该算法可以处理不同长度的序列。

   `DTW` 算法寻找两个序列$A$和$B$之间的最佳对齐 `alignment`，其目标是使得对齐元素之间的距离之和最小。

   假设$A=\{a_1,a_2,\cdots,a_n\},B=\{b_1,b_2,\cdots,b_m\}$，则 `DTW` 算法需要寻找一条从$(a_1,b_1)$出发、到达$(a_n,b_m)$的最短路径。假设路径长度为$K$，第$k$个顶点为$p_k=(a_{i_k},b_{j_k})$，则路径需要满足条件：

   - 边界条件：$(i_1,j_1) = (1,1), (i_{K},j_{K}) = (n,m)$
   - 连续性性：对于下一个顶点$p_{k+1}$，它必须满足$i_{k+1} - i_k \le 1, j_{k+1}-i_k\le1$。即数据对齐时不会出现遗漏。
   - 单调性：对于下一个顶点$p_{k+1}$，它必须满足$i_{k+1}\ge i_k, j_{k+1}\ge j_k$。即数据对齐时都是按顺序依次进行的。
   - 路径最短：$\min\sum_{k=1}^K dist(p_k)$，其中$dist(\cdot)$为单次对齐的距离。

   如下图所示，根据边界条件，该路径需要从左下角移动到右上角；根据单调性和连续性，每一步只能选择右方、上方、右上方三个方向移动一个单位。该问题可以通过动态规划来求解。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/xDAWtFx9OPXT.png?imageslim">
   </p>
   

5. 考虑到序列$s(R_k(u))$和$s(R_k(v))$中的元素为顶点的`degree`，我们使用以下单次对齐距离：

   $dist(a,b) = \frac{\max(a,b)}{\min(a,b)} - 1$

   - 当$a=b$时，$dist(a,b) = 0$。因此如果两个`degree` 序列相同，则每个对齐距离都是零，则序列的距离为零。

   - 如果使用传统的欧式距离，如$dist(a,b) = (a-b)^2$，则`degree`$a=1,b=2$的距离和$a=101,b=102$距离相等。

     实际上 `degree 1 vs degree 2`，相比于 `degree 101 vs degree 102` ，前者的差距要大得多。而使用我们定义的 `dist` 函数有：

     $dist(1,2) = 1 \gg dist(101,102) = \frac {1}{101}$

     这正好是衡量顶点 `degree` 差异的理想特点。

6. 虽然论文采用 `DTW` 来评估两个有序 `degree` 序列之间的相关性，但是`struc2vec` 也支持使用任何距离函数。

#### 13.1.2 加权 multi-layer 图

1. 论文构建了一个 `multi-layer` 加权图来编码顶点之间的结构相似性。定义$\mathcal M$为一个`multi-layer`图，其中 `layer`$k$（$k=0,\cdots,k^\*$）根据顶点的$k$阶邻域来定义。

   `layer`$k$是一个包含顶点集合$V$的带权无向完全图，即包含$C_{|V|^2}$条边。每条边的权重由顶点之间的$k$阶结构相似性来决定：

   $w_k(u,v) = \exp(-f_k(u,v))\quad k=0,\cdots,k^\*$

   - 仅当$f_k(u,v)$有定义时，`layer`$k$中顶点$u,v$之间的边才有定义。
   - `layer`$k$中边的权重与结构距离（即$k$阶相似性）成反比。
   - `layer`$k$中边的权重小于等于`1` ，当且仅当$f_k=0$时权重等于 `1` 。
   - 考虑到$f_k(u,v)$时随着$k$的增加单调增加的，因此同一对顶点$u,v$从`multi-layer` 图的低层到高层，边的权重是逐渐降低的。

2. 对于`multi-layer` 图的不同层之间，我们使用有向边进行连接。每个顶点可以连接上面一层、下面一层对应的顶点。假设顶点$u$在 `layer`$k$记作$u^{(k)}$，则$u^{(k)}$可以通过有向边分别连接到$u^{(k-1)},u^{(k+1)}$。

   对于跨 `layer` 的边，我们定义其边的权重为：

   其中$\Gamma_k(u^{(k)})$为 `layer`$k$中，$u$的所有边的权重大于该层平均边的权重的数量。即：

   $\Gamma_k(u) = \sum_{v\in V}\mathbb I(w_k(u,v) \gt \bar w_k)$

   $\bar w_k = \frac{1}{C_{|V|}^2}\sum_{(u,v)\in C_{|V|}^2} w_k(u,v)$为平均权重。因此$\Gamma_k(u)$衡量了在`layer`$k$，顶点$u$和其它顶点的相似性。

   - 如果$u$在当前层拥有很多结构相似的顶点，则应该切换到更高层进行比较，从而得到更精确的相似性。
   - 根据定义有$w_{u^{(k)},u^{(k+1)}} \ge w_{u^{(k)},u^{(k-1)} }$，因此每个每个顶点连接高层的权重不小于连接低层的权重。
   - 这里采用对数函数，因为顶点$u$超过层内平均权重的边的数量取值范围较大，通过对数可以压缩取值区间。

3. $\mathcal M$具有$k^\*$层，每一层有$|V|$个顶点，层内有$C_{|V|}^2$条边，层之间有$|V|$条边。因此`multi-layer` 图有$k^\* C_{|V|}^2 + 2|V|(k^\*-1)$条边。后续会讨论如何优化从而降低生成$\mathcal M$的计算复杂度，以及存储$\mathcal M$的空间复杂度。

#### 13.1.3 上下文生成

1. 我们通过 `multi-layer` 图$\mathcal M$来生成每个顶点$u\in V$的结构上下文。注意：$\mathcal M$仅仅使用顶点的结构信息，而没有采用任何额外信息来计算顶点之间的结构相似性。

   我们也采用随机游走来生成顶点序列，这里我们考虑有偏的随机游走：根据$\mathcal M$中边的权重来在$\mathcal M$中随机游走。假设当前层位第$k$层：

   - 首先以概率$q\gt 0$来决定：是在当前 `layer` 游走还是需要切换 `layer` 游走。即以概率$q$来留在当前`layer`，以概率$1-q$来跨层。

   - 层间游走：如果需要切换 `layer` ，则是移动到更上一层、还是移动到更下一层，这由层之间的有向边的权重来决定：

     跨层概率正比于层之间的有向边的权重。

   - 层内游走：当随机游走保留在当前层时，随机游走从顶点$u$转移到顶点$v$的概率为：

     $p_k(u,v) = \frac{\exp(-f_k(u,v))}{\sum_{v^\prime\in V,v^\prime\ne u}\exp(-f_k(u,v^\prime))}$

     其中分母是归一化系数。

     因此随机游走更倾向于游走到与当前顶点结构更相似的顶点，避免游走到与当前顶点结构相似性很小的顶点。最终顶点的上下文中包含的更可能是和它结构相似的顶点。

2. 对于每个顶点$u\in V$，我们从第$0$层开始进行随机游走。每个顶点生成以它为起点的$r$条随机游走序列，每条随机游走序列的长度为$l$。

#### 13.1.4 学习 embedding

1. 我们通过 `SkipGram` 来从随机游走序列中训练顶点 `embedding` ，训练的目标是：给定序列中的顶点，最大化其上下文的概率。其中上下文由中心窗口的宽度$w$来给定。
   - 为了降低计算复杂度，我们采用 `Hierarchical Softmax` 技巧。
   - 这里也可以不适用 `SkipGram` ，而似乎用任何其它的文本 `embedding`技术。

#### 13.1.5 优化

1. 为了构建$\mathcal M$我们需要计算每一层每对顶点之间的距离$f_k(u,v)$，而$f_k(u,v)$需要基于 `DTW` 算法在两个有序 `degree` 序列上计算。经典的 `DTW` 算法的计算复杂度为$O(l^2)$，其快速实现的计算复杂度为$O(l)$，其中$l$为最长序列的长度。

   令$d_\max$为网络中的最大 `degree`，则对于任意顶点$u$和邻域大小$k$， `degree` 序列的长度满足$|s(R_k(u)|\lt \min(d_\max^k,|V|)$。

   由于每一层有$C_{|V|}^2$对顶点，因此计算第$k$层所有距离的计算复杂度为$O(|V|^2\times \min(d_\max^k,|V|))$。

   最终构建$\mathcal M$的计算复杂度为$O(k\times |V|^3)$。

   这里我们介绍一系列优化来显著减少计算复杂度和空间复杂度。

2. 优化一：降低有序 `degree` 序列的长度。

   为了降低比较长序列距离的代价，我们对有序`degree` 序列进行压缩：对序列中出现的每个 `degree`，我们保存它出现的次数。压缩后的有序 `degree` 序列由该 `degree`，以及它出现的次数构成的元组来组成。注意：压缩后的有序 `degree` 序列也是根据 `degree` 进行排序。

   由于网络中很多顶点往往具有相同的 `degree`，因此压缩后的有序 `degree` 序列要比原始序列小一个量级。

   令$A^\prime,B^\prime$为压缩后的有序 `degree` 序列，它们的元素都是元组的形式。则我们在`DTW` 中的`dist` 函数修改为：

   $\text{dist}((a_0,a_1),(b_0,b_1)) = \left(\frac{\max(a_0,b_0)}{\min(a_0,b_0)}-1\right)\max(a_1,b_1)$

   其中$(a_0,a_1)\in A^\prime, (b_0,b_1)\in B^\prime$，$a_0,b_0$都是 `degree`，$a_1,b_1$都是`degree` 对应出现的次数。

   注意：使用压缩的 `degree` 序列会使得具有相同 `degree` 的原始序列片段之间的比较。原始的 `DTW` 算法是比较每个 `degree`，而不是比较每个 `degree` 片段，因此上式是原始 `DTW` 的近似。

   由于 `DTW` 现在在$A^\prime,B^\prime$上计算，而它们要比$A,B$短得多，因此可以显著加速 `DTW` 的计算速度。

3. 优化二：降低成对顶点相似度计算的数量。

   在第$k$层，显然没有必要计算每对顶点的相似度。考虑`degree` 相差很大的一对顶点（如`degree=2` 和 `degree=20` ），即使在$k=0$层它们的结构距离也很大。由于顶点之间的结构距离随着$k$的增大而增大，因此当$k\gt 0$时它们的结构距离更大。因此这样的一对顶点在$\mathcal M$中的边的权重很小，则随机游走序列不太可能经过这种权重的边，因此在$\mathcal M$中移除这类边不会显著改变模型。

   我们将每一层需要计算相似度的数量限制为：每个顶点最多与$O(\log|V|)$个顶点计算相似度。令顶点$u$需要计算相似度的邻居顶点集合为$J_u$，则$J_u$需要包含哪些几乎和$u$结构相似的顶点，并且应用到所有 `layer` 。

   我们取和$u$的 `degree` 最相似的顶点作为$J_u$。

   - 首先对全网所有顶点的有序 `degree` 序列进行二分查找，查找当前顶点$u$的 `degree`
   - 然后沿着每个搜索方向（左侧、右侧）获取连续的$\log |V|$个顶点。

   计算$J_u$的计算复杂度为$O(|V|\log |V|)$，因为我们需要对全网顶点根据 `degree` 进行排序。

   现在$\mathcal M$每一层包含$O(|V|\times \log |V|)$条边 ，而不是$O(|V|^2)$条边。

4. 优化三：降低层的数量。

   $\mathcal M$中的层的数量由网络直径$k^\*$来决定。但是实际上当$k$的值足够大时，评估两个顶点之间的结构相似性就没有任何意义。因为当$k$接近$k^\*$时，$s(R_k(u))$和$s(R_k(v))$这两个序列都非常短，这使得$f_k(u,v)$和$f_{k-1}(u,v)$几乎相等。

   因此我们可以将$\mathcal M$的层的数量限制为一个固定的常数$k^\prime \lt k^\*$，从而仅仅采用最重要的一些层来评估结构相似性。这显著减少了构建$\mathcal M$的计算需求和内存需求。

5. 尽管上述优化的组合会影响顶点的结构`embedding`，但是我们实验证明：这些优化的影响非常小，甚至是有益的。因此降低模型计算量和内存需求的好处远远超出了它们的不足。

### 13.2 实验

#### 13.2.1 杠铃图

1. 我们定义$B(h,k)$为 `(h,k)-barbell` 图：

   - 杠铃图包含两个完全图$K_1$和$K_2$，每个完全图包含$h$个顶点
   - 杠铃图包含一个长度为$k$的路径$P$，其顶点记作$\{p_1,\cdots,p_k\}$，该路径连接两个完全图
   - 两个顶点$b_1\in V(K_1)$和$b_2\in V(K_2)$作为 `bridge` ，我们连接$b_1$到$p_1$、$b_2$到$p_k$

   杠铃图中包含大量结构身份`structural identity` 相同的顶点。

   - 定义$C_1=V(K_1)- \{b_1\}, C_2=V(K_2)-\{b_2\}$，则所有的顶点$v\in \{C_1\cup C_2\}$为结构身份相同的。
   - $\{p_i,p_{k-i}\},1\le i\le k-1$顶点对之间也是结构身份相同的。
   - $\{b_1,b_2\}$这对`bridge`也是结构身份相同的。

   下图给出了$B(10,10)$杠铃图，其中结构等价的顶点以相同的颜色表示。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/N9aSSn24PvwQ.png?imageslim">
   </p>
   

2. 我们使用 `DeepWalk,node2vec,struc2vec` 等模型学习杠铃图中顶点的 `embedding` 。这些模型都采用相同的参数：每个顶点开始的随机游走序列数量 `20`、每条游走序列长度 `80`、`SkipGram` 窗口大小为`5` 。对于 `node2vec` 有$p=1,q=2$。

   我们预期 `struc2vec`

   下图给出了各模型针对$B(10,10)$杠铃图学到的 `embedding` 。

   - `DeepWalk` 无法捕获结构等效性。这是符合预期的，因为 `DeepWalk`并不是为结构等效性而设计的。
   - `node2vec` 也无法捕获结构等效性。实际上它是将图中距离相近的顶点放到 `embedding` 空间中相近的位置。
   - `struc2vec` 能够学到间隔良好的等效类别 `embedding` ，从而将结构等效的顶点放在`embedding` 空间中相近的位置。
     - `struc2vec` 还能够捕获结构层次`structural hierarchy`：虽然顶点$b_1$和$p_1,p_2$都不等价，但是在从结构上看$b_1$和$p_1$比$b_1$和$p_2$更相似。表现在 `embedding` 空间上，$b_1$和$p_1$的距离更近。
     - 我们提出的三个优化并为对 `embedding` 的质量产生任何重大影响。实际上优化一的 `embedding` 中，结构等效的顶点甚至更加靠近。
   - 图`b` 给出了 `RolX` 的结果。模型一共确定了六个角色，其中角色`2`和角色`4`捕获了正确的结构类别，蓝色类别被分配到三个类别(角色`1、3、6`) ，角色`5` 包含了剩余的类别。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/w0jSrDzbfReG.png?imageslim">
   </p>
   

#### 13.2.2 Karate 网络

1. `Zachary's Karate Club` 是一个由`34` 个顶点、`78` 条边组成的网络，其中每个顶点代表俱乐部成员、边表示两个成员是否在俱乐部以外产生互动，因此边通常被解释为成员之间的好友关系。

   我们的网络由该网络的两个拷贝$G_1,G_2$组成，其中$u\in V(G_1)$都有一个镜像顶点$v\in V(G_2)$。我们还通过在镜像顶点对 `(1,37)` 之间添加一条边来连通$G_1,G_2$。尽管 `struc2vec` 可以建模分离的连通分量，但是 `DeepWalk` 和 `node2vec` 无法训练。为了与`DeepWalk/node2vec` 基准方法进行公平的比较，我们添加了这条边。

   下面给出了镜像网络，其中结构相等的顶点采用相同的颜色表示。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/tbWNQDk4jlaf.png?imageslim">
   </p>
   

2. 我们使用 `DeepWalk,node2vec,struc2vec` 等模型学习镜像图中顶点的 `embedding` 。这些模型都采用相同的参数：每个顶点开始的随机游走序列数量 `5`、每条游走序列长度 `15`、`SkipGram` 窗口大小为`3` 。对于 `node2vec` 有$p=1,q=2$。

   - `DeepWalk` 和 `node2vec` 学习的 `embedding` 可视化结果如下图所示。可以看到，它们都无法识别结构等效的顶点。

   - `struc2vec` 成功的学到顶点的结构特征，并正确捕获到结构等效性。镜像顶点`pair` 对在`embedding` 空间中距离很近，并且顶点以一个复杂的结构层次聚合在一起。

     另外，顶点`1` 和 `34` 在网络中具有类似领导者的角色，`struc2vec` 成功捕获了它们的结构相似性，即使它们之间不存在边。

   - `RolX` 一共识别出`28` 个角色。注意：顶点`1/34` 被分配到不同角色中，顶点`1/37` 也被分配到不同角色中。一共`34` 对镜像顶点`pair` 对中仅有 `7` 对被正确分配。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/SnOT3vIQATEl.png?imageslim">
     </p>
     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/mQlu7eKe7G8O.png?imageslim">
     </p>
     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/wiEqCI1ovxiY.png?imageslim">
     </p>
     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/JAwHbaDx642s.png?imageslim">
     </p>
     

3. 我们测量了每个顶点和它的镜像顶点之间的距离、每个顶点和子图内其它顶点的距离，下图给出 `node2vec` 和 `struc2vec`学到的`embedding` 的这两种距离分布。横轴为距离，纵轴为超过该距离的顶点对的占比。

   - 对于 `node2vec`，这两个分布几乎相同。这表明`node2vec` 并未明显的区分镜像顶点`pair` 对和非镜像顶点`pair` 对，因此 `node2vec` 无法很好识别结构等效性。

   - 对于 `struc2vec`，这两个分布表现出明显的差异。`94%` 的镜像`pair` 对之间的距离小于 `0.25` ，而`68%` 的非镜像 `pair` 对之间的距离大于 `0.25` 。

     非镜像`pair` 对的平均距离是镜像 `pair` 对平均距离的 `5.6` 倍，而`node2vec` 这一比例几乎为 `1` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/UWe8TPhMmngK.png?imageslim">
   </p>
   

4. 接下来我们评估所有顶点对之间的距离（通过$f_k(u,v)$来衡量），以及它们在 `embedding` 空间中的距离（通过欧式距离来衡量）的相关性。我们分别计算 `Spearman` 相关系数和 `Pearson` 相关系数，结果如下图所示。

   这两个系数表明：对于每一层这两个距离都存在很强的相关性。这表明 `embedding`确实刻画了顶点之间的结构相似性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/BqiLmBje5Jje.png?imageslim">
   </p>
   

#### 13.2.3 分类任务

1. 我们考虑一个空中交通网络，它是一个无权无向图，每个顶点表示机场，每条边表示机场时间是否存在商业航班。我们根据机场的活跃水平来分配`label`，其中活跃水平根据航班流量或者人流量为衡量。

2. 数据集：

   - 巴西空中交通网络：包含 `2016`年`1`月到`2016`年`12`月从国家民航局 `ANAC` 收集的数据。该网络有`131`个顶点、`1038`条边，网络直径为`5`。机场活跃度是通过对应年份的着陆总数与起飞总数来衡量的。
   - 美国空中交通网络：包含 `2016`年 `1`月到 `2016` 年 `10` 月从美国运输统计局收集的数据。该网络有 `1190` 个顶点、`13599` 条边，网络直径为 `8`。机场活跃度是通过对应年份的人流量来衡量的。
   - 欧洲空中交通网络：包含 `2016` 年 `1` 月到 `2016` 年 `11` 月从欧盟统计局收集的数据。该网络包含 `399` 个顶点、`5995` 条边，网络直径为 `5` 。机场活跃度是通过对应年份的着陆总数与起飞总数来衡量的。

   对于每个机场，我们将其活跃度的标签分为四类：对每个数据集我们对活跃度的分布进行取四分位数从而分成四组，然后每一组分配一个类别标签。因此每个类别的样本数量（机场数量）都相同。

   另外机场类别更多的与机场角色密切相关。

3. 我们使用 `struc2vec` 和 `node2vec` 来学习每个网络的顶点 `embedding` ，其中我们通过 `grid search` 来为每个模型选择最佳的超参数。

   然后我们将学到的`embedding` 视为特征来训练一个带 `L2` 正则化的 `one-vs-rest` 逻辑回归分类器。我们还考虑直接将顶点的 `degree` 作为特征来训练一个逻辑回归分类器，从而比较使用原生特征和`embedding` 特征的区别。

   由于每个类别的规模相同，因此我们这里使用测试准确率来评估模型性能。我们将分类模型的数据随机拆分为`80%` 的训练集和 `20%` 的测试集，然后重复实验`10` 次并报告平均准确率。

   结论：

   - `struc2vec` 模型学到的 `embedding`明显优于其它方法，并且其优化几乎没有产生影响。
   - `node2vec` 模型学到的 `embedding`的平均性能甚至略低于直接使用顶点 `degree`，这表明在这个分类任务中，顶点的结构相似性起到了重要作用。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/gq329aGOEDkV.png?imageslim">
   </p>
   

#### 13.2.4 健壮性和可扩展性

1. 为了说明`struc2vec` 对噪音的健壮性，我们从网络中随机删除边从而直接修改其网络结构。

   给定图$G=(V,E)$，我们以概率$s$随机采样$G$图中的边从而生成新的图$G_1$，这相当于图$G$的边以概率$1-s$被删除。

   我们从 `Facebook` 网络中随机采样，该网络包含 `224` 个顶点、`3192` 条边，顶点 `degree` 最大为 `99` 最小为 `1` 。我们采用不同的$s$来生成两个新的图$G_1$和$G_2$，我们重新标记$G_2$中的顶点从而避免两个顶点编号相同。因此这里也存在一种“镜像”关系：原始`Facebook` 网络中的顶点会同时出现在$G_1$和$G_2$中。

   我们将这两个子图的并集来作为 `struc2vec` 的输入。下图给出了不同的$s$值，`struc2vec` 学到的所有顶点 `pair` 对之间的距离分布。其中 `x` 标记的是镜像顶点 `pair` 对之间的距离，`+`标记的是所有顶点 `pair` 对之间的距离。

   - 当$s=1$时，两个距离分布明显不同。所有顶点`pair` 对之间的平均距离要比镜像顶点 `pair` 对之间的平均距离大 `21` 倍。

   - 当$s=0.9$时，两个距离分布仍然非常不同。进一步减小$s$不会显著影响所有顶点 `pair` 对的距离分布，但是会缓缓改变镜像顶点`pair` 对的距离分布。

   - 即使在$s=0.3$时，这意味着原始边同时出现在$G_1$和$G_2$中的概率为 `0.09`，`struc2vec` 仍然会将镜像顶点`pair` 对进行很好的区分。

     这表明即使在存在结构噪音的情况下，`struc2vec` 在识别结构等效性方面仍然具有很好的鲁棒性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/3Cj1O388QQMi.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/raMlfyoHn2wh.png?imageslim">
   </p>
   

2. 我们将采用优化一、优化二的 `struc2vec`应用于 `Erd¨os-R´enyi` 图，我们对每个顶点采样`10` 个随机游走序列，每个序列长度为 `80`，`SkipGram` 窗口为 `10`， `embedding` 维度为 `128` 。为加快训练速度，我们使用带负采样的 `SkipGram` .

   为评估运行时间，我们在顶点数量从`100`到 `100万` 、平均`degree` 为 `10` 的图上训练 `struc2vec`。每个实验我们独立训练`10`次并报告平均执行时间。其中 `training time` 表示训练 `SkipGram` 需要的额外时间。

   下图给出了`log-log` 尺度的训练时间，这表明 `struc2vec` 是超线性的，但是接近$n^{1.5}$（虚线）。因此尽管`struc2vec` 在最坏情况下在时间和空间上都是不利的，但是实际上它仍然可以扩展到非常大的网络。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/6f7mMTbBdU5x.png?imageslim">
   </p>
   

## 十四、GraphWave

1. 图中不同位置的顶点在其局部网络拓扑中可能具有相似的角色，识别这些角色可以提供对网络结构的关键洞察`insight` ，并用于各种机器学习任务，包括作为机器学习问题的输入、识别系统中的关键节点（如社交网络中的关键影响者 `influencers`、传染病扩散图中的关键枢纽`hub` ）。

   直观的讲，具有相似结构角色的顶点在网络中起到相似的功能。如：公司社交网络中的管理者角色，或者细胞分子网络中的酶的角色。但是顶点的这种结构相似性和传统的顶点相似性概念有很大的不同。传统概念中，认为网络位置相近的顶点是相似的。

   如下图所示，尽管顶点$a$和$b$在图中相距甚远，它们具有相似的结构角色。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/sYF747isWxIs.png?imageslim">
   </p>
   

2. 在图上定义顶点的结构角色对应于局部网络邻域的不同拓扑，如链式结构中的顶点、星型结构中的中心、两个簇中间的`bridge` 等等。但是这些结构角色必须预定义，这需要领域内的专业知识，以及需要人工来对图的结构进行观察`inspection` 。

   一种更强大的识别结构相似性的方法是：通过无监督学习对每个顶点$a$学习一个结构`embedding` 向量$\mathbf{\vec x}_a$。对于任意$\epsilon \gt 0$，如果$\text{dist}(\mathbf{\vec x}_a,\mathbf{\vec x}_b) \le \epsilon$则顶点$a,b$被定义为$\epsilon-\text{structural}$相似的。

   因此这种方法必须引入适当的`embedding`方法以及一个合适的距离函数`dist()` 。

   虽然目前已经有几种方法用于学习图中顶点结构 `embedding`，但是这些方法对于拓扑中的小扰动极为敏感，并且对所学到的 `embedding` 的性质缺乏数学上的解释。 并且这些方法依赖于人工手动构建拓扑结构特征，使用`non-scalable` 的启发式方法，甚至有的方法仅返回单个相似性得分而不是一个结构 `embedding` 。

   论文`《Learning Structural Node Embeddings via DiffusionWavelets》` 提出了 `GraphWave` 方法来解决图的结构学习问题。

   `GraphWave` 方法采用来自于图信号处理的技术，基于以顶点为中心的谱图小波扩散 `spectral graph wavelet diffusion`来学习每个顶点的结构`embedding` 。直观的看，每个顶点在图上传播一个能量单位，并根据网络的`response` 来表征其邻域拓扑结构。我们正式证明了该小波系数与图的拓扑特性直接相关，因此这些系数包含了发现顶点相似性的所有信息，从而无需人工构造特征。

3. 根据设计，小波`wavelet` 是作用在图的局部区域的。因此如果两个顶点相距较远，则它们之间的小波是无法进行比较的。这时需要为每对顶点指定一个一对一的映射，从而使得一对顶点之间的小波可以进行直接比较，如计算相关系数或者计算$L_2$距离。

   这种顶点对之间的一一映射计算量太大，因此是难以实现的。所以这些小波方法从未被用于学习结构`embedding` 。

   为了克服这一调整，`GraphWave` 提出将小波视为图上概率分布的新颖方法。这样结构信息体现在`diffusion` 是如何在网络中扩散`spread` ，而不是扩散到何处。

   我们使用这些小波分布函数的经验特征函数`empirical characteristic function` 来 `embed` 这些小波分布，这样可以捕获给定分布的所有矩（包括高阶矩）。

4. 我们从数学上证明了 `GraphWave` 可以发现结构相似的顶点，并且能够抵抗局部网络结构的微小扰动。

   另外，`GraphWave` 的计算复杂度是$O(|E|)$，这使得它可以应用到大型（稀疏）网络。

   通过将 `GraphWave` 和其它几个 `state-of-the-art` 基准方法在真实数据集和合成数据集上进行比较，`GraphWave` 取得了最高 `137%` 的改进。

5. 目前挖掘顶点结构相似性的方法依赖于顶点的人工特征构造，这些方法需要在预先给出每个顶点的局部拓扑属性，如顶点的`degree`、它参数的三角形数量、`k-clique` 的数量、`PageRank` 得分。

   这种方法的一个典型示例是 `RolX` ，它是一种基于矩阵分解的方法。另一种方法是 `struc2vec` ，它使用启发式方法基于网络拓扑度量来构建 `multi-layer` 图，并在`multi-layer` 图上随机游走从而捕获结构信息。

   与这些方法相比，`GraphWave` 不依赖于启发式算法，也不需要显式的人工特征工程或者人工参数调整。另外，`struc2vec,神经网络指纹, Semi-Supervised GCN`等方法是嵌入整个图，而我们的方法仅仅`embed` 单个顶点。

6. `graph diffusion kernel` 图扩散核最近用于各种图的建模任务，`GraphWave` 是第一个应用图扩散核来确定顶点结构角色的论文。核方法已经被证明可以有效捕获几何属性，并成功应用于各种图像处理任务。但是我们这里的图是高度不规则的（与图像的欧式图相反），因此传统的小波方法不适用。

   `GraphWave` 这里使用小波来表征扩散的形状而不是扩散的位置，这一关键洞察使得我们能够发现结构`embedding` 。

### 14.1 模型

1. 给定无向图$G=(V,E)$，其中$V$为顶点集合，$E$为边集合。令邻接矩阵为$\mathbf A$，它是二元的或者带权重的。定义度矩阵$\mathbf D$，其中$D_{i,i} = \sum_j A_{i,j}$。

   顶点的结构`embedding` 问题为：对于每个顶点$v\in V$，我们希望在一个连续的多维空间种学得一个 `embedding`来表示它的结构角色`structural role`。

2. 令$\mathbf L = \mathbf D-\mathbf A$为未归一化的拉普拉斯矩阵，令$\mathbf U$为$\mathbf L$的特征向量`eigenvector` 组成的特征矩阵，则有：

   $\mathbf L = \mathbf U\mathbf\Lambda \mathbf U^T$

   其中$\mathbf\Lambda = \text{diag}(\lambda_1,\cdots,\lambda_N)$，$\lambda_1\le \lambda_2\le\cdots\le \lambda_{|V|}$为$\mathbf L$的特征值`eigenvalue` 。

   令$g_s$为一个`scaling` 参数为$s$的滤波器核 `filter kernel`，这里我们采用热核`heat kernel`$g_s(\lambda) = e^{-\lambda s}$，但是我们的结论适用于任何类型的 `scaling wavelet` 。另外，这里我们假设$s$是给定的，后续会讨论如何选择一个合适的$s$值。

   图信号处理`graph signal processing`将与$g_s$相关的谱图小波`spectral graph wavelet` 定义为：以顶点$v$为中心的狄拉克信号的频域响应。因此谱图小波$\Psi_v$定义为：

   $\Psi_v = \mathbf U \text{diag}(g_s(\lambda_1),\cdots,g_s(\lambda_{|V|}))\mathbf U^T \vec \delta_{v}$

   其中$\vec\delta_v$是顶点$v$的 `one-hot` 向量：顶点$v$对应的位置为`1`，其它位置为`0` 。因此顶点$v$的第$m$个小波系数为$\Psi_{m,v} = \sum_{l=1}^{|V|}g_s(\lambda_l)U_{m,l}U_{v,l}$，它是矩阵$\Psi\in \mathbb R^{|V|\times |V|}$的第$m$行第$v$列。

   可以看到在谱图小波，核$g_s$调试`modulate` 了特征谱，使得响应信号位于空域的局部区域以及频域的局部区域。

3. 如果我们将拉普拉斯矩阵和信号系统中的时域-频域进行类比，则发现：较小特征值对应的特征向量携带变化缓慢的信息，从而鼓励邻居顶点共享相似的信息；较大特征值对应的特征向量携带迅速变化的信息，从而使得邻域顶点之间区别较大。

   因此低通滤波器核`low-pass filter kernel`$g_s$可以看作是一种调制算子`modulation operator` ，它降低了较高的特征值并在图中平滑了信号的变化。

#### 14.1.1 算法

1. 我们首先描述 `GraphWave` 算法，然后对其进行分析。对于每个顶点$v$，`GraphWave` 返回表示其结构`embedding`的一个 `2d` 维度向量$\mathbf{\vec x}_v\in \mathbb R^{2d}$，使得局部网络结构相似的顶点拥有相似的 `embedding` 。

2. `GraphWave` 算法：

   - 输入：

     - 图$G=(V,E)$
     - `scale` 参数$s$
     - $d$个均匀分布的采样点$\{t_1,\cdots,t_d\}$，它们代表分布的矩的阶数

   - 输出：顶点的结构`embedding` 向量$\{\mathbf{\vec x}_v\in \mathbb R^{2d}\}_{v\in V}$

   - 算法步骤：

     - 计算拉普拉斯矩阵$\mathbf L$，并进行特征分解：$\mathbf L = \mathbf U\mathbf\Lambda \mathbf U^T$

     - 计算图谱小波$\Psi = \mathbf Ug_s(\mathbf \Lambda)\mathbf U^T$

     - 对$t\in \{t_1,\cdots,t_d\}$，计算$\phi(t)\in \mathbb R^{|V|}$为$e^{it\Psi}$的 `column-wise` 逐列均值。对于顶点$v\in V$，将$\phi_v(t)$的实部和虚部分配给$\mathbf{\vec x}_v$，其中$\phi_v(t)$为$\phi(t)$的第$v$个元素。

       每个顶点拼接$d$个实部虚部，则得到一个 `2d`维的向量。

3. 算法中，我们首先应用谱图小波来获取每个顶点的 `diffusion` 模型，然后收集到矩阵$\Psi \in \mathbb R^{|V|\times |V|}$中，其中第$v$列的列向量是以顶点$v$为中心的 `heat kernel` 产生的谱图小波。

   - 现有的工作主要研究将小波系数视为`scaling` 参数$s$的函数，而这里我们将小波系数视为局部网络结构的函数：小波系数随着顶点$v$的局部网络结构的变化而变化。特别的，每个小波系数都有深刻的物理意义：$\Psi_{v,m}$表示顶点$m$从顶点$v$接收到的能量的大小。

   - 我们将小波系数视为概率分布，并通过经验特征函数来刻画该分布。我们将证明：具有相似网络邻域结构的顶点$u$和$v$将具有相似的谱图小波系数分布$\Psi_u$和$\Psi_v$。

   - 概率分布$X$的特征函数定义为：

     $\phi_X(t) = \mathbb E[e^{itX}],t\in \mathbb R$

     它完全刻画了分布$X$，因为它捕获了关于分布$X$的所有矩的信息。将它进行泰勒展开得到：

     给定顶点$v$和 `scale s`，$\Psi_v$的经验特征函数定义为：

     $\phi_v(t) = \frac {1}{|V|} \sum_{m=1}^{|V|}e^{it\times \Psi_{v,m}}$

     理论上我们需要计算足够多的点来描述$\phi_v(t)$，但是这里我们采样了$d$个均匀间隔的点，即$\{t_1,\cdots,t_d\}$。最终得到：

     $\mathbf{\vec x}_v = \left(\text{Re}(\phi_v(t_1)),\text{Im}(\phi_v(t_1)),\cdots,\text{Re}(\phi_v(t_d)),\text{Im}(\phi_v(t_d))\right)^T$

   - 注意到我们在经验特征函数$\phi_v(t)$上均匀采样了$d$个点，这创建了一个大小为$2d$的 `embedding`，因此`embedding` 的维度和图的大小无关。

4. `GraphWave` 的输出为图中每个顶点的结构嵌入。我们可以将结构嵌入之间的距离定义为$L_2$距离，即顶点$u$和$v$之间的结构距离定义为：

   $\text{dist}(u,v) = ||\mathbf{\vec x}_u - \mathbf{\vec x}_v||_2$

   根据特征函数的定义，这相当于在小波系数分布上比较不同阶次的矩。

5. `scaling` 参数$s$确定每个顶点$v$周围的网络邻域的半径。较小的$s$会使用较小半径的邻域来确定顶点的结构嵌入；较大的$s$会使得扩散过程在网络上传播得更远，从而导致使用较大半径得邻域来确定顶点的结构嵌入。

   `GraphWave` 还可以采用$s$的不同取值来集成多尺度结构`embedding` ，这是通过串联$J$个不同的 `embedding`$\mathbf{\vec x}_v^{(s_j)}$来实现的，每个`embedding` 都和一个`scale`$s_j$相关，其中$s_j\in [s_\min,s_\max]$。我们接下来提供一个理论上的方法来找到合适的$s_\min,s_\max$。

   在多尺度版本的 `GraphWave` 中，最终顶点$v$聚合的结构`embedding`为：

   $\mathbf{\vec x}_v = \left(\text{Re}(\phi_v^{(s_j)}(t_i)),\text{Im}(\phi_v^{(s_j)}(t_i))\right)^T_{t_i,s_j}$

6. 我们使用切比雪夫多项式来计算$\Psi$，拉普拉斯算子的每个幂具有$O(|E|)$的计算成本，从而产生$O(K\times |E|)$的整体计算复杂度，其中$K$表示切比雪夫多项式逼近的阶数。

   因此 `GraphWave` 的整体计算复杂度是$|E|$的线性的，这使得 `GraphWave` 可以扩展到大型稀疏网络。

#### 14.1.2 理论分析

1. 我们首先通过理论分析表明：谱图小波系数表征了局部网络领域的拓扑结构。然后我们理论上证明：结构等价/相似的顶点具有几乎相等/近似的`embedding` 。我们从数学上证明了 `GraphWave` 的理论正确性。

2. 我们首先建立顶点$v$的谱图小波和顶点$v$的局部网络结构之间的关系。

   由于图拉普拉斯算子的谱是离散的，并且位于区间$[0,\lambda_{|V|}]$之间，根据 `Stone-Weierstrass` 定理可知：核$g_s$在区间$[0,\lambda_{|V|}]$上可以通过一个多项式来逼近。记这个多项式为$P$，则这个多项式逼近是紧的 `tight`，其误差是一致有界的 `uniformaly bounded` 。即：

   $\forall \epsilon \gt 0, \quad \exists P: P(\lambda) = \sum_{k=0}^K\alpha_k\lambda^k,\quad s.t.\; |g_s(\lambda) - P(\lambda)| \le \epsilon\quad \forall \lambda\in [0,\lambda_\max]$

   其中$K$为多项式逼近的阶数，$\alpha_k$为多项式系数，$r(\lambda) = g_s(\lambda) - P(\lambda)$为残差。

   我们可以将顶点$v$的谱图小波以多项式逼近的形式重写为：

   $\Psi_v = \left(\sum_{k=0}^K\alpha_k\mathbf L^k\right)\vec \delta_v + \mathbf Ur(\mathbf\Lambda)\mathbf U^T\vec \delta_v$

   由于$\mathbf U$是单位正交矩阵，$r(\lambda)$是一致有界的，因此我们可以通过`Cauchy-Schwartz`不等式将等式右侧的第二项进行不等式变换 ：

   其中等式左边为$\left(\mathbf Ur(\mathbf\Lambda)\mathbf U^T\vec \delta_v\right)$的第$m$个元素，$\sum_{j=1}^{|V|}U_{m,j}^2 = 1$（因为$\mathbf U$为单位正交矩阵）， 以及：

   $\sum_{j=1}^{|V|}|r(\lambda_j)|^2U_{v,j}^2\le \sum_{j=1}^{|V|} \epsilon ^2U_{v,j}^2 = \epsilon^2$

   因此$\Psi_v$可以由一个$K$阶多项式逼近，该多项式捕获了有关顶点$v$的$K$阶邻域信息。这表明谱图小波主要受到顶点的局部拓扑结构影响，因此小波包含了生成顶点结构`embedding` 所必须的信息。

3. 然后我们证明局部邻域结构相同的顶点具有相似的 `embedding` 。

   假设顶点$u$和$v$的$K$阶邻域相同，其中$K$是一个小于图的直径的整数。这意味着顶点$u$和$v$在结构上是等价的。

   我们首先将$g_s$根据泰勒在零点展开到$K$阶：

   $g(s)\simeq P(\lambda,s) = \sum_{k=0}^K(-1)^k\frac{(s\lambda)^k}{k!}$

   然后对于每个特征值$\lambda$，我们使用`Taylor-Lagrange` 等式来确保存在$c_\lambda \in [0,s]$，使得：

   $|r(\lambda)| = \left|e^{(-\lambda s)}-P(\lambda,s)\right| = \frac{(\lambda s)^{K+1}}{(K+1)!}e^{(-\lambda c_{\lambda})} \le \frac{(\lambda s)^{K+1}}{(K+1)!}$

   如果我们选择$s$使得满足：

   $s\le \frac{((K+1)!\epsilon)^{1/(K+1)}}{\lambda}$

   则可以保证绝对残差$|r(\lambda)|\le \epsilon$对于任意$\lambda$成立。这里$\epsilon$是一个参数，可以根据结构等效的顶点的`embedding` 期望的接近程度来指定，因此也称作$\epsilon-\text{structurally similar}$。另外$s$越小则可以选择$\epsilon$更小，使得上界越紧。

   由于顶点$u$和$v$的邻域是相同的，因此存在一个`one-to-one` 映射$\pi$，它将$u$的$K$阶邻域$\mathcal N_K(u)$映射到$v$的$K$阶邻域$\mathcal N_K(v)$，使得$\mathcal N_K(v) = \pi(\mathcal N_k(u))$。通过随机映射剩余的顶点（$u$的$K$阶邻域以外的顶点），我们将$\pi$作用到整个图$G$上。

   利用图拉普拉斯矩阵的$K$阶多项式近似，我们有：

   - 考虑第二行的第一项。由于顶点$u$和$v$的$K$阶邻域是等价的，则有：

     因此这一项可以约掉。

   - 考虑第二行的第二项、第三项。使用前面的残差分析可以得到它们都有一个一致的上界$\epsilon$，因此有：

     $|\Psi_{m,u} - \Psi_{\pi(m),v}| \le 2\epsilon$

     即：$\Psi_u$中的每个小波系数和它对应的$\Psi_v$中的小波系数的距离不超过$2\epsilon$。

     考虑到我们使用经验特征函数来描述分布，因此这种分布的相似性就转换为`embedding` 的相似性。因此如果选择合适的 `scale` ，则结构上等效的顶点具有$\epsilon-\text{structural}$相似的`embedding` 。

4. 接着我们证明局部邻域结构相似的顶点具有相似的 `embedding` 。

   设顶点$v$的$K$阶领域为$\mathcal N_K(v)$，$\tilde{\mathcal N}_K(v)$为顶点$v$的一个扰动的$K$阶邻域，这是通过对顶点$v$的原始$K$阶邻域重新调整边来得到。

   假设扰动后的图拉普拉斯矩阵为$\tilde{\mathbf L}$，接下来我们证明：如果顶点邻域的扰动很小，则顶点的小波系数的变化也很小。

   假设扰动很小，即：$\forall k\le K, ||\mathbf L^k-\tilde{\mathbf L}^k||_F\le \epsilon$。我们使用核$g_s$的$K$阶泰勒在零点展开来表示扰动图的小波系数：

   $\tilde\Psi_a = \sum_{k=0}^K\alpha_k\tilde{\mathbf L}^k + \tilde{\mathbf U}r(\tilde{\mathbf\Lambda})\tilde{\mathbf U}^T$

   我们使用 `Weyl` 定理将图结构中的扰动与图拉普拉斯算子的特征值变化联系起来。特别地，图的小扰动导致特征值的小扰动。即，对于每个$\tilde\lambda$，$r(\tilde \lambda)$和原始的$r(\lambda)$接近：

   $r(\tilde\lambda) = r(\lambda) + o(\epsilon) \le C\epsilon$

   其中$o(\epsilon)$表示$\epsilon$的一个无穷小量，$C$为一个常数。

   综合所有因素，则有：

   因此在`GraphWave` 中，结构相似的顶点具有相似的 `embedding` 。

#### 14.1.3 scale 参数

1. 我们开发了一个自动的方法来为`heat kernel`$g_s$中的 `scale` 参数$s$找到合适的取值范围，该方法将在 `GraphWave` 的多尺度版本中使用。我们通过分析`heat diffusion` 小波的方差来指定$s_\min$和$s_\max$来作为区间边界。

   直观来看，较小的$s$值使得热传播的时间较短，从而使得热扩散的小波分布没有意义，因为此时只有很少的几个小波系数是非零，绝大多数小波系数都是零。较大的$s$值使得热传播的时间较长，热扩散的小波分布也没有意义，因为此时网络收敛到所有顶点都处于能量为$1/|V|$的同温状态。

   这里我们证明两个命题，从而为热扩散小波分布的方差和收敛速度提供洞察`insight`。然后我们使用这些结论来选择$s_\min$和$s_\max$。

2. 命题一：热扩散小波$\Psi_v(s)$中非对角线的系数的方差与以下指标成正比：

   $\text{VAR}\left[\{\Psi_{m,v}^{(s)}\mid m\ne v\}\right] \propto ^2\Delta_v^{(0)}\Delta_v^{(2s)} - \left(\Delta_v^{(s)}\right)^2$

   其中$\Delta_v^{(s)} = |\Psi_{v,v}^{(s)} - \frac {1}{|V|}|$随着$s$的增加单调递减。这是因为$\Psi_{v,v}^{(s)} = \sum_{l=1}^{|V|}e^{-\lambda_ls}U_{v,l}^2$，它随着$s$的增加单调递减。

   证明：令小波$\Psi_v$的非对角线系数均值为$\tilde\mu_v^{(s)} = \sum_{m\ne v} \Psi_{m,v}^{(s)}/(|V|-1)$。考虑到$\Psi_{v}$为一个概率分布，因此有$\sum_{m\ne v}\Psi_{m,v}^{(s)} = 1-\Psi_{v,v}^{(s)}$。

   根据方差的定义有：

3. 命题一证明了小波系数的方差是$\Delta_v^{(s)}$的函数，因此为了最大化方差我们必须分析$\Delta_v^{(s)}$。为了确保小波系数的分布具有足够大的容量（即分布的多样性足够大），我们需要最大化方差。

   因此我们选择区间$[s_\min,s_\max]$来使得$\Delta_v(s)$足够大，从而使得`diffusion` 既有足够长的时间来扩散、又不至于太长以至于到达网络的收敛状态（所有顶点的温度都相同）。

4. 命题二：热扩散小波系数$\Psi_{m,v}^{(s)}$的收敛上下界为：

   $e^{-\lambda_{|V|}\lceil s \rceil}\Delta_v^{(0)} \le \Delta_v^{(s)} \le e^{\lambda_{2}\lfloor s\rfloor} \Delta_v^{(0)}$

   证明：对于非负的$s$有：

   $\Delta_v^{(s+1)} = \left|\Psi_{v,v}^{(s+1)} - \frac 1{|V|}\right| = \left|\sum_{l=2}^{|V|}e^{-\lambda_l(s+1)}U_{v,l}^2\right|\le e^{-\lambda_2}\left|\Psi_{a,a}^{(s)} - \frac 1{|V|}\right| =e^{-\lambda_2} \Delta_v^{(s)}$

   > 注：这里的证明存疑，但是原始论文并未给出解释。

   对应的有：

   $\Delta_v^{(s+1)}=\left|\Psi_{v,v}^{(s+1)} - \frac 1{|V|}\right|\ge e^{-\lambda_{|V|}}\left|\Psi_{v,v}^{(s)} - \frac 1{|V|}\right|=e^{-\lambda_{|V|}}\Delta_v^{(s)}$

   因此有：

   $e^{-\lambda_{|V|}}\le\frac{\Delta_v^{(s+1)}}{\Delta_v^{(s)}}\le e^{-\lambda_2}$

   对于任意的$s\in \mathbb N$，我们采用数学归纳法可以证明有：

   $e^{-\lambda_{|V|}s}\Delta_v^{(0)} \le \Delta_v^{(s)} \le e^{-\lambda_2 s}\Delta_v^{(0)}$

   由于$\Delta_a^{(s)}$是$s$的连续递增函数，因此我们选择大于等于零且满足条件的$s$进行向下、向上取整。

5. 选择$s_\max$：我们选择$s_\max$使得小波系数被局限在局部邻域上。因此我们限定：

   $\frac{\Delta_a^{(s)}}{\Delta_a^{(0)}}\ge \eta$

   其中$\eta \lt 1$。这确保了$\Delta_a^{(s)}$最多下降到初始值$\Delta_a^{(0)}$的$\eta \* 100%$。理论上如果$s$足够大则所有顶点温度相同，则$\Delta_a^{(s)} = 0$。

   根据命题二有：

   $e^{-\lambda_{|V|} s}\le\frac{\Delta_a^{(s)}}{\Delta_a^{(0)}}\le e^{-\lambda_2 s}$

   令$\frac{\Delta_a^{(s)}}{\Delta_a^{(0)}} = e^{-\lambda s}$，其中$\lambda_2\le \lambda\le \lambda_{|V|}$，则有：$e^{-\lambda s}\ge \eta$。即$s\le -\log (\eta)/\lambda$。

   - $\eta$参数越接近`1` ，则传播时间越短，$s_{\max}$越小。
   - $\eta$参数越接近`0`，则传播时间越长，则$s_\max$越大。

   现在考虑$\lambda$。当大部分特征值趋近于$\lambda_{|V|}$时，$\lambda$逼近于$\lambda_{|V|}$；当大部分特征值趋近于$\lambda_2$时，$\lambda$逼近于$\lambda_2$。为找到这两种收敛情况之间的中间状态，我们将$\lambda$设置为$\lambda_2$和$\lambda_{|V|}$的几何平均值。与算术平均值相反，几何平均值在区间$[\lambda_2,\lambda_{|V|}]$上保持相等的权重，即$\lambda_2$的$\epsilon\%$的变化和$\lambda_{|V|}$的$\epsilon\%$的变化效果相同。因此我们选择$s_\max$为：

   $s_\max = -\frac{\log \eta}{\sqrt{\lambda_2\lambda_{|V|}}}$

6. 选择$s_\min$：我们选择$s_\min$使得每个小波都有足够长的时间来扩散，即限定：

   $\frac{\Delta_a^{(s)}}{\Delta_a^{(0)}}\le \gamma$

   其中$1\ge \gamma \ge \eta$。同样的分析我们有：

   $s_\max = -\frac{\log \gamma}{\sqrt{\lambda_2\lambda_{|V|}}}$

   其中：

   - $\gamma$参数越接近`1`，则传播时间越短，$s_{\min}$越小。
   - $\gamma$参数越接近`0`，则传播时间越长，则$s_\min$越大。

7. 为覆盖适当的 `scale` 区间，论文建议设置$\eta=0.8$以及$\gamma = 0.95$。

### 14.2 实验

1. `GraphWave` 的 `embedding` 独立于下游任务，因此我们在不同任务中对其进行评估。

   我们将 `GraphWave` 和两个结构嵌入的基准方法 `struc2vec、RolX` 进行评估。`struc2vec` 通过在 `multi-layer` 图上的一系列随机游走来发现不同尺度的结构嵌入。`RolX` 通过基于顶点特征的矩阵（如`degree`、三角形数量）的非负矩阵分解的方法来描述每个顶点的角色。

   我们还将`GraphWave` 和其它最新的无监督顶点表示学习方法`node2vec、Deepwalk`方法进行比较，以强调结构`embedding` 方法与这类方法的差异。

2. 对于所有的基准模型，我们使用这些模型的默认参数。对于 `GraphWave` 模型，我们使用多尺度版本，并设置$d=50$，以及使用$[0,100]$内均匀间隔的采样点$t_i$。

#### 14.2.1 杠铃图

1. 杠铃图由两个长链组成的稠密团构成，该图包含 `8` 个不同类别的结构等效顶点，如图`A` 的颜色所示。

   我们通过`PCA` 可视化了不同模型学到的`embedding` ，相同的 `embedding` 具有相同的投影，这使得图 `B~D` 上的点发生重叠。

   - 从图`D` 可以看出， `GraphWave` 可以正确学习结构等效顶点的`embedding`，这为我们的理论提供了支撑（相同颜色的顶点几何完全重叠）。

     相反，`struc2vec` 和 `RolX` 都无法准确的识别结构等效性（相同颜色的顶点并未重叠）。

   - 所有方法都能识别稠密团的顶点（紫色）的结构，但是只有`GraphWave` 能够准确识别链上顶点的结构角色。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/zNYUEdIx2gcV.png?imageslim">
   </p>
   

#### 14.2.2 结构等效图

1. 我们在合成的图上评估这些方法。我们开发了一个程序来生成合成图，这些合成图可以植入结构等价的子图，并且给出每个顶点角色的真实标签。我们的目的是通过这些真实的角色标签来评估这些方法的性能。

   我们的合成图由不同类型的基础形状给出，这些形状类型包括 `house,fan,star`。我们一共评估四种网络结构配置：

   - 在 `house` 配置中，我们选择将`4` 个 `house` 放置在一个长度为 `30` 的圆环上。
   - 在 `varied` 配置中，我们对每个类型的形状选择`8` 个，并随机放置在一个长度为 `40` 的圆环上，从而生成具有更丰富、更复杂结构角色的合成图。
   - 在 `noise` 配置中，我们随机删减边来增加扰动（最多 `10%` ），从而评估`house perturbed, varied perturbed` 的鲁棒性。

   我们对这四种网络结构 `house, varied, house perturbed, varied perturbed`分别构造一个图，然后在这些构造的图上运行不同方法来得到顶点 `embedding` 。

2. 我们通过两种策略来评估每个`embedding`算法的性能：

   - 无监督评估策略：我们评估每种`embedding` 方法将具有相同结构角色的顶点嵌入到一起的能力。我们使用 `agglomerative` 聚类算法（采用 `single linkage` ）来对每种方法学到的 `embedding` 进行聚类，然后通过以下指标来评估聚类质量：

     - 同质性 `homogeneity`：给定聚类结果的条件下，真实结构角色的条件熵。它评估聚类结果和真实结果的差异。

     - 完整性 `completeness`：在所有真实结构角色相等的顶点中，有多少个被分配到同一个聚类。即评估聚类结果的准确性。

     - 轮廓系数 `silhouette score` ：它衡量样本聚类的合理性。

       对于样本$v_i$，假设它到同簇类其它样本的平均距离为$a_i$，则$a_i$越小说明该样本越应该被聚到本簇，因此定义$a_i$为簇内不相似度。

       假设它到其它簇$C_j$的平均距离为$b_{i,j}$，则定义簇间不相似度为$b_j=\min_j\{b_{i,j}\}$。

       定义轮廓系数为：

       $s(i) = \frac{b(i) - a(i)}{\max\{a(i),b(i)\}}$

       当$s(i)$越接近 `1` ，则说明样本$v_i$聚类合理；当$s(i)$越接近 `-1` ，则说明样本$v_i$聚类不合理。

   - 监督评估策略：我们将学到的顶点 `embedding` 作为特征来执行顶点分类，类别为顶点的真实结构角色。

     我们使用`kNN` 模型，对所有样本进行`10` 折交叉验证。对于测试集的每个顶点，我们根据它在训练集中最近邻的 `4` 个顶点来预测该顶点的结构角色，其中”近邻” 是通过 `embedding` 空间的距离来衡量。

     最终我们给出分类的准确率和 `F1`得分作为评估指标。

   所有两种策略的每个实验都重复执行`25`次，并报告评估指标的均值。

3. 模型`embedding` 的效果比较如下图所示。

   - 所有实验中，`node2vec、DeepWalk` 的效果都很差。这是因为这两个方法的重点都是学习顶点的邻近关系，而不是顶点的结构相似性。
   - `GraphWave` 的效果始终优于 `struc2vec` 。
   - 虽然 `RolX` 在部分实验中效果强于 `GraphWave`，但是 `GraphWave` 整体效果较好。
   - 轮廓系数`silhouette score` 表明：`GraphWave` 发现的簇往往更聚集并且簇之间的间隔更好。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/kAYyn2JJGUDG.png?imageslim">
   </p>
   

4. 我们将`GraphWave` 学到的 `embedding` 进行可视化，这提供了一种观察顶点之间结构角色差异的方法。

   如图 `A` 表示一个带 `house` 的环，图`B` 是它的 `GraphWave` 学到的 `embedding` 的 `2` 维`PCA` 投影。这证明了 `GraphWave` 可以准确区分不同结构角色的顶点。

   图 `C` 给出了特征函数$\phi_v(t) = \frac {1}{|V|} \sum_{m=1}^{|V|}e^{it\times \Psi_{v,m}}$，不同颜色对应不同结构角色的顶点。其物理意义为：

   - 位于图图外围的顶点难以在图上扩散信号，因此派生出的小波具有很少的非零系数。因此这类顶点的特征函数是一个较小的环状曲线。
   - 位于图核心上的顶点趋向于将信号扩散得更远，因此这类顶点的特征函数是一个较大的环状曲线。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/uyivEdEMWAAt.png?imageslim">
   </p>
   

#### 14.2.3 跨图的泛化

1. 我们分析`embedding` 如何识别位于不同图上的顶点的结构相似性，这是为了评估`embedding` 能否识别跨图的结构相似性。

2. 我们通过以下过程来生成 `200` 个具有真实顶点结构角色标签的图。

   - 首先以 `0.5` 的概率来选择环形图或者链式图，从而确定了图的骨架。
   - 然后通过均匀随机选择环的大小或者链的长度来决定骨架的大小。
   - 最后随机选择一定数量的小图挂载到骨架上，这些小图以 `0.5` 的概率从 `5` 个顶点的 `house` 或者 `5` 个顶点的`chain` 中选取。

   为了在噪音环境中评估`embedding` 方法，我们还在顶点之间随机添加`10` 个随机边，然后训练每个顶点的 `embedding` 。接着用学到的 `embedding` 来预测每个顶点的真实结构角色。

3. 为了评估每个方法在跨图上的泛化能力，我们使用`10` 折交叉验证，并且对于测试集中的顶点我们选择训练集中和它最近邻（通过`embedding` 来计算距离）的`4` 个邻居顶点来预测该测试顶点的标签。注意：所选择的`4` 个邻居顶点可能位于不同的图上。

   最后我们评估预测的准确率和 `F1` 得分，评估结果如下。结果表明：

   - `GraphWave` 方法均优于所有其它方法，这表明了 `GraphWave` 具有学习结构特征的能力，这种结构特征对于不同的图都有重要意义。
   - 基于结构相似性的 `embedding` 方法要优于基于邻近关系的`embedding`方法。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/qWfaHjI8Vs5t.png?imageslim">
   </p>
   

#### 14.2.4 可扩展性和鲁棒性

1. 为评估`GraphWave` 的可扩展性我们逐渐扩大了合成图的顶点数量，我们给出了这些合成图上运行`GraphWave` 的时间和顶点数量的关系，如下图所示。

   - `GraphWave` 得训练时间是边数量得线性函数，因此它是一种快速算法。

   - `GraphWave` 的潜在瓶颈是通过经验特征函数将小波系数的分布转换成 `embedding` 向量。

     这里利用了小波系数的稀疏性。小波系数通常是稀疏的，因为`GraphWave` 通过合理的选择 `scale`参数$s$使得小波不会被传播得太远。这总稀疏性可以降低`embedding` 过程得计算量，因为这里是一组稀疏矩阵相乘，并且逐元素函数应用到非零元素上。

   - `struc2vec` 无法应用到大图上，对于仅包含 `100` 个顶点的图，它都需要高达`260` 秒来学习顶点的 `embedding` 。

   - `RolX` 可以扩展到类似大小的图上。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/wTLf6379Bpme.png?imageslim">
   </p>
   

2. 接下来我们评估`GraphWave` 对噪音的鲁棒性。我们将噪音采取与 `varied perturbed` 相同的方式注入到图中，噪音水平由随机扰动的边占原始边的百分比给出。

   对于每个图，我们首先使用 `GraphWave`学习顶点`embedding`，然后使用 `affinity propagation` 聚类算法对`embedding` 进行聚类。最后我们根据顶点的真实结构角色来评估聚类质量。

   除了同质性、完整性指标之外，我们还报告检测到的聚类的数量，从而衡量 `GraphWave` 发现的角色的丰富程度，我们随机运行`10` 次并报告平均结果。

   结果如下图所示。结果表明：即使存在强烈的噪音，`GraphWave` 的性能也是逐渐平滑地降低。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/I5iT1RFMHnKv.png?imageslim">
   </p>
   

#### 14.2.5 真实数据集

1. 我们考虑对安然公司员工之间的电子邮件通信网络进行学习，由于公司存在组织架构，因此我们预期结构`GraphWave` 能够学到这种工作职位上的结构等效性。

   该网络中每个顶点对应于安然的员工，边对应于员工之间的`email` 通信。员工的角色有七种，包括`CEO、总裁 president、经理 manager` 等。这些角色给出了网络顶点的真实标签。

   我们用不同模型学习每个顶点的 `embedding`，然后计算每两类员工之间的平均 `L2` 距离，结果如下所示。

   - `GraphWave` 捕获到了安然公司的复杂组织结构。如 `CEO` 和总裁在结构上与其它角色的距离都很远。
   - `struc2vec` 的结果不太成功，每个类别之间的距离几乎均匀分布。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/CCG23cf2hsXN.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/3vmkGfT4nxpV.png?imageslim">
   </p>
   

2. 接下来我们分析一系列航空公司网络。我们考虑刘家在欧洲机场之间运营的航空公司：四家商业航空公司、两家货运航空公司。每家航空公司对应一个`Graph`，其中顶点表示机场/目的地、边表示机场之间的直飞航班。

   这里一共有 `45` 个机场，我们对其标记为枢纽机场`hub airport`、区域枢纽、商业枢纽、重点城市等几个类别，这些类别作为顶点的真实结构角色。

   对每个`Graph`我们学习图中每个机场的 `embedding`，然后将来自不同`Graph` 的同一个机场的 `embedding` 进行拼接，然后使用 `t-SNE` 可视化。我们还将拼接后的 `embedding` 作为 `agglomerative` 聚类的输入，然后评估同质性、完整性、轮廓系数。

   结果如下所示：

   - `struc2vec` 学习的 `embedding` 捕获了不同的航空公司，可以看到各航空公司的顶点几乎各自聚在一起。

     这表明`struc2vec` 无法在不同的航空公司网络之间泛化，因此无法识别那些结构等效、但是不属于同一个航空公司的机场。

   - `RolX` 学到的 `embedding` 的 `t-SNE`投影几乎没有任何明显的模式。

   - `GraphWave` 可以学到顶点的结构等效性，即使这些顶点位于不同的图上。这表明 `GraphWave` 能够学到针对真实世界网络有意义的结构嵌入。

   - 从上表可以看到，`GraphWave` 在聚类的所有三个指标上都优于其它方法。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/pft99BaCaABl.png?imageslim">
   </p>
   

## 十五、NetMF

1. 网络 `embedding` 问题通常形式化为如下问题：给定一个无向带权图$G=(V,E,\mathbf A)$，其中$V$为顶点集合、$E$为边集合、$\mathbf A$为邻接矩阵，任务的目标是学习一个映射$V\rightarrow \mathbb R^d$，该映射将每个顶点映射到一个能够捕获该顶点结构属性的$d$维向量。

   目前已有一些方法，如`DeepWalk,LINE,PTE,node2vec` ，它们在实践中得到了有效的证明，但是背后的理论机制尚不了解。

   事实上在 `Word Embedding` 任务中，带负采样的 `SkipGram` 模型已经被证明等价于一个 `word-context` 矩阵的隐式分解，但是还不清楚`word-context` 矩阵和网络结构之间的关系。另外，尽管 `DeepWalk,LINE,PTE,node2vec` 之间看起来很相似，但是缺乏对其底层连接更深入的理解。

   论文`《Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec》` 证明了 `DeepWalk,LINE,PTE,node2vec` 在理论上等价于隐式矩阵分解，并给出了每个模型的矩阵形式的闭式解。另外论文还发现：

   - 当上下文窗口大小$T=1$时，`LINE` 可以被视为是 `DeepWalk`的特例。
   - `PTE` 作为 `LINE` 的扩展，实际上它是多个网络联合矩阵的隐式分解。
   - `DeepWalk` 的隐式矩阵分解和图拉普拉斯算子之间存在理论联系，基于这种联系作者提出了一个新的算法 `NetMF` 来近似 `DeepWalk` 隐式矩阵分解的闭式解。

   最后作者使用 `SVD` 对每个算法的矩阵进行显式分解，通过实验证明了 `NetMF` 优于其它的几个模型。

### 15.1 模型

#### 15.1.1 LINE

1. 给定一个带权无向图$G=(V,E,\mathbf A)$，`LINE(2nd)` 任务是学到两个`representation` 矩阵 ：

   - `vertex represetation` 矩阵$\mathbf X \in \mathbb R^{|V|\times d}$：第$i$行$\mathbf{\vec x}_i$为顶点$v_i$作为`vertex`时的 `embedding` 向量。
   - `context representation` 矩阵$\mathbf Y \in \mathbb R^{|V|\times d}$：第$i$行$\mathbf{\vec x}_i$为顶点$v_i$作为`contex`时的 `embedding` 向量。

   `LINE(2nd)` 的目标函数为：

   $\mathcal L = \sum_{i=1}^{|V|}\sum_{j=1}^{|V|}A_{i,j} \left(\log \sigma(\mathbf{\vec x}_i\cdot \mathbf{\vec y}_j) + b \mathbb E_{j^\prime \sim P_N}[\log \sigma(-\mathbf{\vec x}_i\cdot \mathbf{\vec y}_{j^\prime})]\right)$

   其中：

   - $\sigma(\cdot)$为 `sigmoid` 函数

   - $b$为负采样系数

   - $P_N$为用于产生负样本的 `noise` 分布，在 `LINE` 原始论文中使用经验分布：$P_N(j)\propto d_j^{3/4}$，其中$d_j$为顶点 `j` 的加权 `degree` ：

     $d_j = \sum_{k=1}^{|V|}A_{j,k}$

2. 本文我们选择$P_N(j)\propto d_j$，因为这种形式的经验分布将得到一个闭式解。定义$\text{vol}_G = \sum_{i=1}^{|V|}\sum_{j=1}^{|V|}A_{i,j}=\sum_{j=1}^{|V|}d_j$为所有顶点的加权 `degree` 之和，则有：

   $P_N(j) = \frac{d_j}{\text{vol}_G}$

   我们重写目标函数为：

   $\mathcal L = \sum_{i=1}^{|V|}\sum_{j=1}^{|V|} A_{i,j}\log \sigma(\mathbf{\vec x}_i\cdot \mathbf{\vec y}_j) + b\sum_{i=1}^{|V|} d_i \mathbb E_{j^\prime \sim P_N}[\log \sigma(-\mathbf{\vec x}_i\cdot \mathbf{\vec y}_{j^\prime})]$

   我们在图$G$的所有顶点上计算，从而得到期望为：

   $\mathbb E_{j^\prime \sim P_N}[\log \sigma(-\mathbf{\vec x}_i\cdot \mathbf{\vec y}_{j^\prime})] = \sum_{j=1 }^{|V|} \frac{d_{j }}{\text{vol}_G} \log \sigma(-\mathbf{\vec x}_i\cdot \mathbf{\vec y}_{j })$

   因此有：

   $\mathcal L = \sum_{i=1}^{|V|}\sum_{j=1}^{|V|}\left(A_{i,j}\log\sigma(\mathbf{\vec x}_i\cdot \mathbf{\vec y}_j) + b \frac{d_id_j}{\text{vol}_G}\log \sigma(\mathbf{-\vec x}_i\cdot \mathbf{\vec y}_j)\right)$

   则对于每一对顶点 `(i,j)`，其局部目标函数`local objective function` 为：

   $\mathcal L(i,j) = A_{i,j}\log \sigma(\mathbf{\vec x}_i\cdot \mathbf{\vec y}_j) + b \frac{d_id_j}{\text{vol}_G}\log \sigma(\mathbf{-\vec x}_i\cdot \mathbf{\vec y}_j)$

   定义$z_{i,j} = \mathbf{\vec x}_i \cdot \mathbf{\vec y}_j$，根据 `《NeuralWord Embedding as Implicit Matrix Factorization》` 的结论：对于一个足够大的 `embedding` 维度， 每个$z_{i,j}$之间可以认为是相对独立的。因此我们有：

   $\frac{\partial \mathcal L}{\partial z_{i,j}} = \frac{\partial \mathcal L(i,j)}{\partial z_{i,j}} = A_{i,j} \sigma(-z_{i,j}) - b \frac{d_id_j}{\text{vol}_G}\sigma(z_{i,j})$

   为求解目标函数极大值，我们令偏导数为零，则有：

   $\exp(2z_{i,j}) - \left(\frac{\text{vol}_GA_{i,j}}{bd_id_j} -1\right)\exp(z_{i,j}) - \frac{\text{vol}_GA_{i,j}}{bd_id_j} = 0$

   这个方程有两个闭式解：

   - $\exp(z_{i,j}) = -1$：其解为虚数，不予考虑。
   - $\exp(z_{i,j}) = \frac{\text{vol}_G A_{i,j}}{bd_id_j}$：有效解。

   因此有：

   $\mathbf{\vec x}_i\cdot \mathbf{\vec y}_j = z_{i,j} = \log \left(\frac{\text{vol}_G A_{i,j}}{bd_id_j}\right)$

   定义对角矩阵$\mathbf D = \text{diag}(d_1,\cdots,d_{|V|})$，则 `LINE(2nd)` 对应于矩阵分解：

   $\mathbf X\mathbf Y^T = \log(\text{vol}_G \mathbf D^{-1}\mathbf A \mathbf D^{-1}) - (\log b)\mathbf I$

   .

#### 15.1.2 PTE

1. `PTE` 将文本网络分为三个子网络，假设单词集合为$\mathbb V$，文档集合为$\mathbb D$，标签集合为$\mathbb L$：

   - `word-word` 子网$G^{w,w}$：每个 `word` 是一个顶点，边的权重为两个 `word` 在大小为 `T` 的窗口内共现的次数。

     假设其邻接矩阵为$\mathbf A^{w,w}$，定义$d_{i\cdot,}^{w,w} = \sum_j A_{i,j}^{w,w}$为$\mathbf A^{w,w}$第$i$行的元素之和， 定义$d_{\cdot,j}^{w,w} = \sum_i A_{i,j}^{w,w}$为$\mathbf A^{w,w}$第$j$列的元素之和。由于$G^{w,w}$为无向图，因此$\mathbf A^{w,w}$为对称矩阵，所以有$d^{w,w}_{i,\cdot} = d^{w,w}_{\cdot,i}$。

     定义对角矩阵$\mathbf D_{row}^{w,w} = \text{diag}(d^{w,w}_{1,\cdot},\cdots,d^{w,w}_{|\mathbb V|,\cdot})$，$\mathbf D_{col}^{w,w} = \text{diag}(d^{w,w}_{\cdot,1},\cdots,d^{w,w}_{\cdot,|\mathbb V|})$，它们分别由$\mathbf A^{w,w}$的各行之和、各列之和组成。

   - `word-document` 子网$G^{d,w}$：每个 `word` 和 `document` 都是一个顶点，边的权重是`word` 出现在文档中的次数。它是一个二部图，因此$\mathbf A^{w,d}$不是对称矩阵，因此$d_{i,\cdot}^{w,d}\ne d_{\cdot,i}^{w,d}$。

     同样的我们定义对角矩阵$\mathbf D_{row}^{w,d} = \text{diag}(d^{w,d}_{1,\cdot},\cdots,d^{w,d}_{|\mathbb V|,\cdot})$，$\mathbf D_{col}^{w,d} = \text{diag}(d^{w,d}_{\cdot,1},\cdots,d^{w,d}_{\cdot,|\mathbb D|})$，它们分别由$\mathbf A^{w,d}$的各行之和、各列之和组成。

   - `word-label` 子网$G^{w,l}$：每个 `word` 和 `label` 都是一个顶点，边的权重为 `word` 出现在属于这个 `label` 的文档的篇数。它也是一个二部图，因此$\mathbf A^{w,l}$不是对称矩阵，因此$d_{i,\cdot}^{w,l}\ne d_{\cdot,i}^{w,l}$。

     同样的我们定义对角矩阵$\mathbf D_{row}^{w,l} = \text{diag}(d^{w,l}_{1,\cdot},\cdots,d^{w,l}_{|\mathbb V|,\cdot})$，$\mathbf D_{col}^{w,l} = \text{diag}(d^{w,l}_{\cdot,1},\cdots,d^{w,l}_{\cdot,|\mathbb L|})$，它们分别由$\mathbf A^{w,d}$的各行之和、各列之和组成。

   `PTE` 的损失函数为：

   其中$(\cdot)^{w,w},(\cdot)^{w,d},(\cdot)^{w,l}$分别为三个子网中的量，$\alpha,\beta,\gamma$为三个超参数来平衡不同子网的损失，$b$为负采样系数。

   根据前面的结论有：

   令：

   则有$\mathbf X \in \in \mathbb R^{3|\mathbb V|\times 3d},\mathbf Y \in \mathbb R^{(|\mathbb V|+|\mathbb D| +|\mathbb L|)\times 3d}$，且有：

2. 根据 `PTE` 论文，$\alpha,\beta,\gamma$需要满足：

   $\alpha \text{vol}_{G_{w,w}} = \beta \text{vol}_{G_{w,d}} = \gamma \text{vol}_{G_{w,l}}$

   这是因为`PTE` 在训练期间执行边采样，其中边是从三个子网中交替采样得到。

#### 15.1.3 DeepWalk

1. `DeepWalk` 首先通过在图上执行随机游走来产生一个 `corpus`$\mathcal D$，然后在$\mathcal D$上训练 `SkipGram` 模型。这里我们重点讨论带负采样的 `SkipGram` 模型`skipgram with negative sampling:SGNS` 。整体算法如下所示：

   - 输入：
     - 图$G(V,E,\mathbf A)$
     - 窗口大小$T$
     - 随机游走序列长度$L$
     - 总的随机游走序列数量$N$
   - 输出：顶点的 `embedding` 矩阵$\mathbf X$
   - 算法步骤：
     - 迭代：$\text{for}\; s = 1,2,\cdots,N$，迭代过程为：
       - 根据先验概率分布$P(w)$随机选择一个初始顶点$w^{}_1$。
       - 在图$G$上从初始顶点$w^{}_1$开始随机游走，采样得到一条长度为$L$的顶点序列$(w_1^{},\cdots,w_L^{})$。
       - 统计顶点共现关系。对于窗口位置$j=1,2,\cdots,L-T$：
         - 考虑窗口内第$r$个顶点$r=1,2,\cdots,T$：
           - 添加 `vertex-context`顶点对$(w_j^{},w_{j+r}^{})$到$\mathcal D$中。
           - 添加 `vertex-context`顶点对$(w_{j+r} ^{},w_{j}^{})$到$\mathcal D$中。
     - 然后在$\mathcal D$上执行负采样系数为$b$的 `SGNS` 。

2. 根据论文 `《NeuralWord Embedding as Implicit Matrix Factorization》` ， `SGNS` 等价于隐式的矩阵分解：

   其中：$|\mathcal D|$为语料库大小；$n(w,c)$为语料库$\mathcal D$中`vertex-context`$(w,c)$共现的次数；$n(w)$为语料库中 `vertex`$w$出现的总次数；$n(c)$为语料库中 `context`$c$出现的总次数；$b$为负采样系数。

3. 接下来的分析依赖于一些关键的假设：

   - 假设图$G$为无向的`undirected`、连接的`connected`、`non-bipartite` 的，这使得$P(w) = d_w/\text{vol}_G$为一个平稳分布。
   - 假设每个随机游走序列的第一个顶点从这个平稳分布$P(w)$中随机选取。

   对于$r=1,2,\cdots,T$，定义：

   - $\mathcal D_{r\rightarrow} = \{(w,c)\mid (w,c)\in \mathcal D, w = w_j^{},c = w_{j+r}^{}\}$，即$\mathcal D_{r\rightarrow}$为$\mathcal D$的子集，它的每个 `context` 顶点$c$都在每个 `vertex` 顶点$w$的右侧$r$步。

     另外定义$n(w,c)_{r\rightarrow}$为$\mathcal D_{r\rightarrow}$中`vertex-context`$(w,c)$共现的次数。

   - $\mathcal D_{r\leftarrow} = \{(w,c)\mid (w,c)\in \mathcal D, w = w_{j+r}^{},c = w_{j}^{}\}$， 即$\mathcal D_{r\rightarrow}$为$\mathcal D$的子集，它的每个 `context` 顶点$c$都在每个 `vertex` 顶点$w$的左侧$r$步。

     另外定义$n(w,c)_{r\leftarrow}$为$\mathcal D_{r\leftarrow}$中`vertex-context`$(w,c)$共现的次数。

   - 定义$\mathbf P = \mathbf D^{-1}\mathbf A$，$\mathbf P^r = \underbrace {\mathbf P\times \cdots \times \mathbf P}_r$（$r$个$\mathbf P$相乘），其中对角矩阵$\mathbf D = \text{diag}(d_1,\cdots,d_{|V|})$，$d_w = \sum_{c}A_{w,c}$，$\text{vol}_G=\sum_{w}\sum_{c}A_{w,c}$。定义$P^r_{w,c}$为$\mathbf P^r$的第$w$行、第$c$列。

     事实上$\mathbf P$定义了一个概率转移矩阵，$P_{w,c}$定义了从顶点$w$经过一步转移到$c$的概率；$P^r_{w,c}$定义了从顶点$w$经过$r$步转移到$c$的概率。

   另外考虑到对称性，我们有$\mathbf A= \mathbf A^{\top}$。

4. 定理一：定义，则当$L\rightarrow \infty$时有：

   其中：$\stackrel{p}{\rightarrow}$表述依概率收敛。

   其物理意义为：

   - 在所有正向转移过程中，`vertex-context`$(w,c)$在语料库中出现的概率等于：$w$出现的概率，乘以从$w$正向转移$r$步骤到达$c$的概率。
   - 在所有正向转移过程中，`vertex-context`$(c,w)$在语料库中出现的概率等于：$c$出现的概率，乘以从$c$正向转移$r$步骤到达$w$的概率。

   > 因为$w,c$相互之间构成对方的上下文，所以这里的结论是对称的。

   证明：

   首先介绍 `S.N. Bernstein` 大数定律：设$Y_1,Y_2,\cdots$为一个随机变量的序列，其中每个随机变量具有有限的期望$\mathbb E[Y_j]\lt K$和有限的方差$\text{Var} (Y_j) \lt K$，并且协方差满足：当$|i-j|\rightarrow \infty$时，$\text{Cov}(Y_i,Y_j)\rightarrow 0$。则大数定律 `law of large numbers:LLN` 成立。

   考虑只有一条随机游走序列（即$N=1$）：$(w_1,\cdots,w_L)$。给定一个 `vertex-context`$(w,c)$，定义随机变量$Y_j,j=1,2,\cdots,L-T$为事件$w_j=w,w_{j+r} = c$发生的指示器 `indicator` 。

   我们观察到：

   - $|\mathcal D_{r\rightarrow}| = L-T$，$\sum_{j=1}^{L-T} Y_j = n(w,c)_{r\rightarrow}$。因此有：

     $\frac{n(w,c)_{r\rightarrow}}{|\mathcal D_{r\rightarrow}|} = \frac{1}{L-T} \sum_{j=1}^{L-T} Y_j$

   - 基于我们对图的假设和随机游走的假设，则有：$Y_j$发生的概率等于$w_j=w$的概率乘以$w$经过$r$步转移到$c$的概率。即：

     $\mathbb E[Y_j] = P(Y_j) = \frac{d_w}{\text{vol}_G}\times P^r_{w, c}$

   - 基于我们对图的假设和随机游走的假设，则有：当$j\gt i+r$时有：

     其中：第一项为$w_i$采样到$w$的概率；第二项为从$w$经过$r$步转移到$c$的概率；第三项为从$c$经过$j-(r+i)$步转移到$w$得概率；第四项为从$w$经过$r$步转移到$c$的概率。

   则有：

   当$|j-i|\rightarrow \infty$时，从$c$经过$\infty$步转移到$w$的概率收敛到它的平稳分布，即$p(w_j=w)$。即：

   $\lim_{|j-i|\rightarrow\infty}P^{j-i-r}_{c,w} = \frac{d_w}{\text{vol}_G}$

   因此有$\lim_{|j- i|\rightarrow \infty} \text{Cov}(Y_i,Y_j) = 0$。因此随机游走序列收敛到它的平稳分布。

   应用大数定律，则有：

   $\frac{n(w,c)_{r\rightarrow}}{|\mathcal D_{r\rightarrow}|} = \frac{1}{L-T} \sum_{j=1}^{L-T} Y_j \stackrel{p}{\rightarrow} \frac{1}{L-T}\sum_{j=1}^{L-T} \mathbb E[Y_j] = \frac{d_w}{\text{vol}_G}P^r_{w,c}$

   类似地，我们有：

   $\frac{n(w,c)_{r\leftarrow}}{|\mathcal D_{r\leftarrow}|} = \frac{d_c}{\text{vol}_G}P^r_{c,w}$

   当$N\gt 1$时，我们定义$Y_j^{},s=1,2,\cdots,N,j=1,2,\cdots,L-T$为事件$w_j^{} = w, w_{j+r}^{(s)} = c$的指示器，同样可以证明相同的结论。

5. 事实上如果随机游走序列的初始顶点分布使用其它分布（如均匀分布），则可以证明：当$j\rightarrow \infty$时，有：

   $\lim_{j\rightarrow\infty} P(w_j=w, w_{j+r} = c) = \frac{d_w}{\text{vol}_G}P^r_{w,c}$

   因此定理一仍然成立。

6. 定理二：当$L\rightarrow\infty$时，有：

   $\frac{n(w,c)}{|\mathcal D|}\stackrel{p}{\rightarrow}\frac{1}{2T}\sum_{r=1}^T\left(\frac{d_w}{\text{vol}_G}P^r_{w,c} + \frac{d_c}{\text{vol}_G}P^r_{c,w}\right)$

   证明：

   注意到$\frac{|\mathcal D_{r\rightarrow}|}{|\mathcal D|}=\frac{|\mathcal D_{r\leftarrow}|}{|\mathcal D|} = \frac{1}{2T}$，应用定理一有：

   进一步的，考察$w$的边际分布和$c$的边际分布，当$L\rightarrow \infty$时，我们有：

   $\frac{n(w)}{|\mathcal D|} \stackrel{p}{\rightarrow} \frac{d_w}{\text{vol}_G},\quad \frac{n(c)}{|\mathcal D|} \stackrel{p}{\rightarrow} \frac{d_c}{\text{vol}_G}$

7. 定理三：在 `DeepWalk` 中，当$L\rightarrow \infty$时有：

   $\frac{n(w,c)|\mathcal D|}{n(w)n(c)}\stackrel{p}{\rightarrow} \frac{\text{vol}_G}{2T}\left(\frac{1}{d_c}\sum_{i=1}^TP^r_{w,c} +\frac{1}{d_w}\sum_{i=1}^TP^r_{c,w} \right)$

   因此`DeepWalk` 等价于因子分解：

   $\mathbf X \mathbf Y^{\top} = \log\left(\frac{\text{vol}_G}{T}\left(\sum_{r=1}^T \mathbf P^r\right)\mathbf D^{-1}\right)- (\log b)\mathbf I$

   证明：

   利用定理二和`continous mapping theorem`，有：

   写成矩阵的形式为：

8. 事实上我们发现，当$T=1$时，`DeepWalk` 就成为了 `LINE(2nd)`， 因此 `LINE(2nd)` 是 `DeepWalk` 的一个特例。

#### 15.1.4 node2vec

1. `node2vec` 是最近提出的 `graph embedding` 方法，其算法如下：

   - 输入：

     - 图$G(V,E,\mathbf A)$
     - 窗口大小$T$
     - 随机游走序列长度$L$
     - 总的随机游走序列数量$N$

   - 输出：顶点

   - 算法步骤：

     - 构建转移概率张量$\mathbf P\in \mathbb R^{|V| \times |V|\times |V|}$
     - 迭代：$\text{for}\; s = 1,2,\cdots,N$，迭代过程为：
       - 根据先验概率分布$Q(w_1,w_2)$随机选择初始的两个顶点$(w^{}_1,w_2^{})$。
       - 在图$G$上从初始顶点$w^{}_1,w^{}_2$开始二阶随机游走，采样得到一条长度为$L$的顶点序列$(w_1^{},\cdots,w_L^{})$。
       - 统计顶点共现关系。对于窗口位置$j=2,\cdots,L-T$：
         - 考虑窗口内第$r$个顶点$r=1,2,\cdots,T$：
           - 添加三元组$(w_j^{},w_{j+r}^{},w_{j-1}^{})$到$\mathcal D$中。
           - 添加三元组$(w_{j+r} ^{},w_{j}^{},w_{j-1}^{})$到$\mathcal D$中。
     - 然后在$\mathcal D^\prime=\{(w,c)\mid (w,c,u)\in \mathcal D \}$上执行负采样系数为$b$的 `SGNS` 。

     注意：这里为了方便分析，我们使用三元组$(w_j,w_{j+r},w_{j-1})$，而不是`vertex-context` 二元组。

2. `node2vec` 的转移概率张量$\mathbf P$采取如下的方式定义：

   - 首先定义未归一化的概率：

     其中$\hat P_{w,v,u}$表示在$w_{j-1}=u,w_j = v$的条件下，$w_{j+1} = w$的概率。

   - 然后得到归一化的概率：

     $P_{w,v,u} = P(w_{j+1}=w\mid w_j=v, w_{j-1} = u) = \frac{\hat P_{w,v,u}}{\sum_{u^\prime}\hat P_{w,v,u^\prime}}$

3. 类似 `DeepWalk` ，我们定义：

   这里$u$为`previous` 顶点。

   定义$n(w,c,u)_{\rightarrow}$为$(w,c,u)$出现在$\mathcal D_{r\rightarrow}$中出现的次数；定义$n(w,c,u)_{\leftarrow}$为$(w,c,u)$出现在$\mathcal D_{r\leftarrow}$中出现的次数。

   定义二阶随机游走序列的平稳分布为$\mathbf Q$，它满足：$\sum_{u} P_{w,v,u}Q_{v,u} = Q_{w,v}$。根据 `Perron-Frobenius` 定理，这个平稳分布一定存在。为了便于讨论，我们定义每个随机游走序列的初始两个顶点服从平稳分布$\mathbf Q$。

   定义高阶转移概率矩阵$P^r_{w,v,u} = P(w_{j+r}=w\mid w_j=v,w_{j-1}=u)$。

   由于篇幅有限，这里给出 `node2vec` 的主要结论，其证明过程类似 `DeepWalk` ：

   因此 `node2vec` 有：

   $\frac{n(w,c)\times |\mathcal D|}{n(w)\times n(c)}\stackrel{p}{\rightarrow } \frac{\frac{1}{2T}\sum_{r=1}^T \left(\sum_u Q_{w,u}P^r_{c,w,u} + \sum_u Q_{c,u}P^r_{w,c,u}\right)}{ \left(\sum_u Q_{w,u}\right)\times \left(\sum_u Q_{c,u}\right)}$

   尽管实现了 `node2vec` 的封闭形式，我们将其矩阵形式的公式留待以后研究。

4. 注意：存储和计算转移概率张量$\mathbf P^r$以及对应的平稳分布代价非常高，使得我们难以对完整的二阶随机游走动力学过程建模。但是最近的一些研究试图通过对$\mathbf Q$进行低秩分解来降低时间复杂度和空间复杂度：

   $Q_{u,v} = \mathbf{\vec Q}_u\cdot \mathbf{\vec Q}_v$

   由于篇幅限制，我们这里主要集中在一阶随机游走框架`DeepWalk` 上。

#### 15.1.5 NetMF

1. 根据前面的分析我们将 `LINE,PTE,DeepWalk,node2vec` 都统一到矩阵分解框架中。这里我们主要研究 `DeepWalk` 矩阵分解，因为它比 `LINE` 更通用、比 `node2vec` 计算效率更高。

2. 首先论文引用了四个额外的定理：

   - 定理四：定义归一化的图拉普拉斯矩阵为$\mathbf L = \mathbf I - \mathbf D^{-1/2}\mathbf A \mathbf D^{-1/2}$，则它的特征值都是实数。

     而且，假设它的特征值从大到小排列 ，则有：

     $2\ge\lambda_1\ge\lambda_2\ge \cdots\ge\lambda_n = 0$

     进一步的，假设该图是连通图 （`connected`），并且顶点数量$n\gt 1$，则有：$\lambda_1\ge \frac{n}{n-1}$。

     证明参考：`Fan RK Chung. 1997. Spectral graph theory. Number 92. American Mathematical Soc.`

   - 定理五：实对称矩阵的奇异值就是该矩阵特征值的绝对值。

     证明参考：`Lloyd N Trefethen and David Bau III. 1997. Numerical linear algebra. Vol. 50. Siam.`

   - 定理六：假设$\mathbf B\in \mathbb R^{n\times n} ,\mathbf C \in \mathbb R^{n\times n}$为两个对称矩阵，假设将$\mathbf B,\mathbf C,\mathbf {BC}$的奇异值按照降序排列，则对于任意$1\le i,j\le n,i+j\le n+1$，以下不等式成立：

     $\sigma_{i+j-1}(\mathbf B\mathbf C) \le \sigma_i(\mathbf B)\times \sigma_j(\mathbf C)$

     其中$\sigma_i(\cdot)$为对应矩阵的奇异值根据降序排列之后的第$i$个奇异值。

     证明参考：`Roger A. Horn and Charles R. Johnson. 1991. Topics in Matrix Analysis. Cambridge University Press. https://doi.org/10.1017/CBO9780511840371`

   - 定理七：给定一个实对称矩阵$\mathbf A$，定义它的瑞利商为：

     $R(\mathbf A,\mathbf{\vec x}) = \frac{\mathbf{\vec x}\cdot (\mathbf A \mathbf{\vec x})}{\mathbf{\vec x}\cdot \mathbf{\vec x}}$

     假设$\mathbf A$的特征值从大到小排列为$\lambda_1\ge\cdots\ge\lambda_n$，则有：

     $\lambda_n = \min_{\mathbf{\vec x}\ne \mathbf{\vec 0}} R(\mathbf A,\mathbf{\vec x}),\quad \lambda_1 = \max_{\mathbf{\vec x}\ne \mathbf{\vec 0}} R(\mathbf A,\mathbf{\vec x})$

     证明参考：`Lloyd N Trefethen and David Bau III. 1997. Numerical linear algebra. Vol. 50. Siam.`

3. 考察 `DeepWalk` 的矩阵分解：

   $\mathbf X \mathbf Y^{\top} = \log\left(\frac{\text{vol}_G}{T}\left(\sum_{r=1}^T \mathbf P^r\right)\mathbf D^{-1}\right)- (\log b)\mathbf I$

   忽略常量以及 `element-wise` 的 `log` 函数，我们关注于矩阵：$\frac{1}{T}\left(\sum_{r=1}^T \mathbf P^r\right)\mathbf D^{-1}$。

   根据定理四，实对称矩阵$\mathbf D^{-1/2}\mathbf A\mathbf D^{-1/2} = \mathbf I - \mathbf L$存在特征值分解$\mathbf U\mathbf\Lambda \mathbf U^\top$，其中$\mathbf U$为正交矩阵、$\mathbf\Lambda=\text{diag}(\lambda_1,\cdots,\lambda_n)$为特征值从大到小构成的对角矩阵。

   根据定理三可知，$1=\lambda_1\ge \lambda_2\cdots\ge\lambda_n\ge -1$，并且$\lambda_n\lt -\frac{1}{n-1}\lt0$。

   考虑到$\mathbf P = \mathbf D^{-1}\mathbf A= \mathbf D^{-1/2}\mathbf A\mathbf D^{-1/2}$，因此有：

   $\frac{1}{T}\left(\sum_{r=1}^T \mathbf P^r\right)\mathbf D^{-1} = \left(\mathbf D^{-1/2}\right)\left(\mathbf U\left(\frac{1}{T}\sum_{r=1}^T \mathbf\Lambda^r\right) \mathbf U^\top\right)\left(\mathbf D^{-1/2}\right)$

   - 我们首先分析$\mathbf U\left(\frac{1}{T}\sum_{r=1}^T \mathbf\Lambda^r\right) \mathbf U^\top$的谱。显然，它具有特征值：

     $\left\{\frac 1T \sum_{r=1}^T \lambda_1^r,\; \frac 1T \sum_{r=1}^T \lambda_2^r,\;\cdots,\;\frac 1T \sum_{r=1}^T \lambda_i^r,\;\cdots\;\frac 1T \sum_{r=1}^T \lambda_n^r\right\}$

     这可以视为对$\mathbf D^{-1/2}\mathbf A\mathbf D^{-1/2}$的特征值$\lambda_i$进行一个映射$f(x) = \frac 1T\sum_{r=1}^T x^r$。这个映射可以视为一个滤波器，滤波器的效果如下图所示。可以看到：

     - 滤波器倾向于保留正的、大的特征值。
     - 随着窗口大小$T$的增加，这种偏好变得更加明显。

     即：随着$T$的增加，滤波器尝试通过保留较大的、正的特征值来近似低阶半正定矩阵。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/E3zCHSeHyubU.png?imageslim">
     </p>
     

   - 然后我们分析$\frac{1}{T}\left(\sum_{r=1}^T \mathbf P^r\right)\mathbf D^{-1}$的谱。

     根据定理五，矩阵$\mathbf U\left(\frac{1}{T}\sum_{r=1}^T \mathbf\Lambda^r\right) \mathbf U^\top$的奇异值可以根据其特征值的绝对值得到。我们将$|\frac 1T \sum_{r=1}^T \lambda_i^r|,i=1,2,\cdots,n$进行降序排列，假设排列的顺序为$\{p_1,p_2,\cdots,p_n\}$，则有：

     $\left|\frac 1T \sum_{r=1}^T \lambda_{p_1}^r\right|\ge \left|\frac 1T \sum_{r=1}^T \lambda_{p_2}^r\right|\ge\cdots\ge \left|\frac 1T \sum_{r=1}^T \lambda_{p_n}^r\right|$

     考虑到每个$d_i$都是正数，因此我们可以将$\mathbf D^{-1/2}$的奇异值根据特征值$\frac{1}{\sqrt d_i}$降序排列。假设排列的顺序为$\{q_1,q_2,\cdots,q_n\}$，则有：

     $\frac{1}{\sqrt{d_{q_1}}}\ge \frac{1}{\sqrt{d_{q_2}}}\ge\cdots\ge\frac{1}{\sqrt{d_{q_n}}}$

     特别的，$d_{q_1} = d_{\min}$为最小的 `degree` 。

     通过应用两次定理五，我们可以发现第$s$个奇异值满足：

     因此$\frac{1}{T}\left(\sum_{r=1}^T \mathbf P^r\right)\mathbf D^{-1}$的第$s$个奇异值的上界为$\frac{1}{d_{\min}}\left|\frac 1T \sum_{r=1}^T\lambda_{p_s}^r\right|$。

     另外，根据瑞利商，我们有：

     应用定理七，我们有：

     $\lambda_{\min}\left(\left(\frac 1T \sum_{r=1}^T \mathbf P^r\right)\mathbf D^{-1}\right) \ge \frac{1}{d_\min}\lambda_\min \left(\mathbf U\left(\frac 1T\sum_{r=1}^T \mathbf\Lambda^r\right)\mathbf U^\top\right)$

4. 为了说明过滤器$f(x) = \frac 1T\sum_{r=1}^T x^r$的效果，我们分析了 `Cora` 数据集对应的引文网络。我们将引文链接视为无向边，并选择最大连通分量 `largest connected component`。

   我们分别给出了$\mathbf D^{-1/2}\mathbf A \mathbf D^{-1/2}$、$\mathbf U\left(\frac {1}{T} \sum_{r=1}^T \mathbf\Lambda^r\right)\mathbf U^\top$以及$\left(\frac 1T \sum_{r=1}^T \mathbf P^r\right)\mathbf D^{-1}$按照降序排列的特征值，其中$T=10$。

   - 对于$\mathbf D^{-1/2}\mathbf A \mathbf D^{-1/2}$，最大的特征值为$\lambda_1= 1$，最小特征值为$\lambda_n = -0.971$。
   - 对于$\mathbf U\left(\frac {1}{T} \sum_{r=1}^T \mathbf\Lambda^r\right)\mathbf U^\top$，我们发现：它的所有负特征值以及一些小的正特征值都被过滤掉了 `filtered out` 。
   - 对于$\left(\frac 1T \sum_{r=1}^T \mathbf P^r\right)\mathbf D^{-1}$，我们发现：
     - 它的奇异值（即特征值的绝对值）被$\mathbf U\left(\frac {1}{T} \sum_{r=1}^T \mathbf\Lambda^r\right)\mathbf U^\top$的奇异值所限制 `bounded` 。
     - 它的最小特征值的被$\mathbf U\left(\frac {1}{T} \sum_{r=1}^T \mathbf\Lambda^r\right)\mathbf U^\top$的特征值所限制 `bounded` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/9vfDoj0BYif7.png?imageslim">
   </p>
   

5. 基于前面的理论分析，我们提出了一个矩阵分解框架 `NetMF` ，它是对 `DeepWalk` 和 `LINE` 的改进。

   为表述方便，我们定义：

   $\mathbf M = \frac{\text{vol}_G}{bT}\left(\sum_{r=1}^T\mathbf P^r\right) \mathbf D^{-1}$

   因此$\mathbf X \mathbf Y^T = \log \mathbf M$对应于 `DeepWalk` 的矩阵分解。

   - 对于很小的$T$，我们直接计算$\mathbf M$并对$\log \mathbf M$进行矩阵分解。

     考虑到直接对$\log \mathbf M$进行矩阵分分解难度很大，有两个原因：

     - 当$M_{i,j} = 0$时，$\log M_{i,j}$的行为未定义。
     - $\log \mathbf M$是一个巨大的稠密矩阵，计算复杂度太高。

     受到 `Shifted PPMI` 启发，我们定义$\mathbf M^\prime = \max(\mathbf M,1)$。这使得$\log \mathbf M^\prime$中每个元素都是有效的，并且$\log \mathbf M^\prime$是稀疏矩阵。然后我们对$\log \mathbf M^\prime$进行奇异值分解，并使用它的 `top`$d$奇异值和奇异向量来构造`embedding` 向量。

   - 对于很大的$T$，直接计算$\mathbf M$的代价太高。我们提出一个近似算法，主要思路是：根据$\mathbf M$和归一化拉普拉斯算子之间的关系来近似$\mathbf M$。

     - 首先我们对$\mathbf D^{-1/2}\mathbf A\mathbf D^{-1/2}$进行特征值分解，通过它的 `top`$h$个特征值和特征向量$\mathbf U_h\mathbf\Lambda_h\mathbf U_h^{\top}$来逼近$\mathbf D^{-1/2}\mathbf A\mathbf D^{-1/2}$。

       由于只有 `top`$h$个特征值被使用，并且涉及的矩阵是稀疏的，因此我们可以使用 `Arnoldi` 方法来大大减少时间。

     - 然后我们通过$\hat {\mathbf M} = \frac{\text{vol}_G}{b }\mathbf D ^{-1/2}\mathbf U_h\left(\frac 1T\sum_{r=1}^T\mathbf \Lambda_h^r\right) \mathbf U_h^\top\mathbf D^{-1/2}$来逼近$\mathbf M$。

6. `NetMF` 算法：

   - 输入：

     - 图$G(V,E,\mathbf A)$
     - 窗口大小$T$

   - 输出：顶点的 `embedding` 矩阵$\mathbf X$

   - 算法步骤：

     - 如果$T$较小，则计算：

       如果$T$较大，则执行特征值分解：$\mathbf D^{-1/2}\mathbf A \mathbf D^{-1/2} \simeq \mathbf U_h\mathbf\Lambda_h\mathbf U_h^\top$。然后计算：

     - 执行$d$维的 `SVD`分解：$\log \mathbf M^\prime = \mathbf U_d \Sigma_d \mathbf V_d^\top$或者$\log \hat{\mathbf M}^\prime = \mathbf U_d \Sigma_d \mathbf V_d^\top$。

     - 返回$\mathbf U_d\sqrt\Sigma_d$作为网络 `embedding`。

7. 对于较大的$T$，可以证明$\hat{\mathbf M}$逼近$\mathbf M$的误差上界，也可以证明$\log \hat{\mathbf M}^\prime$逼近$\log \mathbf M^\prime$的误差上界。

   定理八：令$||\cdot||_F$为矩阵的 `Frobenius` 范数，则有：

   证明：

   - 第一个不等式：可以通过 `F` 范数的定义和前面的定理七来证明。

   - 第二个不等式：不失一般性我们假设$M^\prime_{i,j}\le \hat M_{i,j}^\prime$，则有：

     第一步成立是因为： 对于$x\ge 0$有$\log (1+x)\le x$；第二步成立是因为：$M_{i,j}^\prime = \max(M_{i,j},1) \ge 1$。因此有$\left\|\log \mathbf M^\prime - \log \hat{\mathbf M}^\prime\right\|_F \le \left\|\mathbf M^\prime - \hat{\mathbf M}^\prime\right\|_F$。

     另外，根据$\mathbf M^\prime$和$\hat{\mathbf M}^\prime$的定义有：

     $\left|M_{i,j}^\prime - \hat M_{i,j}^\prime\right| = \left|\max(M_{i,j},1) - \max(\hat M_{i,j},1)\right|\le \left|M_{i,j} - \hat M_{i,j}\right|$

     因此有：$\left\|\mathbf M^\prime - \hat{\mathbf M}^\prime\right\|_F \le \left\|\mathbf M - \hat{\mathbf M}\right\|_F$。

8. `DeepWalk` 尝试通过随机游走来对顶点抽样，从而期待用经验分布来逼近真实的 `vertex-context` 分布。尽管大数定律可以保证这种方式的收敛性，但是实际上由于真实世界网络规模较大，而且实际随机游走的规模有限（随机游走序列的长度、序列的数量），因此经验分布和真实分布之间存在`gap` 。这种 `gap` 会对 `DeepWalk`的性能产生不利影响。

   `NetMF` 通过直接建模真实的 `vertex-context` 分布，从而降低了这种 `gap` ，从而得到比 `DeepWalk` 更好的效果。

### 15.2 实验

1. 作者在多标签顶点分类任务中评估 `NetMF`的性能。

2. 数据集：

   - `BlogCatalog` 数据集：由博客作者提供的社交关系网络。标签代表作者提供的主题类别。
   - `Flickr` 数据集：`Flickr`网站用户之间的关系网络。标签代表用户的兴趣组，如“黑白照片”。
   - `Protein-Protein Interactions:PPI`：该数据集包含蛋白质和蛋白质之间的关联，标签代表基因组。
   - `Wikipedia` 数据集：来自维基百科，包含了英文维基百科 `dump` 文件的前$10^9$个字节中的单词共现网络。顶点的标签表示通过`Stanford POS-Tagger`推断出来的单词词性`Part-of-Speech:POS` 。

3. `Baseline` 模型：我们将 `NetMF(T=1)`、`NetMF(T=10)` 和 `LINE,DeepWalk` 进行比较。

   - 所有模型的 `embedding` 维度都是 `128` 维。
   - 对于 `NetMF(T=10)` ，我们在 `Flickr` 数据集上选择$h=16384$，在其它数据集上选择$h=256$。
   - 对于 `DeepWalk` ，我们选择窗口大小为 `10`、随机游走序列长度 `40`、每个顶点开始的随机游走序列数量为 `80` 。

   我们重点将 `NetMF(T=1)` 和 `DeepWalk` 进行比较，因为二者窗口大小都为 `1` ；重点将 `NetMF(T=10)` 和 `DeepWalk` 进行比较，因为二者窗口大小都为 `10` 。

4. 和 `DeepWalk` 相同的实验步骤，我们首先训练整个网络的 `embedding`，然后随机采样一部分标记样本来训练一个`one-vs-rest` 逻辑回归分类模型，剩余的顶点作为测试集。我们评估测试集的 `Micro-F1`指标和 `Macro-F1` 指标。为了确保实验结果可靠，每个配置我们都重复实验 `10`次，并报告测试集指标的均值。

   对于 `BlogCatalog,PPI,Wikipedia` 数据集，我们考察分类训练集占比 `10%~90%`的情况下，各模型的性能；对于 `Flickr`数据集，我们考察分类训练集占比 `1%~10%` 的情况下，各模型的性能。

   完成的实验结果如下图所示。可以看到：`NetMF(T=1)` 相对于 `LINE(2nd)` 取得了性能的提升，`NetMF(T=10)` 相对于 `DeepWalk` 也取得了性能提升。

   - 在 `BlogCatalog,PPI,Flickr` 数据集中，我们提出的 `NetMF(T=10)`比 `Baseline` 性能更好。这证明了`NetMF` 理论的正确性。
   - 在 `Wikipedia` 数据集中，窗口更小的 `NetMF(T=1)` 和 `LINE(2nd)` 效果更好。这表明：短期依赖足以建模 `Wikipedia` 网络结构。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/ba7bvbzb2kia.png?imageslim">
   </p>
   

   如下表所示，大多数情况下当标记数据稀疏时，`NetMF` 方法远远优于 `DeepWalk` 和 `LINE` 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/1vkvkuS3UNSA.png?imageslim">
   </p>
  

## 十六、NetSMF

1. 现有的流行的 `Graph Embedding` 方法在可扩展性方面存在不足。

   - `LINE` 具有很好的可扩展性，这是因为它仅对一阶邻近度和二阶邻近度建模。但这也是它的缺点：学到的 `embedding` 缺失了网络中的高阶邻近关系。
   - `DeepWalk/node2vec` 基于图上的随机游走和较大上下文尺寸的 `SkipGram` 对较远的顶点（即全局网络结构）进行建模，但是它们处理大型网络的计算代价太高。
   - `NetMF` 证明了 `DeepWalk/LINE` 等价于一个显式的矩阵分解，但是这个矩阵是一个$|V|\times |V|$的稠密矩阵，这使得直接构建和分解这个超大型稠密矩阵的代价非常高。

   鉴于这些方法的局限性（如下表所示），论文 `NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization` 提出了 `NetSMF` 模型，该模型具有计算效率高、能捕获全局上下文、能从理论上证明正确性的优点。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/7RhtcS5CHCdR.png?imageslim">
   </p>
   

   `NetSMF` 的基本思想是：通过一个稀疏矩阵来拟合`NetMF` 中的稠密矩阵，从而降低矩阵构建成本和矩阵分解成本。

   `NetSMF` 主要包含三个步骤：

   - 利用谱图稀疏化 `spectral graph sparsification` 技术对网络的随机游走矩阵多项式 `random-walk matrix-polynomial` 找到稀疏化器 `sparsifier`
   - 基于这个稀疏化器构建一个稀疏矩阵，该稀疏矩阵是原始 `NetMF` 稠密矩阵在谱域上的近似，并且具有更少的非零值。
   - 执行 `Randomized SVD` 分解算法从而高效地分解该稀疏矩阵，从而得到网络顶点的 `embedding` 。

   我们设计的 `NetSMF` 稀疏矩阵在谱域上接近原始的 `NetMF` 稠密矩阵，从而保留了足够的网络信息。这使得我们从稀疏矩阵中学到的 `embedding` 几乎和原始`NetMF` 稠密矩阵中学到的 `embedding` 一样强大。

   另外论文证明了通过稀疏矩阵来逼近原始 `NetMF` 稠密矩阵误差的理论上界，从而理论上证明了 `NetSMF` 的有效性。

   通过各种实验表明：和流行的 `Graph Embeding` 方法相比，`NetSMF` 是唯一同时具有效果好、效率高优点的方法。如 `NetSMF` 在保持和 `NetMF` 效果相差无几的条件下，训练速度快了几个量级。

2. 给定无向带权图$G=(V,E,\mathbf A)$，记$n=|V|,m=|E|$。`NetSMF` 证明了当随机游走序列的长度$L\rightarrow \infty$时，`DeepWalk` 隐式的分解矩阵：

   $\mathbf X \mathbf Y^\top = \log^\circ \left(\frac{\text{vol}_G}{b}\mathbf M\right)\\\text{vol}_G = \sum_i\sum_j A_{i,j},\quad d_i=\sum_{j}A_{i,j},\quad \mathbf D = \text{diag}(d_1,d_2,\cdots,d_n)\\ \mathbf M = \frac 1T\sum_{r=1}^T \left(\mathbf D^{-1}\mathbf A\right)^r\mathbf D^{-1}$

   其中$\text{vol}_G$也称作图的 `volume` ，$\log^\circ(\cdot)$为矩阵的逐元素 `log` 。考虑到目标矩阵非零元素太多，以及$\log 0$是未定义行为，因此`NetMF` 的目标矩阵调整为：

   $\text{trunc_log}^\circ \left(\frac{\text{vol}_G}{b}\mathbf M\right)$

   其中：$\text{trunc_log}(x) = \log \max(1,x)$。

   `NetMF` 通过显式分解该矩阵从而得到顶点的 `embedding` ，但是实践中存在两个挑战：

   - 首先，几乎每一对距离$r\le T$内的顶点都对应于 `NetMF` 目标矩阵中的非零项。考虑到很多社交网络和信息网络都具有 `small-world` 属性：在每一个顶点开始，仅需要几步就可以达到绝大多数顶点。如，截至 `2012` 年，`Facebook`中 `92%` 的可达到的顶点对之间的距离小于等于 `5` 。因此，即使设置了一个较小的上下文窗口（如`DeepWalk` 中默认的$T=10$），`NetMF` 中目标矩阵也具有$O(n^2)$个非零元素。而在大型网络中，这种规模矩阵的构建和分解是不现实的。

   - 其次，计算矩阵的幂$\left(\mathbf D^{-1}\mathbf A\right)^r$涉及到矩阵乘法，其算法复杂度为$O(n^3)$。另外分解$n\times n$的稠密矩阵的计算代价也较高。

     为降低构建目标矩阵的成本，`NetMF` 使用`top` 特征值和特征向量来逼近$\mathbf M$。但是这个近似矩阵仍然是稠密的，使得该策略无法处理大型网络。

### 16.1 模型

1. 谱域相似性`Spectral Similarity` ：假设有两个带权无向图$G=(V,E,\mathbf A)$和$\tilde G=(\tilde V,\tilde E,\tilde{\mathbf A})$，令$\mathbf L = \mathbf D_G - \mathbf A$和$\tilde{\mathbf L} = \mathbf D_{\tilde G} - \tilde{\mathbf A}$分别为它们的拉普拉斯矩阵。如果下面的关系成立，我们就说$G$和$\tilde G$是$(1+\epsilon)$谱相似的`spectrally similar` ：

   $\forall \mathbf{\vec x}\in \mathbb R^n,\quad (1-\epsilon) \mathbf{\vec x}^\top \tilde{\mathbf L}\mathbf{\vec x} \le \mathbf{\vec x}^\top \mathbf L\mathbf{\vec x} \le (1+\epsilon) \mathbf{\vec x}^\top\tilde{\mathbf L}\mathbf{\vec x}$

2. 定理九（继续章节`15.1.5` 的定理编号）：对于随机游走多项式 `random-walk molynomial`$\mathbf H = \mathbf D - \sum_{r=1}^T \alpha_r\mathbf D\left(\mathbf D^{-1} \mathbf A\right)^r$，其中$\alpha_r\ge 0, \sum_{r=1}^T\alpha_r = 1$，则可以在$O(T^2m\epsilon^{-2}\log^2n)$时间复杂度内构造一个$(1+\epsilon)$`spectral sparsifier`$\tilde{\mathbf H}$，其中$\tilde{\mathbf H}$具有$O((n\log n )\epsilon^{-2})$个非零项。

   对于无权图，时间复杂度可以进一步降低到$O(T^2m\epsilon^{-2}\log n)$。

   证明参考：`Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. 2015. Efficient sampling for Gaussian graphical models via spectral sparsification. In COLT ’15. 364–390.` 、`Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. 2015. Spectral sparsification of random-walk matrix polynomials. arXiv preprint arXiv:1502.03496 (2015).`

3. 为构建一个包含$O((n\log n )\epsilon^{-2})$个非零项的 `sparsifier`$\tilde{\mathbf H}$，该稀疏化算法包含两步：

   - 第一步获得一个针对$\mathbf H$的初始化 `sparsifier`，它包含$O(Tm\epsilon^{-2}\log n)$个非零项。
   - 第二步使用标准的谱稀疏化算法`spectral sparsification algorithm` 来进一步降低非零项到$O(\epsilon^{-2}n\log n)$。

   本文我们仅仅应用第一步，因为一个包含$O(Tm\epsilon^{-2}\log n)$个非零项的稀疏矩阵已经足够可用，所以我们并没有采用第二步，这能够避免额外的计算。下面讲到的所有随机游走多项式稀疏化算法`random-walk molynomial sparsification algorithm` 都仅包含第一步。

4. 我们取$\alpha_r = \frac 1T$，因此有：

   $\mathbf H = \mathbf D - \frac 1T\sum_{r=1}^T \mathbf D\left(\mathbf D^{-1} \mathbf A\right)^r$

   我们定义：

   $\mathbf M = \frac{1}{T} \left(\sum_{r=1}^T\mathbf (\mathbf D^{-1}\mathbf A )^r \right) \mathbf D^{-1}$

   因此有：$\mathbf M = \mathbf D^{-1}(\mathbf D - \mathbf H)\mathbf D^{-1}$。

   采用$\mathbf H$的稀疏化版本$\tilde{\mathbf H}$，我们定义$\tilde{\mathbf M} = \mathbf D^{-1}(\mathbf D - \tilde{\mathbf H}) \mathbf D^{-1}$。可以发现$\tilde{\mathbf M}$也是一个稀疏矩阵，其非零元素的规模和$\tilde{\mathbf H}$非零元素规模相当。

   最终我们可以分解这个稀疏矩阵从而获得每个顶点的 `embedding` ：

   $\mathbf X \mathbf Y^\top = \text{trunc_log}^\circ \left(\frac{\text{vol}_G}{b}\tilde{\mathbf M}\right)$

5. 我们正式给出 `NetSMF` 算法，该算法包含三个步骤：

   - 随机游走多项式的稀疏化`Random-Walk Molynomial Sparsification` ：我们首先构建一个网络$\tilde G$，它具有和$G(V,E,\mathbf A)$相同的顶点但是不包含任何边。然后我们重复 `Path-Sampling` 路径采样算法$M$次，从而得到$O(M)$条边，这些边构成了$\tilde{\mathbf H}$的非零项。在每一次迭代过程中：

     - 首先均匀随机选择一条边$e=(u,v)\in E$，均匀随机选择一个路径长度$r\in \{1,\cdots,T\}$。

     - 然后从顶点$u$开始进行$k-1$步随机游走，得到顶点序列$(u,u_{k-2},\cdots,u_0)$；从顶点$v$开始进行$r-k$步随机游走，得到顶点序列$(v,u_{k+1},\cdots,u_r)$。这一过程产生了一个长度为$r$的路径$\mathbf p = (u_0,u_1,\cdots,u_r)$。同时我们计算：

       $Z(\mathbf p) = \sum_{i=1}^r\frac{2}{A_{u_{i-1},u_i}}$

     - 最后我们向图$\tilde G$中添加一条新的边$(u_0,u_r)$，其权重为$\frac{2rm}{MZ(\mathbf p)}$。如果边已经存在，则相同的边进行合并（只需要将权重相加即可）。

       这条新的边代表了图$G$中一条长度为$r$的随机游走序列，权重就是转移概率。

     最终我们计算$\tilde G$的未归一化的拉普拉斯矩阵$\tilde {\mathbf H}$，它具有$O(M)$个非零项。

   - 构建 `NetMF` 稀疏器 `sparsifier` ：即计算$\tilde{\mathbf M} = \mathbf D^{-1}(\mathbf D - \tilde{\mathbf H}) \mathbf D^{-1}$。这一步并未改变非零项的规模。

   - 截断的奇异值分解 `truncated singular value decomposition`：对$\text{trunc_log}^\circ(\frac{\text{vol}_G}{b}\tilde{\mathbf M})$执行$d$维的 `RandomizedSVD`分解。

     即使稀疏矩阵仅有$O(M)$个非零元素，执行精确的 `SVD` 仍然非常耗时。这里我们使用 `Randomized SVD` 进行分解。由于篇幅有限我们无法包含更多细节，简而言之，该算法通过高斯随机矩阵将原始矩阵投影到低维空间，然后在$d\times d$维的小矩阵上执行经典的 `SVD` 分解即可。

6. `NetSFM` 算法：

   - 输入：

     - 图$G= (V,E,\mathbf A)$
     - 非零项的数量$M$
     - `embedding` 维度$d$

   - 输出：顶点 `embedding` 矩阵$\mathbf X\in \mathbb R^{|V|\times d}$

   - 算法步骤：

     - 初始化$\tilde G = (V,\mathbf\Phi,\mathbf 0)$，即只有顶点没有边。

     - 迭代$i=1,\cdots,M$，迭代步骤为：

       - 均匀随机选择一条边$e=(u,v)\in E$

       - 均匀随机选择一个长度$r\in \{1,\cdots,T\}$

       - 随机采样一条路径：$u^\prime,v^\prime,Z \leftarrow \text{PathSampling}(e,r)$

       - 添加边$(u^\prime,v^\prime)$到$\tilde G$中，边的权重为$\frac{2rm}{MZ}$。

         如果有多条边相同则合并成一个，将权重直接相加即可。

     - 计算$\tilde G$的未归一化的拉普拉斯矩阵$\tilde{\mathbf H}$

     - 计算$\tilde{\mathbf M} = \mathbf D^{-1}(\mathbf D - \tilde{\mathbf H}) \mathbf D^{-1}$

     - 对$\text{trunc_log}^\circ(\frac{\text{vol}_G}{b}\tilde{\mathbf M})$执行$d$维的 `RandomizedSVD` 分解，得到$\mathbf U_d,\mathbf \Sigma_d,\mathbf{\vec V}_d$。

     - 返回$\mathbf U_d\sqrt{\mathbf\Sigma}_d$。

7. `PathSampling` 算法：

   - 输入：

     - 图$G= (V,E,\mathbf A)$
     - 边$e=(u,v)$
     - 采样的路径长度$r$

   - 输出：路径的初始顶点$u_0$、结束顶点$u_r$、路径$Z$值

   - 算法步骤：

     - 均匀随机采样一个整数$k\in \{1,\cdots,r\}$

     - 从顶点$u$开始执行$(k-1)$步随机游走，采样到的顶点记作$（u,u_{k-2},\cdots, u_0)$。

     - 从顶点$v$开始执行$(r-k)$步随机游走，采样到的顶点记作$(v,u_{k+1},\cdots,u_r)$。

     - 计算整条路径的路径$Z$值：

       $Z= \sum_{i=1}^r\frac{2}{A_{u_{i-1},u_i}}$

     - 返回$(u_0,u_r,Z)$

8. `Randomized SVD` 算法：

   - 输入：
     - 一个稀疏的对称矩阵$\mathbf K = \text{trunc_log}^\circ(\frac{\text{vol}_G}{b}\tilde{\mathbf M})$。我们以行优先的方式存储矩阵，从而充分利用对称性来简化计算。
     - 目标维度$d$
   - 输出：`SVD` 矩阵分解的$\mathbf U_d,\mathbf\Sigma_d,\mathbf V_d$
   - 步骤：
     - 采样一个高斯随机矩阵$\mathbf O\in \mathbb R^{n\times d}$作为投影矩阵。
     - 计算映射后的矩阵$\mathbf Y = \mathbf K^\top\mathbf O = \mathbf K\mathbf O \in \mathbb R^{n\times d}$
     - 对矩阵$\mathbf Y$进行正交归一化
     - 计算矩阵$\mathbf B = \mathbf K\mathbf Y\in \mathbb R^{n\times d}$
     - 采样另一个高斯随机矩阵$\mathbf P\in \mathbb R^{d\times d}$作为投影矩阵。
     - 计算映射后的矩阵$\mathbf Z = \mathbf B \mathbf P\in \mathbb R^{n\times d}$。
     - 对矩阵$\mathbf Z$进行正交归一化。
     - 计算矩阵$\mathbf C = \mathbf Z^\top \mathbf B\in \mathbb R^{d\times d}$。
     - 对$\mathbf C$执行 `Jacobi SVD` 分解：$\mathbf C = \mathbf U\mathbf\Sigma \mathbf V^\top$
     - 返回$\mathbf Z\mathbf U\in \mathbb R^{n\times d},\mathbf\Sigma\in \mathbb R^{d\times d},\mathbf Y \mathbf V\in \mathbb R^{n\times d}$

9. 基于 `SVD` 分解的一个优点是：我们可以通过 `Cattell’s Scree` 测试来选择合适的 `embedding` 维度$d$：通过从大到小来绘制奇异值，然后选择奇异值明显下降、或者奇异值开始趋向于均匀的$d$。

10. `PathSampling` 算法的说明：

    - 引理：给定一个长度为$r$的路径$\mathbf p = (u_0,\cdots,u_r)$，则`PathSampling` 算法采样到该路径的概率为：

      $\pi(\mathbf p) = \frac{w(\mathbf p)Z(\mathbf p)}{2rm}$

      其中：

      $Z(\mathbf p) = \sum_{i=1}^r\frac{2}{A_{u_{i-1},u_i}},\quad w(\mathbf p) = \frac{\prod_{i=1}^rA_{u_{i-1},u_i}}{\prod_{i=1}^{r-1}D_{u_i}}$

    - 引理：假设采样到一个长度为$r$的路径$\mathbf p = (u_0,\cdots,u_r)$，则新的边$(u_0,u_r)$的权重应该为：

      $\frac{w(\mathbf p)}{\pi(\mathbf p)M}$

    - 根据上述引理，则添加到$\tilde G$中的边的权重为：

      $\frac{w(\mathbf p)}{\pi(\mathbf P)M} = \frac{w(\mathbf p)}{(w(\mathbf p)Z(\mathbf p))/(2rm) \times M} = \frac{2rm}{MZ(\mathbf p)}$

      对于无向图，则$Z(\mathbf P) = 2r$，则权重简化为$\frac{m}{M}$。

11. `NetMF` 和 `NetSMF` 之间的主要区别在于对目标矩阵的近似策略。`NetMF` 使用了一个稠密矩阵来近似，从而带来了时间和空间上的挑战；`NetSFM` 基于谱图稀疏化理论和技术，使用了一个稀疏矩阵来近似。

### 16.2 复杂度

1. 算法复杂度：

   - 第一步：我们需要调用 `PathSampling` 算法$M$次，每次 `PathSampling` 都需要在网络$G$上执行$O(T)$步随机游走。对于无权无向图，随机游走采样一个顶点的计算复杂度为$O(1)$；对于带权无向图，可以使用 `roulette wheel selection` 从而随机游走采样一个顶点的计算复杂度为$O(\log n)$。

     另外我们需要$O(M)$的空间存储$\tilde G$，以及额外的$O(n+m)$空间来存储算法的输入。

   - 第二步：我们需要$O(M)$的时间来计算$\tilde{\mathbf M} = \mathbf D^{-1}(\mathbf D - \tilde{\mathbf H}) \mathbf D^{-1}$以及$\text{trunc_log}^\circ \left(\frac{\text{vol}_G}{b}\tilde{\mathbf M}\right)$。

     另外我们需要$O(n)$空间来存储`degree` 矩阵$\mathbf D$，$O(M)$的空间来存储$\hat{\mathbf M}$。

   - 第三步：我们需要$O(Md)$时间来计算一个稀疏矩阵和一个稠密矩阵的乘积，以及$O(d^3)$来执行 `Jacobi SVD`，以及$O(nd^2)$来计算`Gram-Schmidt` 正交化 。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/cyVcFWTB1Tjo.png?imageslim">
   </p>
   

2. 示例：假设网络$G$的顶点数量$n=10^{6}$，边的数量为$m=10^7$，上下文窗口大小$T=10$，逼近因子`approximation factor`$\epsilon = 0.1$。

   - `NetSMF` 调用 `PathSampling` 算法次数为$M=Tm\epsilon^{-2}\log n\simeq 1.4\times 10^{11}$次，得到一个稀疏矩阵大约$1.4\times 10^{11}$个非零值，稠密度大约为$\frac{M}{n^2}\simeq 14\%$。

     然后我们在 `randomized SVD` 中计算 `sparse-dense` 矩阵乘积，近似矩阵的稀疏性可以大大加快计算速度。

   - `NetMF` 必须构建一个稠密矩阵，它包含$n^2=10^{12}$个非零元素，这比 `NetSMF` 大一个量级。

   通过一个更大的$\epsilon$我们可以进一步降低 `NetSMF` 中近似矩阵的稀疏性，而 `NetMF` 缺乏这种灵活性。

3. `NetSMF` 的每个步骤都可以并行化，从而`scale` 到非常大的网络。

   - 第一步：我们可以同时启动多个 `PathSampling worker` 来独立的、并行的采样多个路径，每个 `worker` 负责采样一批路径。
   - 第二步：我们可以简单直接地并行计算$\tilde{\mathbf M} = \mathbf D^{-1}(\mathbf D - \tilde{\mathbf H}) \mathbf D^{-1}$和$\mathbf K = \text{trunc_log}^\circ(\frac{\text{vol}_G}{b}\tilde{\mathbf M})$。
   - 第三步：我们可以将稀疏矩阵组织为行优先的格式，这种格式可以在稀疏矩阵和稠密矩阵之间进行高效的乘法运算。

### 16.3 近似误差分析

1. 我们假设逼近因子$\epsilon \lt 0.5$。我们假设顶点的 `degree` 是降序排列：$d_\min = d_1\le d_2\le \cdots\le d_n = d_{\max}$。 我们令$\sigma_i(\mathbf \cdot)$为矩阵从大到小排列的奇异值中的第$i$个奇异值。

   我们首先证明一个结论：令$\mathbf F = \mathbf D^{-1/2}\mathbf H \mathbf D^{-1/2}$，以及$\tilde{\mathbf F} = \mathbf D^{-1/2}\tilde {\mathbf H} \mathbf D^{-1/2}$，则有：

   $\forall i\in \{1,\cdots,n\},\quad \sigma_i(\tilde{\mathbf F} - \mathbf F) \lt 4\epsilon$

   证明：

   $\mathbf F = \mathbf D^{-1/2}\left( \mathbf D - \frac 1T\sum_{r=1}^T \mathbf D\left(\mathbf D^{-1} \mathbf A\right)^r\right)\mathbf D^{-1/2} = \mathbf I - \sum_{r=1}^T\frac{1}{T}\left(\mathbf D^{-1/2} \mathbf A\mathbf D^{-1/2}\right)^r$

   它是某个图的归一化的拉普拉斯矩阵，因此有特征值$\lambda_i(\mathbf F)\in [0,2)$。

   由于$\tilde {\mathbf F}$是矩阵$\mathbf F$的$\epsilon$`spectral sparsifier`，因此有：

   $\forall \mathbf{\vec x}\in \mathbb R^n,\quad \frac{1}{1+\epsilon}\mathbf{\vec x}^\top \mathbf F\mathbf{\vec x} \le \mathbf{\vec x}^\top\tilde {\mathbf F}\mathbf{\vec x} \le \frac{1}{1-\epsilon}\mathbf{\vec x}^\top \mathbf F\mathbf{\vec x}$

   令$\mathbf{\vec x} = \mathbf D^{-1/2}\mathbf{\vec y}$，则有：

   其中最后一个不等式是因为$\epsilon \lt 0.5$。

   根据瑞利商的性质（15.1 中的定理七）有：$|\lambda_i(\tilde{\mathbf F} - \mathbf F)|\le 2\epsilon \lambda_i(\mathbf L) \lt 4\epsilon$。

   根据奇异值和特征值的关系（15.1 中的定理五），我们有：$\sigma_i(\tilde{\mathbf F} - \mathbf F) \lt 4\epsilon$。

2. 定理十：$\tilde{\mathbf M} - \mathbf M$的奇异值满足：

   $\forall i \in \{1,\cdots,n\},\quad \sigma_i(\tilde{\mathbf M} - \mathbf M) \le \frac{4\epsilon}{\sqrt{d_id_\min}}$

   证明：

   $\tilde{\mathbf M} - \mathbf M = \mathbf D^{-1}(\tilde{\mathbf H} - \mathbf H)\mathbf D^{-1} = \mathbf D^{-1/2}(\tilde{\mathbf H} - \mathbf H)\mathbf D^{-1/2}$

   根据奇异值的性质（15.1 中的定理六），我们有：

3. 定理十一：令$||\cdot||_F$为矩阵的 `Frobenius` 范数，则有：

   $\left\|\text{trunc_log}^\circ\left(\frac{\text{vol}_G}{b}\tilde{\mathbf M}\right)-\text{trunc_log}^\circ\left(\frac{\text{vol}_G}{b} {\mathbf M}\right)\right\|_F \le \frac{4\epsilon\text{vol}_G}{b\sqrt{d_\min}}\sqrt{\sum_{i=1}^n \frac{1}{d_i}}$

   证明：很明显$\text{trunc_log}^\circ()$函数满足是 `1- Lipchitz` 的。因此我们有：

### 16.3 实验

1. 数据集：

   - `BlogCatalog` 数据集：由博客作者提供的社交关系网络。标签代表作者提供的主题类别。
   - `Flickr` 数据集：`Flickr`网站用户之间的关系网络。标签代表用户的兴趣组，如“黑白照片”。
   - `Protein-Protein Interactions:PPI`：该数据集包含蛋白质和蛋白质之间的关联，标签代表基因组。
   - `Youtube` 数据集：`YouTube` 网站用户之间的社交网络。标签代表用户的视频兴趣组，如“动漫、摔跤”。网络包含 `1138499` 个顶点、`2990443` 条边、`47` 种不同标签。
   - `Open Academic Graph:OAG` 数据集：一个学术网络，它包含 `67,768,244` 位作者和 `895,368,962` 条无向边。顶点标签位每个作者的研究领域，共有 `19`种不同的标签。每位作者可以研究多个领域，所以有多个标签。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/devWSFLPwm8o.png?imageslim">
   </p>
   

2. 模型配置：

   - 所有模型的 `embedding` 维度设置为$d=128$
   - 对于 `NetSMF/NetMF/DeepWalk/node2vec` ，我们将上下文窗口大小设为$T=10$
   - 对于 `LINE`我们仅使用二阶邻近度，即 `LINE(2nd)`，负样本系数为 `5`，边采样的数量为 `100`亿。
   - 对于 `DeepWalk`，随机游走序列的长度为`40`，每个顶点开始的随机游走序列数量为 `80`，负采样系数为 `5`。
   - 对于`node2vec`，随机游走序列的长度为`40`，每个顶点开始的随机游走序列数量为 `80`，负采样系数为 `5` 。参数$p,q$从$\{0.25,0.5,1,2,4\}$中进行 `grid search` 得到。
   - 对于 `NetMF`，我们在 `BlogCatalog,PPI,Flickr` 数据集中设置$h=256$。
   - 对于 `NetSMF`，我们在 `PPI,Flickr,YouTube` 数据集中设置采样数量$M=10^3\times T\times m$；我们在 `BlogCatalog` 数据集中设置采样数量$M=10^4\times T\times M$；我们在 `OAG` 数据集中设置采样数量$M=10\times T\times m$。
   - 对于 `NetMF` 和 `NetSMF`，我们设置负采样系数$b=1$。

3. 和 `DeepWalk` 相同的实验步骤，我们首先训练整个网络的 `embedding`，然后随机采样一部分标记样本来训练一个`one-vs-rest` 逻辑回归分类模型，剩余的顶点作为测试集。我们评估测试集的 `Micro-F1`指标和 `Macro-F1` 指标。为了确保实验结果可靠，每个配置我们都重复实验 `10`次，并报告测试集指标的均值。

   对于 `BlogCatalog,PPI` 数据集，我们考察分类训练集占比 `10%~90%` 的情况下，各模型的性能；对于 `Flickr,YouTube,OAG` 数据集，我们考察分类训练集占比 `1%~10%` 的情况下，各模型的性能。

   完成的实验结果如下图所示。对于训练时间超过1周的模型，我们判定为训练失败，此时并未在图中给出结果。第二张图给出了模型训练时间，`-` 表示模型无法在周内完成训练（时间复杂度太高)；`x` 表示模型因内存不足无法训练（空间复杂度太高）。

   - 我们首先重点对比 `NetSMF` 和 `NetMF`：

     - 在训练速度上：对于大型网络 （`YouTube,OAG`），`NetMF` 因为空间复杂度和时间复杂度太高而无法训练，但是 `NetSMF` 可以在 `24h` 内完成训练；对于中型网络（`Flickr`），二者都可以完成训练，但是 `NetSMF` 的速度要快2倍；对于小型网络，`NetMF` 的训练速度反而更快，这是因为 `NetSMF` 的稀疏矩阵构造和分解的优势被`pipeline` 中其它因素抵消了。
     - 在模型效果上：`NetSMF` 和 `NetMF` 都能产生最佳的效果（和其它方法相比），并且 `NetSMF` 性能相比 `NetMF` 略有下降。

     总之，`NetSMF` 不仅提高了可扩展性，还能保持足够好的性能。这证明了我们谱图稀疏化近似算法的有效性。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/XV0Cgmu6DW6T.png?imageslim">
   </p>
   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/cK2ovgHjtFko.png?imageslim">
   </p>
   

4. 我们用 `Flickr` 数据集的 `10%` 标记顶点作为训练集，来评估`NetSMF` 超参数的影响。

   - 维度$d$：论文使用 `Cattell Scree` 测试。首先从大到小绘制奇异值，然后再图上找出奇异值突变或者趋于均匀的点。从 `Flickr` 数据的奇异值观察到：当$d$增加到 `100` 时，奇异值趋近于零（图`b` 所示）。因此我们在实验中选择$d=2^8=128$。

     我们观察$d=2^4\sim2^8$时，测试集的指标。可以看到确实当$d=128$时模型效果最好。这表明了 `NetSMF` 可以自动选择最佳的 `embedding` 维度。

   - 非零元素数量：理论上$M=O(T\times m\epsilon^{-2}\log n)$，当$M$越大则近似误差越小。不失一般性我们将$M$设置为$k\times T\times m$，其中$k=\{1,10,,100,200,500,1000,2000\}$，我们研究随着$M$的增大模型性能的影响。

     如图 `c` 所示，当增加非零元素数量时，`NetSMF` 效果更好，因为更大的$M$带来更小的误差。但是随着$M$的增加，预测性能提升的边际收益在递减。

   - 并行性：我们将线程数量分别设置为 `1、10、20、30、60`，然后考察`NetSMF` 的训练时间。

     如图`d` 所示，在单线程时`NetSMF` 运行了 `12` 个小时，在`30` 个线程时`NetSMF` 运行了 `48` 分钟，这实现了 `15` 倍的加速比（理想情况 `30`倍）。这种相对较好的亚线性加速比使得 `NetSMF` 能够扩展到非常大规模的网络。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/aHzrJnftQC6l.png?imageslim">
   </p>
   