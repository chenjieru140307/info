# 主题模型

1. 给包含$N$篇文档的定语料库$\mathbb D =\{\mathcal D_1,\mathcal D_2,\cdots,\mathcal D_N \}$，其中$\mathcal D_i$为第$i$篇文档，包含$n_i$个单词。

   语料库的所有单词来自于词汇表$\mathbb V = \{\text{word}_1,\text{word}_2,\cdots,\text{word}_V\}$，其中$V$表示词汇表的大小，第$j$个单词为$\text{word}_j$。

   注意：文档中的单词标记为$w_j$，它表示文档中第$j$个位置的单词为$\text{word}_{w_j}$。如：文档中第1个位置的单词为$w_1 = 100$（假设$\text{word}_{100} = 我$），则文档中第一个位置的单词为 `我` 。

   因此这里将$w$来表示文档中的单词（也称作 `token` ），用$v$表示词表中的单词。

2. `BOW:Bag of Words`：词在文档中不考虑顺序，这称作词袋模型。

## 一、Unigram Model

1. 假设有一个骰子，骰子有$V$个面，每个面对应于词典中的一个单词。 `Unigram Model`是这样生成文档的：

   - 每次抛一次骰子，抛出的面就对应于产生一个单词。
   - 如果一篇文档有$n$个单词，则独立的抛掷$n$次骰子就产生着$n$个单词。

2. 令骰子的投掷出各个面的概率为$\vec\Theta =(\theta_1,\theta_2,\cdots,\theta_V)^{T}$，其中$\theta_v$为抛出的面是第$v$面的概率：$p(\text{word}_v)=\theta_v$。满足约束：

  $\vec\Theta$就是待求的参数。

3. 假设文档包含$n$个单词，这些单词依次为 ：$\{\text{word}_{w_1},\text{word}_{w_2},\cdots,\text{word}_{w_n}\}$，其中$w_i \in \{1,2,\cdots,V\}$。用$(w_1,w_2,\cdots,w_n)$代表文档， 则生成这篇文档的概率为：

  $p(w_1,w_2,\cdots,w_n;\vec\Theta)=p(w_1;\vec\Theta)\prod_{i=2}^{n}p(w_i\mid w_1,w_2,\cdots,w_{i-1};\vec\Theta)$

   在$p(w_i\mid w_1,w_2,\cdots,w_{i-1};\vec\Theta)$中，$w_1,w_2,\cdots,w_{ i-1 }$称作$w_i$的上下文。由于采取的是词袋模型，没有考虑上下文，所以有：

  $p(w_i\mid w_1,w_2,\cdots,w_{i-1};\vec\Theta) =p(w_i;\vec\Theta)$

   于是有：

  $p(w_1,w_2,\cdots,w_n;\vec\Theta)=\prod_{i=1}^{n}p(w_i;\vec\Theta)$

   - 如果考虑了上下文（即抛弃词袋模型），则各种单词的组合会导致爆炸性的复杂度增长。
   - 由于是词袋模型，因此$p(w_1,w_2,\cdots,w_n;\vec\Theta)$并不构成一个概率分布。$p(w_1,w_2,\cdots,w_n;\vec\Theta)$仅仅是生成该文档的一种非归一化概率。

4. 假设单词$\{\text{word}_{w_1},\text{word}_{w_2},\cdots,\text{word}_{w_n}\}$中，有$\tilde n_1$个$\text{word}_1$，有$\tilde n_2$个$\text{word}_2$，...有$\tilde n_V$个$\text{word}_V$，其中$\tilde n_1+\tilde n_2+\cdots+\tilde n_V=n$，则：

  $p(w_1,w_2,\cdots,w_n;\vec\Theta)=\prod_{i=1}^{n}p(w_i;\vec\Theta)=\prod_{v=1}^{V}p(\text{word}_v)^{\tilde n_v}=\prod_{v=1}^{V}\theta_v^{\tilde n_v}$

5. 参数估计：就是估计骰子的投掷出各个面的概率$\vec\Theta=(\theta_1,\theta_2,\cdots,\theta_V)^{T}$

### 1.1 最大似然估计

1. 假设数据集$\mathbb D$包含$N$篇文档$\mathbb D =\{\mathcal D_1,\mathcal D_2,\cdots,\mathcal D_N \}$。对文档$\mathcal D_i$，假设其单词依次为$\{\text{word}_{w_1^{i}},\text{word}_{w_2^{i}},\cdots,\text{word}_{w_{n_i}^{i}}\}$， 用$(w_1^{i},w_2^{i},\cdots,w_{n_i}^{i})$来表示。其中：

   -$v=w_j^i$表示文档$\mathcal D_i$的第$j$个单词为单词$\text{word}_v$。
   -$n_i$表示文档$\mathcal D_i$一共有$n_i$个单词。

2. 由于每篇文档都是独立的且不考虑文档的顺序和单词的顺序，则数据集发生的概率

  $L=p(\mathbb D)=p(w_1^1,\cdots,w_{n_1}^1,\cdots,w_1^N,\cdots,w^N_{n_N};\vec\Theta)=\prod_{i=1}^{N}\prod_{j=1}^{n_i}p(w^{i}_j;\vec\Theta)$

   假设数据集的所有单词$\{\text{word}_{w_1^{1}},\text{word}_{w_2^{1}},\cdots,\text{word}_{w_{n_1}^{1}},\cdots,\text{word}_{w_1^{N}},\text{word}_{w_2^{N}},\cdots,\text{word}_{w_{n_N}^{N}}\}$中，有$\tilde n_1$个$\text{word}_1$，有$\tilde n_2$个$\text{word}_2$，...有$\tilde n_V$个$\text{word}_V$。其中$\tilde n_1+\tilde n_2+\cdots+\tilde n_V=n$，$n$为所有文档的所有单词的数量。则有：

  $L=\prod_{i=1}^{N}\prod_{j=1}^{n_i}p(w^{i}_j;\vec\Theta)=\prod_{v=1}^{V}p(\text{word}_v)^{\tilde n_v}=\prod_{v=1}^{V}\theta_v^{\tilde n_v}$

3. 使用最大似然估计法，也就是最大化对数的$L$：

  $LL=\log L=\log \prod_{v=1}^{V}\theta_v^{\tilde n_v}=\sum_{v=1}^{V}\tilde n_v\log \theta_v$

   于是求解：

   用拉格朗日乘子法求解，其解为：

  $\hat \theta_v=\frac{\tilde n_v}{n},v=1,2,\cdots,V$

   其物理意义为：单词$\text{word}_v$出现的概率$\theta_v$等于它在数据集$\mathbb D$中出现的频率，即：它出现的次数$\tilde n_v$除以数据集$\mathbb D$所有单词数$n$。

### 1.2 最大后验估计

1. 根据贝叶斯学派的观点， 参数$\vec\Theta$也是一个随机变量而不再是一个常量，它服从某个概率分布$p(\vec\Theta)$， 这个分布称作参数$\vec\Theta$的先验分布。

   此时：

   根据前面的推导有：$p(w_1^1,\cdots,w_{n_1}^1,\cdots,w_1^N,\cdots,w^N_{n_N}\mid \vec\Theta)=\prod_{v=1}^{V}\theta^{\tilde n_v}$，则有：

  $p(\mathbb D) =p(w_1^1,\cdots,w_{n_1}^1,\cdots,w_1^N,\cdots,w^N_{n_N}) = \int \prod_{v=1}^{V}\theta^{\tilde n_v}p(\vec\Theta)d\vec\Theta$

2. 此处先验分布$p(\vec\Theta)$有多种选择。注意到数据集条件概率$p(w_1^1,\cdots,w_{n_1}^1,\cdots,w_1^N,\cdots,w^N_{n_N}\mid \vec\Theta)$刚好是多项式分布的形式，于是选择先验分布为多项式分布的共轭分布，即狄利克雷分布：

  $\vec\Theta \sim Dir(\vec\alpha):p(\vec\Theta;\vec \alpha)=\frac {1}{B(\vec\alpha)}\prod_{v=1}^{V} \theta_v^{\alpha_v-1}$

   其中：$\vec \alpha=(\alpha_1,\alpha_2,\cdots,\alpha_V)^{T}$为参数向量，$B(\vec\alpha)$为`Beta`函数：

  $B(\vec\alpha)=\frac{\prod_{v=1}^{V}\Gamma(\alpha_v)}{\Gamma(\sum_{v=1}^{V}\alpha_v)}$

   显然根据定义有：

  $\int p(\vec \Theta;\vec\alpha) d\vec\Theta=\int\frac {1}{B(\vec\alpha)}\prod_{v=1}^{V} \theta_v^{\alpha_v-1} d\vec\Theta=1 \longrightarrow \int\prod_{v=1}^{V} \theta_v^{\alpha_v-1}d\vec\Theta=B(\vec\alpha)$

3. 令$\vec{\tilde{ \mathbf n}}=(\tilde n_1,\tilde n_2,\cdots,\tilde n_V)^{T}$为词频向量，其每个元素代表了对应的单词在数据集$\mathbb D$中出现的次数。

   此时有：

   因此$p(\mathbb D)$仅由$\vec\alpha$决定，记作：$p(\mathbb D) =\frac {B(\vec\alpha+\vec{\tilde{ \mathbf n}})}{B(\vec\alpha)}$

4. 后验概率 ：

   可见后验概率服从狄利克雷分布$Dir(\vec\alpha+\vec{\tilde{ \mathbf n}})$。

5. 因为这时候的参数$\vec\Theta$是一个随机变量，而不再是一个固定的数值，因此需要通过对后验概率$p(\vec\Theta\mid \mathbb D;\vec\alpha)$最大化或者期望来求得。

   - 这里使用期望值$\mathbb E(\vec\Theta\mid \mathbb D;\vec\alpha )$来做参数估计。

     由于后验分布$p(\vec\Theta\mid \mathbb D;\vec\alpha)$服从狄利克雷分布$Dir(\vec\alpha+\vec{\tilde{ \mathbf n}})$， 则有期望：

    $\mathbb E(\vec\Theta\mid \mathbb D;\vec\alpha )=\left(\frac{\tilde n_1 +\alpha_1}{\sum_{v=1}^{V}(\tilde n_v +\alpha_v)},\frac{\tilde n_2 +\alpha_2}{\sum_{v=1}^{V}(\tilde n_v +\alpha_v)},\cdots,\frac{\tilde n_V +\alpha_V}{\sum_{v=1}^{V}(\tilde n_v +\alpha_v)}\right)$

     即参数$\theta_v$的估计值为：

    $\hat \theta_v=\frac{\tilde n_v+\alpha_v}{\sum_{v=1}^{V}(\tilde n_v +\alpha_v)}$

     考虑到$\alpha_v$在狄利克雷分布中的物理意义为：事件的先验的伪计数。因此该估计式物理意义为：估计值是对应事件计数（伪计数+真实计数）在整体计数中的比例。

   - 这里并没有使用最大似然数据集$\mathbb D$来做参数估计，因为$p(\mathbb D) =\frac {B(\vec\alpha+\vec{\tilde{ \mathbf n}})}{B(\vec\alpha)}$中并没有出现参数$\vec\Theta$。

### 1.3 文档生成算法

1. 文档生成算法：根据主题模型求解的参数来生成一篇新的文档。

2. 最大似然模型的 `Unigram Model` 生成文档步骤：

   根据词汇分布$\vec\Theta=(\frac{\tilde n_1}{n},\frac{\tilde n_2}{n},\cdots,\frac{\tilde n_V}{n})^T$，从词汇表$\mathbb V$中独立重复采样$n$次从而获取$n$个单词。则这些单词就生成一篇文档。

3. 最大后验估计的 `Unigram Model`生成文档的步骤为：

   - 根据参数为$\vec\alpha$的狄利克雷分布$p(\vec\Theta;\vec\alpha)\sim Dir(\vec\alpha)$随机采样一个词汇分布$\tilde{\vec\Theta}=(\tilde \theta_1,\tilde \theta_2,\cdots,\tilde \theta_V)^{T}$。

     所谓随机采样一个词汇分布，即：根据狄里克雷分布生成一个随机向量。选择时要求 ：

   - 根据词汇分布$\tilde{\vec\Theta}$，从词汇表$\mathbb V$中独立重复采样$n$次从而获取$n$个单词。则这些单词就生成一篇文档。

## 二、pLSA Model

1. `Unigram Model`模型过于简单。事实上人们写一篇文章往往需要先确定要写哪几个主题。

   如：写一篇计算机方面的文章，最容易想到的词汇是：内存、CPU、编程、算法等等。之所以能马上想到这些词，是因为这些词在对应的主题下出现的概率相对较高。

   因此可以很自然的想到：一篇文章由多个主题构成，而每个主题大概可以用与该主题相关的频率最高的一些词来描述。

   上述直观的想法由`Hoffman`在 1999 年的 `probabilistic Latent Semantic Analysis:pLSA`模型中首先进行了明确的数学化。

2. 主题 `topic`：表示一个概念。具体表示为一系列相关的词，以及它们在该概念下出现的概率。

   - 与某个主题相关性比较强的词，在该主题下出现概率较高
   - 与某个主题相关性比较弱的词，在该主题下出现概率较低

3. 主题示例：给定一组词： `证明,推导,对象,酒庄,内存`，下列三个主题可以表示为：

   - 数学主题：$(0.45,0.35,0.2,0,0)$
   - 计算机主题：$(0.2,0.15,0.45,0,0.2)$
   - 红酒主题：$(0,0,0.2,0.8,0)$

   |        | 证明 | 推导 | 对象 | 酒庄 | 内存 |
   | :----: | :--: | :--: | :--: | :--: | :--: |
   |  数学  | 0.45 | 0.35 | 0.2  |  0   |  0   |
   | 计算机 | 0.2  | 0.15 | 0.45 |  0   | 0.2  |
   |  红酒  |  0   |  0   | 0.2  | 0.8  |  0   |

### 2.1 文档生成算法

1. 假设话题集合$\mathbb T$有$T$个话题，分别为$\mathbb T=\{\text{topic}_1,\text{topic}_2,\cdots,\text{topic}_T\}$。`pLSA` 模型的文档生成规则：

   - 以概率$p(\text{topic}_t)$选中第$t$个话题$\text{topic}_t$，然后在话题$\text{topic}_t$中以概率$p(\text{word}_v\mid \text{topic}_t)$选中第$v$个单词$\text{word}_v$。

   - 重复执行上一步`挑选话题--> 挑选单词`$n$次，则得到一篇包含$n$个单词$\{\text{word}_{w_1},\text{word}_{w_2},\cdots,\text{word}_{w_n}\}$的文档，记作$(w_1 ,w_2 ,\cdots,w_{n} )$。

     其中：$1\le w_j \le V$，$v=w_j$表示文档的第$j$个单词为$\text{word}_v$。

2. 对于包含$N$篇文档的数据集$\mathbb D$，假设所有文档都是如此生成。则数据集$\mathbb D$的生成规则：

   - 以概率$p(\mathcal D_i)$选中第$i$篇文档。这个概率仅仅用于推导原理，事实上随着公式的推导，该项会被消掉。

     > 通常选择均匀选择，即$p(\mathcal D_i) = \frac 1N$

   - 对于第$i$篇文档，以概率$p(\text{topic}_t\mid \mathcal D_i)$选中第$t$个话题$\text{topic}_t$，然后在话题$\text{topic}_t$中以概率$p(\text{word}_v\mid \text{topic}_t)$选中第$v$个单词$\text{word}_v$。

   - 重复执行上一步`挑选话题--> 挑选单词`$n_i$次，则得到一篇包含$n_i$个单词$\{\text{word}_{w_1^i},\text{word}_{w_2^i},\cdots,\text{word}_{w_{n_i}}^i\}$的文档，记作$(w_1^i ,w_2^i ,\cdots,w_{n_i}^i )$。

   - 重复执行上述文档生成规则$N$次，即得到$N$篇文档组成的文档集合$\mathbb D$。

### 2.2 模型原理

1. 令

   -$\varphi_{i,t}$表示：选中第$i$篇文档$\mathcal D_i$的条件下，选中第$t$个话题$\text{topic}_t$的概率
   -$\theta_{t,v}$表示：选中第$t$个话题$\text{topic}_t$的条件下，选中第$v$个单词$\text{word}_v$的概率

   待求的是参数$\Phi$和$\Theta$：

2. 根据`pLSA`概率图模型（盘式记法），结合成对马尔可夫性有：

  $p(\text{word}*v,\mathcal D_i\mid \text{topic}\*t )=p(\text{word}\*v\mid \text{topic}\*t)p(\mathcal D_i \mid \text{topic}\*t)$

   即：文档和单词关于主题条件独立。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/IiATNLT3icdC.png?imageslim">
   </p>
   

3. 对于文档$\mathcal D_i$，给定其中的单词$\text{word}\*v$，有：*

   根据该等式，可以得到：

   $p(\text{word}\*v\mid \mathcal D_i)=\sum\*{t=1}^Tp(\text{topic}\*t\mid \mathcal D_i)p(\text{word}\*v\mid \text{topic}\*t)$

   即：给定文档$\mathcal D_i$的条件下，某个的单词$\text{word}\*v$出现的概率可以分成三步：*

   - 首先得到给定的文档$\mathcal D_i$的条件下，获取某个话题$\text{topic}\*t$的概率*
   - 再得到该话题$\text{topic}\*t$生成该单词$\text{word}\*v$的概率
   - 对所有的话题累加$\sum\*{t=1}^T$即得到给定的单词$\text{word}\*v$在给定文档$\mathcal D_i$中出现的概率

4. 对于给定文档$\mathcal D_i$中主题$\text{topic}\*t$生成的单词$\text{word}\*v$，有：

   则已知文档$\mathcal D_i$中出现了单词$\text{word}\*v$的条件下，该单词由主题$\text{topic}\*t$生成的概率为：

   其物理意义为：给定文档$\mathcal D_i$的条件下，单词$\text{word}\*v$由主题$\text{topic}\*t$生成的概率占单词$\text{word}\*v$出现的概率的比例。

### 2.3 参数求解

1. `pLSA` 模型由两种参数求解方法：矩阵分解、`EM` 算法。

#### 2.3.1 矩阵分解

1. 根据前面的推导，有：$p(\text{word}\*v\mid \mathcal D_i)=\sum\*{t=1}^Tp(\text{topic}\*t\mid \mathcal D_i)p(\text{word}\*v\mid \text{topic}\*t)$。其中文档$\mathcal D_i$和单词$\text{word}\*v$是观测到的，主题$\text{topic}\*t$是未观测到的、未知的。

   令$p^{\mathcal D}\*{i,v}=p(\text{word}\*v\mid \mathcal D_i)$，根据：

   则有：

   $p^{\mathcal D}\*{i,v}=\sum\*{t=1}^{T}\varphi*{i,t} \theta{t,v}$

2. 令：

   则有：$\mathbf P^{\mathcal D}=\Phi \Theta$。

   由于$\mathbf P^{\mathcal D}$是观测的、已知的，所以`pLSA`对应着矩阵分解。其中要求满足约束条件：

   .

#### 2.3.2 EM 算法

1. 在文档$\mathcal D_i$中，因为采用词袋模型，所以单词的生成是独立的。假设文档$\mathcal D_i$中包含单词${\text{word}{w^i_1},\cdots,\text{word}{w^i{n_i}}}$，其中：

   -$n_i$表示文档$\mathcal D_i$的单词总数。
   -$v=w_j^i$表示文档$\mathcal D_i$的第$j$个单词为$\text{word}*v$。*

   *则有：*

   *$p(\text{word}*{w^i_1},\cdots,\text{word}{w^i{n_i}} \mid \mathcal D_i)=\prod{j=1}^{n_i}p(\text{word}{w^i_j} \mid \mathcal D_i)$

2. 根据前面的推导，有：$p(\text{word}*v\mid \mathcal D_i)=\sum*{t=1}^Tp(\text{topic}*t\mid \mathcal D_i)p(\text{word}\*v\mid \text{topic}\*t)$。则：

   $p(\text{word}\*{w^i_1},\cdots,\text{word}\*{w^i*{n_i}} \mid \mathcal D_i)=\prod{j=1}^{n_i}\sum{t=1}^Tp(\text{topic}*t\mid \mathcal D_i)p(\text{word}*{w^i_j}\mid \text{topic}*t)$*

   *则有：*

   *$p(\text{word}*{w^i_1},\cdots,\text{word}{w^i{n_i}} , \mathcal D_i)=p( \mathcal D_i)\prod{j=1}^{n_i}\sum{t=1}^Tp(\text{topic}*t\mid \mathcal D_i)p(\text{word}*{w^i_j}\mid \text{topic}*t)$*

3. *假设文档$\mathcal D_i$的单词${\text{word}*{w^i_1},\cdots,\text{word}{w^i{n_i}}}$中，单词$\text{word}*v$有$c(i,v)$个，$v=1,2,\cdots,V$。 则有：*

   *$p(\text{word}*{w^i_1},\cdots,\text{word}{w^i{n_i}} , \mathcal D_i)=p( \mathcal D_i)\prod{v=1}^{V}\left[\sum{t=1}^Tp(\text{topic}*t\mid \mathcal D_i)p(\text{word}\*v\mid \text{topic}\*t)\right]^{c(i,v)}$

   $c(i,v)$的物理意义为：文档$\mathcal D_i$中单词$\text{word}\*v$的数量。*

4. 考虑观测变量$X_i=(\text{word}\*{w^i_1},\cdots,\text{word}\*{w^i*{n_i}} , \mathcal D_i)$，它表示第$i$篇文档$\mathcal D_i$以及该文档中的$n_i$个单词。

   则有：

  $p(X_i)=p( \mathcal D_i)\prod{v=1}^{V}\left[\sum{t=1}^Tp(\text{topic}*t\mid \mathcal D_i)p(\text{word}\*v\mid \text{topic}\*t)\right]^{c(i,v)}$

   由于文档之间是相互独立的，因此有：

   要使得观测结果发生，则应该最大化$p(X_1,X_2,\cdots,X_N)$。但是这里面包含了待求参数的乘积，其最大化难于求解，因此使用`EM`算法求解。

5. 考虑完全变量$Y_i=(\text{word}\*{w^i_1},\cdots,\text{word}\*{w^i*{n_i}} , \text{topic}{z^i_1},\cdots,\text{topic}{z^i{n_i}} ,\mathcal D_i)$，其中${\text{topic}{z^i_1},\cdots,\text{topic}{z^i{n_i}}}$为文档中$\mathcal D_i$中每位置的单词背后的话题。

   - 由于采用词袋模型，所以生成单词是相互独立的，因此有：

   - 根据$p(\mathcal D_i ,\text{word}*v,\text{topic}\*t )=p(\mathcal D_i)p(\text{topic}\*t\mid \mathcal D_i)p(\text{word}\*v\mid \text{topic}\*t)$有：

     $p(\text{word}\*v,\text{topic}\*t\mid \mathcal D_i)=p(\text{topic}\*t\mid \mathcal D_i)p(\text{word}\*v\mid \text{topic}\*t)$

     于是：

   $p(Y_i)=p( \mathcal D_i)\prod\*{j=1}^{n_i}p(\text{topic}\*{z_j^i}\mid \mathcal D_i)p(\text{word}*{w_j^i}\mid \text{topic}{z_j^i})$

   - 由于文档之间是相互独立的，因此有：

6. 假设在文档$\mathcal D_i$中，单词$\text{word}\*v$不论出现在文档的哪个位置，都是由同一个话题$\text{topic}\*{t}$产生的。

   则有：

   则有：

  $p(Y_1,Y_2,\cdots,Y_N) = \prod{i=1}^{N} p( \mathcal D_i)\prod_{v=1}^V\left[p(\text{topic}*{t}\mid \mathcal D_i)p(\text{word}\*v\mid \text{topic}\*{t})\right]^{c(i,v)}$*

   *则完全数据的对数似然函数为：*

   *$LL = \log p(Y_1,Y_2,\cdots,Y_N) = \sum*{i=1}^N p(\mathcal D_i)+\sum_{i=1}^N\sum_{v=1}^V\left[c(i,v)(\log p(\text{topic}*{t}\mid \mathcal D_i)+\log p(\text{word}\*v\mid \text{topic}\*{t})\right]$*

7. *`E` 步：求取`Q`函数，为$LL$关于后验概率$p(\text{topic}\*t\mid \mathcal D_i,\text{word}\*v)$的期望。

   根据前面的推导，有：

   其中$\tilde \varphi_{i,t},\tilde \theta_{t,v}$均为上一轮迭代的结果，为已知量。

   则有：

8. `M`步：最大化`Q`函数，同时考虑约束条件：

   对每个参数进行求导并使之等于0 ，联立方程求解得到：

9. 文档-主题概率$\varphi_{i,t}$更新方程

   $\varphi_{i,t}=\frac{\sum_{v=1}^{V}c(i,v)p(\text{topic}\*t\mid \mathcal D_i,\text{word}\*v)}{n_i}$

   其物理意义：文档$\mathcal D_i$中每个位置背后的、属于主题$\text{topic}\*t$的频数（按概率计数），除以位置的个数。也就是主题$\text{topic}\*t$的频率。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/lrloEnnWsJK3.png?imageslim">
   </p>
   

10. 主题-单词概率$\theta\*{t,v}$更新方程

    $\theta\*{t,v}=\frac{\sum*{i=1}^{N}c(i,v)p(\text{topic}*t\mid \mathcal D_i,\text{word}\*v)}{\sum\*{{v^\prime}=1}^{V}\sum*{i=1}^{N}c(i,{v^\prime})p(\text{topic}*t\mid \mathcal D_i,\text{word}*{v^\prime})}$

    其物理意义为：单词$\text{word}*v$在数据集$\mathbb D$中属于主题$\text{topic}\*t$的频数（按概率计数），除以数据集$\mathbb D$中属于主题$\text{topic}\*t$的频数（按概率计数）。即单词$\text{word}\*v$的频率。*

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/ymnKrJslQ8zT.png?imageslim">
   </p>
   

11. `pLSA`的 `EM`算法：

    - 输入：文档集合$\mathbb D$，话题集合$\mathbb T$，字典集合$\mathbb V$

    - 输出：参数$\Phi={\varphi\*{i,t}}$和$\Theta={\theta\*{t,v}}$，其中：*

    - *算法步骤：*

      - *初始化： 令$m=0$， 为$\varphi_{i,t}^{}$和$\theta_{t,v}^{}$赋初值，$i=1,2,\cdots,N;v=1,2,\cdots,V;t=1,2,\cdots,T$。*

      - *迭代，迭代收敛条件为参数变化很小或者 `Q`函数的变化很小。迭代步骤如下：*

        - *`E`步：计算$Q$函数。*

          - *先计算后验概率：*

          *$p(\text{topic}\*t\mid \mathcal D_i,\text{word}\*v) ^{}=\frac{ \varphi\*{i,t} ^{} \theta\*{t,v} ^{}}{\sum_{t^{\prime}=1}^{T} \varphi_{i,t^{\prime}} ^{} \theta_{t^{\prime},v} ^{}}$*

          - *再计算$Q$函数：*

            *$Q=\sum_{i=1}^N p(\mathcal D_i)+ \sum_{i=1}^N\sum_{v=1}^Vc(i,v)\sum_{t=1}^Tp(\text{topic}\*t\mid \mathcal D_i,\text{word}\*v) ^{}(\log \varphi\*{i,t}+\log\theta\*{t,v})$*

        - *`M`步：计算$Q$函数的极大值，得到参数的下一轮迭代结果：*

        - *重复上面两步直到收敛*

## *三、LDA Model*

1. *在`pLSA`模型中，参数$\Phi,\Theta$是常数。而在`LDA`模型中，假设$\Phi,\Theta$也是随机变量：*

   - *参数$\vec\varphi^i=(\varphi*{i,1},\varphi{i,2},\cdots,\varphi_{i,T})$为文档$\mathcal D_i$的主题分布（离散型的），其中$i=1,2,\cdots,N$。该分布也是一个随机变量，服从分布$p(\vec\varphi^i)$（连续型的）。
   - 参数$\vec\theta^t=(\theta_{t,1},\theta_{t,2},\cdots,\theta_{t,V})$为主题$\text{topic}*t$的单词分布（离散型的），其中$t=1,2,\cdots,T$。该分布也是一个随机变量，服从分布$p(\vec\theta^t)$（连续型的）。*

   *因此 `LDA` 模型是`pLSA` 模型的贝叶斯版本。*

2. *例：在`pLSA` 模型中，给定一篇文档，假设：*

   - *主题分布为 `{教育：0.5，经济：0.3，交通：0.2}` ，它就是$p( \text{topic}\*t\mid \mathcal D_i)$。
   - 主题`教育`下的主题词分布为 `{大学:0.5,老师:0.2,课程:0.3}` ，它就是$p(\text{word}\*v\mid \text{topic}\*t = \text{教育})$。*

   在`LDA`中：

   - 给定一篇文档，主题分布$p( \text{topic}\*t\mid \mathcal D_i)$不再固定 。可能为 `{教育：0.5，经济：0.3，交通：0.2}` ，也可能为 `{教育：0.3，经济：0.5，交通：0.2}` ，也可能为 `{教育：0.1，经济：0.8，交通：0.1}` 。*

     但是它并不是没有规律的，而是服从一个分布$p(\vec \varphi)$。即：主题分布取某种分布的概率可能较大，取另一些分布的概率可能较小。

   - 主题`教育`下的主题词分布也不再固定。可能为 `{大学:0.5,老师:0.2,课程:0.3}`，也可能为 `{大学:0.8,老师:0.1,课程:0.1}`。

     但是它并不是没有规律，而是服从一个分布$p(\vec\theta)$。即：主题词分布取某种分布的概率可能较大，取另一些分布的概率可能较小。

### 3.1 文档生成算法

1. `LDA`模型的文档生成规则：

   - 根据参数为$\vec\eta$的狄利克雷分布随机采样，对每个话题$\text{topic}\*t$生成一个单词分布$\vec\theta_t=(\theta\*{t,1},\theta\*{t,2},\cdots,\theta\*{t,V})$。每个话题采样一次，一共采样$T$次。*
   - 根据参数为$\vec\alpha$的狄利克雷分布随机采样，生成文档$\mathcal D$的一个话题分布$\vec\varphi=(\varphi{1},\varphi{2},\cdots,\varphi{T})$。每篇文档采样一次。
   - 根据话题分布$p(\text{topic}*t\mid \mathcal D)= \varphi*{t}$来随机挑选一个话题。然后在话题$\text{topic}*t$中，根据单词分布$p(\text{word}\*v\mid \text{topic}\*t)= \theta\*{t,v}$来随机挑选一个单词。
   - 重复执行`挑选话题--> 挑选单词`$n$次，则得到一篇包含$n$个单词${\text{word}\*{w_1},\text{word}*{w_2},\cdots,\text{word}*{w_n}}$的文档，记作$(w_1 ,w_2 ,\cdots,w*{n } )$。其中：$1\le w_j \le V$，$v=w_j$表示文档的第$j$个单词为$\text{word}*v$。*

2. *对于包含$N$篇文档的数据集$\mathbb D$，假设所有文档都是如此生成。则数据集$\mathbb D$的生成规则：*

   - *以概率$p(\mathcal D_i)$选中第$i$篇文档。*
   - *根据参数为$\vec\eta$的狄利克雷分布随机采样，对每个话题$\text{topic}\*t$生成一个单词分布$\vec\theta_t=(\theta\*{t,1},\theta*{t,2},\cdots,\theta_{t,V})$。每个话题采样一次，一共采样$T$次。
   - 生成文档$\mathcal D_i$：
     - 根据参数为$\vec\alpha$的狄利克雷分布随机采样，生成文档$\mathcal D_i$的一个话题分布$\vec\varphi_i=(\varphi_{i,1},\varphi_{i,2},\cdots,\varphi_{i,T})$。每篇文档采样一次。
     - 在文档$\mathcal D_i$中，根据话题分布$p(\text{topic}*t\mid \mathcal D_i)= \varphi*{i,t}$来随机挑选一个话题。然后在话题$\text{topic}*t$中，根据单词分布$p(\text{word}\*v\mid \text{topic}\*t)= \theta\*{t,v}$来随机挑选一个单词。
     - 重复执行`挑选话题--> 挑选单词`$n_i$次，则得到一篇包含$n_i$个单词${\text{word}\*{w_1},\text{word}*{w_2},\cdots,\text{word}*{w*{n_i}} }$的文档，记作$(w_1^i ,w_2^i ,\cdots,w_{n_i}^i )$。
   - 重复执行上述文档生成规则$N$次，即得到$N$篇文档组成的文档集合$\mathbb D$。

3. 由于两次随机采样，导致 `LDA` 模型的解会呈现一定程度的随机性。所谓随机性，就是：当多次运行`LDA`算法，获得解可能会各不相同

   当采样的样本越稀疏，则采样的方差越大，则`LDA`的解的方差越大。

   - 文档数量越少，则文档的话题分布的采样越稀疏。
   - 文档中的单词越少，则话题的单词分布的采样越稀疏。

### 3.2 模型原理

1. 由于使用词袋模型，`LDA` 生成文档的过程可以分解为两个过程：

   -$\vec\alpha\rightarrow \vec\varphi_i\rightarrow {\text{topic}*{z^i_1},\text{topic}*{z^i_2},\cdots,\text{topic}*{z*{n^i_i}}}$：该过程表示，在生成第$i$篇文档$\mathcal D_i$的时候，先从`文档-主题`分布$\vec\varphi_i$中生成$n_i$个主题。

     其中：

     -$t={z^i_j}$表示文档$\mathcal D_i$的第$j$个单词由主题$\text{topic}*t$生成。*
     - *$n_i$表示文档$\mathcal D_i$一共有$n_i$个单词。*

   - *$\vec\eta \rightarrow \vec\theta_t\rightarrow {\text{word}*{w^t_1},\text{word}*{w^t_2},\cdots,\text{word}*{w^t_{n_t}}}$：该过程表示，在已知主题为$\text{topic}*t$的条件下，从`主题-单词`分布$\vec\theta_t$生成$n_t$个单词。*

     *其中：*

     - *$v=w^t_j$表示由主题$\text{topic}\*t$生成的的第$j$个单词为$\text{word}\*v$。
     - $n_t$为由$\text{topic}\*t$生成的单词的数量。*

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/bOC7NLs0XHlq.png?imageslim">
   </p>
   

#### 3.2.1 主题生成过程

1. 主题生成过程用于生成第$i$篇文档$\mathcal D_i$中每个位置的单词对应的主题。

   - $\vec\alpha\rightarrow \vec\varphi_i$：对应一个狄里克雷分布
   - $\vec\varphi_i\rightarrow {\text{topic}\*{z^i_1},\text{topic}\*{z^i_2},\cdots,\text{topic}*{z^i{n_i}}}$：对应一个多项式分布
   - 该过程整体对应一个`狄里克雷-多项式` 共轭结构：

2. 合并文档$\mathcal D_i$中的同一个主题。设$n_z(i,t)$表示文档$\mathcal D_i$中，主题$\text{topic}*t$出现的次数。则有：*

   *$\prod*{j=1}^{n_i} p(\text{topic}{z^i_j}\mid\vec\varphi_i)=\prod_{t=1}^T\varphi_{i,t}^{n_z(i,t)}$

   则有：

   其中$\mathbf{\vec n}*z(i)=(n_z(i,1),n_z(i,2),\cdots,n_z(i,T))$表示文档$\mathcal D_i$中，各主题出现的次数。*

3. *由于语料库中$N$篇文档的主题生成相互独立，则得到整个语料库的主题生成概率：*

   *$p( \text{topic}*{z^1_1},\text{topic}{z^1_2},\cdots,\text{topic}*{z^1*{n_1}},\cdots,\text{topic}*{z^N_1},\cdots,\text{topic}*{z^N_{n_N}}; \vec\alpha)\=\prod_{i=1}^Np( \text{topic}*{z^i_1},\text{topic}*{z^i_2},\cdots,\text{topic}*{z^i*{n_i}}; \vec\alpha)=\prod_{i=1}^{N}\frac{B(\mathbf{\vec n}*z(i)+\vec\alpha)}{B(\vec\alpha)}$*

   *.*

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/R7QxdtbR2v0f.png?imageslim">
   </p>
   

#### *3.2.2 单词生成过程*

1. *单词生成过程用于生成数据集$\mathbb D$中所有文档的所有主题的单词。*

   - *$\vec\eta \rightarrow \vec\theta_t$：对应一个狄里克雷分布*
   - *$\vec\theta_t\rightarrow {\text{word}*{w^t_1},\text{word}*{w^t_2},\cdots,\text{word}*{w^t_{n_t}}}$：对应一个多项式分布，其中$\text{word}*{w^t_i}$为数据集$\mathbb D$中（将所有单词拼接在一起）由主题$\text{topic}\*t$生成的单词。
   - 数据集$\mathbb D$中，由主题为$\text{topic}\*t$生成的所有单词的分布对应一个`狄里克雷-多项式` 共轭结构：

2. 合并主题$\text{topic}\*t$生成的同一个单词。设$n_v(t,v)$表示中主题$\text{topic}\*t$生成的单词中，$\text{word}\*v$出现的次数。则有：

   $\prod\*{i=1}^{n_t}p(\text{word}\*{w_i^t}\mid \vec\theta_t,\text{topic}*{w^t_i}=\text{topic}*t) =\prod*{v=1}^V\theta{t,v}^{n_t(t,v)}$

   则有：

   其中$\mathbf{\vec n}*v(t)=(n_v(t,1),n_v(t,2),\cdots,n_v(t,V))$表示由主题$\text{topic}\*t$生成的单词的词频。

3. 考虑数据集$\mathbb D$中的所有主题，由于不同主题之间相互独立，则有：

4. 这里是按照主题来划分单词，如果按照位置来划分单词，则等价于：

   注意：这里$w$的意义发生了变化：

   - 对于前者，$w^t_j$表示由主题$\text{topic}\*t$生成的第$j$个单词。
   - 对于后者，$w^i_j$表示文档$\mathcal D_i$中的第$j$个单词。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/PPouUAo7ESg4.png?imageslim">
   </p>
   

#### 3.2.3 联合概率

1. 数据集$\mathbb D$的联合概率分布为：

   其中：

   - $\mathbf{\vec n}\*z(i)=(n_z(i,1),n_z(i,2),\cdots,n_z(i,T))$表示文档$\mathcal D_i$中，各主题出现的次数。*
   - $\mathbf{\vec n}\*v(t)=(n_v(t,1),n_v(t,2),\cdots,n_v(t,V))$表示主题$\text{topic}\*t$生成的单词中，各单词出现的次数。

#### 3.2.4 后验概率

1. 若已知文档$\mathcal D_i$中的主题${\text{topic}\*{z^i_1},\text{topic}\*{z^i_2},\cdots,\text{topic}*{z^i_{n_i}}}$，则有：

   则有：$p(\vec\varphi_i\mid\text{topic}{z^i_1},\text{topic}*{z^i_2},\cdots,\text{topic}*{z^i_{n_i}}) \sim Dir(\vec\varphi_i;\mathbf{\vec n}*z(i)+\vec\alpha)$。这说明参数$\vec\varphi_i$的后验分布也是狄里克雷分布。*

2. *若已知主题$\text{topic}\*t$及其生成的单词${\text{word}\*{w^t_1},\cdots,\text{word}*{w^t_{n_t}}}$则有：

   则有：$p(\vec\theta_t\mid \text{topic}*t,\text{word}*{w^t_1},\cdots,\text{word}{w^t_{n_t}}) \sim Dir(\vec\theta_t;\vec{\mathbf n}*{v}(t)+\vec\eta)$。这说明参数$\vec\theta_t$的后验分布也是狄里克雷分布。*

### *3.3 模型求解*

1. *`LDA`的求解有两种办法：变分推断法、吉布斯采样法。*

#### *3.3.1 吉布斯采样*

1. *对于数据集$\mathbb D$：*

   - *其所有的单词${\text{word}*{w^1_1},\cdots,\text{word}*{w^1*{n_1}},\cdots,\text{word}*{w^N_1},\cdots,\text{word}*{w^N_{n_N}}}$是观测的已知数据，记作$\mathbf {WORD}$。
   - 这些单词对应的主题${\text{topic}*{z^1_1},\cdots,\text{topic}*{z^1_{n_1}},\cdots,\text{topic}*{z^N_1},\cdots,\text{topic}*{z^N_{n_N}}}$是未观测数据，记作$\mathbf {TOPIC}$。

   需要求解的分布是：$p(\mathbf{WORD}\mid \mathbf{TOPIC})$。其中：$v=w^i_j$表示文档$\mathcal D_i$的第$j$个单词为$\text{word}*v$，$t=z_j^i$表示文档$\mathcal D_i$的第$j$个单词由主题$\text{topic}\*t$生成。

2. 定义$\mathbf{TOPIC}\*{\neg{(i,j)}}$为：去掉$\mathcal D_i$的第$j$个单词背后的那个生成主题（注：只是对其频数减一）：*

   *定义$\mathbf {WORD}\*{\neg{(i,j)}}$为：去掉$\mathcal D_i$的第$j$个单词：

   根据吉布斯采样的要求，需要得到条件分布：

   $p(\text{topic}\*{z^i_j} \mid \mathbf{TOPIC}\*{\neg{(i,j)}},\mathbf{WORD})$

   根据条件概率有：

   则有：

3. 对于文档$\mathcal D_i$的第$j$个位置，单词$\text{word}\*{w^i_j}$和对应的主题$\text{topic}\*{z^i_j}$仅仅涉及到如下的两个`狄里克雷-多项式`共轭结构：

   - 文档$\mathcal D_i$的主题分布$\vec\alpha \rightarrow \vec\varphi_i$
   - 已知主题为$\text{topic}\*{z^i_j}$的情况下单词的分布$\vec\eta \rightarrow \vec\theta_{z^i_j}$*

   *对于这两个共轭结构，去掉文档$\mathcal D_i$的第$j$个位置的主题和单词时：*

   - *先验分布（狄里克雷分布）：保持不变。*

   - *文档$\mathcal D_i$的主题分布：主题$\text{topic}\*{z^i_j}$频数减少一次，但是该分布仍然是多项式分布。其它$N-1$个文档的主题分布完全不受影响。因此有：

     $p(\mathbf{TOPIC}\*{\neg{(i,j)}};\vec\alpha)=\prod_{i=1}^{N}\frac{B(\mathbf{\vec n}^\prime_z(i)+\vec\alpha)}{B(\vec\alpha)}$*

   - *主题$\text{topic}\*{z^i_j}$的单词分布：单词$\text{word}\*{w^i_j}$频数减少一次，但是该分布仍然是多项式分布。其它$T-1$个主题的单词分布完全不受影响。因此有：*

     *$p(\mathbf{WORD}\*{\neg{(i,j)}}\mid \mathbf{TOPIC}\*{\neg{(i,j)}};\vec\eta)=\prod_{t=1}^T\frac{B(\vec{\mathbf n}^\prime_{v}(t)+\vec\eta)}{B(\vec\eta)}$*

   - *根据主题分布和单词分布有：*

     *$p(\mathbf{TOPIC}\*{\neg{(i,j)}},\mathbf{WORD}\*{\neg{(i,j)}};\vec\alpha,\vec\eta) =\prod_{i=1}^{N}\prod_{t=1}^T\frac{B(\mathbf{\vec n}^\prime_z(i)+\vec\alpha)}{B(\vec\alpha)}\frac{B(\vec{\mathbf n}^\prime_{v}(t)+\vec\eta)}{B(\vec\eta)}$*

     *其中：*

     - *$\mathbf{\vec n}^\prime_z(i)=(n^\prime_z(i,1),n^\prime_z(i,2),\cdots,n^\prime_z(i,T))$表示去掉文档$\mathcal D_i$的第$j$个位置的单词和主题之后，第$i$篇文档中各主题出现的次数。*
     - *$\mathbf{\vec n}^\prime_v(t)=(n^\prime_v(t,1),n^\prime_v(t,2),\cdots,n^\prime_v(t,V))$表示去掉文档$\mathcal D_i$的第$j$个位置的单词和主题之后，数据集$\mathbb D$中，由主题$\text{topic}\*t$生成的单词中各单词出现的次数。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/Apcsn7EJpYS7.png?imageslim">
     </p>
     

4. 考虑$p(\text{topic}\*{z^i_j},\text{word}\*{w^i_j}\mid \mathbf{TOPIC}\*{\neg{(i,j)}},\mathbf{WORD}\*{\neg{(i,j)}})$。记$t=z_j^i,v=w_j^i$，则有：

   考虑到主题生成过程和单词生成过程是独立的，则有：

   考虑到文档$\mathcal D_i$的第$j$个位置的单词背后的主题选择过程和其它文档、以及本文档内其它位置的主题选择是相互独立的，则有：

   考虑到文档$\mathcal D_i$的第$j$个位置的单词选择过程和其它文档、以及本文档内其它位置的单词选择是相互独立的，则有：

   则有：

   根据狄里克雷分布的性质有：

   则有：

   其中：$t=z_j^i$为文档$\mathcal D_i$的第$j$个位置的单词背后的主题在主题表的编号；$v=w_j^i$为文档$\mathcal D_i$的第$j$个位置的单词在词汇表中的编号。

5. 根据上面的推导，得到吉布斯采样的公式（$t=z^i_j, v=w_j^i$）：

   $p(\text{topic}\*{t} \mid \mathbf{TOPIC}\*{\neg{(i,j)}},\mathbf{WORD}) \propto \frac{n_z^{\prime}(i,t)+\alpha_{t}}{\sum_{t'=1}^T\left[n_z^{\prime}(i,t')+\alpha_{t'}\right]}\times \frac{n_v^{\prime}(t,v)+\eta_{v}}{\sum_{v'=1}^V\left[n_v^{\prime}(t,v')+\eta_{v'}\right]}$

   - 第一项刻画了：文档$\mathcal D_i$中，第$j$个位置的单词背后的主题占该文档所有主题的比例（经过$\vec\alpha$先验频数调整）。
   - 第二项刻画了：在数据集$\mathbb D$中，主题$\text{topic}\*{t}$中，单词$\text{word}\*{v}$出现的比例（经过$\vec\eta$先验频数调整）。
   - 它整体刻画了：文档$\mathcal D_i$中第$j$个位置的单词为$\text{word}\*{v}$，且由主题$\text{topic}\*{t}$生成的可能性。

6. 令：

   - $\alpha_{\mathbb D} = \sum_{t^\prime=1}^T\alpha_{t^\prime}$为数据集中所有主题的先验频数之和
   - $\eta_{\mathbb D} = \sum_{v^\prime=1}^V\eta_{v^\prime}$为数据集中所有单词的先验频数之和
   - $\sum_{t'=1}^T n_z^{\prime}(i,t')$表示去掉文档$\mathcal D_i$位置$j$的主题之后，文档$\mathcal D_i$剩下的主题总数。它刚好等于$n_i-1$，其中$n_i$表示文档$\mathcal D_i$中单词总数，也等于该文档中的主题总数。
   - $F(t)$表示：数据集$\mathbb D$中属于主题$t$的单词总数。
   - $\sum_{v'=1}^V n_v^{\prime}(t,v')$表示去掉文档$\mathcal D_i$位置$j$的单词之后，数据集$\mathbb D$中属于主题$t$的单词总数，则它等于$F(t) -1$。

   则有：

   $p(\text{topic}\*{t} \mid \mathbf{TOPIC}\*{\neg{(i,j)}},\mathbf{WORD}) \propto \frac{n_z^{\prime}(i,t)+\alpha_{t}}{n_i-1+\alpha_{\mathbb D}}\times \frac{n_v^{\prime}(t,v)+\eta_{v}}{F(t)-1 +\eta_{\mathbb D}}$

   考虑到对于文档$\mathcal D_i$来讲，$n_i -1+\alpha_\mathbb D$是固定的常数，因此有：

   $p(\text{topic}\*{t} \mid \mathbf{TOPIC}\*{\neg{(i,j)}},\mathbf{WORD}) \propto \left(n_z^{\prime}(i,t)+\alpha_{t}\right) \times \frac{n_v^{\prime}(t,v)+\eta_{v}}{F(t)-1 +\eta_{\mathbb D}}$

7. 事实上，上述推导忽略了一个核心假设：考虑到采取词袋假设，`LDA` 假设同一篇文档中同一个单词（如：`喜欢`）都由同一个主题生成。

   定义$p(\text{topic}\*{t} \mid \mathbf{TOPIC}\*{\neg{(i,v)}},\mathbf{WORD})$为：已知所有单词，以及去掉文档$\mathcal D_i$中单词$\text{word}\*v$出现的所有位置（对某个单词，如`喜欢`，可能在文档中出现很多次）背后的主题的条件下，单词$\text{word}\*v$由主题$\text{topic}\*{t}$生成的概率。

   则有：

   $p(\text{topic}\*{t} \mid \mathbf{TOPIC}\*{\neg{(i,v)}},\mathbf{WORD}) \propto \frac{n\*{z,v}^{\prime}(i,t)+\alpha_{t}}{\sum_{t'=1}^T\left[n_{z,v}^{\prime}(i,t')+\alpha_{t'}\right]}\times \frac{n_{v,v}^{\prime}(t,v)+\eta_{v}}{\sum_{v'=1}^V\left[n_{v,v}^{\prime}(t,v')+\eta_{v'}\right]}$*

   *其中：*

   - *$n_{z,v}^\prime(i,t)$表示：去掉单词$\text{word}\*v$出现的所有位置背后的主题之后，文档$\mathcal D_i$剩余的主题中，属于主题$\text{topic}\*t$的总频数。则根据定义有：

     $\sum\*{t'=1}^T\left[n\*{z,v}^{\prime}(i,t')+\alpha_{t'}\right ] = n_i - n_w(i,v) + \alpha_{\mathbb D}$*

     *其中$n_i$表示文档$\mathcal D_i$中单词总数，也等于该文档中的主题总数；$n_w (i,v)$为文档$\mathcal D_i$中单词$\text{word}\*v$出现的次数。

   - $n\*{v,v}^\prime(t,v)$表示：去掉文档$\mathcal D_i$单词$\text{word}\*v$出现的所有位置背后的主题之后，数据集$\mathbb D$中由主题$t$生成的单词$\text{word}\*v$总数。则根据定义有：

     $\sum\*{v'=1}^V\left[n\*{v,v}^{\prime}(t,v')+\eta_{v'}\right] = F(t) +\eta_{\mathbb D} -n_w(i,v)$*

     *其中$F(t)$表示数据集$\mathbb D$中属于主题$t$的单词总数。*

   *因此得到：*

   *$p(\text{topic}\*{t} \mid \mathbf{TOPIC}\*{\neg{(i,v)}},\mathbf{WORD}) \propto \frac{n_{z,v}^{\prime}(i,t)+\alpha_{t}}{n_i - n_w(i,v)+\alpha_\mathbb D}\times \frac{n_{v,v}^{\prime}(t,v)+\eta_{v}}{F(t) - n_w(i,v) +\eta_\mathbb D}$*

   *这称作基于单词的采样：每个单词采样一次，无论该单词在文档中出现几次。这可以确保同一个文档中，相同的单词由同一个主题生成。*

   *前面的采样方式称作基于位置的采样：每个位置采样一次。这种方式中，同一个文档的同一个单词如果出现在不同位置则其主题很可能会不同。*

#### *3.3.2 模型训练*

1. *定义`文档-主题`计数矩阵$\mathbf T$为：*

   *其中第$i$行代表文档$\mathcal D_i$的主题计数。*

   *定义`主题-单词`计数矩阵$\mathbf W$为：*

   *其中第$t$行代表 主题$\text{topic}\*t$的单词计数

2. `LDA`训练的吉布斯采样算法（基于位置的采样）

   - 输入：

     - 单词词典$\mathbb V$
     - 超参数$\vec\alpha,\vec\eta$
     - 主题数量$T$
     - 语料库$\mathbb D$

   - 输出：

     - 文档-主题分布$\vec\varphi_i$的估计量

     - 主题-单词分布$\vec\theta_t$的估计量

       > 因为这两个参数都是随机变量，因此使用它们的期望来作为一个合适的估计

   - 算法步骤：

     - 设置全局变量：

       - $n^z\*{i,t}$表示文档$\mathcal D_i$中，主题$\text{topic}\*t$的计数。它就是$n_z(i,t)$，也就是$\mathbf T$的第$i$行第$t$列。
       - $n^v\*{t,v}$表示主题$\text{topic}\*t$中，单词$\text{word}\*v$的计数。它就是$n_v(t,v)$，也就是$\mathbf W$的第$t$行第$v$列。
       - $n^z_i$表示各文档$\mathcal D_i$中，主题的总计数。它也等于该文档的单词总数，也就是文档长度，也就是$\mathbf T$的第$i$行求和。
       - $n^v_t$表示单主题$\text{topic}\*t$中，单词的总计数。它也就是$\mathbf W$的第$t$行求和。*

     - 随机初始化：

       - 对全局变量初始化为 0 。
       - 遍历文档：$i \in {1,2,\cdots,N}$
         - 对文档$\mathcal D_i$中的每一个位置$j=1,2,\cdots,n_i$，其中$n_i$为文档$\mathcal D_i$的长度：
           - 随机初始化每个位置的单词对应的主题：$\text{topic}\*{z^i_j}\rightarrow z^i_j=t\sim Mult(\frac 1T)$
           - 增加“文档-主题”计数：$n^z\*{i,t}+=1$*
           - *增加“文档-主题”总数：$n^z_i+=1$*
           - *增加“主题-单词”计数：$n^v*{t,v}+=1$
           - 增加“主题-单词”总数：$n^v_t+=1$

     - 迭代下面的步骤，直到马尔科夫链收敛：

       - 遍历文档：$i \in {1,2,\cdots,N}$

         - 对文档$i$中的每一个位置$j=1,2,\cdots,n_i$，其中$n_i$为文档$\mathcal D_i$的长度：

           - 删除该位置的主题计数，设$t=z_j^i,v=w_j^i$：

           - 根据下面的公式，重新采样得到该单词的新主题$\text{topic}*{z^i_j}$：*

             *$p(\text{topic}*{t} \mid \mathbf{TOPIC}*{\neg{(i,j)}},\mathbf{WORD}) \propto \ \frac{n_z^{\prime}(i,t)+\alpha*{t}}{\sum_{t'=1}^T\left[n_z^{\prime}(i,t')+\alpha_{t'}\right]}\times \frac{n_v^{\prime}(t,v)+\eta_{v}}{\sum_{v'=1}^V\left[n_v^{\prime}(t,v')+\eta_{v'}\right]}$

           - 记新的主题在主题表中的编号为$\tilde t$，则增加该单词的新的主题计数：

       - 如果马尔科夫链收敛，则根据下列公式生成`文档-主题分布`$\Phi$的估计，以及`主题-单词`分布$\Theta$的估计：

3. 如果使用基于单词的采样，则训练过程需要调整为针对单词训练，而不是针对位置训练：

   - 对文档$\mathcal D_i$中的每一个词汇$\text{word}*v, v \in { v*{i,1},v_{i,2},\cdots,v_{i,n_v^i}}$，其中$n^i_v$为出现在文档$\mathcal D_i$的词汇构成的词汇表的大小。：

     - 随机初始化每个词汇对应的主题：$\text{topic}*{z^i_v}\rightarrow z^i_v=t\sim Mult(\frac 1T)$*
     - *增加“文档-主题”计数：$n^z*{i,t}+=n_w(i,v)$
     - 增加“文档-主题”总数：$n^z_i+=n_w(i,v)$
     - 增加“主题-单词”计数：$n^v_{t,v}+=n_w(i,v)$
     - 增加“主题-单词”总数：$n^v_t+=n_w(i,v)$

     其中$n_w(i,v)$表示文档$\mathcal D_i$中单词$\text{word}*v$出现的次数。*

   - *采样公式：*

     *$p(\text{topic}*{t} \mid \mathbf{TOPIC}*{\neg{(i,v)}},\mathbf{WORD}) \propto \frac{n*{z,v}^{\prime}(i,t)+\alpha_{t}}{n_i - n_w(i,v)+\alpha_\mathbb D}\times \frac{n_{v,v}^{\prime}(t,v)+\eta_{v}}{F(t) - n_w(i,v) +\eta_\mathbb D}$

   - 主题更新公式：

4. 通常训练时对$\mathbf T$和$\mathbf D$进行批量更新：每采样完一篇文档或者多篇文档时才进行更新，并不需要每次都更新。

   - 每次更新会带来频繁的更新需求，这会带来工程实现上的难题。如分布式训练中参数存放在参数服务器，频繁更新会带来大量的网络通信，网络延时大幅增加。
   - 每次更新会使得后一个位置（或者后一个单词）的采样依赖于前一个采样，因为前一个采样会改变文档的主题分布。这使得采样难以并行化进行，训练速度缓慢。

   这使得训练时隐含一个假设：在同一篇文档的同一次迭代期间，`文档-主题` 计数、`主题-单词` 矩阵保持不变。即：参数延迟更新。

#### 3.3.3 模型推断

1. 理论上可以通过最大似然估计来推断新的文档$\mathcal D_{new}$的主题分布。设新文档有$n$个单词，分别为$\text{word}*{w_1},\cdots,\text{word}*{w_{n} }$。 假设这些单词背后的主题分别为$\text{topic}*{z_1},\cdots,\text{topic}*{z_{n}}$。则有：

   由于单词的生成是独立的，且主题的单词分布是已经求得的，因此有：

  $p(\text{word}*{w_1},\cdots,\text{word}*{w_{n}}\mid \text{topic}*{z_1},\cdots,\text{topic}*{z_{n}};\vec\eta) = \prod_{i=1}^n p(\text{word}*{w_i}\mid \text{topic}*{z_{i}};\vec\eta)=\prod_{i=1}^n \hat\theta_{z_i,w_i}$

   由于主题的选择是独立的，但是文档的主题分布未知，该主题分布是从狄里克雷分布采样。因此有：

   其中$n_z(t)$为文档中主题$\text{topic}*t$的频数。*

   *因此有：*

   *$\mathcal L(\mathcal D*{new}) = \prod_{i=1}^n \hat\theta_{z_i,w_i}\times B(\vec\alpha)\prod_{t=1}^T\varphi_{t}^{n_z(t)+\alpha_t-1} \propto \prod_{i=1}^n \hat\theta_{z_i,w_i}\prod_{t=1}^T\varphi_{t}^{n_z(t)+\alpha_t-1}$

   由于$\text{topic}*{z_i}$取值空间有$T$个，则新文档中可能的主题组合有$T^n$种，因此最大似然$\max \mathcal L(\mathcal D*{new})$计算量太大而无法进行。

2. 有三种推断新文档主题分布的策略。假设训练文档集合为$\mathbb D_1$，待推断的文档集合为$\mathbb D_2$，二者的合集为$\mathbb D_{all} = \mathbb D_1\bigcup \mathbb D_2$。

   - 完全训练：

     - 首先单独训练$\mathbb D_1$到模型收敛。
     - 然后加入$\mathbb D_2$，并随机初始化新文档的主题，继续训练模型到收敛。

     这种做法相当于用$\mathbb D_1$的训练结果为$\mathbb D$的主题进行初始化（$\mathbb D_2$的部分仍然保持随机初始化）。其推理的准确性较高，但是计算成本非常高。

   - 固定主题：

     - 首先单独训练$\mathbb D_1$到模型收敛。
     - 然后加入$\mathbb D_2$，并随机初始化新文档的主题，继续训练模型到收敛。训练过程中固定$\mathbb D_1$的主题。

     这种做法只需要在第二轮训练中更新$\mathbb D_2$的主题。

   - 固定单词：

     - 首先单独训练$\mathbb D_1$到模型收敛。
     - 然后训练一篇新文档$\mathcal D_{new}$。训练过程中，使用训练集合$\mathbb D_1$的主题-单词计数矩阵$\mathbf W$。

     这种做法可以在线推断，它每次只处理一篇新文档（前面两个版本每次处理一批新文档）。

## 四、LDA优化

1. 标准`LDA` 的模型训练过程中，对于文档$\mathcal D_{i}$（令$t=z^i_j, v=w_j^i$，采用基于位置的更新） ：

  $p(\text{topic}*{t} \mid \mathbf{TOPIC}*{\neg{(i,j)}},\mathbf{WORD}) \propto \left(n_z^{\prime}(i,t)+\alpha_{t}\right) \times \frac{n_v^{\prime}(t,v)+\eta_{v}}{F(t) -1 +\eta_{\mathbb D}}$

   常规的采样方式是：

   - 计算$p_t = p(t)= p(\text{topic}*{t} \mid \mathbf{TOPIC}*{\neg{(i,j)}},\mathbf{WORD}),t=1,2,\cdots,T$
   - 将采样空间划分为：第一段$[0,p_1 )$， 第二段$[p_1,p_1+p_2)$，... 第$T$段$\left[\sum_{t=1}^{T-1}p_t, \sum_{t=1}^Tp_t \right)$，称作分桶。
   - 在$\left[0, \sum_{t=1}^Tp_t\right]$之间均匀采样一个点，该点落在哪个桶内则返回对应桶的主题。

2. 常规`LDA` 采样算法：

   - 在$\left[0, \sum_{t=1}^Tp_t\right]$之间均匀采样一个点，假设为$x$。其算法复杂度为$O(T)$，因为需要计算$\sum_{t=1}^Tp_t$。
   - 查找下标$K$，使得满足$\sum_{t=1}^{K-1}p_t \le x\lt \sum_{t=1}^Kp_t$，返回$K$。通过二分查找，其算法复杂度为$O(\log T)$。

3. 事实上如果执行非常多次采样，则常规`LDA` 采样算法第一步的成本可以分摊到每一次采样。因此整体算法复杂度可以下降到$O(\log T)$。如果仅执行一次采样，则整体算法复杂度为$O(T)$。

  $p(t)$与$v$有关，而$v = w_j^i$在文档$\mathcal D_i$的不同位置$j$取值不同。这意味着每一次采样的概率分布都不同，因此采样每个主题的算法复杂度为$O(T)$。

4. 有三种算法对采样过程进行了加速：

   - `SparseLDA` 和 `AliasLDA` 使用基于位置的采样，这是为了方便对采样概率进行有效分解。
   - `LightLDA` 使用基于单词的采样，这种采样更能够满足`LDA` 的假设：`LDA` 假设同一篇文档中同一个单词都由同一个主题生成。

### 4.1 SparseLDA

1. `sparseLDA` 推断比传统 `LDA` 快大约20倍，且内存消耗更少。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/U2RLQKNIVUlO.png?imageslim">
   </p>
   

#### 4.1.1 概率分解

1. `SparseLDA` 主要利用了`LDA` 的稀疏性。事实上，真实场景下的一篇文档只会包含少数若干个主题，一个词也是参与到少数几个主题中。因此可以将采样公式分解（令$t=z^i_j, v=w_j^i$，采用基于位置的更新）：

   令：

  $R^i = \sum_{t=1}^T r(t);\quad S^i = \sum_{t=1}^T s(t);\quad Q^i = \sum_{t=1}^T q(t)$

   其中：$R ^i$仅仅和主题的单词统计有关；$S ^i$和文档$\mathcal D_i$的主题统计、主题的单词统计都有关；$Q ^i$和文档$\mathcal D_i$的主题统计、主题的单词统计、文档$\mathcal D_i$的单词统计都有关。

   假设$\eta_1=\eta_2=\cdots=\eta$为常数，即：主题的所有单词的先验频数都为常数$\eta$。现在仅考虑一篇文档的训练，因此忽略角标$i$，有：

2. 令$U=\sum_{t=1}^T p_t$，原始`LDA` 的采样方式为：

   - 将$[0, U)$划分为$T$个桶，将每个桶根据其$(r,s,q)$分解继续划分为三个子桶。
   - 根据从均匀分布$(0 \sim U)$中采样一个点。该点落在那个桶，则采样该桶对应的主题。

   假设该点落在桶$t$，则有三种情况：

   - 若该点落在 `r` 对应的子桶，其概率为$\frac{p_t}{U}\times \frac{r(t)}{p_t}$。
   - 若该点落在 `s` 对应的子桶，其概率为$\frac{p_t}{U}\times \frac{s(t)}{p_t}$。
   - 若该点落在 `q` 对应的子桶，其概率为$\frac{p_t}{U}\times \frac{q(t)}{p_t}$。

3. 考虑重新组织分桶，按照$R,S,Q$划分为 `[0,R)、[R,R+S)、[R+S,U)` 三个桶。将第一个桶根据$r(1),\cdots,r(T)$划分为 `T` 个子桶； 将第二个桶根据$s(1),\cdots,s(T)$划分为 `T` 个子桶；将第三个桶根据$q(1),\cdots,q(T)$划分为 `T` 个子桶。

   则上述采样过程的三种情况等价于：从均匀分布$(0 \sim U)$中采样一个点。该点落在那个桶，则就在桶内的子桶中继续采样主题。

   - 该桶落在 `R` 桶的子桶 `t` ，其概率为：$\frac{R}{U}\times \frac{r(t)}{R}$。
   - 该桶落在 `S` 桶的子桶 `t` ，其概率为：$\frac{S}{U}\times \frac{s(t)}{S}$。
   - 该桶落在 `Q` 桶的子桶 `t` ，其概率为：$\frac{Q}{U}\times \frac{q(t)}{Q}$。 <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/FP5xyQsrg36f.png?imageslim">
   </p>
   

#### 4.1.2 采样过程

1. `sparseLDA` 采样过程中假设：在同一篇文档的同一次迭代期间，`文档-主题` 计数、`主题-单词` 矩阵保持不变。即：参数延迟更新。

2. `sparseLDA` 采样过程：从均匀分布$(0 \sim U)$中采样一个点$x$。

   - 如果$x\lt R$，则从$(0\sim R)$中继续均匀采样一个点，设该点落在子桶$[r(t-1),r(t))$，则返回主题$\text{topic}*t$。*

     *考虑到$r(t )= \frac{ \alpha*{t}\times \eta}{F(t) -1 +\eta V}$对于同一篇文档中的所有位置都相同，这使得对桶$R$采样的平摊算法复杂度为$O(\log T)$。

   - 如果$R\lt x\le R+S$，则从$(0\sim S)$中继续均匀采样一个点，设该点落在子桶$[s(t-1),s(t))$，则返回主题$\text{topic}*t$。*

     *此时只需要考虑当前文档中出现的主题，因为对于其它未出现的主题有$n_z^{\prime}(t)=0$，则$s(t) = \frac{n_z^{\prime} (t)\times \eta }{F(t) -1 +\eta V }=0$。*

     *因此对桶$S$采样的算法复杂度为$O(T_d)$，$T_d$为文档中出现的主题数量。 同时内存消耗更少，因为只需要保存非零值对应的分量。*

   - *如果$R+S\lt x\le U$，则从$(0\sim Q)$中继续均匀采样一个点，设该点落在子桶$[q(t-1),q(t))$，则返回主题$\text{topic}\*t$。

     此时只需要考虑使得当前单词$\text{word}\*v$的 `主题-单词` 计数非零的那些主题。因为对于其它主题有$n_v^{\prime}(t,v)= 0$，则$q(t) = \frac{\left(n_z^{\prime}(t)+\alpha\*{t}\right)\times n_v^{\prime}(t,v)}{F(t) -1 +\eta V} = 0$。

     因此对桶$Q$采样的算法复杂度为$O(T\*{v})$，$T*{v}$为当前单词$\text{word}*v$涉及到的主题数量 。 同时内存消耗更少，因为只需要保存非零值对应的分量。*

   - *整体推断的算法复杂度为$O(T_d+T*{v} + \log T)$。

3. 对于小数据集，$T_d +T_{v} +\log T\ll T$，但是对于大数据集可能发生$T_{v} \rightarrow T$。即：单词$\text{word}*v$出现在了几乎所有主题中。*

   *假设单词$\text{word}\*v$由主题$t$生成的可能性为$\delta$，则$n$篇文档中$\text{word}\*v$至少由主题$t$生成的一次的概率为：

   $\lim \*{n \rightarrow \infty}1-(1-\delta )^n = 1$*

   因此对于非常大的数据集对于 `sparseLDA`是不利的。

### 4.2 AliasLDA

#### 4.2.1 概率分解

1. `AliasLDA` 通过引入 `Alias Table` 和 `Metropolis-Hastings` 方法来进一步加快采样速度。

2. `AliasLDA` 对采样公式进行不同的分解（令$t=z^i_j, v=w_j^i$，采用基于位置的更新）：

   其中$Q$与文档无关。

3. `AliasLDA` 采样过程是：从均匀分布$(0 \sim U)$中采样一个点$x$。

   - 如果$x<=S$，则从$(0\sim S)$中继续均匀采样一个点，设该点落在子桶$[s(t-1),s(t))$，则返回主题$\text{topic}\*t$。*

     此时只需要考虑当前文档中出现的主题，因为对于其它未出现的主题有$n^{\prime}\*z(t)=0$，则$s(t) = n_z ^{\prime}(t) \frac{n_v^{\prime} (t,v)+\eta}{F(t) -1+\eta V}=0$。*

     因此对桶$S$采样的算法复杂度为$O(T_d)$，$T_d$为文档中出现的主题数量。 同时内存消耗更少，因为只需要保存非零值对应的分量。

   - 如果$x>S$，则从$(0\sim Q)$中继续均匀采样一个点，设该点落在子桶$[q(t-1),q(t))$，则返回主题$\text{topic}\*t$。*

     如果能够使得该子采样的算法复杂度为$O(1)$，则整体采样的算法复杂度为$O(T_d)$，要大大优于 `sparseLDA`的$O(T_d+T\*{v} + \log T)$。

#### 4.2.2 Alias Table

1. `Alias Table` 用于将离散的、非均匀分布转换成离散的、均匀的分布。这是为了采样方便：离散均匀分布的采样时间复杂度为$O(1)$。

2. 假设一个离散、非均匀分布为${P(X=1)=\frac 12,P(X=2)=\frac{1}{3},P(X=3)=\frac{1}{12},P(X=4)=\frac{1}{12} }$。

   常规的采样方式为：

   - 分桶：`1:[0,1/2), 2:[1/2,5/6), 3:[5/6,11/12), 4:[11/12,1)` 。
   - 采样：从`0~1` 之间均匀生成一个随机数 `x` ，`x` 落在哪个桶（二分查找），则返回对应的桶的编号。

   其平均每次采样的复杂度为$O(\log N)$，其中$N$为事件的数量。

3. 一个改进的方法是：

   - 建立四个分桶，桶的编号分别为 `1~4` 。
   - 从`1~4` 中均匀采样一个整数，决定落在哪个桶。假设在第$i$个桶。
   - 从 `0~1` 之间均匀采样一个小数$x$，则：
     - 计算非归一化概率：$p_i = \frac{P(X=i)}{\max\*{j}P(X=j)}$（即：归一化概率除以其中的最大值）。*
     - *若$x\le p_i$，则接受采样，返回事件$i$；若$x\gt p_i$，则拒绝接受，重新采样（重新选择分桶、重新采样小数）。*

   *理想情况下其算法复杂度为$O(1)$，平均情况下算法复杂度为$O(N)$。*

   *可以看到：事件${X=1,X=2,X=3,X=4}$被选中的概率（非归一化）为：${\frac 14\times 1,\frac 14\times \frac 23,\frac 14\times \frac 16,\frac 14\times \frac 16}$，归一化之后就是${\frac 12,\frac 13,\frac{1}{12},\frac{1}{12} }$。*

    

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/sG4Kz93jgjEk.png?imageslim">
   </p>
   

4. *`Alias Table` 在上述思想指导下更进一步，它对概率除以均值而不是最大值。*

   - *构建 `Alias Table` （算法复杂度$O(N )$）：*

     - *初始化数组$P*{array} = [p_1,p_2,\cdots,p_N]$，第$i$个桶的容量$p_i$表示事件$i$发生的概率；$G{array}=[0,0,\cdots,0]$，$G{array}[i]$存放第$i$个桶中的另外一个事件的编号。

       第$i$个桶要么完全由事件$i$组成，要么由事件$i$和另外一个事件组成（由$G{array}[i]$给出）。每个桶最多包含2个事件。

     - 将每个桶的容量乘以$N$：$P{array}[i] \leftarrow P_{array}[i]\times N$。即：计算非归一化概率。

     - 构建队列 `A`，存放容量大于1的桶编号；构建队列 `B`，存放容量小于1的桶编号。

     - 处理每个桶，直到满足条件：队列 `B` 为空。处理方式为：

       从队列 `A` 取出一个桶，假设为$i$；从队列 `B` 取出一个桶，假设为$j$。将$j$填充到容量为1，填充的容量从$i$消减。假设消减的容量为$\Delta_{i\rightarrow j }$

       - 记录容量：$P_{array}[i] \leftarrow P_{array}[i]- \Delta_{i\rightarrow j }$；登记：$G_{array}[j] = i$。

       注意：此时不需要更新$P_{array}[j]$，$P_{array}[j] \lt 1$存放的是事件$j$的非归一化概率，也等于它在桶中的比例。

       - 若$P_{array}[i] \gt 1$，则继续放入队列`A`；若$P_{array}[i] \lt 1$，则放入队列`B` ；若$P_{array}[i] = 1$，则桶$i$处理完成。

       最终每个桶的容量都是1，桶内最多存放两个事件（由$G_{array}$记录）。

   - 从`1~N` 中均匀采样一个整数，决定落在哪个桶。

   - 从 `0~1` 之间均匀采样一个小数$x$。假设在第$j$个桶：若$x \le P_{array}[j]$，则返回事件$j$；否则返回事件$G_{array}[j]$。

<p align="center">
   <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/ca8NjXTqLjR0.png?imageslim">
</p>


1. `Alias Table` 的算法复杂度：
   - 构建 `Alias Table` 步骤复杂度$O(N)$，采样步骤复杂度$O(1)$
   - 如果采样1次，则算法复杂度为$O(N)$。如果采样非常多次，则构建 `Alias Table` 的代价会被平均，整体的平摊复杂度为$O(1)$。

#### 4.2.3 MH 采样算法

1. 如果直接对$q(t) = \alpha_{t} \frac{n^{\prime}*v (t,v)+\eta}{F(t) -1+\eta V}$采用 `Alias Table` 采样，则会发现一个严重的问题：对于文档中不同位置$j$的单词$v=w_j$不同，因此概率分布$q(t)$随位置$j$发生变化。这就相当于采样 1 次的 `Alias Table` 算法，完全没有发挥出 `Alias Table` 的优势。*

   *解决方式是：提出一个不随位置$j$变化的概率分布$\bar q(t)$，它近似原始分布$q(t)$但是更容易采样。然后采用基于 `MH` 的 `MCMC` 采样算法。*

2. *标准的 `MH` 算法需要构建状态转移矩阵，但是这里的状态转移比较特殊：状态转移概率仅与最终状态有关，与前一个状态无关，即：$q(i) = q(i\leftarrow j),\bar q(i) = \bar q(i\leftarrow j)$。*

   - *算法输入： 近似概率分布$\bar q(t)$，原始概率分布$q(t)$*

   - *算法输出：满足原始分布的采样序列${x_0,x_1,\cdots,x*{n},x_{n+1},\cdots}$

   - 算法步骤：

     - 从概率分布$\bar q(t)$中随机采样一个样本$x_0$

     - 对$i=1,2,\cdots$执行迭代。迭代步骤如下：

       - 根据$[\bar q_1,\bar q_2,\cdots,\bar q_T]$采样出候选样本$x^{*}$*

       - *计算接受率$\alpha(x^{*})$：

        $\alpha( x^{*} )=\min \left(1,\frac{q( x^{*})\bar q( x *{i-1} )}{q( x \*{i-1})\bar q( x^{\*} )} \right)$

       - 根据均匀分布从$(0,1)$内采样出阈值$u$，如果$u \le \alpha( x^{\*})$，则接受$x^{\*}$， 即$x\*{i}= x^{\*}$。否则拒绝$x^{\*}$， 即$x*{i}= x_{i-1}$。

         > 由于$\bar q(t)$近似于$q(t)$，因此大多数情况下会接受$x^{*}$。*

     - *返回采样序列${x_0,x_1,\cdots}$。*

     > *注意：返回的序列中，只有充分大的$k$之后的序列${ x_{k}, x_{k+1},\cdots}$才是服从原始分布的采样序列。*

#### *4.2.4 AliasLDA*

1. *`AliasLDA` 综合采用了 `AliasTable` 和 `MH`采样算法。对分桶$Q$采样的算法步骤：*

   - *构建不随文档中的位置$j$变化的概率分布$\bar q(t)$，算法复杂度$O(T)$。*

   - *根据$\bar q(t)$构建 `AliasTable` ，算法复杂度$O(T)$。*

   - *循环：遍历文档的所有位置$j=1,2,\cdots,n$：*

     *对于文档的当前位置$j$根据前述的 `MH` 算法采样出主题$t$，其算法复杂度为$O(1)$。*

   *考虑到前面两步的代价可以平摊到每次采样过程，因此平摊算法复杂度为$O(1)$。*

### *4.3 LightLDA*

1. *`web-scale` 规模的预料库非常庞大也非常复杂，通常需要高容量的主题模型（百万主题、百万词汇表，即：万亿参数级别）来捕捉长尾语义信息。否则，当主题太少时会丢失这些长尾语义信息。*

   *这种规模数据集的训练需要上千台机器的集群，而`LightLDA` 允许少量的机器来训练一个超大规模的`LDA` 主题模型，它可以用 8 台机器训练一个包含百万主题&百万词汇表（万亿级参数）、包含千亿级单词的数据集。*

2. *`LightLDA` 主要采用了以下方法：*

   - *更高效的`MH` 采样算法，其算法复杂度为$O(1)$，并且与当前最先进的`Gibbs` 采样器相比收敛效率快了近一个量级。*
   - *`structure-aware` 的模型并行方案，其利用主题模型中的依赖关系，产生一个节省机器内存和网络通信的采样策略。*
   - *用混合数据结构来存储模型，其针对高频单词和低频单词采用不同的数据结构，从而使得内存可以放置更大规模的模型，同时保持高效的推断速度。*
   - *有界异步数据并行方案，其可以降低网络同步和通信的成本，从而允许通过参数服务器对海量数据进行有效的分布式处理。*

#### *4.3.1 structure-aware 模型并行*

1. *数据并行：分割数据集，将不同文档集交给不同的 `worker` 来处理。所有 `worker`共享同一份模型参数，即：共享同一份全局 `主题-单词` 矩阵$\mathbf W$。*

   *如：`YahooLDA` 和 基于参数服务器的`LDA`实现就是采取数据并行。*

2. *数据并行+模型并行：分割数据集，将不同文档集交给不同的 `worker` 来处理。同时分割模型，不同 `worker` 处理不同的 `主题-单词` 分布。即：每个`worker` 只会看到并处理本地文档出现过的单词的 `主题-单词` 分布。*

   *如：`PLDA` 和 `Peacock` ，以及 `LDA` 就是采取这种策略。*

3. *为了解决模型内存需求太大与`worker` 内存较小的问题，`LightLDA` 提出了`structure-aware` 模型并行方案。*

   - *在每个`worker` 中，采样算法执行之前先将本 `worker` 分割到的数据集进一步划分为数据块 `Data Block 1,Data Block 2,...` 。同时计算每个数据块包含的单词的词汇表，并将该词汇表作为元数据附加到数据块上。*

     *该操作只需要线性扫描，其计算复杂度为$O(1)$。*

   - *在`worker` 对 `Data Block i` 执行采样时：*

     - *首先加载数据块 `Data Block i` 到 `worker` 的内存，取得该 `block` 的元数据。*

     - *然后根据元数据，将该数据块的词汇表划分为集合$\mathbb V_1,\mathbb V_2,\cdots$。*

     - *对每个词表集合$\mathbb V_j$，拉取参数服务器中的全局 `主题-单词` 矩阵$\mathbf W$中涉及$\mathbb V_j$的单词的部分，称作模型切片，记作$\mathbf W_j$。因此`worker` 仅仅需要保存模型的一个切片。*

       *然后对 `Data Block i` 中的所有文档执行采样并更新$\mathbf W_j$，采样时仅仅考虑出现在$\mathbb V_j$中的单词。*

     - *当对$\mathbf W_j$更新完毕时，将本次更新同步到参数服务器中的全局 `主题-单词` 矩阵$\mathbf W$。然后继续处理下一个词汇集合$\mathbb V_{j+1}$。*

   - *一旦对 `Data Block i` 处理完毕，继续采样下一个数据块 `Data Block i+1` 。*

   *如下图所示为 `Data Block 2` 的采样过程，$S_2$表示该数据块包含的文档数量。在`d1,d2,...` 中的箭头给出了主题采样的顺序。每个文档通过灰色块表明出现对应的单词（出现一次或多次）、白色块表明没有出现对应的单词。*

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/bWgLdkTyxxjj.png?imageslim">
   </p>
   

4. *`structure-aware` 除了降低`worker` 内存需求之外，还通过以下方式减轻网络通信瓶颈：*

   - *在处理数据块 `Data Block i` 时，只有当前模型切片相关的所有单词都被采样时，才会移动到下一个模型切片。*

     *这使得在处理数据块 `Data Block i` 时，每个模型切片只需要被换入换出内存一次，而不是反复换入换出内存。模型切片的换入换出需要和参数服务器进行同步，因此多次交换会增大通信压力。*

   - *在利用当前模型切片对数据块采样时，可以同步进行下一个模型切片的加载（从参数服务器拉取到本地）、上一个模型切片的同步更新（从本地推送到参数服务器）。*

     *这进一步降低了网络通信的时间延迟。*

#### *4.3.2 高效的 `MH` 采样算法*

1. *`AliasLDA` 采样的计算复杂度为$O(T_d)$，因此它不擅长处理比较长的文档，比如网页。这是因为：在初始迭代中，文档每个位置的主题都是随机初始化的，因此对于较长的文档$T_d$非常大。这会使得`AliasLDA` 在迭代初期非常缓慢，消耗大量时间。*

2. *当利用分布$\bar q(t)$对$q(t)$进行采样时，一个好的$\bar q(t)$需要满足两个条件（从而加速采样速度）：从$\bar q(t)$中采样相对更加容易；马尔科夫链更快混合`mix`。*

   *因此在设计分布$\bar q(t)$时，存在一个折中：*

   - *如果分布$\bar q(t)$和$q(t)$更接近，则马尔科夫链混合的更快，但是从$\bar q(t)$采样的代价可能与$q(t)$相差无几。*
   - *如果分布$\bar q(t)$非常容易采样但是与$q(t)$很不一样，则马尔科夫链混合的很慢。*

3. *所谓马尔科夫链的良好的混合意味着：马尔科夫链能够收敛，且收敛速度不会太长。*

   *当拒绝率太高时，`MCMC` 采样很可能长期处于某些值附近（如下图所示的平摊区域），搜索整个参数空间要花非常多的时间。下图中，横轴为迭代次数，纵轴表示采样结果。*

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/XrrVV6WGi48w.png?imageslim">
   </p>
   

4. *与 `AliasLDA` 和 `SparseLDA` 不同，`LightLDA` 用两个近似分布$\bar q_1(t),\bar q_2(t)$来对$p(t)$进行采样。*

   *考虑采样概率：*

   - *定义 ：*

     *$\bar q_1(t) = \frac{n_z(t)+\alpha*{t}}{n+\alpha_\mathbb D}$

     其中$n_z(t)$为文档内主题$t$的频次，$n$为文档长度。则接受率为：

    $\alpha_1( x^{} )=\min \left(1,\frac{p( x^{*})\bar q_1( x _{i-1} )}{p( x _{i-1})\bar q_1( x^{*} )} \right)$

   - 定义：

    $\bar q_2(t) = \frac{n_v (t,v)+\eta}{F(t) +\eta V}$

     其中$n (t,v)$表示数据集$\mathbb D$中由主题$t$生成的单词中单词$\text{word}*v$的次数，$F(t)$表示数据集$\mathbb D$中由主题$t$生成的单词总频数。则接受率为：*

     *$\alpha_2( x^{\*} )=\min \left(1,\frac{p( x^{\*})\bar q_2( x \*{i-1} )}{p( x \*{i-1})\bar q_2( x^{\*} )} \right)$

5. 为了提高对$\bar q_1$的采样效率，对$\bar q_1$进一步拆分：

   因此采样步骤为：

   - 首先从均匀分布$(0 \sim Q_{1,1}+Q_{1,2})$中采样一个点。
   - 如果该点落在$(0,Q_{1,1})$之间，则使用$\bar q_{1,1}(t)$进行采样。此时无需建立 `AliasTable`，直接从文档中均匀随机选择一个单词，其背后的主题就是采样得到的主题，该分布刚好就是$\bar q_{1,1}(t)$。
   - 如果该点落在$[Q_{1,1},Q_{1,1}+Q_{1,2})$之间，则使用$\bar q_{1,2}(t)$进行采样。
     - 如果$\alpha_1=\alpha_2=\cdots=\alpha$为常数，则此时$\bar q_{1,2}(t)$为均匀分布，无需建立 `AliasTable` 。
     - 如果$\alpha_t$并不是全部相等，则由于$\bar q_{1,2}(t)$与单词、文档都无关，因此可以建立全局$\bar q_{1,2}(t)$`AliasTable`，它可以跨单词、跨数据块共享，其平均时间、空间复杂度为$O(1)$。

   因此从$\bar q_1$采样的平摊时间、空间复杂度均为$O(1)$。

6. 为了提高对$\bar q_2$的采样效率，采用类似于 `AliasLDA` 的`Alias Table` 方式，从而使得对$\bar q_2$的采样复杂度为$O(1)$。

   - 注意：虽然在文档内部，$\bar q_2(t)$在不同位置处不同，是变化的，因此不满足 `Alias Table`的要求。但是在同一个数据块中，对于同一个单词$v$，$\bar q_2(t)$是相同的，是不变的，因此满足 `Alias Table`的要求。因此$\bar q_2(t)$是在同一个数据块内，跨文档共享。

   - 建立$\bar q_2(t)$的空间复杂度是$O(T)$，因为它需要存储$T$个分桶的边界，以及桶内的值。为了降低空间复杂度，可以进一步拆分：

     其采样步骤为：

     - 首先从均匀分布$(0 \sim Q_{2,1}+Q_{2,2})$中采样一个点。
     - 如果该点落在$(0,Q_{2,1})$之间，则使用$q_{2,1}(t)$进行采样。此时只需要考虑单词$v$涉及的主题即可，空间复杂度为$O(T_v)$。
     - 如果该点落在$[Q_{2,1},Q_{2,1}+Q_{2,2})$之间，则使用$q_{2,2}(t)$进行采样。它与单词$v$无关，因此可以在所有单词之间共享，这使得平均的空间复杂度为$O(1)$。

     因此对$\bar q_2$的采样的平摊时间复杂度为$O(1)$，平摊空间复杂度为$O(T_v)$。

7. 考虑到$p(t) = p_1(t) \times p_2(t)$，$p_1(t)$为`文档-主题` 概率，与$\bar q_1(t)$比较接近；$p_2(t)$为`主题-单词` 概率，与$\bar q_2(t)$比较接近。

   - 如果仅仅用$\bar q_1(t)$采样，则有可能出现$p_1(t)$较小、$p_2(t)$很大的情形。此时$p(t)$较大，而$\bar q_1(t)$较小，使得接受率较低，收敛速度较慢。
   - 同理：如果仅仅用$\bar q_2(t)$采样，则有可能出现$p_2(t)$较小、$p_1(t)$很大的情形。此时$p(t)$较大，而$\bar q_2(t)$较小，使得接受率较低，收敛速度较慢。

   因此：虽然单独利用$\bar q_1(t)$或者单独利用$\bar q_2(t)$都可以对$p(t)$进行采样，但是单独使用时收敛速度较慢。

8. `LightLDA` 联合采用$\bar q_1(t), \bar q_2(t)$来加速采样的收敛速度，它轮流在二者之间采样。这等价于用$\bar q_c(t) = \bar q_1(t) \times \bar q_2(t)$进行采样，其接受率较大、收敛速度较快。

   - 算法输入：原始函数分布$p(t)$，`文档-主题`近似分布$\bar q_1(t)$，`主题-单词`近似分布$\bar q_2(t)$

   - 算法输出：满足原始分布的采样序列${x_0,x_1,\cdots,x_{n},x_{n+1},\cdots}$

   - 算法步骤：

     - 从$\bar q_1(t)$中随机采样一个样本$x_{0}$

     - 对$i=1,2,\cdots$执行迭代。迭代步骤如下：

       - 根据$\bar q_1 (t)$采样出候选样本$x^{1,\*}$，计算接受率$\alpha(x^{1,\*})$：

         $\alpha( x^{1,\*} )=\min \left(1,\frac{p( x^{1,\*})\bar q_1( x \*{i-1} )}{p( x \*{i-1})\bar q_1( x^{1,\*} )} \right)$

         根据均匀分布从$(0,1)$内采样出阈值$u$，如果$u \le \alpha( x^{1,\*})$，则接受$x^{1,\*}$， 即$x\*{i-1}^{tmp}= x^{1,\*}$。否则拒绝$x^{\*}$， 即$x*{i-1}^{tmp}= x_{i-1}$。

       - 根据$\bar q_2 (t)$采样出候选样本$x^{2,*}$，计算接受率$\alpha(x^{2,*})$：

        $\alpha( x^{2,*} )=\min \left(1,\frac{p( x^{2,*})\bar q_1( x *{i-1}^{tmp} )}{p( x^{tmp} \*{i-1})\bar q_1( x^{2,\*} )} \right)$

         根据均匀分布从$(0,1)$内采样出阈值$u$，如果$u \le \alpha( x^{2,\*})$，则接受$x^{2,\*}$， 即$x_i= x^{2,\*}$。否则拒绝$x^{\*}$， 即$x_i= x\*{i-1}^{tmp}$。*

     - *返回采样序列${x_0,x_1,\cdots }$。*

#### *4.3.3 混合数据结构*

1. *对于百万词汇表 x 百万主题，如果每一项（`单词-主题`计数）是 `32 bit`，则需要 `4 TB`内存才能存放。假设一台机器有 `128 G`内存，则至少需要 `32` 台参数服务器。因此如果能够大幅降低内存，则对硬件资源的需求也能够下降。*

2. *考虑词频统计，在数十亿网页文件中：*

   - *在删除停用词之后，几乎所有单词的词频不超过 32 位无符号整数的上限。因此`主题-单词` 矩阵的元素采用 32 位无符号整数存放。*
   - *大多数单词出现的次数少于$T$次，$T$为主题数量（论文中为一百万）。这意味着`主题-单词` 矩阵中，大多数单词列是稀疏的，如果采用稀疏表示( 如`hash` 映射) 将显著减少内存需求。*

3. *采用稀疏表示存在一个问题：其随机访问性能很差，这会损害`MCMC` 采样的速度。为了保持高采样性能、低内存需求，`LightLDA` 提出了混合数据结构：*

   - *对于词频较高的单词，其对应的主题计数列是不做修改的、方便随机访问的 `dense` 表示。这部分单词占据词表的 `10%` ，几乎覆盖了语料库中 `95%` 的位置。*
   - *对于词频较低的长尾单词，其对应的主题计数列经过 `hash` 映射的 `sparse` 表示，使得内存需求大幅降低。这部分单词占据词表的 `90%`，仅仅覆盖了语料库中 `5%` 的位置。*

   *这就使得：*

   - *在`MCMC` 采样中，对大多数`主题-单词` 列的访问都是 `dense` 表示，这使得采样的吞吐量很高。*
   - *在存储方面，大多数`主题-单词`列的存储是`sparse` 表示，这使得内存需求较低。*

#### *4.3.4 系统实现*

1. *`LightLDA` 使用开源的分布式机器学习系统 `Petuum` 来实现，特别使用其参数服务器来进行有界异步数据并行。*

2. *参数服务器用于存放 `LDA` 的两类模型参数：`主题-单词` 矩阵$\mathbf W$，主题频次向量$\mathbf{\vec w} = (n^v_1,n^v_2,\cdots,n^v_T)$，其中$n^v_t$为主题$t$生成的单词的总数（它也等于$\mathbf W$第$t$行的求和）。*

3. *因为语料库的规模远大于模型的大小，所以在网络中交换文档的代价对网络压力非常大。因此`LightLDA` 并不是交换数据，而是交换模型：先将语料库随机分配到各个`worker` 的磁盘上，然后在整个算法期间每个 `worker` 仅仅访问本地磁盘上的部分语料库。*

   > *交换数据的做法是：每轮迭代时，将语料库分发到不同 `worker`，每个 `worker` 在不同迭代步处理的数据会有所不同。*

4. *`LightLDA` 整体架构如下图所示：*

   - *各参数的意义位（这里采用论文中的符号表示）：*
     - *$n_d$：文档$d$的长度；$D_n$：数据分片中包含的文档数量。$z*{di}$：文档$d$的第$i$个词汇的主题。
     -$n_{kv}$：语料库中主题$k$生成的单词$v$的数量，即$W[k,v]$。$n_k$：语料库中主题$k$的频次，即$\mathbf W$第$t$行的求和。$n_{k,d}$：文档$d$的主题$k$的频次。
     -${w_{d_i},z_{d_i}}*{d=1,i=1}^{D_n,n_d}$：数据分片。${n*{k,v}}*{k=1,v=1}^{K,V_S}$：模型分片。${n_k}*{k=1}^K$：主题频次向量。
   - 数据分片从 `worker` 的磁盘加载到 `worker` 的内存，模型分片由 `worker` 从参数服务器进行获取。
   - 考虑到每个数据分片可能仍然非常巨大，无法一次性加载进`worker`内存。因此 `LightLDA` 将每个数据分片继续划分成数据块 `Data Block` ，并将这些数据块依次加载进内存处理。

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/w3TQDs4WpICC.png?imageslim">
   </p>
   

5. 为了加速并行处理，`LightLDA` 进行了如下优化：

   - 通过流水线来消除IO的延迟：在采样的同时可以加载数据块、拉取下一个模型分片、更新上一个模型分片。

     > 这需要根据采样器的吞吐量、数据块大小、模型切片大小仔细安排参数。

   - 为防止模型切片之间的数据不均衡（热门词和冷门词），需要对词汇表按照词频进行排序，然后对单词进行混洗来生成模型切片。

   - 在生成数据块时，将文档中的单词按照它们在词表中的顺序进行排序，从而确保同一个模型切片的所有单词在数据块中是连续的。这个操作只需要执行一次，与 `LDA` 整体消耗时间相比该操作非常快。

   - 使用有界异步数据并行来消除在相邻迭代边界处发生的网络等待时间。

6. `LightLDA` 在每个 `worker` 内部开启多线程来进一步加速算法：

   - 每个数据块在内存中被划分为不相交的区域，每个区域由不同的线程进行采样。
   - 所有线程之间共享模型切片，且采样期间模型切片不变。在将模型切片更新到参数服务器之前，对各线程的更新结果进行聚合。

   通过保持模型切片不变，这避免了竞争条件和加锁/解锁等并发问题。虽然这种延迟更新方式需要更多的迭代步才能收敛，但是由于这种方式消除了并发问题，提高了采样器吞吐率，使得每个迭代步的速度快得多，因此整体上仍然可以加速收敛速度。

## 五、sentence-LDA

### 5.1 sentence-LDA

1. `LDA` 认为每个单词对应一个主题，但是针对短文本可能每句话表示一个主题，这就是`Sentence-LDA` 的基本假设。

2. `Sentence-LDA` 的文档生成过程：

   - 根据参数为$\vec\eta$的狄利克雷分布随机采样，对每个话题$\text{topic}*t$生成一个单词分布$\vec\theta_t=(\theta*{t,1},\theta_{t,2},\cdots,\theta_{t,V})$。每个话题采样一次，一共采样$T$次。

   - 对每篇文档$\mathcal D_i$：

     - 根据参数为$\vec\alpha$的狄利克雷分布随机采样，生成文档$\mathcal D_i$的一个话题分布$\vec\varphi_i=(\varphi_{i,1},\varphi_{i,2},\cdots,\varphi_{i,T})$。每篇文档采样一次。

     - 对文档$\mathcal D_i$中的每个句子：

       - 从话题分布中$\vec\varphi_i$中采样一个话题$\text{topic}*t$，然后从话题的单词分布$\vec\theta_t$采样$n*{i,s}$个单词 。$s$为句子的编号。

         此时这些单词的话题都是$\text{topic}*t$。*

       - *重复生成$n_i$个句子，得到一篇包含$n_i$个句子的文档。*

     - *重复执行上述文档生成规则$N$次，即得到$N$篇文档组成的文档集合$\mathbb D$。*

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/r0NKdAX59HqN.png?imageslim">
   </p>
   

3. *`Sentence-LDA` 的吉布斯采样概率为：*

   *各参数的意义为：*

   - *$p(\text{topic}\*{t} \mid \mathbf{TOPIC}\*{\neg{(i,s)}},\mathbf{WORD})$：已知所有单词，以及除了第$i$篇文档的第$s$个句子之外的所有句子的主题的情况下，第$i$篇文档的第$s$个句子的主题为$\text{topic}\*t$的概率。
   - $n\*{s}^{\prime}(i,t)$：排除第$i$篇文档的第$s$句之外，第$i$篇文档中属于主题$t$的句子的数量。*
   - *$n_v^{\prime}(t,v)$：排除第$i$篇文档的第$s$句之外，数据集$\mathbb D$中属于主题$t$的词汇$v$的数量。*
   - *$\alpha_t$：文档-主题计数中，主题$t$的先验计数；$\eta_v$：主题-单词计数中，单词$v$的先验计数。*
   - *$m_s$：第$i$篇文档的第$s$句的单词总数；$m_{s,v}$：第$i$篇文档的第$s$句的词汇$v$的总数。*

4. *`Sentence-LDA` 得到的文档-主题概率分布为：*

   *$\hat \varphi_{i,t} = \frac{n_{s} (i,t)+\alpha_t}{\sum_{t^\prime=1}^ Tn_{s} (i,t^\prime) +\alpha_{t^\prime}}$*

   *主题-单词概率分布为：*

   *$\hat\theta_{t,v} = \frac{ n_v (t,v)+\eta_v }{ \sum_{v^\prime=1}^V(n_v (t,v^\prime)+\eta_v^\prime) }$*

   *其中：*

   - *$n_{s} (i,t)$：文档$i$中，属于主题$t$的句子数量；$n_v(t,v)$：数据集$\mathbb D$中，属于主题$t$的单词$v$的数量。*
   - *$\hat\varphi_{i,t},\hat\theta_{t,v}$其实分别是$\varphi_{i,t}, \theta_{t,v}$的期望值，因为$\varphi_{i,t}, \theta_{t,v}$均为随机变量。*

### *5.2 ASUM*

1. *论文 `Aspect and sentiment unification model for online review analysis` 中提出了 `sentence-LDA` 以及扩展了 `sentence-LDA` 的 `ASUM`模型。*

   *`ASUM`（`Aspect and Sentiment Unification Model` ）同时对评论的主题以及评论的情感进行建模。它认为客户撰写评论的方式为：（以餐馆评论为例）：*

   - *首先决定餐馆评价的好坏概率分布，如：70%是满意的，30%是不满意的。*
   - *然后对每个情感给出其评价主题概率分布。如：满意的主题概率分布为：50%是服务，25%是食物，25% 是价格。*
   - *最后对每个句子，表达一个主题和一个情感。即：每个句子中所有的单词背后都是同一个主题，也是同一个情感。*

2. *`ASUM` 文档生成过程：*

   - *对每一个`主题-情感` 对（情感为$\text{senti}\*e$，主题为$\text{topic}\*t$），从$\vec\beta$的狄利克雷分布随机采样，得到该主题和该情感下的单词分布：$\vec\theta\*{e,t}=(\theta\*{e,t,1},\theta_{e,t,2},\cdots,\theta_{e,t,V})$。每个`主题-情感`采样一次，一共采样$E\times T$次。*

     *其中$E$为情感的总数。*

   - *对每篇文档$\mathcal D_i$：*

     - *根据参数为$\vec\gamma$的狄利克雷分布随机采样，生成文档$\mathcal D_i$的一个情感分布$\vec\pi_i=(\pi_{i,1},\pi_{i,2},\cdots,\pi_{i,E})$。每篇文档采样一次。*

     - *对每个情感$e$，根据参数为$\vec\alpha$的狄利克雷分布随机采样，生成生成文档$\mathcal D_i$的一个话题分布$\vec\varphi_{i,e}=(\varphi_{i,e,1},\varphi_{i,e,2},\cdots,\varphi_{i,e,T})$。每篇文档采样一次。*

     - *对文档$\mathcal D_i$中的每个句子：*

       - *从情感分布$\vec\pi_i$中采样一个情感$e$，然后从话题分布中$\vec\varphi_{i,e}$中采样一个话题$\text{topic}\*t$，然后从`主题-情感`的单词分布$\vec\theta\*{e,t}$采样$n_{i,s}$个单词 。$s$为句子的编号。*

         *此时这些单词的话题都是$\text{topic}\*t$。

       - 重复生成$n_i$个句子，得到一篇包含$n_i$个句子的文档。

     - 重复执行上述文档生成规则$N$次，即得到$N$篇文档组成的文档集合$\mathbb D$。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/BgrOIWXpOQvP.png?imageslim">
     </p>
     

3. 与 `LDA` 模型不同，`ASUM` 模型中的$\vec\beta$参数是非对称的：如 `good,great` 不大可能会出现在负面情感中，`bad,annoying` 不大可能出现在正面情感中。

4. `ASUM` 的吉布斯采样概率为：

   各参数的意义为：

   - $p(\text{senti}\*{e},\text{topic}\*{t} \mid \mathbf{SENTI}\*{\neg{(i,s)}},\mathbf{TOPIC}\*{\neg{(i,s)}},\mathbf{WORD})$：已知所有单词、除了第$i$篇文档的第$s$个句子之外的所有句子的主题，以及除了第$i$篇文档的第$s$个句子之外的所有句子的情感的情况下，第$i$篇文档的第$s$个句子的情感为$\text{senti}\*e$、主题为$\text{topic}\*t$的概率。*
   - $n_x^\prime(i,e)$：排除第$i$篇文档的第$s$句之外，第$i$篇文档中属于情感$e$的句子的数量。
   - $n\*{s}^{\prime}(i,e,t)$：排除第$i$篇文档的第$s$句之外，第$i$篇文档中情感$e$的且属于主题$t$的句子的数量。
   - $n_v^{\prime}(e,t,v)$：排除第$i$篇文档的第$s$句之外，数据集$\mathbb D$中属于情感$e$且属于主题$t$的词汇$v$的数量。
   - $\gamma_e$：文档-情感计数中，情感$e$的先验计数；$\alpha_t$：文档-主题计数中，主题$t$的先验计数；$\eta\*{e,v}$：主题-单词计数中，属于情感$e$的单词$v$的先验计数。*
   - *$m_s$：第$i$篇文档的第$s$句的单词总数；$m_{s,v}$：第$i$篇文档的第$s$句的词汇$v$的总数。*

5. *`ASUM` 得到的文档-情感概率分布为：*

   *$\hat\pi_{i,e} = \frac{n_{x} (i,e)+\gamma_e}{\sum_{e^\prime=1}^ E n_{x} (i,e^\prime) +\gamma_{e^\prime}}$*

   *文档的情感-主题分布为：*

   *$\hat \varphi_{i,e,t} = \frac{n_{s} (i,e,t)+\alpha_{e,t}}{\sum_{t^\prime=1}^ Tn_{s} (i,e,t^\prime) +\alpha_{e,t^\prime}}$*

   *主题-单词概率分布为：*

   *$\hat\theta_{e,t,v} = \frac{ n_v (e,t,v)+\eta_{e,v} }{ \sum_{v^\prime=1}^V(n_v (t,e,v^\prime)+\eta_{e,v^\prime}) }$*

   *其中：*

   - *$n_{x} (i,e)$：文档$i$中，属于情感$e$的句子数量；$n_{s} (i,e, t)$：文档$i$中，属于情感$e$和主题$t$的句子数量；$n_v(e,t,v)$：数据集$\mathbb D$中，属于情感$e$和主题$t$的单词$v$的数量。*
   - *$\alpha_{e,t}$：文档中，属于情感$e$和主题$t$的 `情感-主题`二元对的先验计数。*
   - *$\hat\pi_{i,e}, \hat\varphi_{i,e,t},\hat\theta_{e,t,v}$其实分别是$\pi_{i,e}, \varphi_{i,e,t}, \theta_{e,t,v}$的期望值，因为$\pi_{i,e}, \varphi_{i,e,t}, \theta_{e,t,v}$均为随机变量。*

6. *`ASUM` 和 `SLDA`可以用于以下用途：*

   - *利用 `SLDA` 进行评论的主题抽取。*

   - *利用 `ASUM` 进行 `情感-主题` 的抽取。*

   - *自适应的扩展特定主题下的情感词。*

     - *首先进行`情感-主题` 合并。以词的分布为向量，计算`情感-主题` 的两两余弦相似度。如果结果超过一个阈值，则认为二者是相同的。*
     - *计算词的出现概率。如果一个单词在所有`情感-主题` 下都有高概率，则它是一个通用词；如果它仅仅在一个`情感-主题` 下有高概率，则它是一个特定主题下的情感词。*

   - *无监督情感分类。根据$\hat \pi_{i,e}$中，各情感的分布来执行分类。*

     *其中需要引入先验知识：$e$的取值是多少才代表正面情感。这需要观察`情感-主题` 词的分布，由人工指定。*

## *六、模型讨论*

1. *`pLSA` 容易陷入过拟合。`pLSA` 认为：*

   - *文档-主题分布$\vec\varphi_i,i=1,2,\cdots,N$不是随机变量，而是未知的常量。*
   - *主题-单词分布$\vec\theta_t,t=1,2,\cdots,T$也不是随机变量，也是未知的常量。*

   *`pLSA` 通过拟合训练数据集来求解这些参数，这意味着这些参数只能表征当前的训练集的文档的特征。对于未知的文档，`pLSA` 认为它也符合训练集的文档特征。*

   *事实上这就是一种过拟合，尤其是当训练集的文档数量太少时，非常容易陷入过拟合。*

   *`LDA` 会给$\vec\varphi_i,\vec\theta_t$加入一些先验性的知识。当数据量较小，先验性的知识会占据主导地位；当数据量较大，真实数据占据主导地位。*

2. *以人口抽样问题为类比。*

   - *`pLSA` 认为：人口的男女比例是一个常数。*

     *给出一个人口集合，`pLSA` 先统计男女比例（假如训练集是从医院获取的）。假如结果为 `2 :1`， 则 `pLSA`会断言：所有的人口比例都是 `2 : 1`。*

     *于是训练集越小，`pLSA` 越容易陷入过拟合，离真实结果也就越远。*

   - *`LDA` 会首先假设男女比例为 `1000:1000`。 然后再统计人口集合中男女的人数，最终得到的结果。*

     *假设人口集合中男女的人数分别为 `200:100`，则最终`LDA` 得到男女比例为 `1200:1100` 。虽然该结果离真实的结果可能有偏差，但是它比 `pLSA` 的结果要更好。*

3. *当数据量足够大的时候， `pLSA` 跟 `LDA` 的结果相差无几。这是因为当数据量足够大时，真实数据的信息会淹没掉先验知识。*

4. *假设有词表共有 3 个单词，主题表共有 3个主题。下面一张图形比较了 `Unigram Model`，`pLSA` 与 `LDA` 的区别。*

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200604/pHQ4iuP4CkOh.jpg?imageslim">
   </p>
   

   *最外的三角形为单词三角形。内部每个点（如$p_2$）表示一个单词概率分布，表示产生 `word1,word2,word3` 这三个词的概率的大小；靠内的三角形为主题三角形。内部每个点表示一个主题概率分布，表示产生 `topic1,topic2,topic3`这三个主题的概率的大小。*

   - *`Unigram` 模型：该模型由生成各单词的概率决定，因此单词三角形内部任意一点（如$p_2$）代表了一个 `Unigram` 模型。*

   - *`pLSA` 模型：主题三角形内任意一些点（如带叉的点所示）就是一个 `pLSA` 模型。*

     *模型先根据主题概率分布采样一个主题，然后根据该主题的单词概率分布采样一个单词。重复执行`选择主题-选择单词`的过程，即可得到一篇文档。*

   - *`LDA` 模型：主题三角形内，每一条曲线表示了一个 `pLDA` 模型。*

     *模型首先根据曲线（它代表主题概率分布的分布，即主题概率分布选择某类分布的概率较大）选择主题三角形内的一个点（它代表一个主题概率分布）；然后根据该主题概率分布随机采样一个主题；然后根据该主题的单词概率分布采样一个单词。重复执行 `选择主题 - 选择单词` 的过程，即可得到一篇文档。*

     *`topic`分布的分布，即 `topic` 分布取某些值的概率较大，取另外一些值的概率较小。它刻画了`LDA` 模型选择主题的过程。*