

# 奇异值与特征值有什么关系

<span style="color:red;">奇异值这个地方没懂。</span>

那么奇异值和特征值是怎么对应起来的呢？


我们将一个矩阵 $A$ 的转置乘以 $A$，并对 $A^TA​$ 求特征值 $\lambda$，对应的特征向量 $\nu$，则有下面的形式：

$$
(A^TA)V = \lambda V
$$

这里 $V​$ 就是上面的右奇异向量，另外还有：

$$
\sigma_i = \sqrt{\lambda_i}, u_i=\frac{1}{\sigma_i}A\mu_i
$$

这里的 $\sigma​$ 就是奇异值，$u​$ 就是上面说的左奇异向量。<span style="color:red;">为什么这个  $\sigma​$ 叫奇异值呢？$\mu_i$ 是什么？为什么是左奇异向量？这个式子是怎么得到的？过程补充下。而且，奇异值有什么用处吗？</span>

​奇异值 $\sigma​$ 跟特征值类似，在矩阵 $\sum​$ 中也是从大到小排列，而且 $\sigma​$ 的减少特别的快，在很多情况下，前 10%甚至 1%的奇异值的和就占了全部的奇异值之和的 99%以上了。也就是说，我们也可以用前 $r​$（$r​$ 远小于 $m、n​$）个的奇异值来近似描述矩阵，即部分奇异值分解：

$$
A_{m\times n}\approx U_{m \times r}\sum_{r\times r}V_{r \times n}^T
$$

<span style="color:red;">这个式子没看懂。</span>

右边的三个矩阵相乘的结果将会是一个接近于 $A$ 的矩阵，在这儿，$r$ 越接近于 $n$，则相乘的结果越接近于 $A$。<span style="color:red;">什么叫接近于 $A$ ？怎么衡量这种接近？</span>







# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
