

# 可以补充进来的


- 向量的求导要单独拿出来，把矩阵论的向量求导那一篇整合进来。
- 实对称阵不同特征值的特征向量正交，这个在一些机器学习的公式推理的时候用到很多次了，要再理解下。
- 感觉还需要整理 梳理。
- 很多地方不明白，而且为什么奇异值分解不能放在这里？而是要单开？还是说上面有一部分是属于奇异值分解的？


# 矩阵变换与特征值

数学中，字母上面加横线读作拔(a)表示求平均的意思。



# 相似变换：把矩阵看作线性映射就容易理解了


如果 $T:V\rightarrow V$ 是一个线性变换，那么对于 $V$ 的两组基 $\alpha $ 与 $\widetilde{\alpha}=\alpha \cdot P$ ，线性变换 $T$ 的矩阵分别为：

$$A_{\alpha}(T)\, and \, A_{\widetilde{\alpha} }(T)=P^{-1}\cdot A_{\alpha}(T)\cdot P$$

<span style="color:red;">厉害了，这个是由线性映射与矩阵 这个知识点得到的。因为这里是 P 与 Q 是相等的。所以根据基变换下的矩阵变换公式得来的。</span>


## 方阵的相似变换


如果两个方阵  $A$  和  $\widetilde{A}$  满足， $\widetilde(A)=P^{-1}AP$  。那么这两个方阵就互为相似矩阵。

相似矩阵的几何意义是同一个线性变换在不同的基下的表达形式。<span style="color:red;">嗯 是的厉害。当研究对象是线性变换的时候，我们只关心矩阵在相似变换下不变的几何性质。是的，你其实不是关心两个不同的基是什么，而只是关心这种相似变换的 T。</span>


## 那么相似变换下不变的性质都有什么呢？



* 行列式(det)：<span style="color:red;">行列式是 det 吗？到这个地方才讲到行列式，行列式其实是相似变换下的不变量。因为这个不变量很重要，因此很多书把这个提到第一个来讲。为什么是不变量呢？证明如下：</span>

$$\begin{aligned}(P^{-1}AP)&=det(P^{-1})det(A)det(P)\\&=det(P^{-1})det(P)det(A)\\&=det(A)\end{aligned}$$


* 迹(trace)：<span style="color:red;">什么是迹？</span>


$$tr(AB)=tr(BA)$$

$$tr(P^{-1}AP)=tr(APP^{-1})=tr(A\cdot I)=tr(A)$$


* 秩(rank) 秩就是一个矩阵的极大线性无关组中的向量个数。




## 最重要的一个相似变换的性质：特征值


最重要的一个相似变换不变的性质是什么？是特征值，前面的三个都可以用特征值来表示。




  * 特征值：特征方程 $det(A-\lambda I)=0$ 的根。如果 $det(A-\lambda I)=0$ ，那么 $det(P^{-1}(A-\lambda I)P)=0$ ，于是 $det(P^{-1}AP-\lambda I)=0$  **厉害**


特征值是最重要的相似不变量，利用这个相似不变量可以方便的得出上面的所有的不变量。


  * 行列式是什么？行列式就是所有特征值相乘。

  * 迹是什么？是所有特征值相加。

  * 秩是什么？是非 0 特征值的个数。


所以有了特征值的信息之后，前面的三个都比较简单了。**没想到特征值的作用这么大。**

下面简单介绍一下上面一些概念的几何意义：




  * 行列式是什么？如果 T 是从 V 到 V 的线性变换，那么取一组基你就得到矩阵，那么行列式就是 V 里面的单位体积的东西经过映射之后就得到了行列式。行列式就是体积膨胀的系数。

  * 迹的几何意义是：把这个映射看成是流体的向量场，流体的向量场所显示的流动的体积膨胀系数。

  * 秩说的是：比如 V 本来的维数是 n，映射到的子空间的维数就是秩？**没明白，在 Wiki 查一下。**




#




#




# 相和变换（二次型）


假设  $V$  是一个实系数线性空间，那么线性空间上的度量指的是空间中向量的内积关系  $G(v_1,v_2)$  。如果  $\alpha \{\alpha_1,\cdot ,\alpha_k \}$  是空间 V 的一组基，那么这个内积一般可以用一个对称矩阵  $H_\alpha =[h_{ij}]_{n \times n}$  来表示。**为什么这个内积可以用这个对称矩阵来表示？ 为什么度量是向量的内积关系？**

$$h_{ij}=G(\alpha_i,\alpha_j)$$

这个时候对于任意两个变量 $v_1,v_2$ ，如果 $v_1=\alpha \cdot x_1$ ， $v_2=\alpha \cdot x_2$ ，那么：

$$G(v_1,v_2)=x_1^TH_{\alpha}x_2$$

**这个 $H_\alpha $ 是什么？**


## 方阵的相合变换


如果两个对称方阵  $A$  和  $\widetilde{A}$  满足  $\widetilde{A}=P^TAP$  。那么这两个方阵就互为相合矩阵。

相似矩阵的几何意义是同一个内积结构在不同基下的表示形式。之前介绍的相似变换是同一个线性变换在不同的基下的表示。

**这个地方怎么又说是相似矩阵了？不是相合矩阵吗？而且 $\widetilde{A}$ 是什么？**






# 方阵的正交相似变换


有了相似变换和相和变换之后，才讲正交相似变换。

正交相似变换同时满足相似与相合变换的条件，也就是说它同时保持了矩阵的相似与相和不变量。

如果两个对称方阵  $A$  和  $\widetilde{A}$  满足  $\widetilde{A}=P^TAP$  。而且  $P$  是正交矩阵：  $P^T=P^{-1}$  。那么这  $A$  和  $\widetilde{A}$  就互为正交相似。即既是相和的又是相似的。


## 方阵的正交相似标准型


任何一个对称矩阵  $A$  都可以正交相似于一个对角矩阵  $D$  。即总存在一个正交矩阵  $P$  ，使得  $A=P^TDP$  。这个公式就是我们要学的 PCA，即主成分分析的数学原理。

为什么要做这些相似相和的东西？就是给定一个矩阵 A，你希望它相似一个非常简单的矩阵，而这个简单的矩阵几乎蕴含了你原来矩阵里面的所有的信息。什么是最简单的矩阵？对角矩阵就是最简单的矩阵。而这个定理告诉你，任何一个对称矩阵都可以正交相似与一个对角矩阵。就是说，这个对称矩阵从一定意义上来说都是非常简单的。然后 PCA 就是说：本来你只有这个 A，然后你通过找到 D 来简化这个问题。










# 正交阵：


若 n 阶矩阵 A 满足 $A^TA=I$ ，成 A 为正交矩阵，简称正交阵。A是正交阵的充要条件：A的列(行)向量都是单位向量，且两两正交。

A是正交阵，x为向量，则 $A\cdot x$ 称作正交变换。正交变换不改变向量长度。


## 两道思考题：


若 A、B都是 n 阶正交阵，那么，A×B是正交阵吗？是的。

正交阵和对称阵，能够通过何种操作获得一定意义下的联系？昨天学的协方差矩阵就是对称阵。






# 特征值和特征向量




## 定义：


A是 n 阶矩阵，若数λ和 n 维非 0 列向量 x 满足 Ax=λx，那么，数λ称为 A 的特征值，x称为 A 的对应于特征值λ的特征向量。

根据定义，立刻得到 $(A-λI)x = 0$ ，令关于λ 的多项式 $|A-λI|$ 为 0，方程 $|A-λI|=0$ 的根为 A 的特征值；将根λ_0 带入方程组(A-λI)x = 0 ，求得到的非零解，即λ_0 对应的特征向量。


## 特征值的性质：


设 n 阶矩阵 $A=(a_{ij} )$ 的特征值为 $λ_1 ,λ_2 ,...λ_n $ ，则




  *  $λ_1 +λ_2 +...+λ_n =a_{11} +a_{22}+…+a_{nn}$ 

  *  $λ_1 \;λ_2 …λ_n =|A|$ 


矩阵 A 主行列式的元素和，称作矩阵 A 的迹。


**特征值的意义，在聚类和谱聚类里面由好的实践方面的应用。后面学到回来补充下。**





## 两个点：


已知λ是方阵 A 的特征值，则：**用定义怎么解决？**




  *  $\lambda^2$  是 $A^2$ 的特征值

  * A可逆时， $\lambda^{-1}$ 是 $A^{-1}$ 的特征值。




## 不同特征值 的 特征向量


设 $λ_1 ,λ_2 ,...,λ_m $ 是方阵 A 的 m 个特征值， $p_1 ,p_2 ,...,p_m$ 是依次与之对应的特征向量，若 $λ_1 ,λ_2 ,...,λ_m $ 各不相等，则 $p_1 ,p_2 ,...,p_m$ 线性无关。

总结与思考




  * 不同特征值对应的特征向量，线性无关。

  * 若方阵 A 是对称阵呢？结论是否会加强？**什么叫线性无关的结论得到加强？嗯，加强到正交了。**


    * 协方差矩阵、二次型矩阵、无向图的邻接矩阵等都是对称阵

    * 在谱聚类中将会有所涉及







## 引理：


实对称阵的特征值是实数

设复数λ为对称阵 A 的特征值，复向量 x 为对应的特征向量，即 Ax=λx(x≠0)，现在证明附属λ一定是实数：

用 $\overline{\lambda}$  表示 $\lambda$ 的共轭复数， $\overline{x}$  表示 x 的共轭复向量，而 A 是实矩阵，有 $\overline{A}=A$  ，如果 $\lambda$ 是实数，说明 $\overline{\lambda}$  = $\lambda$ 

证明：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/Cdh44ifbff.png?imageslim">
</p>

这个是想告诉大家是对称阵的特征值一定是实数。

而因为实数λ带入方程组(A- λ I)x=0，该方程组为实系数方程组，因此，实对称阵的特征向量只能是实向量。


## 实对称阵不同特征值的特征向量正交


证明如下：

令实对称矩阵为 A，它的两个不同的特征值 $λ_1$  ， $λ_2$  对应的特征向量分别是 $μ_1$   $μ_2$  ；其中， $λ_1$   $λ_2$   $μ_1$   $μ_2$  都是实数或是实向量。




  * 则有： $Aμ 1 =λ_1 μ_1$  ， $ Aμ_2 =λ_2 μ_2$ 

  *  $(Aμ_1 )^T =(λ_1 μ_1 )^ T$  ，从而： $μ_1^TA=λ_1 μ_1^T$ 

  * 所以： $μ_1^T Aμ_2 =λ_1 μ_1^T μ_2$ 

  * 同时， $μ_1^T Aμ_2 =μ_1^T (Aμ_2 )=μ_1^T λ_2 μ_2 = λ_2 μ_1^T μ_2$ 

  * 所以， $λ_1 μ_1^T μ_2 =λ_2 μ_1^T μ_2$ 

  * 故： $(λ_1 -λ_2 ) μ_1^T μ_2 =0$ 

  * 而 $λ_1 ≠λ_2 $ ，所以 $μ_1^T μ_2 =0$ ，即： $μ_1$  ， $μ_2 $ 正交。




## 最终结论


设 A 为 n 阶的对称阵，则必有正交阵 P，使得：

$$P^{-1}AP=P^TAP=\Lambda$ 

 $\Lambda$ 是以 A 的 n 个特征值为对角元的对角阵。

一个对称阵总有一个正交阵，把它对角化。该变换称为“合同变换”， $A$ 和 $\Lambda$ 互为合同矩阵。




# 二次型


含有 n 个变量的二次齐次函数，称为二次型。二次型本身跟线性代数没有关系。一个二次型对应一个对称阵，而上面说了对称阵可以由正交阵对角化，从而二次型可以化成只有 n 个变量平方项的标准型，而这个正交阵，对应着坐标系的旋转变化。**嗯也是。用的地方比如：可以将方程化简看是椭圆还是双曲线什么的。**


# 正定阵 还是不熟悉？什么是正定？什么是半正定？




## 正定阵的定义


对于 n 阶方阵 A，若任意 n 阶向量 x，都有 x^TAx>0，则称 A 是正定阵。可以看成是 1 阶的正数负数到方阵的推广。

若条件变成 x^TAx≥0，则 A 称作半正定阵，类似还有负定阵，半负定阵。


## 正定阵的判定：


对称阵 A 为正定阵；A的特征值都为正；A的顺序主子式（主对角线上的）大于 0；这三个命题等价。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/fe186Ccgii.png?imageslim">
</p>

用在那里呢？taylor展开式的时候，如果是 n 元的变量，求一阶导数，则是一个 n 阶的向量，求二阶导数就是一个 n^2的一个方阵，这个方阵判定的话就是用的正定负定。**没明白？**


## 两道题目：


给定凸锥的定义如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/dfmJJdcA1D.png?imageslim">
</p>

试证明：n阶半正定方阵的集合为凸锥。[note][凸锥](http://106.15.37.116/2018/03/31/ai-convex-optimization-optimization-and-convex-optimization/)[/note]这个是凸优化方面的一个题目。这个证明题本身没有价值，价值在于通过证明过程理解定义。考察的是半正定阵的定义。

利用定义证明：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/3hB60kL644.png?imageslim">
</p>




## 向量的导数


A为 m×n的矩阵， x为 n×1的列向量，则 Ax 为 m×1的列向量，记 $\overrightarrow{y}=A\cdot \overrightarrow{x}$ 。思考：

$$\frac{\partial \overrightarrow{y} }{\partial \overrightarrow{x} }=?$$

推导：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/3i62FlCJab.png?imageslim">
</p>

**上面的，矩阵的求偏导的计算方式是什么样子的？**

理论与直接推广：向量偏导公式


- $\frac{\partial A \overrightarrow{x} }{\partial \overrightarrow{x} }=A^T$ 
- $\frac{\partial A^T \overrightarrow{x} }{\partial \overrightarrow{x} }=A$ 
- $\frac{\partial A \overrightarrow{x} }{\partial \overrightarrow{x}^T}=A$ 


在线性回归中将直接使用该公式。使用的时候注意回来补充一下怎么用的。


## 标量对向量的导数


A为 $n\times n$ 的矩阵，x为 $n\times 1$ 的列向量，记  $y=\overrightarrow{x}^T\cdot A\cdot \overrightarrow{x}$ 

这个时候 y 是一个数。这时可得：**怎么求出来的？**

$$\frac{\partial y}{\partial \overrightarrow{x} }=\frac{\partial(\overrightarrow{x}^T\cdot A\cdot \overrightarrow{x})}{\partial \overrightarrow{x} }=(A^T+A)\cdot \overrightarrow{x}$$

如果 A 为对称阵，则有：

$$\frac{\partial y}{\partial \overrightarrow{x} }=\frac{\partial(\overrightarrow{x}^T\cdot A\cdot \overrightarrow{x})}{\partial \overrightarrow{x} }=2A\cdot \overrightarrow{x}$$



### 注意： **这个没讲**




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/g00JIIgdC8.png?imageslim">
</p>



