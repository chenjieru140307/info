

# 特征值分解 特征值 特征向量

特征值与特征向量：

- 通过特征值分解，我们可以得到特征值(eigenvalues)与特征向量(eigenvectors)。
- 特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。

含义：

- 如果 $\nu$ 是方阵 $A$ 的特征向量
- $\lambda$ 为特征向量 $\nu$ 对应的特征值。
- 那么：$A\nu = \lambda \nu$

特征值分解：

- 特征值分解是将一个矩阵分解为如下形式：

$$
A=Q\sum Q^{-1}
$$

说明：

- $Q$ 是这个矩阵 $A$ 的特征向量组成的矩阵
- $\sum$ 是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。

疑问：

- 所有的矩阵都可以进行特征分解吗？






# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
