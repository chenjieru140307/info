# 矩阵

## 矩阵

矩阵$\mathbf X\in \mathbb R^{m\times n}$都表示为：

$$\mathbf X = \begin{bmatrix} x_{1,1}&x_{1,2}&\cdots&x_{1,n}\\ x_{2,1}&x_{2,2}&\cdots&x_{2,n}\\ \vdots&\vdots&\ddots&\vdots\\ x_{m,1}&x_{m,2}&\cdots&x_{m,n}\\ \end{bmatrix}$$

简写为：$(x_{i,j})_{m\times n}$或者$[x_{i,j}]_{m\times n}$。

## 迹

矩阵的迹：

- 设矩阵$\mathbf A=(a_{i,j})_{m\times n}$，则$\mathbf A$的迹为：$tr(\mathbf A)=\sum_{i}a_{i,i}$。

迹的性质有：

- $\mathbf A$ 的 `F` 范数等于$\mathbf A\mathbf A^T$的迹的平方根：$||\mathbf A||_F=\sqrt{tr(\mathbf A \mathbf A^{T})}$。
  - F 范数是矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的 L2 范数，它的优点在于它是一个凸函数，可以求导求解。$\Vert A\Vert_F=\sqrt{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^2)}$
- $\mathbf A$的迹等于$\mathbf A^T$的迹：$tr(\mathbf A)=tr(\mathbf A^{T})$。
- 交换律：假设$\mathbf A\in \mathbb R^{m\times n},\mathbf B\in \mathbb R^{n\times m}$，则有：$tr(\mathbf A\mathbf B)=tr(\mathbf B\mathbf A)$。
- 结合律：$tr(\mathbf A\mathbf B\mathbf C)=tr(\mathbf C\mathbf A\mathbf B)=tr(\mathbf B\mathbf C\mathbf A)$。
- 多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相乘的迹是相同的。当然，我们需要考虑挪动之后矩阵乘积依然定义良好：$\operatorname{tr}(\boldsymbol{A} \boldsymbol{B} \boldsymbol{C})=\operatorname{tr}(\boldsymbol{C} \boldsymbol{A} \boldsymbol{B})=\operatorname{tr}(\boldsymbol{B} \boldsymbol{C} \boldsymbol{A})$
  - 或者更一般地：$
\operatorname{tr}\left(\prod_{i=1}^{n} \boldsymbol{F}^{(i)}\right)=\operatorname{tr}\left(\boldsymbol{F}^{(n)} \prod_{i=1}^{n-1} \boldsymbol{F}^{(i)}\right)$
  - 即使循环置换后矩阵乘积得到的矩阵形状变了，迹运算的结果依然不变。
    - 举例，假设矩阵 $\boldsymbol{A} \in \mathbb{R}^{m \times n}$ ，矩阵 $\boldsymbol{B} \in \mathbb{R}^{n \times m}$ ，我们可以得到：$\operatorname{tr}(\boldsymbol{A} \boldsymbol{B})=\operatorname{tr}(\boldsymbol{B} \boldsymbol{A})$。尽管 $\boldsymbol{A} \boldsymbol{B} \in \mathbb{R}^{m \times m}$ 和 $\boldsymbol{B} \boldsymbol{A} \in \mathbb{R}^{n \times n}$。
- 标量在迹运算后仍然是它自己：$a=\operatorname{tr}(a)$ 。






## 矩阵的范数


矩阵：$A_{m\times n}$，其元素为 $a_{ij}$。

$$
\Vert{A}\Vert_p :=\sup_{x\neq 0}\frac{\Vert{Ax}\Vert_p}{\Vert{x}\Vert_p}
$$


- 1 范数（列范数）：矩阵的每一列上的元素绝对值先求和，再从中取个最大的，（列和最大）。$\Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}|$
- 2 范数：矩阵 $A^TA$ 的最大特征值开平方根。$\Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)}$ 。其中， $\lambda_{max}(A^T A)$ 为 $A^T A​$ 的特征值绝对值的最大值。
- 无穷范数（行范数）：矩阵的每一行上的元素绝对值先求和，再从中取个最大的，（行和最大）。$\Vert A\Vert_{\infty}=\max_{1\le i \le m}\sum_{j=1}^n |{a_{ij}}|$
- 核范数：矩阵的奇异值（将矩阵 svd 分解）之和，这个范数可以用来低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩）
- L0 范数：矩阵的非 0 元素的个数，通常用它来表示稀疏，L0 范数越小 0 元素越多，也就越稀疏。
- L1 范数：矩阵中的每个元素绝对值之和，它是 L0 范数的最优凸近似，因此它也可以表示稀疏。
- F 范数（Frobenius）：矩阵的各个元素平方之和再开平方根。它的优点在于它是一个凸函数，可以求导求解。矩阵的`F`范数：设矩阵 $\mathbf A=(a_{i,j})_ {m\times n}$，则其`F` 范数为：$||\mathbf A||_ F=\sqrt{\sum_{i,j}a_{i,j}^{2}}$ 
  - 它通常也叫做矩阵的 $L_2$ 范数，是向量的 $L_2$ 范数的推广。
- L21 范数：矩阵先以每一列为单位，求每一列的 F 范数（也可认为是向量的 2 范数），然后再将得到的结果求 L1 范数（也可认为是向量的 1 范数），很容易看出它是介于 L1 和 L2 之间的一种范数。
- p 范数：$\Vert A\Vert_p=\sqrt[p]{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^p)}$


举例：$A=[-1, 2, -3; 4, -6, 6]$

- 1 范数先得到 $[5,8,9]$，再取最大的最终结果就是：9。
- 2 范数得到的最终结果是：10.0623。
- 无穷范数（行范数）先得到 $[6；16]$，再取最大的最终结果就是：16
- 核范数结果就是：10.9287。
- L0 范数最终结果就是：6
- L1 范数最终结果就是：22
- F 范数最终结果就是：10.0995。
- L21 范数最终结果就是：17.1559。


疑问：

- sup 是什么？


## 转置

矩阵乘积的转置：

$$
(\boldsymbol{A} \boldsymbol{B})^{\top}=\boldsymbol{B}^{\top} \boldsymbol{A}^{\top}
$$


## 单位矩阵和逆矩阵


### 单位矩阵


单位矩阵 identity matrix 


单位矩阵：

- 所有沿主对角线的元素都是 $1$，而其他位置的所有元素都是 $0$。
  - 举例：$\boldsymbol{I}_{3}$： $\left[ \begin{array}{lll}{1} & {0} & {0} \\ {0} & {1} & {0} \\ {0} & {0} & {1}\end{array}\right]$
- 任意向量和单位矩阵相乘，都不会改变。$\forall \boldsymbol{x} \in \mathbb{R}^{n}, \boldsymbol{I}_{n} \boldsymbol{x}=\boldsymbol{x}$


### 逆矩阵

逆矩阵：

- 矩阵 $\boldsymbol{A}$ 的矩阵逆记作 $\boldsymbol{A}^{-1}$
- $\boldsymbol{A}^{-1}$ 满足如下条件：$\boldsymbol{A}^{-1} \boldsymbol{A}=\boldsymbol{I}_{n}$


作用：

- 对于大多数矩阵 $\boldsymbol{A}$ ，我们都能通过矩阵逆解析地求解：$\boldsymbol{A} \boldsymbol{x}=\boldsymbol{b}$

求解如下：

- $\boldsymbol{A} \boldsymbol{x}=\boldsymbol{b}$
- $\boldsymbol{A}^{-1} \boldsymbol{A} \boldsymbol{x}=\boldsymbol{A}^{-1} \boldsymbol{b}$
- $\boldsymbol{I}_{n} \boldsymbol{x}=\boldsymbol{A}^{-1} \boldsymbol{b}$
- $\boldsymbol{x}=\boldsymbol{A}^{-1} \boldsymbol{b}$

注意：

- 这取决于我们能否找到一个逆矩阵 $\boldsymbol{A}^{-1}$


疑问：

- 逆矩阵 $\boldsymbol{A}^{-1}$ 存在的条件是什么？
- 怎么对逆矩阵进行求解？





## 对角矩阵 对称矩阵 正交矩阵


### 对角矩阵 diagonal matrix

对角矩阵：

- 只在主对角线上含有非零元素，其他位置都是零。
- 矩阵 $\boldsymbol{D}$ 是对角矩阵，当且仅当对于所有的 $i \neq j, \quad D_{i, j}=0$。
- 单位矩阵是一个对角矩阵：单位矩阵，其对角元素全部是 $1$。


注意：

- 并非所有的对角矩阵都是方阵。长方形的矩阵也有可能是对角矩阵。

表示：

- 我们用 $diag(\boldsymbol{v})$ 表示对角元素由向量 $\boldsymbol{v}$ 中元素给定的一个对角方阵。

关注对角矩阵的原因：

- 对角矩阵的乘法计算很高效。
  - 计算乘法 $\operatorname{diag}(\boldsymbol{v}) \boldsymbol{x}$，我们只需要将 $\boldsymbol{x}$ 中的每个元素 $x_i$ 放大 $v_i$ 倍。即，$\operatorname{diag}(\boldsymbol{v}) \boldsymbol{x}=\boldsymbol{v} \odot \boldsymbol{x}$。 
  - 非方阵的对角矩阵没有逆矩阵，但我们仍然可以高效地计算它们的乘法。对于一个长方形对角矩阵 $\boldsymbol{D}$ 而言，乘法 $\boldsymbol{D} \boldsymbol{x}$ 会涉及 $\boldsymbol{x}$ 中每个元素的缩放，如果 $\boldsymbol{D}$ 是瘦长型矩阵，那么在缩放后的末尾添加一些零；如果 $\boldsymbol{D}$ 是胖宽型矩阵，那么在缩放后去掉最后一些元素。
- 计算对角方阵的逆矩阵也很高效。
  - 对角方阵的逆矩阵存在，当且仅当对角元素都是非零值，在这种情况下，$\operatorname{diag}(\boldsymbol{v})^{-1}=\operatorname{diag}\left(\left[1 / v_{1}, \ldots, 1 / v_{n}\right]^{\top}\right)$。

应用：

- 在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法，但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的并且简明扼要的算法。


### 对称矩阵 symmetric

对称矩阵：

- 转置和自己相等的矩阵，即：$\boldsymbol{A}=\boldsymbol{A}^{\top}$


应用：

- 当某些不依赖参数顺序的双参数函数生成元素时，对称矩阵经常会出现。
  - 例如，如果 $\boldsymbol{A}$ 是一个距离度量矩阵，$\boldsymbol{A}_{i, j}$ 表示点 $i$ 到点 $j$ 的距离，那么 $A_{i, j}=A_{j, i}$ ，因为距离函数是对称的。（具体作用是什么？）


### 正交矩阵


单位向量：

- 单位向量(unit vector)是具有单位范数(unit norm)的向量，即：$\|x\|_{2}=1$

向量正交：

- 如果 $\boldsymbol{x}^{\top} \boldsymbol{y}=0$ ，那么向量 $\boldsymbol{x}$ 和向量 $\boldsymbol{y}$ 互相正交(orthogonal)。
- 如果两个正交向量都有非零范数，那么这两个向量之间的夹角是 90 度。

相互正交与标准正交：

- 在 $\mathbb{R}^{n}$ 中，至多有 $n$ 个范数非零向量互相正交。
- 如果这些向量不但互相正交，而且范数都为 $1$，那么我们称它们是标准正交(orthonormal)。

正交矩阵：

- 正交矩阵 (orthogonal matrix) 指行向量和列向量是分别标准正交的方阵，即：$\boldsymbol{A}^{\top} \boldsymbol{A}=\boldsymbol{A} \boldsymbol{A}^{\top}=\boldsymbol{I}$
- 这意味着，正交矩阵满足：$\boldsymbol{A}^{-1}=\boldsymbol{A}^{\top}$
- **注意：** 正交矩阵的行列向量不仅是正交的，还是标准正交的。对于行向量或列向量互相正交但不是标准正交的矩阵，没有对应的专有术语。


关注正交矩阵的原因：

- 关注正交矩阵的原因是因为求逆计算代价小。


疑问：

- 我们什么时候会使用到正交矩阵呢？一般是怎么使用的呢？




## 矩阵求导

应用：

- 在解决应用数学的许多领域常见的最小化问题中十分有用。
- 梯度矩阵经常被应用在估计理论的最小化问题中，比如卡尔曼滤波算法的推导。

**矩阵对标量求导：**

矩阵函数 $Y$ 对标量 $x$ 的导数也被称为**切矩阵**：

$${\frac {\partial \mathbf {Y} }{\partial x}}={\begin{bmatrix}{\frac {\partial y_{11}}{\partial x}}&{\frac {\partial y_{12}}{\partial x}}&\cdots &{\frac {\partial y_{1n}}{\partial x}}\\{\frac {\partial y_{21}}{\partial x}}&{\frac {\partial y_{22}}{\partial x}}&\cdots &{\frac {\partial y_{2n}}{\partial x}}\\\vdots &\vdots &\ddots &\vdots \\{\frac {\partial y_{m1}}{\partial x}}&{\frac {\partial y_{m2}}{\partial x}}&\cdots &{\frac {\partial y_{mn}}{\partial x}}\\\end{bmatrix}}$$

说明：

- $Y$ 是 m*n

**标量对矩阵求导：**

$${\frac {\partial y}{\partial \mathbf {X} }}={\begin{bmatrix}{\frac {\partial y}{\partial x_{11}}}&{\frac {\partial y}{\partial x_{21}}}&\cdots &{\frac {\partial y}{\partial x_{p1}}}\\{\frac {\partial y}{\partial x_{12}}}&{\frac {\partial y}{\partial x_{22}}}&\cdots &{\frac {\partial y}{\partial x_{p2}}}\\\vdots &\vdots &\ddots &\vdots \\{\frac {\partial y}{\partial x_{1q}}}&{\frac {\partial y}{\partial x_{2q}}}&\cdots &{\frac {\partial y}{\partial x_{pq}}}\\\end{bmatrix}}$$


说明：

- $\mathbf {X}$ 是 p*q

即：

$$
\nabla _{\mathbf {X} }y(\mathbf {X} )={\frac {\partial y(\mathbf {X} )}{\partial \mathbf {X} }}
$$


**其他：**



类似地，标量函数 $f(X)$ 关于矩阵 $X$ 在方向 $Y$ 的方向导数可写成

$$\nabla _{\mathbf {Y} }f=\operatorname {tr} \left({\frac {\partial f}{\partial \mathbf {X} }}\mathbf {Y} \right)$$

疑问：

- 没明白，还有矩阵对矩阵的求导还没有明白。



## 矩阵运算

1. 给定两个矩阵$\mathbf A=(a_{i,j}) \in \mathbb R^{m\times n},\mathbf B=(b_{i,j}) \in \mathbb R^{m\times n}$，定义：

   - 阿达马积 `Hadamard product`（又称作逐元素积）：

     $$\mathbf A \circ \mathbf B =\begin{bmatrix} a_{1,1}b_{1,1}&a_{1,2}b_{1,2}&\cdots&a_{1,n}b_{1,n}\\ a_{2,1}b_{2,1}&a_{2,2}b_{2,2}&\cdots&a_{2,n}b_{2,n}\\ \vdots&\vdots&\ddots&\vdots\\ a_{m,1}b_{m,1}&a_{m,2}b_{m,2}&\cdots&a_{m,n}b_{m,n}\end{bmatrix}$$

   - 克罗内积`Kronnecker product`：

     $$\mathbf A \otimes \mathbf B =\begin{bmatrix}a_{1,1}\mathbf B&a_{1,2}\mathbf B&\cdots&a_{1,n}\mathbf B\\ a_{2,1}\mathbf B&a_{2,2}\mathbf B&\cdots&a_{2,n}\mathbf B\\ \vdots&\vdots&\ddots&\vdots\\ a_{m,1}\mathbf B&a_{m,2}\mathbf B&\cdots&a_{m,n}\mathbf B \end{bmatrix}$$

2. 设$\mathbf {\vec x},\mathbf {\vec a},\mathbf {\vec b},\mathbf {\vec c}$为$n$阶向量，$\mathbf A,\mathbf B,\mathbf C,\mathbf X$为$n$阶方阵，则有：

   $$\frac{\partial(\mathbf {\vec a}^{T}\mathbf {\vec x}) }{\partial \mathbf {\vec x} }=\frac{\partial(\mathbf {\vec x}^{T}\mathbf {\vec a}) }{\partial \mathbf {\vec x} } =\mathbf {\vec a} $$
   $$\frac{\partial(\mathbf {\vec a}^{T}\mathbf X\mathbf {\vec b}) }{\partial \mathbf X }=\mathbf {\vec a}\mathbf {\vec b}^{T}=\mathbf {\vec a}\otimes\mathbf {\vec b}\in \mathbb R^{n\times n} $$
   $$\frac{\partial(\mathbf {\vec a}^{T}\mathbf X^{T}\mathbf {\vec b}) }{\partial \mathbf X }=\mathbf {\vec b}\mathbf {\vec a}^{T}=\mathbf {\vec b}\otimes\mathbf {\vec a}\in \mathbb R^{n\times n} $$
   $$\frac{\partial(\mathbf {\vec a}^{T}\mathbf X\mathbf {\vec a}) }{\partial \mathbf X }=\frac{\partial(\mathbf {\vec a}^{T}\mathbf X^{T}\mathbf {\vec a}) }{\partial \mathbf X }=\mathbf {\vec a}\otimes\mathbf {\vec a} $$
   $$\frac{\partial(\mathbf {\vec a}^{T}\mathbf X^{T}\mathbf X\mathbf {\vec b}) }{\partial \mathbf X }=\mathbf X(\mathbf {\vec a}\otimes\mathbf {\vec b}+\mathbf {\vec b}\otimes\mathbf {\vec a}) $$
   $$\frac{\partial[(\mathbf A\mathbf {\vec x}+\mathbf {\vec a})^{T}\mathbf C(\mathbf B\mathbf {\vec x}+\mathbf {\vec b})]}{\partial \mathbf {\vec x}}=\mathbf A^{T}\mathbf C(\mathbf B\mathbf {\vec x}+\mathbf {\vec b})+\mathbf B^{T}\mathbf C(\mathbf A\mathbf {\vec x}+\mathbf {\vec a}) $$
   $$\frac{\partial (\mathbf {\vec x}^{T}\mathbf A \mathbf {\vec x})}{\partial \mathbf {\vec x}}=(\mathbf A+\mathbf A^{T})\mathbf {\vec x} $$
   $$\frac{\partial[(\mathbf X\mathbf {\vec b}+\mathbf {\vec c})^{T}\mathbf A(\mathbf X\mathbf {\vec b}+\mathbf {\vec c})]}{\partial \mathbf X}=(\mathbf A+\mathbf A^{T})(\mathbf X\mathbf {\vec b}+\mathbf {\vec c})\mathbf {\vec b}^{T} $$
   $$\frac{\partial (\mathbf {\vec b}^{T}\mathbf X^{T}\mathbf A \mathbf X\mathbf {\vec c})}{\partial \mathbf X}=\mathbf A^{T}\mathbf X\mathbf {\vec b}\mathbf {\vec c}^{T}+\mathbf A\mathbf X\mathbf {\vec c}\mathbf {\vec b}^{T}$$

1. 如果$f$是一元函数，则：

   - 其逐元向量函数为：$f(\mathbf{\vec x}) =(f(x_1),f(x_2),\cdots,f(x_n))^{T}$。

   - 其逐矩阵函数为：

     $$f(\mathbf X)=\begin{bmatrix} f(x_{1,1})&f(x_{1,2})&\cdots&f(x_{1,n})\\ f(x_{2,1})&f(x_{2,2})&\cdots&f(x_{2,n})\\ \vdots&\vdots&\ddots&\vdots\\ f(x_{m,1})&f(x_{m,2})&\cdots&f(x_{m,n})\\ \end{bmatrix}$$

   - 其逐元导数分别为：

     $$f^{\prime}(\mathbf{\vec x}) =(f^{\prime}(x1),f^{\prime}(x2),\cdots,f^{\prime}(x_n))^{T}\\ f^{\prime}(\mathbf X)=\begin{bmatrix} f^{\prime}(x_{1,1})&f^{\prime}(x_{1,2})&\cdots&f^{\prime}(x_{1,n})\\ f^{\prime}(x_{2,1})&f^{\prime}(x_{2,2})&\cdots&f^{\prime}(x_{2,n})\\ \vdots&\vdots&\ddots&\vdots\\ f^{\prime}(x_{m,1})&f^{\prime}(x_{m,2})&\cdots&f^{\prime}(x_{m,n})\\ \end{bmatrix}$$

2. 各种类型的偏导数：

   - 标量对标量的偏导数：$\frac{\partial u}{\partial v}$。

   - 标量对向量（$n$维向量）的偏导数 ：$\frac{\partial u}{\partial \mathbf {\vec v}}=(\frac{\partial u}{\partial v_1},\frac{\partial u}{\partial v_2},\cdots,\frac{\partial u}{\partial v_n})^{T}$。

   - 标量对矩阵($m\times n$阶矩阵)的偏导数：

     $$\frac{\partial u}{\partial \mathbf V}=\begin{bmatrix} \frac{\partial u}{\partial V_{1,1}}&\frac{\partial u}{\partial V_{1,2}}&\cdots&\frac{\partial u}{\partial V_{1,n}}\\ \frac{\partial u}{\partial V_{2,1}}&\frac{\partial u}{\partial V_{2,2}}&\cdots&\frac{\partial u}{\partial V_{2,n}}\\ \vdots&\vdots&\ddots&\vdots\\ \frac{\partial u}{\partial V_{m,1}}&\frac{\partial u}{\partial V_{m,2}}&\cdots&\frac{\partial u}{\partial V_{m,n}} \end{bmatrix}$$

   - 向量（$m$维向量）对标量的偏导数：$\frac{\partial \mathbf {\vec u}}{\partial v}=(\frac{\partial u_1}{\partial v},\frac{\partial u_2}{\partial v},\cdots,\frac{\partial u_m}{\partial v})^{T}$。

   - 向量（$m$维向量）对向量 ($n$维向量) 的偏导数（雅可比矩阵，行优先）

     $$\frac{\partial \mathbf {\vec u}}{\partial \mathbf {\vec v}}=\begin{bmatrix} \frac{\partial u_1}{\partial v_1}&\frac{\partial u_1}{\partial v_2}&\cdots&\frac{\partial u_1}{\partial v_n}\\ \frac{\partial u_2}{\partial v_1}&\frac{\partial u_2}{\partial v_2}&\cdots&\frac{\partial u_2}{\partial v_n}\\ \vdots&\vdots&\ddots&\vdots\\ \frac{\partial u_m}{\partial v_1}&\frac{\partial u_m}{\partial v_2}&\cdots&\frac{\partial u_m}{\partial v_n} \end{bmatrix}$$

     如果为列优先，则为上面矩阵的转置。

   - 矩阵($m\times n$阶矩阵)对标量的偏导数

   $$\frac{\partial \mathbf U}{\partial v}=\begin{bmatrix} \frac{\partial U_{1,1}}{\partial v}&\frac{\partial U_{1,2}}{\partial v}&\cdots&\frac{\partial U_{1,n}}{\partial v}\\ \frac{\partial U_{2,1}}{\partial v}&\frac{\partial U_{2,2}}{\partial v}&\cdots&\frac{\partial U_{2,n}}{\partial v}\\ \vdots&\vdots&\ddots&\vdots\\ \frac{\partial U_{m,1}}{\partial v}&\frac{\partial U_{m,2}}{\partial v}&\cdots&\frac{\partial U_{m,n}}{\partial v} \end{bmatrix}$$

3. 对于矩阵的迹，有下列偏导数成立：

  $$\frac{\partial [tr(f(\mathbf X))]}{\partial \mathbf X }=(f^{\prime}(\mathbf X))^{T} $$
  $$\frac{\partial [tr(\mathbf A\mathbf X\mathbf B)]}{\partial \mathbf X }=\mathbf A^{T}\mathbf B^{T} $$
  $$\frac{\partial [tr(\mathbf A\mathbf X^{T}\mathbf B)]}{\partial \mathbf X }=\mathbf B\mathbf A $$
  $$\frac{\partial [tr(\mathbf A\otimes\mathbf X )]}{\partial \mathbf X }=tr(\mathbf A)\mathbf I $$
  $$\frac{\partial [tr(\mathbf A\mathbf X \mathbf B\mathbf X)]}{\partial \mathbf X }=\mathbf A^{T}\mathbf X^{T}\mathbf B^{T}+\mathbf B^{T}\mathbf X \mathbf A^{T}$$

  $$\frac{\partial [tr(\mathbf X^{T} \mathbf B\mathbf X \mathbf C)]}{\partial \mathbf X }=\mathbf B\mathbf X \mathbf C +\mathbf B^{T}\mathbf X \mathbf C^{T}$$

  $$\frac{\partial [tr(\mathbf C^{T}\mathbf X^{T} \mathbf B\mathbf X \mathbf C)]}{\partial \mathbf X }=(\mathbf B^{T}+\mathbf B)\mathbf X \mathbf C \mathbf C^{T}$$

  $$\frac{\partial [tr(\mathbf A\mathbf X \mathbf B\mathbf X^{T} \mathbf C)]}{\partial \mathbf X }= \mathbf A^{T}\mathbf C^{T}\mathbf X\mathbf B^{T}+\mathbf C \mathbf A \mathbf X \mathbf B $$

  $$\frac{\partial [tr((\mathbf A\mathbf X\mathbf B+\mathbf C)(\mathbf A\mathbf X\mathbf B+\mathbf C))]}{\partial \mathbf X }= 2\mathbf A ^{T}(\mathbf A\mathbf X\mathbf B+\mathbf C)\mathbf B^{T}$$

4. 假设$\mathbf U= f(\mathbf X)$是关于$\mathbf X$的矩阵值函数（$f:\mathbb R^{m\times n}\rightarrow \mathbb R^{m\times n}$），且$g(\mathbf U)$是关于$\mathbf U$的实值函数（$g:\mathbb R^{m\times n}\rightarrow \mathbb R$），则下面链式法则成立：

   $$\frac{\partial g(\mathbf U)}{\partial \mathbf X}= \left(\frac{\partial g(\mathbf U)}{\partial x_{i,j}}\right)_{m\times n}=\begin{bmatrix} \frac{\partial g(\mathbf U)}{\partial x_{1,1}}&\frac{\partial g(\mathbf U)}{\partial x_{1,2}}&\cdots&\frac{\partial g(\mathbf U)}{\partial x_{1,n}}\\ \frac{\partial g(\mathbf U)}{\partial x_{2,1}}&\frac{\partial g(\mathbf U)}{\partial x_{2,2}}&\cdots&\frac{\partial g(\mathbf U)}{\partial x_{2,n}}\\ \vdots&\vdots&\ddots&\vdots\\ \frac{\partial g(\mathbf U)}{\partial x_{m,1}}&\frac{\partial g(\mathbf U)}{\partial x_{m,2}}&\cdots&\frac{\partial g(\mathbf U)}{\partial x_{m,n}}\\ \end{bmatrix}\\ =\left(\sum_{k}\sum_{l}\frac{\partial g(\mathbf U)}{\partial u_{k,l}}\frac{\partial u_{k,l}}{\partial x_{i,j}}\right)_{m\times n}=\left(tr\left[\left(\frac{\partial g(\mathbf U)}{\partial \mathbf U}\right)^{T}\frac{\partial \mathbf U}{\partial x_{i,j}}\right]\right)_{m\times n}$$
