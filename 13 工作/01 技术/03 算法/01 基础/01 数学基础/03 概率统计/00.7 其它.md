# 概率论与随机过程


# 其它

1. 假设随机变量$X,Y$满足$Y=g(X)$，且函数$g(\cdot)$满足：处处连续、可导、且存在反函数。 则有：

   $$p_X(x)=p_Y(g(x)) \left|\frac{\partial g(x)}{\partial x}\right|$$

   或者等价地（其中$g^{-1}(\cdot)$为反函数）：

   $$p_Y(y)=p_X(g^{-1}(y)) \left|\frac{\partial x}{\partial y}\right|$$

   - 如果扩展到高维空间，则有：

     $$p_X(\mathbf{\vec x})=p_Y(g(\mathbf{\vec x})) \left|\det\left(\frac{\partial g(\mathbf{\vec x})}{\partial \mathbf{\vec x}}\right)\right|$$

   - 并不是$p_Y(y)=p_X(g^{-1}(y))$，这是因为$g(\cdot)$引起了空间扭曲，从而导致$\int p_X(g(x))dx \neq 1$。

     根据$|p_Y(g(x))dy|=|p_X(x)dx|$，求解该方程，即得到上述解。

2. 机器学习中不确定性有三个来源：

   - 模型本身固有的随机性。如：量子力学中的粒子动力学方程。

   - 不完全的观测。即使是确定性系统，当无法观测所有驱动变量时，结果也是随机的。

   - 不完全建模。有时必须放弃一些观测信息。

     如机器人建模中：虽然可以精确观察机器人周围每个对象的位置，但在预测这些对象将来的位置时，对空间进行了离散化。则位置预测将带有不确定性。