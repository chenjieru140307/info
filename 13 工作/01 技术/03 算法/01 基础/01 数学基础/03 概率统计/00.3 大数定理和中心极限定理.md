
# 大数定律及中心极限定理

## 切比雪夫不等式

1. 切比雪夫不等式：假设随机变量$X$具有期望$\mathbb E[X] =\mu$， 方差$Var(X)=\sigma^{2}$，则对于任意正数$\varepsilon$，下面的不等式成立：

   $$P\{|X-\mu| \ge \varepsilon\} \le \frac {\sigma^{2}}{\varepsilon^{2}}$$

   - 其意义是：对于距离$\mathbb E[X]$足够远的地方 （距离大于等于$\varepsilon$），事件出现的概率是小于等于$\frac {\sigma^{2}}{\varepsilon^{2}}$。即事件出现在区间$[\mu-\varepsilon , \mu+\varepsilon]$的概率大于$1- \frac {\sigma^{2}}{\varepsilon^{2}}$。

     该不等式给出了随机变量$X$在分布未知的情况下， 事件$\{|X-\mu| \le \varepsilon\}$的下限估计。如：$P\{|X-\mu| \lt 3\sigma\} \ge 0.8889$。

   - 证明：

     $$P\{|X-\mu| \ge \varepsilon\}=\int_{|x-\mu| \ge \varepsilon}p(x)dx \le \int_{|x-\mu| \ge \varepsilon} \frac{|x-\mu|^{2}}{\varepsilon^{2}}p(x)dx \\ \le \frac {1}{\varepsilon^{2}}\int_{-\infty}^{\infty}(x-\mu)^{2}p(x)dx=\frac{\sigma^{2}}{\varepsilon^{2}}$$

2. 切比雪夫不等式的特殊情况：设随机变量$X_1,X_2,\cdots,X_n,\cdots$相互独立，且具有相同的数学期望和方差：$\mathbb E[X_k] =\mu, Var[X_k] =\sigma^{2}$。 作前$n$个随机变量的算术平均：$\overline X =\frac {1}{n} \sum _{k=1}^{n}X_k$， 则对于任意正数$\varepsilon$有：

   $$\lim_{n\rightarrow \infty}P\{|\overline X-\mu| \lt \varepsilon\}=\lim_{n\rightarrow \infty}P\{|\frac{1}{n}\sum_{k=1}^{n}X_k-\mu| \lt \varepsilon\} =1$$

   证明：根据期望和方差的性质有：$\mathbb E[\overline X]=\mu$，$Var[\overline X]=\frac{\sigma^2}{n}$。根据切比雪夫不等式有：

   $$P\{|\overline X-\mu| \ge \varepsilon\} \le \frac{\sigma^2}{n\varepsilon^2}$$

   则有$\lim_{n\rightarrow \infty}P\{|\overline X-\mu| \ge \varepsilon\} = 0$，因此有：$\lim_{n\rightarrow \infty}P\{|\overline X-\mu| \lt \varepsilon\} =1$。

## 大数定理

1. 依概率收敛：设$Y_1,Y_2,\cdots,Y_n,\cdots$是一个随机变量序列，$a$是一个常数。

   若对于任意正数$\varepsilon$有 ：$\lim_{n\rightarrow \infty}P\{|Y_{n}-a| \le \varepsilon \}=1$，则称序列$Y_1,Y_2,\cdots,Y_n,\cdots$依概率收敛于$a$。记作：$Y_{n} \stackrel{P}{\rightarrow} a$

2. 依概率收敛的两个含义：

   - 收敛：表明这是一个随机变量序列，而不是某个随机变量；且序列是无限长，而不是有限长。
   - 依概率：表明序列无穷远处的随机变量$Y_{\infty}$的分布规律为：绝大部分分布于点$a$，极少数位于$a$之外。且分布于$a$之外的事件发生的概率之和为0。

3. 大数定理一： 设随机变量$X_1,X_2,\cdots,X_n,\cdots$相互独立，且具有相同的数学期望和方差：$\mathbb E[X_k] =\mu, Var[X_k] =\sigma^{2}$。 则序列：$\overline X =\frac {1}{n} \sum _{k=1}^{n}X_k$依概率收敛于$\mu$， 即$\overline X \stackrel{P}{\rightarrow} \mu$。

   注意：这里并没有要求随机变量$X_1,X_2,\cdots,X_n,\cdots$同分布。

4. 伯努利大数定理： 设$n_A$为$n$次独立重复实验中事件$A$发生的次数，$p$是事件$A$在每次试验中发生的概率。则对于任意正数$\varepsilon$有：

   $$\lim_{n \rightarrow \infty}P\{|\frac{n_{A}}{n}-p| \lt \varepsilon\}=1 \\ or: \quad \lim_{n \rightarrow \infty}P\{|\frac{n_{A}}{n}-p| \ge \varepsilon\}=0$$

   即：当独立重复实验执行非常大的次数时，事件$A$发生的频率逼近于它的概率。

5. 辛钦定理：设随机变量$X_1,X_2,\cdots,X_n,\cdots$相互独立，服从同一分布，且具有相同的数学期望：$\mathbb E[X_k] =\mu$。 则对于任意正数$\varepsilon$有：

   $$\lim_{n\rightarrow \infty}P\{|\frac{1}{n}\sum_{k=1}^{n}X_k-\mu| \lt \varepsilon\} =1$$

   - 注意：这里并没有要求随机变量$X_1,X_2,\cdots,X_n,\cdots$的方差存在。
   - 伯努利大数定理是亲钦定理的特殊情况。

## 中心极限定理

1. 独立同分布的中心极限定理：设随机变量$X_1,X_2,\cdots,X_n$独立同分布，且具有数学期望和方差：$\mathbb E[X_k] =\mu, Var[X_k] =\sigma^{2}$， 则随机变量之和$\overline {SX_n}=\sum_{k=1}^{n} X_k$的标准变化量：

   $$Y_n=\frac{\overline {SX_n}-\mathbb E[\overline {SX_n}] }{\sqrt{Var[\overline {SX_n}] }}=\frac{\overline {SX_n}-n\mu}{\sqrt n \sigma}$$

   的概率分布函数$F_n(x)$对于任意$x$满足：

   $$\lim_{n\rightarrow \infty}F_n(x)=\lim_{n\rightarrow \infty}P\{Y_n \le x\}=\lim_{n\rightarrow \infty}P\{\frac{\sum_{k=1}^{n} X_k-n\mu}{\sqrt n \sigma} \le x\}\\ = \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}}e^{-t^{2}/2}dt=\Phi(x)$$

   - 其物理意义为：均值方差为$\mu,\sigma^{2}$的独立同分布的随机变量$X_1,X_2,\cdots,X_n$之和$\overline {SX_n}=\sum_{k=1}^{n} X_k$的标准变化量$Y_n$，当$n$充分大时，其分布近似于标准正态分布。

     即：$\overline {SX_n}=\sum_{k=1}^{n} X_k$在$n$充分大时，其分布近似于$N(n\mu,n\sigma^{2})$。

   - 一般情况下，很难求出$n$个随机变量之和的分布函数。因此当$n$充分大时，可以通过正态分布来做理论上的分析或者计算。

2. `Liapunov` 定理：设随机变量$X_1,X_2,\cdots,X_n,\cdots$相互独立，具有数学期望和方差：$\mathbb E[X_k] =\mu_k,Var[X_k] =\sigma_k^{2}$。记：$B_n^{2}=\sum_{k=1}^{n}\sigma_k^{2}$。 若存在正数$\delta$，使得当$n \rightarrow \infty$时，$\frac{1}{B_n^{2+\delta}}\sum_{k=1}^{n}\mathbb E [|X_k-\mu_k|^{2+\delta}] \rightarrow 0$。则随机变量之和$\overline {SX_n}=\sum_{k=1}^{n} X_k$的标准变化量:

   $$Z_n=\frac{\overline {SX_n}-\mathbb E[\overline {SX_n}] }{\sqrt{Var [\overline {SX_n}] }}=\frac{\overline {SX_n}-\sum_{k=1}^{n}\mu_k}{B_n}$$

   的概率分布函数$F_n(x)$对于任意$x$满足：

   $$\lim_{n\rightarrow \infty}F_n(x)=\lim_{n\rightarrow \infty}P\{Z_n \le x\}=\lim_{n\rightarrow \infty}P\{\frac{\sum_{k=1}^{n} X_k-\sum_{k=1}^{n}\mu_k}{B_n} \le x\}\\ = \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}}e^{-t^{2}/ 2}dt=\Phi(x)$$

   - 其物理意义为：相互独立的随机变量$X_1,X_2,\cdots,X_n,\cdots$之和$\overline {SX_n}=\sum_{k=1}^{n} X_k$的衍生随机变量序列$Z_n=\frac{\overline {SX_n}-\sum_{k=1}^{n}\mu_k}{B_n}$，当$n$充分大时，其分布近似与标准正态分布。
   - 这里并不要求$X_1,X_2,\cdots,X_n,\cdots$同分布。

1. `Demoiver-Laplace`定理：设随机变量序列$\eta_n,n=1,2,...$服从参数为$(n, p)$的二项分布，其中$0 \lt p \lt 1$。则对于任意$x$, 有：

   $$\lim_{n\rightarrow \infty}P\{\frac{\eta_n-np}{\sqrt{np(1-p)}} \le x\}=\int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}}e^{-t^{2}\mid 2}dt=\Phi(x)$$

   该定理表明，正态分布是二项分布的极限分布。当$n$充分大时，可以利用正态分布来计算二项分布的概率。
