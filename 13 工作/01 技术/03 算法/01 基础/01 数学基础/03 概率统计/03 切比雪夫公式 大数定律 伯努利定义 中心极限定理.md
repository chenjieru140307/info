# 大数定理 中心极限定理

- 切比雪夫公式
- 大数定理
- 伯努利定理
- 中心极限定理


## 切比雪夫公式：

切比雪夫不等式

缘由：

- 想看看随机变量的值落在期望值附近的概率至少是多少？


即：

- 设随机变量 $X$ 的期望是 $\mu$，方差是 $\sigma^2$，对于任意整数$\epsilon$，试估计概率 $P\{\mid X-\mu\mid <\epsilon\}$ 的下限：


解：以连续型随机变量为例


$$
\begin{aligned}
P\{| X-\mu | \geq \varepsilon\} 
=&\int_{|X-\mu| \geq \varepsilon} f(x) d x 
\\\leq& \int_{|X-\mu|>\varepsilon} \frac{|X-\mu|^{2}}{\varepsilon^{2}} f(x) d x 
\\=&\frac{1}{\varepsilon^{2}} \int_{|X-\mu| \geq \varepsilon}(X-\mu)^{2} f(x) d x 
\\\leq& \frac{1}{\varepsilon^{2}} \int_{-\infty}^{+\infty}(X-\mu)^{2} f(x) d x 
\\=&\frac{\sigma^{2}}{\varepsilon^{2}}
\end{aligned}
$$

此时：

$$
\begin{aligned}
P\{| X-\mu |<\varepsilon\} 
=&1-P\{| X-\mu | \geq \varepsilon\} 
\\\geq& 1-\frac{\sigma^{2}}{\varepsilon^{2}}
\end{aligned}
$$

说明：

- 第四行到第五行：$\int_{-\infty}^{+\infty}(X-\mu)^{2} f(x) d x$ 是方差的定义，因此等于 $\sigma^{2}$。


其中，切比雪夫不等式为：

$$P\{\mid X-\mu\mid \geq\varepsilon\}\leq \frac{\sigma^2}{\varepsilon^2}$$

说明：

- 即当期望给定，方差给定，则 $x$ 不会偏离期望太远，而且没有说 $x$ 是什么分布。
- $X$ 的方差越小，事件 $\{\mid X-\mu\mid \geq\varepsilon\}$ 发生的概率越大。即：$X$ 取得值基本上集中在期望 $\mu$ 附近。


应用：

- 该不等式说明了方差的含义。而且，可用该不等式证明大数定理。（怎么证明？）


## 大数定理


大数定理：

- $X$ 是随机变量，$μ$ 是 $X$ 的期望，$\sigma^2$ 是 $X$ 的方差。 $\{X_k\}_{k=1}^{\infty }$ 是服从 $X$ 的独立同分步随机变量，那么 $\overline{X_n}=\frac{\sum_{k=1}^{n}X_k}{n}$ 依概率收敛于 $μ$ 。
- 也就是说，此时，对于任何 $\varepsilon >0$ ，有：

$$\lim_{n\rightarrow \infty}P(|\overline{X_n}-μ|>ε)=0$$


意义：

- 当 $n$ 很大时，随机变量 $X_1,X_2,\cdots X_n$ 的平均值 $Y_n$ 在概率意义下无限接近期望$\mu$。出现偏离的可能性由，但是很小，当 $n$ 无限大时，这种可能性的概率为 0。
- 即统计平均与期望非常接近，也即 统计平均值 依概率收敛于期望。



## 伯努利定理 ：由大数定理得出的重要推论

伯努利定理：

- 一次实验中事件 $A$ 发生的概率为 $p$，重复 $n$ 次独立实验中，事件 $A$ 发生了 $n_A$ 次，则 $p$、$n$、$n_A$ 的关系满足：

$$
lim_{n\rightarrow \infty}P\{\mid\frac{n_A}{n}-p\mid <\varepsilon\}=1,\; \forall \varepsilon>0
$$


说明：

- 这就是大数定理 结合 一个给定的分布二项分布得到的一个公式。**这个公式很重要啊，相当于化学里的原子的发现。**
- 定理表明事件 $A$ 发生的频率 $\frac{n_A}{n}$ 依概率收敛于事件 $A$ 的概率 $p$ ，以严格的数学形式表达了频率的稳定性。
  - 一旦我们用频率推断概率，实际上用的就是这个伯努利定理！
    - 朴素贝叶斯做垃圾邮件分类
    - 正态分布的参数估计




## 中心极限定理：概率论的第二个大定理


可以认为是大数定理的加强版。

中心极限定理：

- 设随机变量 $X_1,X_2,\cdots X_n$ 互相独立，服从同一分布，并且具有相同的期望 $\mu$ 和方差 $\sigma^2$，
- 则随机变量：$Y_n=\frac{\sum_{i=1}^{n} X_i-n\mu}{\sqrt{n}\sigma}$ 也就是：$Y_n=\frac{\sqrt{n} }{\sigma}(\bar{X}_n-\mu)$ 的分布收敛到标准正态分布。
- 也就是说，对任何$\varepsilon>0$，有：$\underset{n\rightarrow \infty}{lim} P(Z_n<z)=\Phi(z),\;\forall z$ （这里是不是写错了？）
  - 其中$\Phi$是标准正态分布的分布函数。
- 而且容易得到：$\sum_{i=1}^{n}X_i$ 收敛到正态分布 $N(n\mu,n\sigma^2)$。


意义：

- 这个定理想告诉你的是，当你想通过$\bar{X}_n$来估计这个$\mu$的时候，到底估计的有多准。

大数定理和中心极限定理：

- 大数定理和中心极限定理，一个告诉你如何用样本估计 $\mu$，一个告诉你估计的能有多准。这就是为什么这两个定理为什么这么重要。



举例：

- 有一批样本(字符串)，其中 $a-z$ 开头的比例是固定的，但是量很大，需要从中随机抽样。样本量 $n$，总体中 $a$ 开头的字符串占比 1%，需要每次抽到的 $a$ 开头的字符串占比(0.99%,+1.01%)，样本量 $n$ 至少是多少？
- 问题可以重新表述一下：大量存在的两点分布 Bi(1,p)，其中，Bi 发生的概率为 0.01，即 p=0.01。取其中的 n 个，使得发生的个数除以总数的比例落在区间(0.0099,0.0101)，则 n 至少是多少？

解：（没有很清楚）

- 首先，两点分布 $\mathrm{B}$ 的期望为 $\mu=\mathrm{p}$，方差为 $\sigma^{2}=p(1-p)$
- 其次，当 $n$ 较大时，随机变量 $Y=\sum_{i=1}^{n} B_{i}$ 近似服从正态分布，事实上， $X=\frac{Y-n \mu}{\sqrt{n} \sigma}=\frac{\sum_{i=1}^{n} B_{i}-n \mu}{\sqrt{n} \sigma}$ 近似服从标准正态分布。
- 从而

$$
\begin{aligned}
P\left\{a \leq \frac{\sum_{i=1}^{n} B_{i}}{n} \leq b\right\} \geq 1-\alpha \\\Rightarrow P\left\{\frac{\sqrt{n}(a-\mu)}{\sigma} \leq \frac{\sum_{i=1}^{n} B_{i}-n \mu}{\sqrt{n} \sigma} \leq \frac{\sqrt{n}(b-\mu)}{\sigma}\right\} \geq 1-\alpha
\\\Rightarrow \Phi\left(\frac{\sqrt{n}(b-\mu)}{\sigma}\right)-\Phi\left(\frac{\sqrt{n}(a-\mu)}{\sigma}\right) \geq 1-\alpha
\end{aligned}
$$


- 上面式子中，$\mu=0.01$，$\sigma^{2}=0.0099$，$\mathrm{a}=0.0099$，$\mathrm{b}=0.0101$ ，$\alpha=0.05$ 或 $0.01$ （显著性水平的一般取值），差标准正态分布表，很容易计算得到 $n$ 的最小值
- 注：直接使用二项分布，也能得到结论



意义：

- 实际问题中，很多随机现象 由于可以看做许多因素的独立影响的综合反应，所以，往往近似服从正态分布。
  - 城市耗电量：大量用户的耗电量总和
  - 测量误差：许多观察不到的、微小误差的总和
    - 注意：是多个随机变量的和才可以，有些问题是乘性误差，则需要鉴别或者取对数后再使用。怎么处理？ 这个地方的误差，在线性回归中就用到了，在线性回归中将 $\varepsilon$ 认为是测量误差，认为它服从正态分布。从而来论证最小二乘法的合理性。（还没有很清楚）


