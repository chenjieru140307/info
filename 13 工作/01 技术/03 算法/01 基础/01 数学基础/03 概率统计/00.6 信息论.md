
# 信息论

1. 信息论背后的原理是：从不太可能发生的事件中能学到更多的有用信息。

   - 发生可能性较大的事件包含较少的信息。
   - 发生可能性较小的事件包含较多的信息。
   - 独立事件包含额外的信息 。

2. 对于事件$X=x$，定义自信息`self-information`为：$I(x)=-\log P(x)$。

   自信息仅仅处理单个输出，但是如果计算自信息的期望，它就是熵：

   $$H(X)=\mathbb E_{X\sim P(X)}[I(x)]=-\mathbb E_{X\sim P(X)}[\log P(x)]$$

   记作$H(P)$。

   - 熵刻画了按照真实分布$P$来识别一个样本所需要的编码长度的期望（即平均编码长度）。

     如：含有4个字母 `(A,B,C,D)` 的样本集中，真实分布$P=(\frac 12,\frac 12,0,0)$，则只需要1位编码即可识别样本。

   - 对于离散型随机变量$X$，假设其取值集合大小为$K$，则可以证明：$0\le H(X)\le \log K$。

3. 对于随机变量$Y$和$X$，条件熵$H(Y\mid X)$表示：已知随机变量$X$的条件下，随机变量$Y$的不确定性。

   它定义为：$X$给定条件下$Y$的条件概率分布的熵对$X$的期望：

   $$H(Y\mid X) = \mathbb E_{X\sim P(X)}[ H(Y\mid X=x)]=-\mathbb E_{(X,Y)\sim P(X,Y)} \log P(Y\mid X)$$

   - 对于离散型随机变量，有：

     $$H(Y\mid X) = \sum_xp(x) H(Y\mid X=x)=-\sum_x\sum_y p(x,y)\log p(y\mid x)$$

   - 对于连续型随机变量，有：

     $$H(Y\mid X) = \int p(x) H(Y\mid X=x) dx=-\int\int p(x,y)\log p(y\mid x) dx dy$$

4. 根据定义可以证明：$H(X,Y)=H(Y\mid X)+H(X)$。

   即：描述$X$和$Y$所需要的信息是：描述$X$所需要的信息加上给定$X$条件下描述$Y$所需的额外信息。

5. `KL`散度（也称作相对熵）：对于给定的随机变量$X$，它的两个概率分布函数$P(X)$和$Q(X)$的区别可以用 `KL`散度来度量：

   $$D_{KL}(P||Q)=\mathbb E_{X\sim P(X)}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb E_{X\sim P(X)}\left[\log P(x) -\log Q(x) \right]$$

   - `KL`散度非负：当它为 0 时，当且仅当 `P`和`Q`是同一个分布（对于离散型随机变量），或者两个分布几乎处处相等（对于连续型随机变量）。

   - `KL`散度不对称：$D_{KL}(P||Q) \neq D_{KL}(Q||P)$。

     直观上看对于$D_{KL}(P||Q)$，当$P(x)$较大的地方，$Q(x)$也应该较大，这样才能使得$P(x)\log\frac {P(x)}{Q(x)}$较小。

     对于$P(x)$较小的地方，$Q(x)$就没有什么限制就能够使得$P(x)\log\frac {P(x)}{Q(x)}$较小。这就是`KL`散度不满足对称性的原因。

6. 交叉熵`cross-entropy`：$H(P,Q)=H(P)+D_{KL}(P||Q)=-\mathbb E_{X\sim P(X)}\log Q(x)$。

   - 交叉熵刻画了使用错误分布$Q$来表示真实分布$P$中的样本的平均编码长度。
   -$D_{KL(P||Q)}$刻画了错误分布$Q$编码真实分布$P$带来的平均编码长度的增量。

7. 示例：假设真实分布$P$为混合高斯分布，它由两个高斯分布的分量组成。如果希望用普通的高斯分布$Q$来近似$P$，则有两种方案：

   

   <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/aPzaO2Jl0uMW.jpg?imageslim">
   </p>
   

   - 如果选择$Q_1^*$，则：

     - 当$P(x)$较大的时候$Q(x)$也必须较大 。如果$P(x)$较大时$Q(x)$较小，则$P(x) \log \frac{P(x)}{Q(x)}$较大。
     - 当$P(x)$较小的时候$Q(x)$可以较大，也可以较小。

     因此$Q_1^*$会贴近$P(x)$的峰值。由于$P(x)$的峰值有两个，因此$Q_1^*$无法偏向任意一个峰值，最终结果就是$Q_1^*$的峰值在$P(x)$的两个峰值之间。

     <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/36cRz2zOMuEk.png?imageslim">
     </p>
     

   - 如果选择$Q_2^*$，则：

     - 当$P(x)$较小的时候，$Q(x)$必须较小。如果$P(x)$较小的时$Q(x)$较大，则$Q(x)\log\frac{Q(x)}{P(x)}$较大。
     - 当$P(x)$较大的时候，$Q(x)$可以较大，也可以较小。

     因此$Q_2^*$会贴近$P(x)$的谷值。最终结果就是$Q_2^*$会贴合$P(x)$峰值的任何一个。

      <p align="center">
         <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200603/djivL2J8b8Id.png?imageslim">
      </p>
     

   - 绝大多数场合使用$D_{KL}(P||Q)$，原因是：当用分布$Q$拟合$P$时我们希望对于常见的事件，二者概率相差不大。
