# 实践准备


一些建议使用:

- 使用适当的日志和有意义的变量名
  - 在TensorFlow中，你可以通过名称来跟踪不同的变量，并在TensorBoard 中可视化图形。最重要的是，在每个训练步骤中，你都能记录相关的值，比如：step_number、accuracy、loss、learning_rate，甚至有时候还包括一些更具体的值，比如 mean_intersection_over_union。之后，就可以画出每一步的损失曲线。
- 确保您的网络连接正确
  - 使用 TensorBoard 或其他 debug 技术确保图中的每个操作的输入和输出都准确无误，还要确保在将数据和标签送入网络之前对其进行适当的预处理和配对。
- 实施数据增强技术
  - 虽然这一点并不是对所有情况都适用，不过如果你在搞图像相关的神经网络，用简单的数据增强技术处理一下图像，例如镜像、旋转、随机裁剪和重新缩放、添加噪声、弹性变形等，大部分时候出来的效果都有巨大提升。
- 对所有层使用权重初始化和正则化
  - 不要把权重初始化为相同的值
  - 一般情况下，如果你在权重初始化时遇到问题，你可以考虑在神经网络中添加**批量标准化层（Batch Normalization Layer）**。
    - 论文：[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
- 确保正则化条款不会压倒损失函数中的其他项。
  - 关闭正则化，找出损失的数量级，然后适当调整正则化权重。确保在增加正则化强度时，损失也在增加。
- 尝试过拟合一个小数据集。
  - 关闭正则化/丢失/数据增强，拿出训练集的一小部分，让神经网络练它几个世纪，确保可以实现零损失，不然就很可能是错误的。
  - 在某些情况下，将损失驱动为零非常具有挑战性，例如，如果您的损失涉及每个像素的 softmax-ed logits和ground truth labels 之间的交叉熵，那么在语义分割中可能真的难以将其降低到0。相反，你应该争取达到接近100％的准确度。
  - 可以在tf.metrics.accuracy这里了解如何通过获取softmax-ed logits的argmax并将其与ground truth labels进行比较来计算。
- 在过拟合上述小数据集的同时，找到合理的学习率。
  - Yoshua Bengio的论文中给到了结论：**最佳学习率通常接近最大学习率的一半，不会引起训练标准的差异**，这个观察结果是设置学习率的启发。
    - 例如，从较大的学习率开始，如果训练标准发散，就用最大学习率除以3再试试，直到观察不到发散为止。
- 执行梯度检查。
  - 如果您在图表中使用自定义操作，则梯度检查尤其重要。斯坦福CS231n中介绍了梯度检查的方法。






增强卷积神经网络泛化能力的一些方法：



- 使用更多数据
  - 在有条件的前提下，尽可能多地获取训练数据是最理想的方法，更多的数据可以让模型得到充分的学习，也更容易提高泛化能力
- 使用更大批次
  - 在相同迭代次数和学习率的条件下，每批次采用更多的数据将有助于模型更好的学习到正确的模式，模型输出结果也会更加稳定
- 调整数据分布
  - 大多数场景下的数据分布是不均匀的，模型过多地学习某类数据容易导致其输出结果偏向于该类型的数据，此时通过调整输入的数据分布可以一定程度提高泛化能力。<span style="color:red;">怎么调整数据分布？</span>
- 调整目标函数
  - 在某些情况下，目标函数的选择会影响模型的泛化能力，如目标函数 $f(y,y')=|y-y'|$ 在某类样本已经识别较为准确而其他样本误差较大的侵害概况下，不同类别在计算损失结果的时候距离权重是相同的，若将目标函数改成 $f(y,y')=(y-y')^2$ 则可以使误差小的样本计算损失的梯度比误差大的样本更小，进而有效地平衡样本作用，提高模型泛化能力。<span style="color:red;">这个一般都是用的交叉熵吧？</span>
- 调整网络结构
  - 在浅层卷积神经网络中，参数量较少往往使模型的泛化能力不足而导致欠拟合，此时通过叠加卷积层可以有效地增加网络参数，提高模型表达能力；在深层卷积网络中，若没有充足的训练数据则容易导致模型过拟合，此时通过简化网络结构减少卷积层数可以起到提高模型泛化能力的作用
- 数据增强
  - 数据增强又叫数据增广，在有限数据的前提下通过平移、旋转、加噪声等一些列变换来增加训练数据，同类数据的表现形式也变得更多样，有助于模型提高泛化能力，需要注意的是数据变化应尽可能不破坏元数数据的主体特征(如在图像分类任务中对图像进行裁剪时不能将分类主体目标裁出边界)。
- 权值正则化
  - 权值正则化就是通常意义上的正则化，一般是在损失函数中添加一项权重矩阵的正则项作为惩罚项，用来惩罚损失值较小时网络权重过大的情况，此时往往是网络权值过拟合了数据样本(如 $Loss=f(WX+b,y')+\frac{\lambda}{\eta}\sum{|W|}$)。<span style="color:red;">这个式子之前好像没见到吧？里面的参数是什么意思？</span>
- 屏蔽网络节点
  - 该方法可以认为是网络结构上的正则化，通过随机性地屏蔽某些神经元的输出让剩余激活的神经元作用，可以使模型的容错性更强。





将全连接层在输入输出不变的情况下改为卷积层：

- 比如：
  - VGG16 中第一个全连接层是 $25088 * 4096$ 的数据尺寸，将它转化为 $512 * 7 * 7 * 4096$ 的数据尺寸，即一个 K=4096的全连接层，输入数据体的尺寸是 $7 * 7 * 512$，这个全连接层可以被等效地看做一个 $F=7, P=0, S=1, K=4096$ 的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致 $7*7$，这样输出就变为 $1 * 1 * 4096$，本质上和全连接层的输出是一样的。
    - 注：输出激活数据体深度是由卷积核的数目决定的(K=4096)。
