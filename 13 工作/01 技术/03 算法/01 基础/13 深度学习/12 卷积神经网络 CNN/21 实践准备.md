# 实践准备


一些建议使用:

- 使用适当的日志和有意义的变量名
  - 在TensorFlow中，你可以通过名称来跟踪不同的变量，并在TensorBoard 中可视化图形。最重要的是，在每个训练步骤中，你都能记录相关的值，比如：step_number、accuracy、loss、learning_rate，甚至有时候还包括一些更具体的值，比如 mean_intersection_over_union。之后，就可以画出每一步的损失曲线。
- 确保您的网络连接正确
  - 使用 TensorBoard 或其他 debug 技术确保图中的每个操作的输入和输出都准确无误，还要确保在将数据和标签送入网络之前对其进行适当的预处理和配对。
- 实施数据增强技术
  - 虽然这一点并不是对所有情况都适用，不过如果你在搞图像相关的神经网络，用简单的数据增强技术处理一下图像，例如镜像、旋转、随机裁剪和重新缩放、添加噪声、弹性变形等，大部分时候出来的效果都有巨大提升。
- 对所有层使用权重初始化和正则化
  - 不要把权重初始化为相同的值
  - 一般情况下，如果你在权重初始化时遇到问题，你可以考虑在神经网络中添加**批量标准化层（Batch Normalization Layer）**。
    - 论文：[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
- 确保正则化条款不会压倒损失函数中的其他项。
  - 关闭正则化，找出损失的数量级，然后适当调整正则化权重。确保在增加正则化强度时，损失也在增加。
- 尝试过拟合一个小数据集。
  - 关闭正则化/丢失/数据增强，拿出训练集的一小部分，让神经网络练它几个世纪，确保可以实现零损失，不然就很可能是错误的。
  - 在某些情况下，将损失驱动为零非常具有挑战性，例如，如果您的损失涉及每个像素的 softmax-ed logits和ground truth labels 之间的交叉熵，那么在语义分割中可能真的难以将其降低到0。相反，你应该争取达到接近100％的准确度。
  - 可以在tf.metrics.accuracy这里了解如何通过获取softmax-ed logits的argmax并将其与ground truth labels进行比较来计算。
- 在过拟合上述小数据集的同时，找到合理的学习率。
  - Yoshua Bengio的论文中给到了结论：**最佳学习率通常接近最大学习率的一半，不会引起训练标准的差异**，这个观察结果是设置学习率的启发。
    - 例如，从较大的学习率开始，如果训练标准发散，就用最大学习率除以3再试试，直到观察不到发散为止。
- 执行梯度检查。
  - 如果您在图表中使用自定义操作，则梯度检查尤其重要。斯坦福CS231n中介绍了梯度检查的方法。
