
## 卷积神经网络特点：

- 稀疏交互 Sparse Interaction
- 参数共享 Parameter Sharing
- 等变表示


稀疏交互：

- 全连接：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190414/VJX71MPMNFM4.png?imageslim">
    </p>
- 卷积网络：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190414/2nAxgtHMwUwA.png?imageslim">
    </p>
- 说明：
  - 卷积核尺度远小于输入的维度，这样每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重（即产生交互），称这种特性为稀疏交互。
  - 假设网络中相邻两层分别具有 $m$ 个输入和 $n$ 个输出，
    - 全连接网络中的权值参数矩阵将包含 $m×n$ 个参数。
    - 对于卷积网络，如果限定每个输出与前一层神经元的连接数为 $k$ ，那么该层的参数总量为 $k×n$。
  - 在实际应用中，一般 $k$ 值远小于 $m$ 就可以取得较为可观的效果；而此时优化过程的时间复杂度将会减小几个数量级，过拟合的情况也得到了较好的改善。
  - 尽管直接连接都是很稀疏的，但处在更深的层中的单元可以间接地连接到全部或者大部分输入图像。
- 稀疏交互的物理意义是：
  - 通常图像、文本、语音等现实世界中的数据都具有局部的特征结构，我们可以先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。
    - 以人脸识别为例，
      - 最底层的神经元可以检测出各个角度的边缘特征
      - 位于中间层的神经元可以将边缘组合起来得到眼睛、鼻子、嘴巴等复杂特征
      - 位于上层的神经元可以根据各个器官的组合检测出人脸的特征
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190414/4rY92eCd7XK0.png?imageslim">
    </p>


参数共享：

- 参数共享是指在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。
  - 全连接网络中，计算每层的输出时，权值参数矩阵中的每个元素只作用于某个输入元素一次；
  - 在卷积神经网络中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上。



- 平移等变
  - 参数共享的物理意义：使得卷积层具有平移等变性
    - 假如图像中有一只猫，那么无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。
    - 特别地，当函数 $f(x)$ 与 $g(x)$ 满足 $f(g(x))=g(f(x))$ 时，我们称 $f(x)$ 关于变换 $g$ 具有等变性。将 $g$ 视为输入的任意平移函数，令 $I$ 表示输入图像（在整数坐标上的灰度值函数），平移变换后得到 $I^{\prime}=g(I)$ 。
      - 例如，我们把猫的图像向右移动 $l$ 像素，满足 $I^{\prime} (x, y)=I(x-l, y)$。我们令 $f$ 表示卷积函数，根据其性质，我们很容易得到 $g(f(I))=f\left(I^{\prime} \right)=f(g(I))$ 。也就是说，在猫的图片上先进行卷积，再向右平移 $l$ 像素的输出，与先将图片向右平移 $l$ 像素再进行卷积操作的输出结果是相等的。
  - 用处  
    - 当处理时间序列数据时，这意味着通过卷积可以得到一个由输入中出现不同特征的时刻所组成的时间轴。如果我们把输入中的一个事件向后延时，在输出中仍然会有完全相同的表示，只是时间延后了。
    - 图像与之类似，卷积产生了一个 2 维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。当处理多个输入位置时，一些作用在邻居像素的函数是很有用的。
      - 例如在处理图像时，在卷积网络的第一层进行图像的边缘检测是很有用的。相同的边缘或多或少地散落在图像的各处，所以应当对整个图像进行参数共享。但在某些情况下，我们并不希望对整幅图进行参数共享。例如，在处理已经通过剪裁而使其居中的人脸图像时，我们可能想要提取不同位置上的不同特征（处理人脸上部的部分网络需要去搜寻眉毛，处理人脸下部的部分网络就需要去搜寻下巴了）。


注意：

- 卷积对其他的一些变换并不是天然等变的，例如对于图像的放缩或者旋转变换，需要其他的一些机制来处理这些变换。（什么机制？）



## 对卷积与池化的理解

卷积与池化作为一种无限强的先验。

先验概率分布：

- 先验概率分布刻画了在我们看到数据之前我们认为什么样的模型是合理的信念。
- 先验的强或者弱取决于先验中概率密度的集中程度：
  - 弱先验具有较高的熵值
    - 例如方差很大的高斯分布。这样的先验允许数据对于参数的改变具有或多或少的自由性。
  - 强先验具有较低的熵值
    - 例如方差很小的高斯分布。这样的先验在决定参数最终取值时起着更加积极的作用。
- 一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对于这些参数的值给出了多大的支持。


卷积的使用，与先验概率分布：

- 可以把卷积的使用当作是对网络中一层的参数引入了一个无限强的先验概率分布。
  - 我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重以外，其余的权重都为零。
  - 这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性。
- 类似的，使用池化也是一个无限强的先验：
  - 每一个单元都具有对少量平移的不变性。
- 卷积和池化只有当先验的假设合理且正确时才有用：
  - 卷积和池化可能导致欠拟合。
    - 如果一项任务依赖于保存精确的空间信息，那么在所有的特征上使用池化将会增大训练误差。一些卷积网络结构为了既获得具有较高不变性的特征又获得当平移不变性不合理时不会导致欠拟合的特征，被设计成在一些通道上使用池化而在另一些通道上不使用。
    - 当一项任务涉及到要对输入中相隔较远的信息进行合并时，那么卷积所利用的先验可能就不正确了。（有什么例子吗？）
- 比较卷积模型只能以其他卷积模型为标准


## 全连接、局部连接、全卷积与局部卷积

（局部卷积是什么？补充例子）

- 全连接：
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/8cDl5QUmUK3S.png?imageslim">
    </p>
  - 层间神经元完全连接，每个输出神经元可以获取到所有输入神经元的信息，有利于信息汇总，常置于网络末层；连接与连接之间独立参数，大量的连接大大增加模型的参数规模。
- 局部连接
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/IhivGn7lG7PM.png?imageslim">
    </p>
  - 层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，超过这个范围的神经元则没有连接；连接与连接之间独立参数，相比于全连接减少了感受域外的连接，有效减少参数规模。
- 全卷积
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/tnSoeGxqFa69.png?imageslim">
    </p>
  - 层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，连接所采用的参数在不同感受域之间共享，有利于提取特定模式的特征；相比于局部连接，共用感受域之间的参数可以进一步减少参数量。
- 局部卷积
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/yF1hlH0XODPT.png?imageslim">
    </p>
  - 层间神经元只有局部范围内的连接，感受域内采用全连接的方式，而感受域之间间隔采用局部连接与全卷积的连接方式；相比与全卷积成倍引入额外参数，但有更强的灵活性和表达能力；相比于局部连接，可以有效控制参数量。
