

# 池化

## 卷积层


卷积层一般包含三个步骤：

- 1 并行地计算多个卷积产生一组线性激活响应。
- 2 每一个线性激活响应将会通过一个非线性的激活函数，例如整流线性激活函数。这一级有时也被称为探测级。
- 3 使用池化函数来进一步调整这一层的输出。

如图：

<p align="center">
    <img width="30%" height="70%" src="http://images.iterate.site/blog/image/20200624/4zouGGKKsxf1.png?imageslim">
</p>



## 池化

池化：

- 池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。
  - 池化操作的本质是降采样。
  - 主要针对非重叠区域
    - 重叠区域的池化，是采用比窗口宽度更小的步长，使得窗口在每次滑动时存在重叠的区域。
- 池化与先验：
  - 使用池化可以看作是增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性。
    - 当这个假设成立时，池化可以极大地提高网络的统计效率。
- 池化层与卷积的连接：
  - 对空间区域进行池化产生了平移不变性，但当我们对分离参数的卷积的输出进行池化时，特征能够学得应该对于哪种变换具有不变性。
    - 比如，卷积核作为过滤器，在匹配到目标的时候，进行激活，而随后的池化层，无论前面那个被激活，最大池化单元都具有大的激活。
- 可以使用池化操作降低图像维度的原因：
  - 本质上是因为图像具有一种 “静态性” 的属性，这个意思是说在一个图像区域有用的特征极有可能在另一个区域同样有用。
  - 因此，为了描述一个大的图像，很直观的想法就是对不同位置的特征进行聚合统计。
    - 例如，可以计算图像在固定区域上特征的平均值 (或最大值)来代表这个区域的特征。
- CNN的变形稳定性和池化无关，滤波器平滑度才是关键。
  - 传统观点认为，CNN 中的池化层导致了对微小平移和变形的稳定性。Deepmind 论文中说：CNN 的变形稳定性仅在初始化时和池化相关，在训练完成后则无关；并指出，滤波器的平滑度才是决定变形稳定性的关键因素。
    - [论文](https://arxiv.org/pdf/1804.04438.pdf)
- 作用：
  - 显著降低参数量
    - 可以降低卷积层输出的特征维度，有效减少网络参数的同时还可以防止过拟合现象。
  - 能够保持对平移、伸缩、旋转操作的不变性。
    - 平移不变性是指输出结果对输入的小量平移基本保持不变。例如，输入为（1，5，3），最大池化将会取 5，如果将输入右移一位得到（0，1，5），输出的结果仍将为 5。
      - 局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时。
        - 例如，当判定一张图像中是否包含人脸时，我们并不需要知道眼睛的精确像素位置，我们只需要知道有一只眼睛在脸的左边，有一只在右边就行了。但在一些其他领域，保存特征的具体位置却很重要。例如当我们想要寻找一个由两条边相交而成的拐角时，我们就需要很好地保存边的位置来判定它们是否相交。
    - 对伸缩的不变性（一般称为尺度不变性）可以这样理解，如果原先神经元在最大池化操作之后输出 5，那么在经过伸缩（尺度变换）之后，最大池化操作在该神经元上很大概率的输出仍然是 5。因为神经元感受的是邻域输入的最大值，而并非某一个确定的值。
    - 旋转不变性可以参照图 9.19。图中的神经网络由 3 个学得的过滤器和一个最大池化层组成。这 3 个过滤器分别学习到不同旋转方向的 “5”。当输入中出现 “5” 时，无论进行何种方向的旋转，都会有一个对应的过滤器与之匹配并在对应的神经元中引起大的激活。最终，无论哪个神经元获得了激活，在经过最大池化操作之后输出都会具有大的激活。

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190414/2EdEpp2X37Xh.png?imageslim">
    </p>

- 副总用：
  - 可能会使得一些 需要利用自顶向下信息 的神经网络结构变得复杂
    - 例如卷积玻尔兹曼机和自编码器。一些可微网络中需要的在池化单元上进行类逆运算。
- 池化的选择：
  - 一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导。将特征一起动态地池化也是可行的，例如，对于感兴趣特征的位置运行聚类算法。这种方法对于每幅图像产生一个不同的池化区域集合。另一种方法是先学习一个单独的池化结构，再应用到全部的图像中。(没有很明白）)
    - NetVLAD 池化 是吗？

- 常用池化
  - 一般池化 General Pooling 
    - 如图：
        <p align="center">
            <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/o3iss32MWmXU.png?imageslim">
        </p>
    - 通常包括最大池化(Max Pooling)和平均池化(Mean Pooling)。
      - 均值池化（mean pooling）
        - 通过对邻域内特征数值求平均来实现
        - 能够抑制由于邻域大小受限造成估计值方差增大的现象，
        - 特点是对背景的保留效果更好。
      - 最大池化（max pooling）
        - 通过取邻域内特征的最大值来实现
        - 能够抑制网络参数误差造成估计均值偏移的现象
        - 特点是更好地提取纹理信息。
  - 重叠池化 Overlapping Pooling
    - 如图：
      <p align="center">
          <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/prEf5VXD3Ifu.png?imageslim">
      </p>
    - 与一般池化操作相同，但是池化范围 $P_{size}$ 与滑窗步长 $stride$ 关系为 $P_{size}>stride$，同一区域内的像素特征可以参与多次滑窗提取，得到的特征表达能力更强，但计算量更大。
  - 空间金字塔池化 Spatial Pyramid Pooling
    - 如图：
      <p align="center">
          <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/B8jQmlnCq3hc.png?imageslim">
      </p>
    - 在进行多尺度目标的训练时，卷积层允许输入的图像特征尺度是可变的，紧接的池化层若采用一般的池化方法会使得不同的输入特征输出相应变化尺度的特征，而卷积神经网络中最后的全连接层则无法对可变尺度进行运算，因此需要对不同尺度的输出特征采样到相同输出尺度。空间金字塔池化主要考虑了多尺度信息的描述，例如同时计算 $1×1$、$2×2$、$4×4$ 的矩阵的池化并将结果拼接在一起作为下一网络层的输入。
      - 举例：SPPNet 就引入了空间池化的组合，对不同输出尺度采用不同的滑窗大小和步长以确保输出尺度相同 $(win_{size}=\lceil \frac{in}{out}\rceil; stride=\lfloor \frac{in}{out}\rfloor; )$，同时用如金字塔式叠加的多种池化尺度组合，以提取更加丰富的图像特征。常用于多尺度训练和目标检测中的区域提议网络(Region Proposal Network)的兴趣区域(Region of Interest)提取。
  - 其他常用的池化函数包括 $L^2$ 范数以及基于据中心像素距离的加权平均函数 等。
- 池化层与卷积层比较：
  - 类似：
    - 都是对感受域内的特征进行提取，并且根据步长设置获取到不同维度的输出。
  - 区别：
    - 结构
      - 零填充时输出维度不变，而通道数改变
      - 通常特征维度会降低，通道数不变
    - 稳定性
      - 输入特征发生细微改变时，输出结果会改变
      - 感受域内的细微变化不影响输出结果
    - 作用 
      - 感受域内提取局部关联特征。
      - 感受域内提取泛化特征，降低维度。
    - 参数量
      - 与卷积核尺寸、卷积核个数相关
      - 不引入额外参数 







## NetVLAD 池化

（没有理解）

NetVLAD 池化：

- NetVLAD 是一个局部特征聚合的方法。
- 缘由：
  - 在传统的网络里面，例如 VGG，最后一层卷积层输出的特征都是类似于 Batchsize x 3 x 3 x 512的这种东西，然后会经过 FC 聚合，或者进行一个 Global Average Pooling（NIN里的做法），或者怎么样，变成一个向量型的特征，然后进行 Softmax or 其他的 Loss。
  - 这种方法说简单点也就是输入一个图片或者什么的结构性数据，然后经过特征提取得到一个长度固定的向量，之后可以用度量的方法去进行后续的操作，比如分类啊，检索啊，相似度对比等等。
  - 那么 NetVLAD 考虑的主要是最后一层卷积层输出的特征这里，我们不想直接进行欠采样或者全局映射得到特征，对于最后一层输出的 $W \times H \times D$，设计一个新的池化，去聚合一个 “局部特征”，这即是 NetVLAD的作用。
- 介绍：
  - NetVLAD 的一个输入是一个 $W \times H \times D$ 的图像特征，例如 VGG-Net 最后的 3 x 3 x 512 这样的矩阵，在网络中还需加一个维度为 Batchsize。
  - NetVLAD 还需要另输入一个标量 $K$ 即表示 VLAD 的聚类中心数量，它主要是来构成一个矩阵 $C$，是通过原数据算出来的每一个 $W \times H$ 特征的聚类中心，$C$ 的 shape 即 $C: K \times D$，
  - 根据三个输入，VLAD 是计算下式的 $V$:（怎么制定这个 K？）
    $$
    V(j, k) = \sum_{i=1}^{N}{a_k(x_i)(x_i(j) - c_k(j))}
    $$
  - 说明:
    - $j$ 表示维度，从 $1$ 到 $D$，
    - 可以看到 $V$ 的 $j$ 是和输入与 $c$ 对应的，对每个类别 $k$，都对所有的 $x$ 进行了计算，如果 $x_i$ 属于当前类别 $k$，$a_k=1$，否则 $a_k=0$，计算每一个 $x$ 和它聚类中心的残差，然后把残差加起来，即是每个类别 $k$ 的结果，
    - 最后分别 $L2$ 正则后拉成一个长向量后再做 $L2$ 正则，正则非常的重要，因为这样才能统一所有聚类算出来的值，而残差和的目的主要是消减不同聚类上的分布不均，两者共同作用才能得到最后正常的输出。

输入与输出如下图所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/Un8pe7CWMdpn.png?imageslim">
</p>


中间得到的 $K$ 个 $D$ 维向量即是对 $D$ 个 $x$ 都进行了与聚类中心计算残差和的过程，最终把 $K$ 个 $D$ 维向量合起来后进行即得到最终输出的 $K \times D$ 长度的一维向量。

而 VLAD 本身是不可微的，因为上面的 $a$ 要么是 $0$ 要么是 $1$，表示要么当前描述 $x$ 是当前聚类，要么不是，是个离散的，NetVLAD 为了能够在深度卷积网络里使用反向传播进行训练，对 $a$ 进行了修正。

那么问题就是如何重构一个 $a$，使其能够评估当前的这个 $x$ 和各个聚类的关联程度？用 softmax 来得到：

$$
a_k = \frac{e^{W_k^T x_i + b_k}}{e^{W_{k'}^T x_i + b_{k'}}}
$$

将这个把上面的 $a$ 替换后，即是 NetVLAD 的公式，可以进行反向传播更新参数。

所以一共有三个可训练参数，上式 $a$ 中的 $W: K \times D$，上式 $a$ 中的 $b: K \times 1$，聚类中心 $c: K \times D$，而原始 VLAD 只有一个参数 c。

最终池化得到的输出是一个恒定的 $K \times D$ 的一维向量（经过了 L2 正则），如果带 Batchsize，输出即为 $Batchsize \times (K \times D)$ 的二维矩阵。

NetVLAD 作为池化层嵌入 CNN 网络即如下图所示，

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/LMwAalfz3e6c.png?imageslim">
</p>


原论文中采用将传统图像检索方法 VLAD 进行改进后应用在 CNN 的池化部分作为一种另类的局部特征池化，在场景检索上取得了很好的效果。<span style="color:red;">什么是场景检索？</span>

后续相继又提出了 ActionVLAD、ghostVLAD 等改进。
