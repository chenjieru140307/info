# 卷积核

## 卷积核的类型：

- 标准卷积
  - 示意图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/kcnARFrWSUWv.png?imageslim">
    </p>
  - 最常用的卷积核，连续紧密的矩阵形式可以提取图像区域中的相邻像素之间的关联关系，$3\times3$ 的卷积核可以获得 $3\times3$ 像素范围的感受视野
- 扩张卷积（带孔卷积或空洞卷积）  
  - 示意图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/sJeejjrGUJwl.png?imageslim">
    </p>
  - 引入一个称作扩张率（Dilation Rate）的参数，使同样尺寸的卷积核可以获得更大的感受视野，相应的在相同感受视野的前提下比普通卷积采用更少的参数。
    - 同样是 $3\times3$ 的卷积核尺寸，扩张卷积可以提取 $5\times5$ 范围的区域特征，在实时图像分割领域广泛应用。
- 转置卷积
  - 示意图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/XG7LTnqBzwiY.png?imageslim">
    </p>
  - 先对原始特征矩阵进行填充使其维度扩大到适配卷积目标输出维度，然后进行普通的卷积操作的一个过程，<span style="color:red;">怎么进行填充的？</span>其输入到输出的维度变换关系恰好与普通卷积的变换关系相反，但这个变换并不是真正的逆变换操作，通常称为转置卷积(Transpose Convolution)而不是反卷积(Deconvolution)。<span style="color:red;">到底什么是反卷积？之前在可视化特征那部分看到这个词。</span>
    - 转置卷积常见于目标检测领域中对小目标的检测和图像分割领域还原输入图像尺度。<span style="color:red;">怎么用在对于小目标的检测的？怎么在图像分割领域还原输入图像尺度的？</span> 
- 可分离卷积
  - 示意图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/hvkcv4x5ucD6.png?imageslim">
    </p> <span style="color:red;">为什么可分离卷积使用的是这么一个图？</span>
  - 标准的卷积操作是同时对原始图像 $H\times W\times C$ 三个方向的卷积运算，假设有 $K$ 个相同尺寸的卷积核，这样的卷积操作需要用到的参数为 $H\times W\times C\times K$ 个；若将长宽与深度方向的卷积操作分离出变为 $H\times W$ 与 $C$ 的两步卷积操作，<span style="color:red;">怎么分离成两步卷积操作的？细节补充下。</span>则同样的卷积核个数 $K$，只需要 $(H\times W + C)\times K$ 个参数，便可得到同样的输出尺度。
    - 可分离卷积(Seperable Convolution)通常应用在模型压缩或一些轻量的卷积神经网络中，如 MobileNet$^{[1]}$、Xception$^{[2]}$ 等。<span style="color:red;">模型压缩是什么？一般怎么压缩？MobileNet 和 Xception 详细了解下，怎么轻量了。</span>


## 卷积核相关参数

- 卷积核大小 (Kernel Size)：卷积核的大小定义了卷积的感受野
  - 在过去常设为 5，如 LeNet-5；现在多设为 3，通过堆叠 $3\times3$ 的卷积核来达到更大的感受域
    - 卷积核为奇数有以下好处：
      - 保证锚点刚好在中间，方便以 central pixel为标准进行滑动卷积，避免了位置信息发生偏移 。
      - 保证在填充（Padding）时，在图像之间添加额外的零层，图像的两边仍然对称。
- 卷积核步长 (Stride) ：定义了卷积核在卷积过程中的步长
  - 常见设置为 1，表示滑窗距离为 1，可以覆盖所有相邻位置特征的组合；当设置为更大值时相当于对特征组合降采样<span style="color:red;">什么是特征组合降采样？</span>
- 填充方式 (Padding) ：在卷积核尺寸不能完美匹配输入的图像矩阵时需要进行一定的填充策略
  - 设置
    - 'SAME'：表示对不足卷积核大小的边界位置进行某种填充（通常零填充）以保证卷积输出维度与与输入维度一致；
      - 可以有效地保留原始的输入特征信息。
      - 此时又称宽卷积。
    - 'VALID' 并不进行任何填充，在输入特征边缘位置若不足以进行卷积操作，则对边缘信息进行舍弃。
      - 因此在步长为 $1$ 的情况下该填充方式的卷积层输出特征维度可能会略小于输入特征的维度。
      - 此时又称窄卷积。
  - 零填充设定：
    - 不允许填充，只允许卷积核访问那些图像中能够完全包含整个核的位置。
      - 在 MATLAB 的术语中，这称为有效卷积。
      - 在这种情况下，输出的大小在每一层都会缩减。这限制了网络中能够包含的卷积层的层数。
    - 只进行足够的零填充来保持输出和输入具有相同的大小。
      - 在 MATLAB 的术语中，这称为相同卷积。
      - 在这种情况下，只要硬件支持，网络就能包含任意多的卷积层，这是因为卷积运算不改变下一层的结构。
      - 然而，输入像素中靠近边界的部分相比于中间部分对于输出像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。
    - 进行足够多的零填充使得每个像素在每个方向上恰好被访问了 $k$ 次，最终输出图像的宽度为 $m+k-1$。
      - 在 MATLAB 中称为全卷积。
      - 可以认为全卷积是在输入的两端各填充 $k-1$ 个零，使得输入的每个像素都恰好被核访问 $k$ 次，最终得到的输出的宽度为 $[m+2(k-1)]-k+1=m+k-1$。
      - 在这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致学得一个在卷积特征映射的所有位置都表现不错的单核更为困难。
- 输入通道数 (In Channels)：指定卷积操作时卷积核的深度
  - 默认与输入的特征矩阵通道数（深度）一致；在某些压缩模型中会采用通道分离的卷积方式。<span style="color:red;">什么是通道分离的卷积方式？什么是压缩模型？</span>
- 输出通道数 (Out Channels) ：指定卷积核的个数
  - 若设置为与输入通道数一样的大小，可以保持输入输出维度的一致性；若采用比输入通道数更小的值，则可以减少整体网络的参数量<span style="color:red;">一般是越来越多吧？参数量的减少可以通过池化来减少吧？</span> |

输出维度计算：

$$
O_d =\begin{cases} \lceil \frac{(I_d - k_{size})+ 1)}{s}\rceil ,& \text{padding=VALID}\\ \lceil \frac{I_d}{s}\rceil,&\text{padding=SAME} \end{cases}
$$

说明：

- $I_d$ 为输入维度，$O_d$ 为输出维度，$k_{size}$ 为卷积核大小，$s$ 为步长


## 感受野

- 感受野被定义为卷积神经网络特征所能看到输入图像的区域，换句话说特征输出受感受野区域内的像素点的影响。

计算：

- $RF_{l+1}=RF_{l}+\left(kernel\_size_{l+1}-1\right)\times{feature\_stride}_{l}$
- 其中：
  - RF 表示特征感受野大小，
  - l表示层数,
  - $feature\_stride_{l}=\prod_{i=1}^{l}stride_{i}$ 表示输入层，
  - $R F_{0}=1$
  - $feature\_stride_{0}=1$
- 如果有 dilated conv 的话，计算公式
  - $R F_{l+1}=R F_{l}+\left(k e r n e l\_ s i z e_{l+1}-1\right) \times feature\_stride_{l} \times\left({dilation}_{l+1}-1\right)$

举例：

- 一个三层的神经卷积神经网络，每一层卷积核的 *kernel_size* = 3 ，*stride* = 1，为了方便，将二维简化为一维，
- 第一层特征，感受野为 3
  - 图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200620/uwgiTuvG6mn0.png?imageslim">
    </p>
  - 计算：
    - $RF_{1}=RF_{0}+\left( {kernel\_size}_{1}-1\right) \times feature\_stride_{0}=1+(3-1) \times 1=3$




- 第二层特征，感受野为 5
  - 图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200620/b8RahQUa6YO3.png?imageslim">
    </p>
  - 计算：
    - $R F_{2}=R F_{1}+\left(k e r n e l\_s i z e_{2}-1\right) \times feature\_stride_{1}=3+(3-1) \times 1=5$
- 第三层特征，感受野为 7
  - 图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200620/KYyN1U5LfBBD.png?imageslim">
    </p>
  - 计算：
    - $R F_{3}=R F_{2}+\left(k e r n e l\_s i z e_{3}-1\right) \times feature\_stride_{2}=5+(3-1) \times 1=7$


说明：

- 上面所述的是理论感受野，而特征的有效感受野（实际起作用的感受野）实际上是远小于理论感受野的。
- 如下图所示，有效感受野示例：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200620/y9Yn0RBR22Me.png?imageslim">
</p>

- 有效感受野背后的原因：
  - 以一个两层的卷积神经网络为例
    - 每一层卷积核的 *kernel_size* = 3 ，*stride* = 1 的网络为例，
  - 则，该网络的理论感受野为 5，计算流程可以参加下图。
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200620/TwC2ESxTcL79.png?imageslim">
    </p>
  - 其中：
    - $x$ 为输入，$w$ 为卷积权重，$o$ 为经过卷积后的输出特征。
  - 说明：
    - 可见，$x_{1,1}$ 只影响第一层 feature map 中的 $o_{1,1}^{1}$ ；而 $x_{3,3}$ 会影响第一层 feature map 中的所有特征，即 $o_{1,1}^{1}, o_{1,2}^{1}, o_{1,3}^{1}, o_{2,1}^{1}, o_{2,2}^{1}, o_{2,3}^{1}, o_{3,1}^{1}, o_{3,2}^{1}, o_{3,3}^{1}$。
    - 第一层的输出全部会影响第二层的 $o_{1,1}^{2}$。
    - 于是 $x_{1,1}$ 只能通过 $o_{1,1}^{1}$ 来影响 $o_{1,1}^{2}$ ；而 $x_{3,3}$ 能通过 $o_{1,1}^{1}, o_{1,2}^{1}, o_{1,3}^{1}, o_{2,1}^{1}, o_{2,2}^{1}, o_{2,3}^{1}, o_{3,1}^{1}, o_{3,2}^{1}, o_{3,3}^{1}$ 来影响 $o_{1,1}^{2}$ 。
    - 显而易见，虽然 $x_{1,1}$ 和 $x_{3,3}$ 都位于第二层特征感受野内，但是二者对最后的特征 $o_{1,1}^{2}$ 的影响却大不相同，输入中越靠感受野中间的元素对特征的贡献越大。

应用：

- 目标检测中，anchor 的设定。
  - 一些目标检测网络基于anchor的，比如SSD系列，v2以后的yolo，还有faster rcnn系列。基于 anchor 的目标检测网络会预设一组大小不同的anchor，比如 32x32、64x64、128x128、256x256，这么多anchor，我们应该放置在哪几层比较合适呢？这个时候感受野的大小是一个重要的考虑因素。
  - 放置 anchor 层的特征感受野应该跟 anchor 大小相匹配，感受野比anchor 大太多不好，小太多也不好。如果感受野比 anchor 小很多，就好比只给你一只脚，让你说出这是什么鸟一样。如果感受野比 anchor 大很多，则好比给你一张世界地图，让你指出故宫在哪儿一样。（觉得，anchor 是不是不是很好，需要手动设定。）






## 常见卷积核的作用：

- 图像与不同卷积核的卷积可以用来执行边缘检测、锐化和模糊等操作。
- 输出原图:
  - 卷积核：$\begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$
  - 图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/pX6GkP9s7AIB.jpg?imageslim">
    </p>
- 进行边缘检测（突出边缘差异）
  - 卷积核：$\begin{bmatrix} 1 & 0 & -1 \\ 0 & 0 & 0 \\ -1 & 0 & 1 \end{bmatrix}$
  - 图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/IanWUAWXQaBr.jpg?imageslim">
    </p>
- 进行边缘检测（突出中间值）
  - 卷积核：$\begin{bmatrix} -1 & -1 & -1 \\ -1 & 8 & -1 \\ -1 & -1 & -1 \end{bmatrix}$
  - 图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/Bb5CjF2Cfp1S.jpg?imageslim">
    </p> 
- 图像锐化
  - 卷积核：$\begin{bmatrix} 0 & -1 & 0 \\ -1 & 5 & -1 \\ 0 & -1 & 0 \end{bmatrix}$
  - 图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/6DYdGe2iDeS2.jpg?imageslim">
    </p>
- 方块模糊
  - 卷积核：$\begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix} \times \frac{1}{9}$
  - 图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/wGO6Piu1rkvo.jpg?imageslim">
    </p>
- 高斯模糊
  - 卷积核：$\begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \end{bmatrix} \times \frac{1}{16}$
  - 图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/IgxXhR5VFf7s.jpg?imageslim">
    </p>
