# 卷积计算


## 卷积计算

卷积前的图像深度为 1 时，卷积计算：

- 卷积计算公式如下：
    $$
    a_{i,j} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b )
    $$
- 说明：
  - $x_{i,j}$ 表示图像第 $i$ 行第 $j$ 列元素。
  - $w_{m,n}$ 表示 filter​ 第 $m$ 行第 $n$ 列权重。 
  - $w_b$ 表示 filter 的偏置项。 
  - $a_i,_j$ 表示 feature map 第 $i$ 行第 $j$ 列元素。 
  - $f$ 表示激活函数，这里以 $ReLU$ 函数为例。


举例：

- 假设有一个 $5\times5$ 的图像，使用一个 $3\times3$ 的 filter 进行卷积，想得到一个 $3\times3$ 的 Feature Map
- 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/85eGrIDIxhRb.png?imageslim">
    </p>

- 则：步长为 $1$ 时，计算 feature map 元素 $a_{0,0}​$ 如下：

    $$
    \begin{aligned}
    a_{0,0} &= f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{0+m, 0+n} + w_b )\\
    &= relu(w_{0,0} x_{0,0} + w_{0,1} x_{0,1} + w_{0,2} x_{0,2} + w_{1,0} x_{1,0} + w_{1,1} x_{1,1} + w_{1,2} x_{1,2} + w_{2,0} x_{2,0} + w_{2,1} x_{2,1} + w_{2,2} x_{2,2}) \\
    &= 1 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 1 \\
    &= 4
    \end{aligned}
    $$

- 计算过程图示如下：

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/6KKobKwqjbed.png?imageslim">
    </p>

- 以此类推，计算出全部的 Feature Map。

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/Ikz0GkyPV7Vd.png?imageslim">
    </p>


- 当步幅为 2 时，Feature Map计算如下

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/834YzQ0tTAau.png?imageslim">
    </p>


卷积前的图像深度为 $D​$ 时，卷积计算：

- 此时，相应的 filter 的深度也必须为 $D​$。
  - 注意：是 filter 的深度，不是 filter 的个数。
- 卷积计算公式：
    $$
    a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)
    $$
- 其中：
  - $D$ 是深度；$F$ 是 filter 的大小；
  - $w_{d,m,n}$ 表示 filter 的第 $d$ 层第 $m$ 行第 $n$ 列权重；
  - $a_{d,i,j}$ 表示 feature map 的第 $d$ 层第 $i$ 行第 $j$ 列像素
- 此时：
  - 每个卷积层可以有多个 filter。
  - 每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。
  - 卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。

举例：

- 如图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/49jrJ6h5Od7R.png?imageslim">
</p>


- 说明：
  - 显示了包含两个 filter 的卷积层的计算。
  - $7*7*3$ 输入，经过两个 $3*3*3$ filter 的卷积(步幅为 $2$)，得到了 $3*3*2$ 的输出。
  - 图中的 Zero padding 是 $1$，也就是在输入元素的周围补了一圈 $0$。

疑问：

- 上面的 -5 是不是有问题？结果不是 `2*-1+2*0+2*0+-=-2` 吗？



图像大小、步幅和卷积后的 Feature Map大小：

- 它们满足下面的关系：
    $$
    W_2 = (W_1 - F + 2P)/S + 1\\
    H_2 = (H_1 - F + 2P)/S + 1
    $$
- 其中：
  - $W_1$ 是卷积前图像的宽度；$H_1$ 是卷积前图像的宽度。
  - $W_2$ 是卷积后 Feature Map 的宽度；$H_2$ 卷积后 Feature Map 的高度；
  - $F$ 是 filter 的宽度；
  - $P$ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 $0$，如果 $P$ 的值是 $1$，那么就补 $1$ 圈 $0$；
  - $S$ 是步幅；

举例：

- 假设图像宽度 $W_1 = 5$，filter 宽度 $F=3$，Zero Padding $P=0$，步幅 $S=2$
- 则
    $$
    \begin{aligned}
    W_2 =& (W_1 - F + 2P)/S + 1
    \\=& (5-3+0)/2 + 1
    \\=& 2
    \end{aligned}
    $$
- 即：
  - Feature Map 宽度是 2。
- 同样，我们也可以计算出 Feature Map 高度也是 2。


# 计算 正向传播与反向传播

（没有很完整，需要完整计算，并配合程序进行验证）



举例：


卷积神经网络：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190919/8MDHmDzzaqwd.png?imageslim">
</p>

- 设：
  - 卷积核 filter1 的 stride = 1，激活函数为 Relu。
  - 池化层滑动窗口为 2 * 2，池化层没有激活函数。
  - 输入是一个 $4*4$ 的 image，经过两个 $2*2$ 的卷积核进行卷积运算后，变成两个 $3*3$ 的 feature_map

- 前向传播
  - 输入层 -> 卷积层
    - 如图：

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190919/h1weSnSb7cFz.png?imageslim">
    </p>

    - 计算：卷积层某个神经元的输入输出为：
    $$
    \begin{aligned} \ in_{c_{11}}&= conv (input,filter)\\  &= i_{11} \times f_{11} + i_{12} \times f_{12} +i_{21} \times f_{21} + i_{22} \times f_{22}\\  &=1 \times 1 + 0 \times (-1) +1 \times 1 + 1 \times (-1)=1
    \\out_{c_{11}} &= activators(in_{c_{11}}) \\ &=\max(0,in_{c_{11}}) = 1
    \end{aligned}
    $$
    - 其他神经元计算方式相同
  - 卷积层 -> 池化层
    - 如图：

    <p align="center">
        <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20190919/HnOpJ2ukI6AH.png?imageslim">
    </p>

    - 计算：池化层某个神经元的输入输出为：

    $$\begin{aligned}
    in_{m_{11}} &= max(c_{11},c_{12},c_{21},c_{22}) = 1
    \\ out_{m_{11}} &= in_{m_{11}} = 1
    \end{aligned}
    $$
  - 池化层 -> 全连接层
    - 池化层的输出到 flatten 层把所有元素 “拍平”，然后到全连接层。
  - 全连接层 -> 输出层
    - 全连接层到输出层就是正常的神经元与神经元之间的邻接相连，通过 softmax 函数计算后输出到 output，得到不同类别的概率值，输出概率值最大的即为该图片的类别。
- 反向传播
  - 池化层输出时不需要经过激活函数，是一个滑动窗口的最大值，一个常数，那么它的偏导是 1。池化层相当于对上层图片做了一个压缩，这个反向求误差敏感项时与传统的反向传播方式不同。从卷积后的 feature_map反向传播到前一层时，由于前向传播时是通过卷积核做卷积运算得到的 feature_map，所以反向传播与传统的也不一样，需要更新卷积核的参数。下面我们介绍一下池化层和卷积层是如何做反向传播的。



### 卷积层的反向传播

- 由前向传播可得：
  - 每一个神经元的值都是上一个神经元的输入作为这个神经元的输入，经过激活函数激活之后输出，作为下一个神经元的输入，
  - 用 $i$ 表示 input 层，$o$ 表示 feature_map 层。
  - 用 $in$ 表示某一层的输入，$out$ 表示某一层的输出。
  - 则：
    - $in_{i_{11}}$ 就是 input 层 1,1 这个神经元的输入。
    - $out_{i_{11}}$ 就是 input 层 1,1 这个神经元的输出
    - $in_{c_{11}}$ 就是 conv 层 1,1 这个神经元的输入
    - $out_{c_{11}}$ 就是 conv 层 1,1 这个神经元的输出，
- 此时：

    $$
    \begin{aligned} out_{i_{11}} &= activators(in_{i_{11}})
    \\&=\max(0,in_{i_{11}}) 
    \\ \ in_{c_{11}}&= conv (input,filter)
    \\  &= out_{i_{11}} \times f_{11} + out_{i_{12}} \times f_{12} +out_{i_{21}} \times f_{21} + out_{i_{22}} \times f_{22}
    \\  out_{c_{11}} &= activators(in_{c_{11}}) 
    \\ &=\max(0,in_{c_{11}}) 
    \end{aligned}
    $$

- 计算 $i_{11}$ 的误差项 $\delta_{i_{11}}$：

    $$
    \begin{aligned}
    \delta_{i_{11}} &= \frac{\partial E}{\partial in_{i_{11}}} 
    \\&=\frac{\partial E}{\partial out_{i_{11}}} \cdot \frac{\partial out_{i_{11}}}{\partial in_{i_{11}}} 
    \end{aligned}
    $$

  - 计算 $\frac{\partial E}{\partial out_{i_{11}}}$
    - 先把 input 层做完卷积运算后的 feature_map 如下:

        $$\begin{aligned}
        in_{c_{11}} = out_{i_{11}} \times f_{11} + out_{i_{12}} \times f_{12} +out_{i_{21}} \times f_{21} + out_{i_{22}} \times f_{22} 
        \\ in_{c_{12}} = out_{i_{12}} \times f_{11} + out_{i_{13}} \times f_{12} +out_{i_{22}} \times f_{21} + out_{i_{23}} \times f_{22} 
        \\ in_{c_{12}} = out_{i_{13}} \times f_{11} + out_{i_{14}} \times f_{12} +out_{i_{23}} \times f_{21} + out_{i_{24}} \times f_{22} 
        \\ in_{c_{21}} = out_{i_{21}} \times f_{11} + out_{i_{22}} \times f_{12} +out_{i_{31}} \times f_{21} + out_{i_{32}} \times f_{22} 
        \\ in_{c_{22}} = out_{i_{22}} \times f_{11} + out_{i_{23}} \times f_{12} +out_{i_{32}} \times f_{21} + out_{i_{33}} \times f_{22} 
        \\ in_{c_{23}} = out_{i_{23}} \times f_{11} + out_{i_{24}} \times f_{12} +out_{i_{33}} \times f_{21} + out_{i_{34}} \times f_{22} 
        \\ in_{c_{31}} = out_{i_{31}} \times f_{11} + out_{i_{32}} \times f_{12} +out_{i_{41}} \times f_{21} + out_{i_{42}} \times f_{22} 
        \\ in_{c_{32}} = out_{i_{32}} \times f_{11} + out_{i_{33}} \times f_{12} +out_{i_{42}} \times f_{21} + out_{i_{43}} \times f_{22} 
        \\ in_{c_{33}} = out_{i_{33}} \times f_{11} + out_{i_{34}} \times f_{12} +out_{i_{43}} \times f_{21} + out_{i_{44}} \times f_{22} 
        \end{aligned}
        $$

    - 依次对输入元素 $out_{i_{i,j}}$ 求偏导

        $$
        \begin{aligned}
        \frac{\partial E}{\partial out_{i_{11}}}&=\frac{\partial E}{\partial in_{c_{11}}} \cdot \frac{\partial in_{c_{11}}}{\partial out_{i_{11}}}
        \\ &=\delta_{c_{11}} \cdot f_{11} 
        \\
        \frac{\partial E}{\partial out_{i_{12}}}&=\frac{\partial E}{\partial in_{c_{11}}} \cdot \frac{\partial in_{c_{11}}}{\partial out_{i_{12}}} +\frac{\partial E}{\partial in_{c_{12}}} \cdot \frac{\partial in_{c_{12}}}{\partial out_{i_{12}}}\\ &=\delta_{c_{11}} \cdot f_{12}+\delta_{c_{12}} \cdot f_{11}
        \\
        \frac{\partial E}{\partial out_{i_{13}}}&=\frac{\partial E}{\partial in_{c_{12}}} \cdot \frac{\partial in_{c_{12}}}{\partial out_{i_{13}}} +\frac{\partial E}{\partial in_{c_{13}}} \cdot \frac{\partial in_{c_{13}}}{\partial out_{i_{13}}}\\ &=\delta_{c_{12}} \cdot f_{12}+\delta_{c_{13}} \cdot f_{11}
        \\
        \frac{\partial E}{\partial out_{i_{21}}}&=\frac{\partial E}{\partial in_{c_{11}}} \cdot \frac{\partial in_{c_{11}}}{\partial out_{i_{21}}} +\frac{\partial E}{\partial in_{c_{21}}} \cdot \frac{\partial in_{c_{21}}}{\partial out_{i_{21}}}\\ &=\delta_{c_{11}} \cdot f_{21}+\delta_{c_{21}} \cdot f_{11}
        \\
        \frac{\partial E}{\partial out_{i_{22}}}&=\frac{\partial E}{\partial in_{c_{11}}} \cdot \frac{\partial in_{c_{11}}}{\partial out_{i_{22}}} +\frac{\partial E}{\partial in_{c_{12}}} \cdot \frac{\partial in_{c_{12}}}{\partial out_{i_{22}}}\\ &+\frac{\partial E}{\partial in_{c_{21}}} \cdot \frac{\partial in_{c_{21}}}{\partial out_{i_{22}}}+\frac{\partial E}{\partial in_{c_{22}}} \cdot \frac{\partial in_{c_{22}}}{\partial out_{i_{22}}}\\ &=\delta_{c_{11}} \cdot f_{22}+\delta_{c_{12}} \cdot f_{21}+\delta_{c_{21}} \cdot f_{12}+\delta_{c_{22}} \cdot f_{11}
        \end{aligned}$$

    - 观察一下上面几个式子的规律，归纳如下：


        $${ \left[ \begin{array}{ccc} 0& 0& 0& 0& 0& \\      0& \delta_{c_{11}} & \delta_{c_{12}} & \delta_{c_{13}}&0\\      0&\delta_{c_{21}} & \delta_{c_{22}} & \delta_{c_{23}} &0\\      0&\delta_{c_{31}} & \delta_{c_{32}} & \delta_{c_{33}} &0\\      0& 0& 0& 0& 0& \\ \end{array}  \right ] \cdot  \left[ \begin{array}{ccc}      f_{22}&  f_{21} \\      f_{12}&  f_{11} \\ \end{array} \right]}= \left[ \begin{array}{ccc}     \frac{\partial E}{\partial out_{i_{11}}}& \frac{\partial E}{\partial out_{i_{12}}}& \frac{\partial E}{\partial out_{i_{13}}}& \frac{\partial E}{\partial out_{i_{14}}} \\      \frac{\partial E}{\partial out_{i_{21}}}& \frac{\partial E}{\partial out_{i_{22}}}& \frac{\partial E}{\partial out_{i_{23}}}& \frac{\partial E}{\partial out_{i_{24}}} \\      \frac{\partial E}{\partial out_{i_{31}}}& \frac{\partial E}{\partial out_{i_{32}}}& \frac{\partial E}{\partial out_{i_{33}}}& \frac{\partial E}{\partial out_{i_{34}}} \\      \frac{\partial E}{\partial out_{i_{41}}}& \frac{\partial E}{\partial out_{i_{42}}}& \frac{\partial E}{\partial out_{i_{43}}}& \frac{\partial E}{\partial out_{i_{44}}} \\ \end{array} \right]$$

    - 可见：
      - 对卷积核进行 180°翻转，然后与这一层的误差敏感项矩阵 ${\delta_{c_{i,j}}}$ 周围补零后的矩阵做卷积运算后，就可以得到 ${\frac{\partial E}{\partial out_{i_{11}}}}$
      - 即
        $$
        \frac{\partial E}{\partial out_{i_{i,j}}} = \sum_m \cdot \sum_n f_{m,n}\delta_{c_{i+m,j+n}}
        $$

  - 求第二项 $\frac{\partial out_{i_{11}}}{\partial in_{i_{11}}}$
    - 由：$out_{i_{11}} = activators(in_{i_{11}})$
    - 则：$\frac{\partial out_{i_{11}}}{\partial in_{i_{11}}} =f'(in_{i_{11}})$
  - 则，$i_{11}$ 的误差项 $\delta_{i_{11}}$ 为：
    $$
    \begin{aligned}
    \delta_{i_{11}} &= \frac{\partial E}{\partial in_{i_{11}}} 
    \\ &=\frac{\partial E}{\partial out_{i_{11}}} \cdot \frac{\partial out_{i_{11}}}{\partial in_{i_{11}}} 
    \\ &=\sum_m \cdot \sum_n f_{m,n}\delta_{i+m,j+n} \cdot f'(in_{i_{11}}) 
    \end{aligned}
    $$
  - 其余误差项类似求得。
- 求权重的梯度：
  - 权重的梯度为：
    $$\begin{aligned}
    \frac{\partial E}{\partial f_{11}}&=\frac{\partial E}{\partial in_{c_{11}}} \cdot \frac{\partial in_{c_{11}}}{\partial f_{11}}+...+\frac{\partial E}{\partial in_{c_{33}}} \cdot \frac{\partial in_{c_{33}}}{\partial f_{11}}
    \\ &=\delta_{11} \cdot f_{11} +...+ \delta_{33} \cdot f_{11} \end{aligned}$$
  - 即：
    $$
    \begin{aligned}
    \frac{\partial E}{\partial f_{i,j}} = \sum_m\sum_n\delta_{m,n}out_{c_{i+m,j+n}}
    \end{aligned}
    $$
  - 偏置项的梯度为：
    $$\begin{aligned}
    \frac{\partial E}{\partial b} &=\frac{\partial E}{\partial in_{c_{11}}} \frac{\partial in_{c_{11}}}{\partial w_b} +\frac{\partial E}{\partial in_{c_{12}}} \frac{\partial in_{c_{12}}}{\partial w_b}
    \\ &+\frac{\partial E}{\partial in_{c_{21}}} \frac{\partial in_{c_{21}}}{\partial w_b} +\frac{\partial E}{\partial in_{c_{22}}} \frac{\partial in_{c_{22}}}{\partial w_b}
    \\ &=\delta_{11}+\delta_{12}+\delta_{21}+\delta_{22}
    \\ &=\sum_i\sum_j\delta_{i,j} 
    \end{aligned}
    $$
  - 可见：
    - 偏置项的偏导等于这一层所有误差敏感项之和。
  - 得到了权重和偏置项的梯度后，就可以根据梯度下降法更新权重和梯度了。

　　　　
池化层的反向传播：

- 下图：

<p align="center">
    <img width="60%" height="70%" src="http://images.iterate.site/blog/image/20190919/exgNmvXQQVP9.png?imageslim">
</p>

- 说明：
  - 左边是上卷积层输出的 feature_map，右边是池化层的输入。
- 假设，对于左上角的滑动窗口，最大值是 $out_{c_{11}}$，则：
  - $in_{m_{11}} = max(out_{c_{11}},out_{c_{12}},out_{c_{21}},out_{c_{22}})$
  - 此时：
    - $\frac{\partial in_{m_{11}}}{\partial out_{c_{11}}} = 1$
    - $\frac{\partial in_{m_{11}}}{\partial out_{c_{12}}}=\frac{\partial in_{m_{11}}}{\partial out_{c_{21}}}=\frac{\partial in_{m_{11}}}{\partial out_{c_{22}}} = 0$
  - 所以：
    - $\delta_{c_{11}} = \frac{\partial E}{\partial out_{c_{11}}} = \frac{\partial E}{\partial in_{m_{11}}} \cdot \frac{\partial in_{m_{11}}}{\partial out_{c_{11}}} =\frac{\partial E}{\partial in_{m_{11}}}$（为什么要求 $\delta_{c_{11}}$？这个的意义是什么？）
    - $\delta_{c_{12}} = \delta_{c_{21}} =\delta_{c_{22}} = 0$
- 这样就求出了池化层的误差敏感项矩阵。
- 同理可以求出每个神经元的梯度并更新权重。




## 不同的卷积

- 标准卷积
  - 计算时，对卷积核进行 180 的旋转，同时卷积核中心与需计算的图像像素对齐，输出结构为中心对齐像素的一个新的像素值。（为什么需要旋转？之前好像没有注意）
  - 举例：
    - 如图:

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/tN8XXkmlfdWR.png?imageslim">
    </p>

    - 旋转：

    <p align="center">
        <img width="25%" height="70%" src="http://images.iterate.site/blog/image/20200624/kv3BUf4XVcrn.png?imageslim">
    </p>

    - 计算：

    $$\begin{aligned}
    y[0,0] &=x[-1,-1] \cdot h[1,1]+x[0,-1] \cdot h[0,1]+x[1,-1] \cdot h[-1,1] \\
    &+x[-1,0] \cdot h[1,0]+x[0,0] \cdot h[0,0]+x[1,0] \cdot h[-1,0] \\
    &+x[-1,1] \cdot h[1,-1]+x[0,1] \cdot h[0,-1]+x[1,1] \cdot h[-1,-1] \\
    &=0 \cdot 1+0 \cdot 2+0 \cdot 1+0 \cdot 0+1 \cdot 0+2 \cdot 0+0 \cdot(-1)+4 \cdot(-2)+5 \cdot(-1) \\
    &=-13
    \end{aligned}$$


- 反卷积 Deconvolution （是转置卷积 Transpose Convolution 吗？）
  - 示意图：

  <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/NABlYSP0FvTI.png?imageslim">
  </p>

  - 说明：
    - 输入：2x2， 卷积核：4x4， 滑动步长：3， 输出：7x7
  - 过程：
    - 输入图片每个图像进行一次全卷积，根据全卷积大小计算可以知道每个图像的卷积后大小为 1+4-1=4，即 4x4 大小的特征图。（为什么不是 5 ？）
      - 输入有 4 个图像所以 4 个 4x4 的特征图。
    - 将 4 个特征图进行步长为 3 的相加；输出的位置和输入的位置相同。
      - 步长为 3 是指每隔 3 个像素进行相加，重叠部分进行相加，即输出的第 1 行第 4 列是由红色特阵图的第一行第四列与绿色特征图的第一行第一列相加得到，其他如此类推。
  - 反卷积后的尺寸大小：
    - 由卷积核大小与滑动步长决定：
    - $out = (in - 1) * s + k$
    - 其中:
      - in 是输入大小， 
      - k 是卷积核大小， 
      - s 是滑动步长， 
      - out 是输出大小
    - 上图过程就是：
      - $(2 - 1) * 3 + 4 = 7$
  - 应用：
    - （补充）



- 空洞卷积 Dilated Convolutions
  - 缘由：
    - 以前的 CNN 主要问题总结：
      - Up-sampling /池化层
      - 内部数据结构丢失；空间层级化信息丢失。
      - 小物体信息无法重建
        - 假设有四个池化层则任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。
    - 举例：
      - 在图像分割领域：
        - 图像输入到 CNN，如 FCN 中，FCN先像传统的 CNN 那样对图像做卷积再池化，降低图像尺寸的同时增大感受野，但是由于图像分割预测是 pixel-wise 的输出，所以要将池化后较小的图像尺寸上采样 到原始的图像尺寸进行预测，
        - 上采样一般采用 deconv 反卷积操作
        - 之前的池化操作使得每个要预测的 pixel 都能看到较大感受野信息。
        - 因此图像分割 FCN 中有两个关键，一个是池化减小图像尺寸增大感受野，另一个是上采样 扩大图像尺寸。
        - 而在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么，那么能不能设计一种新的操作，不通过池化也能有较大的感受野看到更多的信息呢？答案就是 dilated conv。
  - 作用：
    - 在不做池化的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。避免了信息损失。
  - 应用：
    - 在图像需要全局信息或者语音文本需要较长的 sequence 信息依赖的问题中，都能很好的应用 dilated conv，
      - 比如图像分割、语音合成 WaveNet、机器翻译 ByteNet 中。
  - 说明：
    - 如图：
      <p align="center">
          <img width="60%" height="70%" src="http://images.iterate.site/blog/image/20190722/1LxePT8Y9ktc.png?imageslim">
      </p>
    - 说明：
      - Dilated Convolution，卷积核为 $3 \times  3$，dilation rate $2$
    - 下图:

      <p align="center">
          <img width="90%" height="70%" src="http://images.iterate.site/blog/image/20190722/az2P7sNcUg10.jpg?imageslim">
      </p>

    - 说明：
      - a. 对应 $3\times 3$ 的 1-dilated conv，和普通的卷积操作一样。
      - b. 对应 $3\times 3$ 的 2-dilated conv，实际的卷积 kernel size还是 $3\times 3$，但是空洞为 $1$，
        - 也就是对于一个 $7\times 7$ 的图像 patch，只有 $9$ 个红色的点和 $3\times 3$ 的 kernel 发生卷积操作，其余的点略过。也可以理解为 kernel 的 size 为 $7\times 7$，但是只有图中的 $9$ 个点的权重不为 $0$，其余都为 $0$。 
        - 可以看到虽然 kernel size 只有 $3\times 3$，但是这个卷积的感受野已经增大到了 $7\times 7$（如果考虑到这个 2-dilated conv 的前一层是一个 1-dilated conv的话，那么每个红点就是 1-dilated 的卷积输出，所以感受野为 $3\times 3$，所以 1-dilated和 2-dilated 合起来就能达到 $7\times 7$ 的 conv。）。
      - c. 是 4-dilated conv 操作，同理跟在两个 1-dilated和 2-dilated conv 的后面，能达到 $15\times 15$ 的感受野。
        - 对比传统的 conv 操作，$3$ 层 $3\times 3$ 的卷积加起来，stride 为 $1$ 的话，只能达到 $(kernel-1) * layer+1=7$ 的感受野，也就是和层数 layer 成线性关系，而 dilated conv的感受野是指数级的增长。

