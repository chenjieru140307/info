


卷积神经网络应用于文本分类任务：

- 卷积神经网络的核心思想是捕捉局部特征。
- 对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于 N-gram。
  - 卷积神经网络的优势在于能够自动地对 N-gram 特征进行组合和筛选，获得不同抽象层次的语义信息。
  - 由于在每次卷积中采用了共享权重的机制，因此它的训练速度相对较快，在实际的文本分类任务中取得了非常不错的效果。
- 举例：
  - 用卷积神经网络模型进行文本表示，并最终用于文本分类的网络结构：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190414/kY6AFuI8C6u7.png?imageslim">
    </p>

  - 说明：
    - 输入层是一个 $N×K$ 的矩阵
      - 其中 $N$ 为文章所对应的单词总数，$K$ 是每个词对应的表示向量的维度。
      - 每个词的 $K$ 维向量可以是预先在其他语料库中训练好的，也可以作为未知的参数由网络训练得到。
        - 这两种方法各有优势，
          - 一方面，预先训练的词嵌入可以利用其他语料库得到更多的先验知识；
          - 另一方面，由当前网络训练的词向量能够更好地抓住与当前任务相关联的特征。
        - 因此，图中的输入层实际采用了两个通道的形式，即有两个 $N×K$ 的输入矩阵，其中一个用预先训练好的词嵌入表达，并且在训练过程中不再发生变化；另外一个也由同样的方式初始化，但是会作为参数，随着网络的训练过程发生改变。
    - 第二层为卷积层。
      - 在输入的 $N×K$ 维矩阵上，我们定义不同大小的滑动窗口进行卷积操作：$c_{i}=f\left(w \cdot x_{i:i+h-1}+b\right)$。
        - 其中：
          - $x_{i:i+h-1}$ 代表由输入矩阵的第 $i$ 行到第 $i+h−1$ 行所组成的一个大小为 $h×K$ 的滑动窗口， 
          - $w$ 为 $K×h$ 维的权重矩阵，
          - $b$ 为偏置参数。
      - 假设 $h$ 为 $3$，则每次在 $2×K$ 的滑动窗口上进行卷积，并得到 $N−2$ 个结果，再将这 $N−2$ 个结果拼接起来得到 $N−2$ 维的特征向量。
      - 每一次卷积操作相当于一次特征向量的提取，通过定义不同的滑动窗口，就可以提取出不同的特征向量，构成卷基层的输出。<span style="color:red;">嗯，不同的滑动窗口提取到的特征向量是怎么拼接的？</span>
    - 第三层为池化层
      - 图中所示的网络采用了 1-Max 池化，即为从每个滑动窗口产生的特征向量中筛选出一个最大的特征，然后将这些特征拼接起来构成向量表示。
      - 也可以选用 K-Max 池化（选出每个特征向量中最大的 K 个特征），或者平均池化（将特征向量中的每一维取平均）等，
      - **达到的效果都是将不同长度的句子通过池化得到一个定长的向量表示**。
    - 得到文本的向量表示之后，后面的网络结构就和具体的任务相关了。本例中展示的是一个文本分类的场景，因此最后接入了一个全连接层，并使用 Softmax 激活函数输出每个类别的概率。

