


假设我们现在有一个CNN，测试后发现它的表现比我们预想的差很多。

可以按如下诊断：

- 故障排除前的准备


![img](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC96DwSbWcbYNjNcF8rrSMoPSnwSPlTBeEsltreywUb1JJzpO25QnUXuEgyeUGAgsjFzuICmYxEWw/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

# 损失没有改善怎么办？

如果，训练了好几个Epoch，损失还是没有变小，甚至还越来越大，就要：

**1.****确认你用的损失函数是合适的，你优化的张量也是对的**。常用损失函数列表传送门：t.cn/RkZXji1。

**2.****用个好点的优化器**。这里也有常见优化器的列表：t.cn/RDKwbNA。

**3.****确认变量真的在训练**。要检查这个，就得看**张量板**的直方图。

或者写个脚本，在几个不同的训练实例 (training instances) 中，算出每个张量的**范数**。

如果变量**没在训练**，请看下节，“变量没在训练怎么办？”。

**4.****调整初始学习率**，实施适当的学习率计划。

如果损失**越来越大**，可能是初始学习率**太大**；如果损失**几乎不变**，可能是初始学习率**太小**。

![img](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC96DwSbWcbYNjNcF8rrSMo3ia0uZMlKOvQV0KDy8zd0lmgmTzTeyM7SS5VBtRhAAhZrfGJcwkTV3w/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

不管怎样，一旦找到好用的初始学习率，就要进行**学习率衰减**计划。

像AMA这样的优化器，**内部**就有学习率衰减机制。但它可能衰减得不够激烈，还是自己做一个比较好。

**5****.确认没有过拟合**。做个**学习率 vs 训练步数**的曲线，如果像是**抛物线**，可能就过拟合了。解决方法参见下文“过拟合怎么办？”章节

# 变量没在训练怎么办？

像上文说的，看了**张量板**直方图，或者写了脚本来算每个张量的**范数**之后，把**范数是常数**的那些张量拎出来。

![img](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC96DwSbWcbYNjNcF8rrSMoEIhvFChprG9jFiam0icibaicuA0pvNUvxBUkEWanxKdDJLL1h1PrYx8NsA/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

如果有个变量不训练：

**1.确认TF把它看做可训练 (trainable) 的变量**。详情可以查看**TF GraphKeys**：t.cn/R18Do6Y。

**2.****确认没发生梯度消失**。

如果**下游变量** (更靠近output的变量) 训练**正常**，而**上游变量**不训练，大概就是梯度消失了。

解决方案见下文，“梯度消失/梯度爆炸”章节。

**3.****确认ReLU (线性整流函数) 还在放电**。

如果**大部分**神经元电压都保持在**零**了，可能就要改变一下**权重初始化**策略了：尝试用一个**不那么激烈**的学习率衰减，并且减少**权重衰减正则化**。

# 梯度消失/梯度爆炸

**1.****考虑用个好点的权重初始化策略**。尤其是在，训练之初梯度就不怎么更新的情况下，这一步尤为重要。

**2.****考虑换一下激活函数**。比如**ReLU**，就可以拿**Leaky ReLU**或者**MaxOut**激活函数来代替。

**3.****如果是RNN (递归神经网络) 的话，就可以用LSTM block**。详情可参照此文：t.cn/RI6Qe7t

![img](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC96DwSbWcbYNjNcF8rrSMoFwMgLy4o2rfmibTZ8WLbI3icN6LaX7HLoXBbJfk7KgTywzGRAMO03ooA/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

# 过拟合怎么办？

过拟合，就是神经网络**记住**训练数据了。如果网络在**训练集**和**验证集**上，准确度差别很大，可能它就过拟合了。详情可见 (*Train/Val accuracy*) ：t.cn/RAkUzJP。

**1.**做个**数据扩增**。可上翻至本文第一节。

**2.**做个**Dropout**。在训练的每一步，都抛弃一些神经元。详情请见：t.cn/RkZodZo。

**3.****增加****正则化**。

**4.**做个**批量归一化**。详情请见：t.cn/RNunyfR。

**5.**做个**早停止 (early stopping)** 。因为，过拟合可能是训练了**太多Epoch**造成的。详情可见：t.cn/RkZKjEQ。

**6.**还不行的话，就用个**小一点的网络**吧。不过，没到万不得已，还是别这样。

![img](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC96DwSbWcbYNjNcF8rrSMoicxnThBbrAPuYqiaLuEubYMvjvuRJ752GINiaZMLrJaZ7Xp4bT5YJt4Kg/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

# 还能调些什么？

**1.****考虑用个带权重的损失函数**。

比如，在图像的**语义分割**中，神经网络要给每一个像素归类。其中一些类别，可能很少有像素属于它。

如果，给这些**不常被光顾**的类别，加个权重，mean_iou这项指标就会好一些。

**2.**改变**网络架构**。之前的网络，可能太深，可能太浅了。

**3.**考虑**把几个模型集成起来用**。

**4.**用**跨步卷积 (strided convolution)** 来代替最大池化/平均池化。

**5.**做个完整的**超参数搜索**。

**6.**改变**随机种子 (random seeds)** 。

**7.**上面的步骤全都不管用的话，还是再去**多找点数据**吧。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC96DwSbWcbYNjNcF8rrSMoricL2HrNBISBukQ1rgXiaLpGmRnZGgC8UDibz0xvF5Jt8AF10v25eQX5w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

最后，祝大家的CNN都能吃喝不愁，健康成长。

原文链接：

https://gist.github.com/zeyademam/0f60821a0d36ea44eef496633b4430fc#before-troubleshooting




# 相关

- [CNN手把手维修攻略：你的网络不好好训练，需要全面体检](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247502664&idx=4&sn=90216832c1995df9347a0eddc4a4312c&chksm=e8d07c3adfa7f52c6fb7ad79150e9eca89b06098a5dd9fbf3ba466435591060ea8f13e115ef6&mpshare=1&scene=1&srcid=0817jgx6G53uK6rnlnZrt5nV#rd)
