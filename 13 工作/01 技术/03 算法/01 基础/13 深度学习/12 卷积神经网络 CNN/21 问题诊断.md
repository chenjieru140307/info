


假设我们现在有一个CNN，测试后发现它的表现比我们预想的差很多。

可以按如下诊断：

- 损失没有改善怎么办？
  - 如果，训练了好几个Epoch，损失还是没有变小，甚至还越来越大，就要：
    - 1.确认你用的损失函数是合适的，你优化的张量也是对的
    - 2.用个好点的优化器。
    - 3.确认变量真的在训练。要检查这个，就得看张量板的直方图。
      - 或者写个脚本，在几个不同的训练实例 (training instances) 中，算出每个张量的范数。
      - 如果变量没在训练，请看下节，“变量没在训练怎么办？”。
    - 4.调整初始学习率，实施适当的学习率计划。
      - 如果损失越来越大，可能是初始学习率太大；如果损失几乎不变，可能是初始学习率太小。
      - 不管怎样，一旦找到好用的初始学习率，就要进行学习率衰减计划。
      - 像AMA这样的优化器，内部就有学习率衰减机制。但它可能衰减得不够激烈，还是自己做一个比较好。
    - 5.确认没有过拟合。做个学习率 vs 训练步数的曲线，如果像是抛物线，可能就过拟合了。
- 变量没在训练怎么办？
  - 像上文说的，看了张量板直方图，或者写了脚本来算每个张量的范数之后，把范数是常数的那些张量拎出来。
  - 如果有个变量不训练：
    - 1.确认TF把它看做可训练 (trainable) 的变量。
    - 2.确认没发生梯度消失。
      - 如果下游变量 (更靠近output的变量) 训练正常，而上游变量不训练，大概就是梯度消失了。
    - 3.确认ReLU (线性整流函数) 还在放电。
      - 如果大部分神经元电压都保持在零了，可能就要改变一下权重初始化策略了：尝试用一个不那么激烈的学习率衰减，并且减少权重衰减正则化。
- 梯度消失/梯度爆炸
  - 1.考虑用个好点的权重初始化策略。尤其是在，训练之初梯度就不怎么更新的情况下，这一步尤为重要。
  - 2.考虑换一下激活函数。比如ReLU，就可以拿Leaky ReLU或者MaxOut激活函数来代替。
  - 3.如果是RNN (递归神经网络) 的话，就可以用LSTM block。

- 过拟合怎么办？
  - 过拟合，就是神经网络记住训练数据了。如果网络在训练集和验证集上，准确度差别很大，可能它就过拟合了。
    - 1.做个数据扩增。可上翻至本文第一节。
    - 2.做个Dropout。在训练的每一步，都抛弃一些神经元。
    - 3.增加正则化。
    - 4.做个批量归一化。
    - 5.做个early stopping) 。因为，过拟合可能是训练了太多Epoch造成的。
    - 6.还不行的话，就用个小一点的网络吧。不过，没到万不得已，还是别这样。

- 还能调些什么？
  - 1.考虑用个带权重的损失函数。
    - 比如，在图像的语义分割中，神经网络要给每一个像素归类。其中一些类别，可能很少有像素属于它。
    - 如果，给这些不常被光顾的类别，加个权重，mean_iou这项指标就会好一些。
  - 2.改变网络架构。之前的网络，可能太深，可能太浅了。
  - 3.考虑把几个模型集成起来用。
  - 4.用跨步卷积 (strided convolution) 来代替最大池化/平均池化。
  - 5.做个完整的超参数搜索。
  - 6.改变随机种子 (random seeds) 。
  - 7.上面的步骤全都不管用的话，还是再去多找点数据吧。


