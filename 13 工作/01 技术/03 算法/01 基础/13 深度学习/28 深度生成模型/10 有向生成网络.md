
# 有向生成网络


如\chap?所讨论的，有向图模型构成了一类突出的图模型。虽然有向图模型在更大的机器学习社群中非常流行，但在较小的深度学习社群中，大约直到 2013 年它们都掩盖在无向模型（如 RBM）的光彩之下。

在本节中，我们回顾一些传统上与深度学习社群相关的标准有向图模型。

我们已经描述过部分有向的模型——深度信念网络。我们还描述过可以被认为是浅度有向生成模型的稀疏编码模型。尽管在样本生成和密度估计方面表现不佳，在深度学习的背景下它们通常被用作特征学习器。我们接下来描述多种深度完全有向的模型。




## sigmoid信念网络


sigmoid信念网络 是一种具有特定条件概率分布的有向图模型的简单形式。一般来说，我们可以将 sigmoid信念网络视为具有二值向量的状态 $\boldsymbol s$，其中状态的每个元素都受其祖先影响：


$$
\begin{aligned}
p(s_i) = \sigma \Bigg( \sum_{j<i} W_{j,i} s_j + b_i \Bigg).
\end{aligned}
$$

sigmoid信念网络最常见的结构是被分为许多层的结构，其中原始采样通过一系列多个隐藏层进行，然后最终生成可见层。这种结构与深度信念网络非常相似，但它们在采样过程开始时的单元彼此独立，而不是从受限玻尔兹曼机采样。这种结构由于各种原因而令人感兴趣。一个原因是该结构是可见单元上概率分布的通用近似，即在足够深的情况下，可以任意良好地近似二值变量的任何概率分布（即使各个层的宽度受限于可见层的维度 ）。


虽然生成可见单元的样本在 sigmoid信念网络中是非常高效的，但是其他大多数操作不是很高效。给定可见单元，对隐藏单元的推断是难解的。因为变分下界涉及对包含整个层的团求期望，均匀场推断也是难以处理的。这个问题一直困难到足以限制有向离散网络的普及。


在 sigmoid信念网络中执行推断的一种方法是构造专用于 sigmoid信念网络的不同下界 。这种方法只适用于非常小的网络。另一种方法是使用学成推断机制，如 节 学成近似推断 中描述的。Helmholtz机  结合了一个 sigmoid信念网络与一个预测隐藏单元上均匀场分布参数的推断网络。sigmoid信念网络的现代方法 仍然使用这种推断网络的方法。因为潜变量的离散本质，这些技术仍然是困难的。人们不能简单地通过推断网络的输出反向传播，而必须使用相对不可靠的机制即通过离散采样过程进行反向传播（如 节 通过离散随机操作的反向传播 所述）。最近基于重要采样、重加权的醒眠 或双向~Helmholtz机~ 的方法使得我们可以快速训练 sigmoid信念网络，并在基准任务上达到最好的表现。

sigmoid信念网络的一种特殊情况是没有潜变量的情况。在这种情况下学习是高效的，因为没有必要将潜变量边缘化到似然之外。一系列称为自回归网络的模型将这个完全可见的信念网络泛化到其他类型的变量（除二值变量）和其他结构（除对数线性关系）的条件分布。
自回归网络将在 节 自回归网络 中描述。



## 可微生成器网络


许多生成模型基于使用可微生成器网络的想法。这种模型使用可微函数 $g(\boldsymbol z;\boldsymbol \theta^{(g)})$ 将潜变量 $\mathbf z$ 的样本变换为样本 $\mathbf x$ 或样本 $\mathbf x$ 上的分布，可微函数通常可以由神经网络表示。这类模型包括将生成器网络与推断网络配对的变分自编码器、将生成器网络与判别器网络配对的生成式对抗网络，以及孤立地训练生成器网络的技术。


生成器网络本质上仅是用于生成样本的参数化计算过程，其中的体系结构提供了从中采样的可能分布族以及选择这些族内分布的参数。

作为示例，从具有均值 $\boldsymbol mu$ 和协方差 $\boldsymbol Sigma$ 的正态分布绘制样本的标准过程是将来自零均值和单位协方差的正态分布的样本 $\boldsymbol z$ 馈送到非常简单的生成器网络中。 这个生成器网络只包含一个仿射层：


$$\begin{aligned}
 \boldsymbol x = g(\boldsymbol z) = \boldsymbol mu + \boldsymbol L \boldsymbol z ,
\end{aligned}$$

其中 $\boldsymbol L$ 由 $\boldsymbol Sigma$ 的~Cholesky~分解给出。


伪随机数发生器也可以使用简单分布的非线性变换。例如，**逆变换采样**(inverse transform sampling)从 $U(0,1)$ 中采一个标量 $z$，并且对标量 $x$ 应用非线性变换。在这种情况下，$g(z)$ 由累积分布函数 $F(x) = \int_{-\infty}^{x} p(v) dv$ 的反函数给出。如果我们能够指定 $p(x)$，在 $x$ 上积分，并取所得函数的




伪随机数发生器也可以使用简单分布的非线性变换。例如，逆变换采样(inverse transform sampling)(Devroye,2013)从 U(0,1)中采一个标量 z，并且对标量 x 应用非线性变换。在这种情况下，g(z)由累积分布函数的反函数给出。如果我们能够指定 p(x)，在 x 上积分，并取所得函数的反函数，我们不用通过机器学习就能从 p(x)进行采样。
为了从更复杂的分布(难以直接指定、难以积分或难以求所得积分的反函数)中生成样本，我们使用前馈网络来表示非线性函数 g 的参数族，并使用训练数据来推断参数以选择所期望的函数。
我们可以认为 g 提供了变量的非线性变化，将 z 上的分布变换成 x 上想要的分布。
回顾式(3.47)，对于可求反函数的、可微的、连续的 g，
这隐含地对 x 施加概率分布：
当然，取决于 g 的选择，这个公式可能难以评估，因此我们经常需要使用间接学习 g 的方法，而不是直接尝试最大化 logp(x)。
在某些情况下，我们使用 g 来定义 x 上的条件分布，而不是使用 g 直接提供 x 的样本。例如，我们可以使用一个生成器网络，其最后一层由 sigmoid 输出组成，可以提供 Bernoulli 分布的平均参数：
在这种情况下，我们使用 g 来定义 p(x|z)时，通过边缘化 z 来对 x 施加分布：
两种方法都定义了一个分布 pg(x)，并允许我们使用第 20.9节中的重参数化技巧来训练 pg 的各种评估准则。
表示生成器网络的两种不同方法(发出条件分布的参数相对直接发射样品)具有互补的优缺点。当生成器网络在 x 上定义条件分布时，它不但能生成连续数据，也能生成离散数据。当生成器网络直接提供采样时，它只能产生连续的数据(我们可以在前向传播中引入离散化，但这样做意味着模型不再能够使用反向传播进行训练)。直接采样的优点是，我们不再被迫使用条件分布(可以容易地写出来并由人类设计者进行代数操作的形式)。
基于可微生成器网络的方法是由分类可微前馈网络中梯度下降的成功应用而推动的。在监督学习的背景中，基于梯度训练学习的深度前馈网络在给定足够的隐藏单元和足够的训练数据的情况下，在实践中似乎能保证成功。这个同样的方案能成功转移到生成式建模上吗？
生成式建模似乎比分类或回归更困难，因为学习过程需要优化难以处理的准则。在可微生成器网络的情况中，准则是难以处理的，因为数据不指定生成器网络的输入 z 和输出 x。在监督学习的情况下，输入 x 和输出 y 同时给出，并且优化过程只需学习如何产生指定的映射。在生成建模的情况下，学习过程需要确定如何以有用的方式排布 z 空间，以及额外的如何从 z 映射到 x。
Dosovitskiy et al.(2015)研究了一个简化问题，其中 z 和 x 之间的对应关系已经给出。具体来说，训练数据是计算机渲染的椅子图。潜变量 z 是渲染引擎的参数，描述了椅子模型的选择、椅子的位置以及影响图像渲染的其他配置细节。使用这种合成的生成数据，卷积网络能够学习将图像内容的描述 z 映射到渲染图像的近似 x。这表明当现代可微生成器网络具有足够的模型容量时，足以成为良好的生成模型，并且现代优化算法具有拟合它们的能力。困难在于当每个 x 的 z 的值不是固定的且在每次训练前是未知时，如何训练生成器网络。
在接下来的章节中，我们讨论仅给出 x 的训练样本，训练可微生成器网络的几种方法。
20.10.3　变分自编码器
变分自编码器(variational auto-encoder,VAE)(Kingma,2013; Rezende et al.,2014)是一个使用学好的近似推断的有向模型，可以纯粹地使用基于梯度的方法进行训练。
为了从模型生成样本，VAE首先从编码分布 pmodel(z)中采样 z。然后使样本通过可微生成器网络 g(z)。最后，从分布 pmodel(x;g(z))=pmodel(x|z)中采样 x。然而在训练期间，近似推断网络(或编码器)q(z|x)用于获得 z，而 pmodel(x|z)则被视为解码器网络。



变分自编码器背后的关键思想是，它们可以通过最大化与数据点 x 相关联的变分下界 L(q)来训练：
在式(20.76)中，我们将第一项视为潜变量的近似后验下可见和隐藏变量的联合对数似然性(正如 EM 一样，不同的是我们使用近似而不是精确后验)。第二项则可视为近似后验的熵。当 q 被选择为高斯分布，其中噪声被添加到预测平均值时，最大化该熵项促使该噪声标准偏差的增加。更一般地，这个熵项鼓励变分后验将高概率质量置于可能已经产生 x 的许多 z 值上，而不是坍缩到单个估计最可能值的点。在式(20.77)中，我们将第一项视为在其他自编码器中出现的重构对数似然，第二项试图使近似后验分布 q(z|x)和模型先验 pmodel(z)彼此接近。
变分推断和学习的传统方法是通过优化算法推断 q，通常是迭代不动点方程(第 19.4节)。这些方法是缓慢的，并且通常需要以闭解形式计算 Ez～qlogpmodel(z,x)。变分自编码器背后的主要思想是训练产生 q 参数的参数编码器(有时也称为推断网络或识别模型)。只要 z 是连续变量，我们就可以通过从 q(z|x)=q(z;f(x;θ))中采样 z 的样本反向传播，以获得相对于θ的梯度。学习则仅包括相对于编码器和解码器的参数最大化 L。L中的所有期望都可以通过蒙特卡罗采样来近似。
变分自编码器方法是优雅的，理论上令人愉快的，并且易于实现。它也获得了出色的结果，是生成式建模中的最先进方法之一。它的主要缺点是从在图像上训练的变分自编码器中采样的样本往往有些模糊。这种现象的原因尚不清楚。一种可能性是，模糊性是最大似然的固有效应，因为我们需要最小化 DKL(pdata||pmodel)。如图 3.6所示，这意味着模型将为训练集中出现的点分配高的概率，但也可能为其他点分配高的概率。还有其他原因可以导致模糊图像。模型选择将概率质量置于模糊图像而不是空间的其他部分的部分原因是，实际使用的变分自编码器通常在 pmodel(x;g(z))使用高斯分布。最大化这种分布似然性的下界与训练具有均方误差的传统自编码器类似，这意味着它倾向于忽略由少量像素表示的特征或其中亮度变化微小的像素。如 Theis et al.(2015)和 Huszar(2015)指出的，该问题不是 VAE 特有的，而是与优化对数似然或 DKL(pdata||pmodel)的生成模型共享的。现代 VAE 模型另一个麻烦的问题是，它们倾向于仅使用 z 维度中的小子集，就像编码器不能够将具有足够局部方向的输入空间变换到边缘分布与分解前匹配的空间。
VAE框架可以直接扩展到大范围的模型架构。相比玻尔兹曼机，这是关键的优势，因为玻尔兹曼机需要非常仔细地设计模型来保持易解性。VAE可以与广泛的可微算子族一起良好工作。一个特别复杂的 VAE 是深度循环注意写者(DRAW)模型(Gregor et al.,2015)。DRAW使用一个循环编码器和循环解码器并结合注意力机制。DRAW模型的生成过程包括顺序访问不同的小图像块并绘制这些点处的像素值。我们还可以通过在 VAE 框架内使用循环编码器和解码器定义变分 RNN(Chung et al.,2015b)来扩展 VAE 以生成序列。从传统 RNN 生成样本仅在输出空间涉及非确定性操作。而变分 RNN 还具有由 VAE 潜变量捕获的潜在更抽象层的随机变化性。
VAE框架已不仅仅扩展到传统的变分下界，还有重要加权自编码器(importance-weighted autoencoder)(Burda et al.,2015)的目标：
这个新的目标在 k=1时等同于传统的下界。然而，它也可以被解释为基于提议分布 q(z |x)中 z 的重要采样而形成的真实 log pmodel(x)估计。重要加权自编码器目标也是 log pmodel(x)的下界，并且随着 k 增加而变得更紧。
图 20.6　由变分自编码器学习的高维流形在二维坐标系中的示例(Kingma and Welling,2014a)。我们可以在纸上直接绘制两个可视化的维度，因此可以使用二维潜在编码训练模型来了解模型的工作原理(即使我们认为数据流形的固有维度要高得多)。图中所示的图像不是来自训练集的样本，而是仅仅通过改变二维“编码”z，由模型 p(x|z)实际生成的图像 x(每个图像对应于“编码”z位于二维均匀网格的不同选择)。(左)Frey人脸流形的二维映射。其中一个维度(水平)已发现大致对应于面部的旋转，而另一个(垂直)对应于情绪表达。(右)MNIST流形的二维映射
变分自编码器与 MP-DBM和其他涉及通过近似推断图的反向传播方法有一些有趣的联系(Goodfellow et al.,2013d;Stoyanov et al.,2011;Brakel et al.,2013)。这些以前的方法需要诸如均匀场不动点方程的推断过程来提供计算图。变分自编码器被定义为任意计算图，这使得它能适用于更广泛的概率模型族，因为它不需要将模型的选择限制到具有易处理的均匀场不动点方程的那些模型。变分自编码器还具有增加模型对数似然边界的优点，而 MP-DBM和相关模型的准则更具启发性，并且除了使近似推断的结果准确外很少有概率的解释。变分自编码器的一个缺点是它仅针对一个问题学习推断网络，即给定 x 推断 z。较老的方法能够在给定任何其他变量子集的情况下对任何变量子集执行近似推断，因为均匀场不动点方程指定如何在所有这些不同问题的计算图之间共享参数。
变分自编码器的一个非常好的特性是，同时训练参数编码器与生成器网络的组合迫使模型学习一个编码器可以捕获的可预测的坐标系。这使得它成为一个优秀的流形学习算法。图 20.6展示了由变分自编码器学到的低维流形的例子。图中所示的情况之一，算法发现了存在于面部图像中两个独立的变化因素：旋转角和情绪表达。
20.10.4　生成式对抗网络
生成式对抗网络(generative adversarial network,GAN)(Goodfellow et al.,2014c)是基于可微生成器网络的另一种生成式建模方法。
生成式对抗网络基于博弈论场景，其中生成器网络必须与对手竞争。生成器网络直接产生样本 x=g(z;θ(g))。其对手，判别器网络(discriminator network)试图区分从训练数据抽取的样本和从生成器抽取的样本。判别器发出由 d(x;θ(d))给出的概率值，指示 x 是真实训练样本而不是从模型抽取的伪造样本的概率。
形式化表示生成式对抗网络中学习的最简单方式是零和游戏，其中函数 v(θ(g),θ(d))确定判别器的收益。生成器接收-v(θ(g),θ(d))作为它自己的收益。在学习期间，每个玩家尝试最大化自己的收益，因此收敛在
v的默认选择是
这驱使判别器试图学习将样品正确地分类为真的或伪造的。同时，生成器试图欺骗分类器以让其相信样本是真实的。在收敛时，生成器的样本与实际数据不可区分，并且判别器处处都输出。然后就可以丢弃判别器。
设计 GAN 的主要动机是学习过程既不需要近似推断，也不需要配分函数梯度的近似。当 maxdv(g,d)在θ(g)中是凸的(例如，在概率密度函数的空间中直接执行优化的情况)时，该过程保证收敛并且是渐近一致的。
不幸的是，在实践中由神经网络表示的 g 和 d 以及 maxdv(g,d)不凸时，GAN中的学习可能是困难的。Goodfellow(2014)认为不收敛可能会引起 GAN 的欠拟合问题。一般来说，同时对两个玩家的成本梯度下降不能保证达到平衡。例如，考虑价值函数 v(a,b)=ab，其中一个玩家控制 a 并产生成本 ab，而另一玩家控制 b 并接收成本-ab。如果我们将每个玩家建模为无穷小的梯度步骤，每个玩家以另一个玩家为代价降低自己的成本，则 a 和 b 进入稳定的圆形轨迹，而不是到达原点处的平衡点。注意，极小极大化游戏的平衡不是 v 的局部最小值。相反，它们是同时最小化的两个玩家成本的点。这意味着它们是 v 的鞍点，相对于第一个玩家的参数是局部最小值，而相对于第二个玩家的参数是局部最大值。两个玩家可以永远轮流增加然后减少 v，而不是正好停在玩家没有能力降低其成本的鞍点。目前不知道这种不收敛的问题会在多大程度上影响 GAN。
Goodfellow(2014)确定了另一种替代的形式化收益公式，其中博弈不再是零和，每当判别器最优时，具有与最大似然学习相同的预期梯度。因为最大似然训练收敛，这种 GAN 博弈的重述在给定足够的样本时也应该收敛。不幸的是，这种替代的形式化似乎并没有提高实践中的收敛，可能是由于判别器的次优性或围绕期望梯度的高方差。
在真实实验中，GAN博弈的最佳表现形式既不是零和，也不等价于最大似然，而是 Good-fellow et al.(2014c)引入的带有启发式动机的不同形式化。在这种最佳性能的形式中，生成器旨在增加判别器发生错误的对数概率，而不是旨在降低判别器进行正确预测的对数概率。这种重述仅仅是观察的结果，即使在判别器确信拒绝所有生成器样本的情况下，它也能导致生成器代价函数的导数相对于判别器的对数保持很大。
稳定 GAN 学习仍然是一个开放的问题。幸运的是，当仔细选择模型架构和超参数时，GAN学习效果很好。Radford et al.(2015)设计了一个深度卷积 GAN(DCGAN)，在图像合成的任务上表现非常好，并表明其潜在的表示空间能捕获到变化的重要因素，如图 15.9所示。图 20.7展示了 DCGAN 生成器生成的图像示例。
图 20.7　在 LSUN 数据集上训练后，由 GAN 生成的图像。(左)由 DCGAN 模型生成的卧室图像，经 Radford et al.(2015)许可转载。(右)由 LAPGAN 模型生成的教堂图像，经 Denton et al.(2015)许可转载
GAN学习问题也可以通过将生成过程分成许多级别的细节来简化。我们可以训练有条件的 GAN(Mirza and Osindero,2014)，并学习从分布 p(x|y)中采样，而不是简单地从边缘分布 p(x)中采样。Denton et al.(2015)表明一系列的条件 GAN 可以被训练为首先生成非常低分辨率的图像，然后增量地向图像添加细节。由于使用拉普拉斯金字塔来生成包含不同细节水平的图像，这种技术被称为 LAPGAN 模型。LAPGAN生成器不仅能够欺骗判别器网络，而且能够欺骗人类观察者，实验主体将高达 40%的网络输出识别为真实数据。请看图 20.7中 LAPGAN 生成器生成的图像示例。
GAN训练过程中一个不寻常的能力是它可以拟合向训练点分配零概率的概率分布。生成器网络学习跟踪特定点在某种程度上类似于训练点的流形，而不是最大化该点的对数概率。有点矛盾的是，这意味着模型可以将负无穷大的对数似然分配给测试集，同时仍然表示人类观察者判断为能捕获生成任务本质的流形。这不是明显的优点或缺点，并且只要向生成器网络最后一层所有生成的值添加高斯噪声，就可以保证生成器网络向所有点分配非零概率。以这种方式添加高斯噪声的生成器网络从相同分布中采样，即，从使用生成器网络参数化条件高斯分布的均值所获得的分布中采样。
Dropout似乎在判别器网络中很重要。特别地，在计算生成器网络的梯度时，单元应当被随机地丢弃。使用权重除以二的确定性版本的判别器其梯度似乎不是那么有效。同样，从不使用 Dropout 似乎会产生不良的结果。
虽然 GAN框架被设计为用于可微生成器网络，但是类似的原理可以用于训练其他类型的模型。例如，自监督提升(self-supervised boosting)可以用于训练 RBM 生成器以欺骗逻辑回归判别器(Welling et al.,2002)。
20.10.5　生成矩匹配网络
生成矩匹配网络(generative moment matching network)(Li et al.,2015; Dziugaite et al.,2015)是另一种基于可微生成器网络的生成模型。与 VAE和 GAN不同，它们不需要将生成器网络与任何其他网络配对，例如不需要与用于 VAE 的推断网络配对，也不需要与 GAN 的判别器网络配对。
生成矩匹配网络使用称为矩匹配(moment matching)的技术训练。矩匹配背后的基本思想是以如下的方式训练生成器 —— 令模型生成的样本的许多统计量尽可能与训练集中的样本相似。在此情景下，矩(moment)是对随机变量不同幂的期望。例如，第一矩是均值，第二矩是平方值的均值，以此类推。多维情况下，随机向量的每个元素可以被升高到不同的幂，因此使得矩可以是任意数量的形式
其中是一个非负整数的向量。
在第一次检查时，这种方法似乎在计算上是不可行的。例如，如果我们想匹配形式为 xixj 的所有矩，那么我们需要最小化在 x 的维度上是二次的多个值之间的差。此外，甚至匹配所有第一和第二矩将仅足以拟合多变量高斯分布，其仅捕获值之间的线性关系。我们使用神经网络的野心是捕获复杂的非线性关系，这将需要更多的矩。GAN通过使用动态更新的判别器避免了穷举所有矩的问题，该判别器自动将其注意力集中在生成器网络最不匹配的统计量上。
相反，我们可以通过最小化一个被称为最大平均偏差(maximum mean discrepancy,MMD)(Sch¨olkopf and Smola,2002;Gretton et al.,2012)的代价函数来训练生成矩匹配网络。该代价函数通过向核函数定义的特征空间隐式映射，在无限维空间中测量第一矩的误差，使得对无限维向量的计算变得可行。当且仅当所比较的两个分布相等时，MMD代价为零。
从可视化方面看，来自生成矩匹配网络的样本有点令人失望。幸运的是，它们可以通过将生成器网络与自编码器组合来改进。首先，训练自编码器以重构训练集。接下来，自编码器的编码器用于将整个训练集转换到编码空间。然后训练生成器网络以生成编码样本，这些编码样本可以经解码器映射到视觉上令人满意的样本。
与 GAN 不同，代价函数仅关于一批同时来自训练集和生成器网络的实例定义。我们不可能将训练更新作为一个训练样本或仅来自生成器网络的一个样本的函数，这是因为必须将矩计算为许多样本的经验平均值。当批量大小太小时，MMD可能低估采样分布的真实变化量。有限的批量大小都不足以大到完全消除这个问题，但是更大的批量大小减少了低估的量。当批量大小太大时，训练过程就会慢得不可行，因为计算单个小梯度步长必须一下子处理许多样本。
与 GAN 一样，即使生成器网络为训练点分配零概率，也可以使用 MMD 训练生成器网络。
20.10.6　卷积生成网络
当生成图像时，将卷积结构引入生成器网络通常是有用的(见 Goodfellow et al.(2014c)或 Dosovitskiy et al.(2015)的例子)。为此，我们使用卷积算子的“转置”，如第 9.5节所述。这种方法通常能产生更逼真的图像，并且比不使用参数共享的全连接层使用更少的参数。
用于识别任务的卷积网络具有从图像到网络顶部的某些概括层(通常是类标签)的信息流。当该图像通过网络向上流动时，随着图像的表示变得对于有害变换保持不变，信息也被丢弃。在生成器网络中，情况恰恰相反。要生成图像的表示通过网络传播时必须添加丰富的详细信息，最后产生图像的最终表示，这个最终表示当然是带有所有细节的精细图像本身(具有对象位置、姿势、纹理以及明暗)。在卷积识别网络中丢弃信息的主要机制是池化层，而生成器网络似乎需要添加信息。由于大多数池化函数不可逆，我们不能将池化层求逆后放入生成器网络。更简单的操作是仅仅增加表示的空间大小。似乎可接受的方法是使用 Dosovitskiy　et al.(2015)引入的“去池化”。该层对应于某些简化条件下最大池化的逆操作。首先，最大池化操作的步幅被约束为等于池化区域的宽度。其次，每个池化区域内的最大输入被假定为左上角的输入。最后，假设每个池化区域内所有非最大的输入为零。这些是非常强和不现实的假设，但它们允许我们对最大池化算子求逆。去池化的逆操作分配一个零张量，然后将每个值从输入的空间坐标 i 复制到输出的空间坐标 i×k。整数值 k 定义池化区域的大小。即使驱动去池化算子定义的假设是不现实的，后续层也能够学习补偿其不寻常的输出，所以由整体模型生成的样本在视觉上令人满意。
20.10.7　自回归网络
自回归网络是没有潜在随机变量的有向概率模型。这些模型中的条件概率分布由神经网络表示(有时是极简单的神经网络，例如逻辑回归)。这些模型的图结构是完全图。它们可以通过概率的链式法则分解观察变量上的联合概率，从而获得形如 P(xd|xd-1,…,x1)条件概率的乘积。这样的模型被称为完全可见的贝叶斯网络(fully-visible Bayes networks,FVBN)，并成功地以许多形式使用——首先是对每个条件分布逻辑回归(Frey,1998)，然后是带有隐藏单元的神经网络(Bengio and Bengio,2000b; Larochelle and Murray,2011)。在某些形式的自回归网络中，例如在第 20.10.10中描述的 NADE(Larochelle and Murray,2011)，我们可以引入参数共享的一种形式，它能带来统计优点(较少的唯一参数)和计算优势(较少计算量)。这是深度学习中反复出现的主题——特征重用的另一个实例。
20.10.8　线性自回归网络
自回归网络的最简单形式是没有隐藏单元、没有参数或特征共享的形式。每个 P(xi|xi-1,…,x1)被参数化为线性模型(对于实值数据的线性回归，对于二值数据的逻辑回归，对于离散数据的 softmax 回归)。这个模型由 Frey(1998)引入，当有 d 个变量要建模时，该模型有(d2)个参数，如图 20.8所示。
如果变量是连续的，线性自回归网络只是表示多元高斯分布的另一种方式，只能捕获观察变量之间线性的成对相互作用。


图 20.8　完全可见的信念网络从前 i-1个变量预测第 i 个变量。(上)FVBN的有向图模型。(下)对数 FVBN 相应的计算图，其中每个预测由线性预测器作出
线性自回归网络本质上是线性分类方法在生成式建模上的推广。因此，它们具有与线性分类器相同的优缺点。像线性分类器一样，它们可以用凸损失函数训练，并且有时允许闭解形式(如在高斯情况下)。像线性分类器一样，模型本身不提供增加其容量的方法，因此必须使用其他技术(如输入的基扩展或核技巧)来提高容量。
20.10.9　神经自回归网络
神经自回归网络(Bengio and Bengio,2000a,b)具有与逻辑自回归网络相同的从左到右的图模型(见图 20.8)，但在该图模型结构内采用不同的条件分布参数。新的参数化更强大，它可以根据需要随意增加容量，并允许近似任意联合分布。新的参数化还可以引入深度学习中常见的参数共享和特征共享原理来改进泛化能力。设计这些模型的动机是避免传统表格图模型引起的维数灾难，并与图 20.8共享相同的结构。在表格离散概率模型中，每个条件分布由概率表表示，其中所涉及的变量的每个可能配置都具有一个条目和一个参数。通过使用神经网络，可以获得两个优点。
(1)通过具有(i-1)×k个输入和 k 个输出的神经网络(如果变量是离散的并有 k 个值，使用 one-hot编码)参数化每个 P(xi|xi-1,…,x1)，让我们不需要指数量级参数(和样本)的情况下就能估计条件概率，然而仍然能够捕获随机变量之间的高阶依赖性。
(2)不需要对预测每个 xi 使用不同的神经网络，如图 20.9所示的从左到右连接，允许将所有神经网络合并成一个。等价地，它意味着为预测 xi 所计算的隐藏层特征可以重新用于预测 xi+k(k>0)。因此隐藏单元被组织成第 i 组中的所有单元仅依赖于输入值 x1,…,xi的特定的组。用于计算这些隐藏单元的参数被联合优化以改进对序列中所有变量的预测。这是重用原理的一个实例，这是从循环和卷积网络架构到多任务和迁移学习的场景中反复出现的深度学习原理。
如在第 6.2.2.1节中讨论的，使神经网络的输出预测 xi 条件分布的参数，每个 P(xi|xi-1,…,x1)就可以表示一个条件分布。虽然原始神经自回归网络最初是在纯粹离散多变量数据(带有 sigmoid 输出的 Bernoulli 变量或 softmax 输出的 Multinoulli 变量)的背景下评估，但我们可以自然地将这样的模型扩展到连续变量或同时涉及离散和连续变量的联合分布。
图 20.9　神经自回归网络从前 i-1个变量预测第 i 个变量 xi，但经参数化后，作为 x1,…,xi函数的特征(表示为 hi 的隐藏单元的组)可以在预测所有后续变量 xi+1,xi+2,…,xd时重用
20.10.10　NADE
神经自回归密度估计器(neural auto-regressive density estimator,NADE)是最近非常成功的神经自回归网络的一种形式(Larochelle and Murray,2011)。与 Bengio and Bengio(2000b)的原始神经自回归网络中的连接相同，但 NADE 引入了附加的参数共享方案，如图 20.10所示。不同组 j 的隐藏单元的参数是共享的。
从第 i 个输入 xi 到第 j 组隐藏单元的第 k 个元素的权重是组内共享的：
其余 j<i的权重为 0。


神经自回归密度估计器(NADE)的示意图。隐藏单元被组织在组 h(j)中，使得只有输入 x1,…,xi参与计算 h(i)和预测 P(xj|xj-1,…,x1)(对于 j>i)。NADE使用特定的权重共享模式区别于早期的神经自回归网络：被共享于所有从 xi 到任何 j≥i组中第 k 个单元的权重(在图中使用相同的线型表示复制权重的每个实例)。注意向量(W1,i,W2,i,…,Wn,i)记为 W:,i
Larochelle and Murray(2011)选择了这种共享方案，使得 NADE 模型中的正向传播与在均匀场推断中执行的计算大致相似，以填充 RBM 中缺失的输入。这个均匀场推断对应于运行具有共享权重的循环网络，并且该推断的第一步与 NADE 中的相同。使用 NADE 的唯一区别是，连接隐藏单元到输出的输出权重独立于连接输入单元和隐藏单元的权重进行参数化。在 RBM 中，隐藏到输出的权重是输入到隐藏权重的转置。NADE架构可以扩展为不仅仅模拟均匀场循环推断的一个时间步，而是 k 步。这种方法称为 NADE-k(Raiko et al.,2014)。
如前所述，自回归网络可以被扩展成处理连续数据。用于参数化连续密度的特别强大和通用的方法是混合权重为αi(组 i 的系数或先验概率)，每组条件均值为μi和每组条件方差为的高斯混合体。一个称为 RNADE 的模型(Uria et al.,2013)使用这种参数化将 NADE 扩展到实值。与其他混合密度网络一样，该分布的参数是网络的输出，由 softmax 单元产生混合的权量概率以及参数化的方差，因此可使它们为正的。由于条件均值μi和条件方差之间的相互作用，随机梯度下降在数值上可能会表现不好。为了减少这种困难，Uria et al.(2013)在后向传播阶段使用伪梯度代替平均值上的梯度。
另一个非常有趣的神经自回归架构的扩展摆脱了为观察到的变量选择任意顺序的需要(Murray and Larochelle,2014)。在自回归网络中，该想法是训练网络能够通过随机采样顺序来处理任何顺序，并将信息提供给指定哪些输入被观察的隐藏单元(在条件条的右侧)，以及哪些是被预测并因此被认为是缺失的(在条件条的左侧)。这是不错的性质，因为它允许人们非常高效地使用训练好的自回归网络来执行任何推断问题(即从给定任何变量的子集，从任何子集上的概率分布预测或采样)。最后，由于变量的许多顺序是可能的(对于 n 个变量是 n!)，并且变量的每个顺序 o 产生不同的 p(x|o)，我们可以组成许多 o 值模型的集成：
这个集成模型通常能更好地泛化，并且为测试集分配比单个排序定义的单个模型更高的概率。
在同一篇文章中，作者提出了深度版本的架构，但不幸的是，这立即使计算成本像原始神经自回归网络一样高(Bengio and Bengio,2000b)。第一层和输出层仍然可以在(nh)的乘法-加法操作中计算，如在常规 NADE 中，其中 h 是隐藏单元的数量(图 20.10和图 20.9中的组 hi 的大小)，而它在 Bengio and Bengio(2000b)中是(n2h)。然而，对于其他隐藏层的计算量是(n2h2)(假设在每个层存在 n 组 h 个隐藏单元，且在 l 层的每个“先前”组参与预测 l+1层处的“下一个”组)。如在 Murray and Larochelle(2014)中，使 l+1层上的第 i 个组仅取决于第 i 个组，l层处的计算量将减少到(nh2)，但仍然比常规 NADE 差 h 倍。




# 相关

- 《深度学习》花书
