
# 对数似然梯度

通过最大似然学习无向模型特别困难的原因在于配分函数依赖于参数。对数似然相对于参数的梯度具有一项对应于配分函数的梯度：

$$
\nabla_{\boldsymbol{\theta}} \log p(\mathbf{x} ; \boldsymbol{\theta})=\nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x} ; \boldsymbol{\theta})-\nabla_{\boldsymbol{\theta}} \log Z(\boldsymbol{\theta})
$$

这是机器学习中非常著名的正相(positive phase)和负相(negative phase)的分解。

对于大多数感兴趣的无向模型而言，负相是困难的。没有潜变量或潜变量之间很少相互作用的模型通常会有一个易于计算的正相。RBM的隐藏单元在给定可见单元的情况下彼此条件独立，是一个典型的具有简单正相和困难负相的模型。

正相计算困难，潜变量之间具有复杂相互作用的情况将主要在近似推断中讨论。

本章主要探讨负相计算中的难点。

让我们进一步分析 $\log Z$ 的梯度：

$$
\begin{aligned} & \nabla_{\boldsymbol{\theta}} \log Z \\ &=\frac{\nabla_{\boldsymbol{\theta}} Z}{Z} \\=& \frac{\nabla_{\boldsymbol{\theta}} \sum_{\boldsymbol{x}} \tilde{p}(\mathbf{x})}{Z} \\=& \frac{\sum_{\mathbf{x}} \nabla_{\boldsymbol{\theta} \tilde{p}(\mathbf{x})}}{Z} \end{aligned}
$$

对于保证所有的 $\mathbf{x}$ 都有 $p(\mathbf{x})>0$ 的模型，我们可以用 $\exp (\log \tilde{p}(\mathbf{x}))$ 代替 $\tilde{p}(\mathbf{x})$：


$$
\begin{aligned} & \frac{\sum_{\mathbf{x}} \nabla_{\boldsymbol{\theta}} \exp (\log \tilde{p}(\mathbf{x}))}{Z} \\=& \frac{\sum_{\mathbf{x}} \exp (\log \tilde{p}(\mathbf{x})) \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x})}{Z} \\ &=\frac{\sum_{\mathbf{x}} \tilde{p}(\mathbf{x}) \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x})}{Z} \\ &=\sum_{\mathbf{x}} p(\mathbf{x}) \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x}) \\ &=\mathbb{E}_{\mathbf{x} \sim p(\mathbf{x})} \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x}) \end{aligned}
$$

上述推导对离散的 $\boldsymbol{x}$ 进行求和，对连续的 $\boldsymbol{x}$ 进行积分也可以得到类似结果。

在连续版本的推导中，使用在积分符号内取微分的莱布尼兹法则可以得到等式：

$$
\nabla_{\boldsymbol{\theta}} \int \tilde{p}(\mathbf{x}) d \boldsymbol{x}=\int \nabla_{\boldsymbol{\theta}} \tilde{p}(\mathbf{x}) d \boldsymbol{x}
$$

该等式只适用于 $\tilde{p}$ 和 $\nabla_{\boldsymbol{\theta}} \tilde{p}(\mathbf{x})$ 上的一些特定规范条件。在测度论术语中，这些条件是：

1. 对每一个 $\theta$ 而言，未归一化分布 $\tilde{p}$ 必须是 $\boldsymbol{x}$ 的勒贝格可积函数。
2. 对于所有的 $\theta$ 和几乎所有 $\boldsymbol{x}$，梯度 $\nabla_{\boldsymbol{\theta}} \tilde{p}(\mathbf{x})$ 必须存在。
3. 对于所有的 $\theta$ 和几乎所有的 $\boldsymbol{x}$，必须存在一个可积函数 $R(\boldsymbol{x})$ 使得 $\max _{i}\left|\frac{\partial}{\partial \theta_{i}} \tilde{p}(\mathbf{x})\right| \leq R(\boldsymbol{x})$。

幸运的是，大多数感兴趣的机器学习模型都具有这些性质。

等式：

$$
\nabla_{\boldsymbol{\theta}} \log Z=\mathbb{E}_{\mathbf{x} \sim p(\mathbf{x})} \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x})\tag{18.15}
$$

是使用各种蒙特卡罗方法近似最大化(具有难计算配分函数模型的)似然的基础。

蒙特卡罗方法为学习无向模型提供了直观的框架，我们能够在其中考虑正相和负相。在正相中，我们增大从数据中采样得到的 $\log \tilde{p}(\mathbf{x})$。在负相中，我们通过降低从模型分布中采样的 $\log \tilde{p}(\mathbf{x})$ 来降低配分函数。

在深度学习文献中，经常会看到用能量函数 $\tilde{p}(\mathbf{x})=\exp (-E(\mathbf{x}))$ 来参数化。在这种情况下，正相可以解释为压低训练样本的能量，负相可以解释为提高模型抽出的样本的能量，如图 18.1所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190831/Gfm6GHCwOTnL.png?imageslim">
</p>


# 相关

- 《深度学习》花书
