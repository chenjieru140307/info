# 参数设定

## Batch_Size 选择：

（重新整理）

- 如果数据集比较小，可采用全数据集的形式，好处是：
  - 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。
  - 由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。（没明白）
- 对于更大的数据集，假如采用全数据集的形式，坏处是：
  - 随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。
  - 以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。（没明白 Rprop 是什么？）
  - 此时：
    - 若 Batch_Size = 1
      - 即：每次只训练一个样本
      - 此时，线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。
    - 在合理的范围内，增大 Batch_Size ：
      - 内存利用率提高了，大矩阵乘法的并行化效率提高。
      - 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
      - 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。
    - 过大的 Batch_Size:
      - 内存利用率提高了，但是内存容量可能撑不住了。
      -  跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。（为什么达到相同的精度需要更长时间？）
  - Batch_Size 对训练效果的影响
    - Batch_Size 太小，模型表现效果极其糟糕(error飙升)。
    - 随着 Batch_Size 增大，处理相同数据量的速度越快。
    - 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。
    - 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。
    - 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。（为什么会最优？）
