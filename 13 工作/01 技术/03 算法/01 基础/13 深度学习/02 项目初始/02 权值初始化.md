# 权值初始化

一般方法：

- 常数初始化(constant)
  - 把权值或者偏置初始化为一个常数。例如设置为 0，偏置初始化为 0 较为常见，权重很少会初始化为 0。TensorFlow中也有 zeros_initializer、ones_initializer 等特殊常数初始化函数。
- 高斯初始化(gaussian)
  -  给定一组均值和标准差，随机初始化的参数会满足给定均值和标准差的高斯分布。高斯初始化是很常用的初始化方式。特殊地，在 TensorFlow 中还有一种截断高斯分布初始化（truncated_normal_initializer），其主要为了将超过两个标准差的随机数重新随机，使得随机数更稳定。
- 均匀分布初始化(uniform)
  - 给定最大最小的上下限，参数会在该范围内以均匀分布方式进行初始化，常用上下限为（0，1）。
- xavier 初始化(uniform)
  - 在 batchnorm 还未出现之前，要训练较深的网络，防止梯度弥散，需要依赖非常好的初始化方式。
  - xavier 就是一种比较优秀的初始化方式，也是目前最常用的初始化方式之一。其目的是为了使得模型各层的激活值和梯度在传播过程中的方差保持一致。
  - 本质上 xavier 还是属于均匀分布初始化，但与上述的均匀分布初始化有所不同，xavier 的上下限将在如下范围内进行均匀分布采样：$[-\sqrt{\frac{6}{n+m}},\sqrt{\frac{6}{n+m}}]$
    - 其中，$n$ 为所在层的输入维度，$m$ 为所在层的输出维度。
- kaiming 初始化（msra 初始化）
  - kaiming 初始化，在 caffe 中也叫 msra 初始化。kaiming 初始化和 xavier 一样都是为了防止梯度弥散而使用的初始化方式。
  - kaiming 初始化的出现是因为 xavier 存在一个不成立的假设。xavier 在推导中假设激活函数都是线性的，而在深度学习中常用的 ReLu 等都是非线性的激活函数。而 kaiming 初始化本质上是高斯分布初始化，与上述高斯分布初始化有所不同，其是个满足均值为 0，方差为 2/n的高斯分布：$[0,\sqrt{\frac{2}{n}}]$
    - 其中，$n$ 为所在层的输入维度。


全都初始化同一个值时存在的问题：

- 如果令所有权重都初始化为同一值，此时神经网络计算出来的输出值是一样的，神经网络在进行反向传播算法计算出来的梯度值也一样，并且参数更新值也一样。此时，网络对称。

举例：


- 以一个三层网络为例：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/122mu9WDeWSX.png?imageslim">
</p>


- 它的表达式为：

$$
a_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})
$$

$$
a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)})
$$

$$
a_3^{(2)} = f(W_{31}^{(1)} x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)})
$$

$$
h_{W,b}(x) = a_1^{(3)} = f(W_{11}^{(2)} a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)})
$$

$$
xa_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 +
$$

- 如果每个权重都一样，那么在多层网络中，从第二层开始，每一层的输入值都是相同的了也就是 $a1=a2=a3=....$，既然都一样，就相当于一个输入了。

- 如果是反向传递算法（如果这里不明白请看上面的连接），其中的偏置项和权重项的迭代的偏导数计算公式如下

$$
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b;x,y) = a_j^{(l)} \delta_i^{(l+1)}
$$
$$
\frac{\partial}{\partial b_{i}^{(l)}} J(W,b;x,y) = \delta_i^{(l+1)}
$$

- $\delta$ 的计算公式

$$
\delta_i^{(l)} = (\sum_{j=1}^{s_{t+1}} W_{ji}^{(l)} \delta_j^{(l+1)} ) f^{\prime}(z_i^{(l)})
$$


- 如果用的是 sigmoid 函数

$$
f^{\prime}(z_i^{(l)}) = a_i^{(l)}(1-a_i^{(l)})
$$

- 把后两个公式代入，可以看出所得到的梯度下降法的偏导相同，不停的迭代，不停的相同，不停的迭代，不停的相同......，最后就得到了相同的值（权重和截距）。

