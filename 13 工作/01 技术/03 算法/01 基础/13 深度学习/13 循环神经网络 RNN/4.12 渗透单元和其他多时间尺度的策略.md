


# 渗漏单元和其他多时间尺度的策略

处理长期依赖的一种方法是设计工作在多个时间尺度的模型，使模型的某些部分在细粒度时间尺度上操作并能处理小细节，而其他部分在粗时间尺度上操作并能把遥远过去的信息更有效地传递过来。存在多种同时构建粗细时间尺度的策略。这些策略包括在时间轴增加跳跃连接，"渗漏单元"使用不同时间常数整合信号，并去除一些用于建模细粒度时间尺度的连接。


## 时间维度的跳跃连接

增加从遥远过去的变量到目前变量的直接连接是得到粗时间尺度的一种方法。使用这样跳跃连接的想法可以追溯到{Lin-ieeetnn96}，紧接是向前馈网络引入延迟的想法。在普通的循环网络中，循环从时刻 $t$ 的单元连接到时刻 $t+1$ 单元。构造较长的延迟循环网络是可能的。

正如我们在 节 长期依赖 看到，梯度可能\emph{关于时间步数}呈指数消失或爆炸。引入了 $d$ 延时的循环连接以减轻这个问题。现在导数指数减小的速度与 $\frac{\tau}{d}$ 相关而不是 $\tau$。既然同时存在延迟和单步连接，梯度仍可能成 $t$ 指数爆炸。这允许学习算法捕获更长的依赖性，但不是所有的长期依赖都能在这种方式下良好地表示。



## 渗漏单元和一系列不同时间尺度

获得导数乘积接近 1 的另一方式是设置\emph{线性}自连接单元，并且这些连接的权重接近 1。

我们对某些 $v$ 值应用更新 $\mu^{(t)} \gets \alpha \mu^{(t-1)} + (1-\alpha) v^{(t)}$ 累积一个滑动平均值 $\mu^{(t)}$，其中 $\alpha$ 是一个从 $ \mu^{(t-1)}$ 到 $ \mu^{(t)}$ 线性自连接的例子。当 $\alpha$ 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 $\alpha$ 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。

$d$~时间步的跳跃连接可以确保单元总能被 $d$ 个时间步前的那个值影响。使用权重接近 1 的线性自连接是确保该单元可以访问过去值的不同方式。线性自连接通过调节实值 $\alpha$ 更平滑灵活地调整这种效果，而不是调整整数值的跳跃长度。

这个想法由 {Mozer-nips92}和 {ElHihi+Bengio-nips8}提出。在回声状态网络中，渗漏单元也被发现很有用。

我们可以通过两种基本策略设置渗漏单元使用的时间常数。一种策略是手动将其固定为常数，例如在初始化时从某些分布采样它们的值。另一种策略是使时间常数成为自由变量，并学习出来。在不同时间尺度使用这样的渗漏单元似乎能帮助学习长期依赖。


## 删除连接

处理长期依赖另一种方法是在多个时间尺度组织 RNN 状态的想法，信息在较慢的时间尺度上更容易长距离流动。

这个想法与之前讨论的时间维度上的跳跃连接不同，因为它涉及主动 **删除** 长度为一的连接并用更长的连接替换它们。以这种方式修改的单元被迫在长时间尺度上运作。而通过时间跳跃连接是 **添加** 边。收到这种新连接的单元，可以学习在长时间尺度上运作，但也可以选择专注于自己其他的短期连接。


强制一组循环单元在不同时间尺度上运作有不同的方式。一种选择是使循环单元变成渗漏单元，但不同的单元组关联不同的固定时间尺度。这由 {Mozer-nips92}提出，并被成功应用于 {Pascanu+al-ICML2013-small}。另一种选择是使显式且离散的更新发生在不同的时间，不同的单元组有不同的频率。这是 {ElHihi+Bengio-nips8}和 {Koutnik-et-al-ICML2014}的方法。它在一些基准数据集上表现不错。



# 相关

- 《深度学习》花书
