
一些循环神经网络形式：



- 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络。
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/vbaHrTaPtk2L.png?imageslim">
    </p>
  - 说明：
    - 计算循环网络(将 $\boldsymbol x$ 值的输入序列映射到输出值 $\boldsymbol o$ 的对应序列)训练损失的计算图。
    - 损失 $L$ 衡量每个 $\boldsymbol o$ 与相应的训练目标 $\boldsymbol y$ 的距离。
      - 当使用 softmax 输出时，我们假设 $\boldsymbol o$ 是未归一化的对数概率。损失 $L$ 内部计算 $\hat{\boldsymbol y} = \text{softmax}(\boldsymbol o)$，并将其与目标 $\boldsymbol y$ 比较。
    - RNN输入到隐藏的连接由权重矩阵 $\boldsymbol U$ 参数化，
    - 隐藏到隐藏的循环连接由权重矩阵 $\boldsymbol W$ 参数化
    - 隐藏到输出的连接由权重矩阵 $\boldsymbol V$ 参数化。
- 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络。（没有很明白）
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/dnSYuK8LS1LA.png?imageslim">
    </p>
  - 说明：
    - 此类 RNN 的唯一循环是从输出到隐藏层的反馈连接。
    - 在每个时间步 $t$，输入为 $\boldsymbol x_t$，隐藏层激活为 $\boldsymbol h^{(t)}$，输出为 $\boldsymbol o^{(t)}$，目标为 $\boldsymbol y^{(t)}$，损失为 $L^{(t)}$。
    - 这样的 RNN 没有上面的 RNN 那样强大（只能表示更小的函数集合）。
      - 上面的 RNN 可以选择将其想要的关于过去的任何信息放入隐藏表示 $\boldsymbol h$ 中并且将 $\boldsymbol h$ 传播到未来。
      - 此图中的 RNN 被训练为将特定输出值放入 $\boldsymbol o$ 中，并且 $\boldsymbol o$ 是允许传播到未来的唯一信息。
    - 此处没有从 $\boldsymbol h$ 前向传播的直接连接。之前的 $\boldsymbol h$ 仅通过产生的预测间接地连接到当前。$\boldsymbol o$ 通常缺乏过去的重要信息，除非它非常高维且内容丰富。这使得该图中的 RNN 不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化，如导师驱动过程和输出循环网络述。
- 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/1G80vekC6syt.png?imageslim">
    </p>
  - 说明：
    - 关于时间展开的循环神经网络，在序列结束时具有单个输出。
    - 这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标（如此处所示），或者通过更下游模块的反向传播来获得输出 $\boldsymbol o^{(t)}$ 上的梯度。


## 上面第二种形式网络的训练

（这个需要进行强调吗？）



导师驱动过程和输出循环网络


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/dnSYuK8LS1LA.png?imageslim">
</p>


仅在一个时间步的输出和下一个时间步的隐藏单元间存在循环连接的网络

特点：

- 没有那么强大了（因为缺乏隐藏到隐藏的循环连接）。
  - 例如，它不能模拟通用图灵机。
    - 因为这个网络缺少隐藏到隐藏的循环，它要求输出单元捕捉用于预测未来的关于过去的所有信息。因为输出单元明确地训练成匹配训练集的目标，它们不太能捕获关于过去输入历史的必要信息，除非用户知道如何描述系统的全部状态，并将它作为训练目标的一部分。
- 优点在于，任何基于比较时刻 $t$ 的预测和时刻 $t$ 的训练目标的损失函数中的所有时间步都解耦了。
  - 因此训练可以并行化，即在各时刻 $t$ 分别计算梯度。因为训练集提供输出的理想值，所以没有必要先计算前一时刻的输出。

- 训练过程：
  - 由输出反馈到模型而产生循环连接的模型可用导师驱动过程进行训练。
  - 训练模型时，导师驱动过程不再使用最大似然准则，在训练模型时的时刻 $t+1$ 接收真实值 $y^{(t)}$ 作为输入。我们可以通过检查两个时间步的序列得知这一点。
  - 条件最大似然准则是

    $$\begin{aligned}
    &\log p(\boldsymbol y^{(1)},\boldsymbol y^{(2)} \mid \boldsymbol x^{(1)}, \boldsymbol x^{(2)} ) \\
    &= \log  p(\boldsymbol y^{(2)} \mid \boldsymbol y^{(1)}, \boldsymbol x^{(1)}, \boldsymbol x^{(2)} )  + \log p(\boldsymbol y^{(1)} \mid \boldsymbol x^{(1)}, \boldsymbol x^{(2)}) .
    \end{aligned}$$

  - 在这个例子中，同时给定迄今为止的 $\boldsymbol x$ 序列和来自训练集的前一 $\boldsymbol y$ 值，我们可以看到在时刻 $t=2$ 时，模型被训练为最大化 $\boldsymbol y^{(2)}$ 的条件概率。因此最大似然在训练时指定正确反馈，而不是将自己的输出反馈到模型。
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/x8zAhSxsQfl2.png?imageslim">
    </p>

  - 说明：
    - 导师驱动过程的示意图。
    - 导师驱动过程是一种训练技术，适用于输出与下一时间步的隐藏状态存在连接的 RNN。
    - (左) 训练时，我们将训练集中正确的输出 $\boldsymbol y^{(t)}$ 反馈到 $\boldsymbol h^{(t+1)}$。
    - (右) 当模型部署后，真正的输出通常是未知的。在这种情况下，我们用模型的输出 $\boldsymbol o^{(t)}$ 近似正确的输出 $\boldsymbol y^{(t)}$，并反馈回模型。



对于导师驱动：

- 使用导师驱动过程的最初动机：
  - 是为了在缺乏隐藏到隐藏连接的模型中避免通过时间反向传播。
    - 只要模型一个时间步的输出与下一时间步计算的值存在连接，导师驱动过程仍然可以应用到这些存在隐藏到隐藏连接的模型。
    - 然而，只要隐藏单元成为较早时间步的函数，BPTT 算法是必要的。因此训练某些模型时要同时使用导师驱动过程和 BPTT。
- 缺点：
  - 如果之后网络在**开环**(open-loop)模式下使用，即网络输出（或输出分布的样本）反馈作为输入，那么完全使用导师驱动过程进行训练的缺点就会出现。
    - 在这种情况下，训练期间该网络看到的输入与测试时看到的会有很大的不同。
    - 减轻此问题的一种方法是同时使用导师驱动过程和自由运行的输入进行训练，
      - 例如在展开循环的输出到输入路径上预测几个步骤的正确目标值。通过这种方式，网络可以学会考虑在训练时没有接触到的输入条件（如自由运行模式下，自身生成自身），以及将状态映射回使网络几步之后生成正确输出的状态。
    - 另外一种方式是通过随意选择生成值或真实的数据值作为输入以减小训练时和测试时看到的输入之间的差别。这种方法利用了课程学习策略，逐步使用更多生成值作为输入。

