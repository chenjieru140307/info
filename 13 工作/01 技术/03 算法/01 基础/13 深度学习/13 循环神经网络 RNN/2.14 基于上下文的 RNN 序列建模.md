

## 基于上下文的 RNN 序列建模

上一节描述了没有输入 $\boldsymbol x$ 时，关于随机变量序列 $y^{(t)}$ 的 RNN 如何对应于有向图模型。当然，如 $\boldsymbol{a}^{(t)}=\boldsymbol{b}+\boldsymbol{W} \boldsymbol{h}^{(t-1)}+\boldsymbol{U} \boldsymbol{x}^{(t)}$ 所示的 RNN 包含一个输入序列 $\boldsymbol x^{(1)},\boldsymbol x^{(2)},\dots,\boldsymbol x^{(\tau)}$。一般情况下，RNN 允许将图模型的观点扩展到不仅代表 $y$ 变量的联合分布也能表示给定 $\boldsymbol x$ 后 $y$ 条件分布。如在 节 使用最大似然学习条件分布 的前馈网络情形中所讨论的，任何代表变量 $P(\boldsymbol y;\boldsymbol \theta)$ 的模型都能被解释为代表条件分布 $P(\boldsymbol y \mid \boldsymbol \omega)$ 的模型，其中 $\boldsymbol \omega=\boldsymbol \theta$。我们能像之前一样使用 $P(\boldsymbol y \mid \boldsymbol \omega)$ 代表分布 $P(\boldsymbol y \mid \boldsymbol x)$ 来扩展这样的模型，但要令 $\boldsymbol \omega$ 是关于 $\boldsymbol x$ 的函数。在 RNN 的情况，这可以通过不同的方式来实现。此处，我们回顾最常见和最明显的选择。


之前，我们已经讨论了将 $t =1, \dots, \tau$ 的向量 $\boldsymbol x^{(t)}$ 序列作为输入的 RNN。另一种选择是只使用单个向量 $\boldsymbol x$ 作为输入。当 $\boldsymbol x$ 是一个固定大小的向量时，我们可以简单地将其看作产生 $\boldsymbol y$ 序列 RNN 的额外输入。将额外输入提供到 RNN 的一些常见方法是：



- 在每个时刻作为一个额外输入，或
- 作为初始状态 $\boldsymbol h^{(0)}$，或
- 结合两种方式。



第一个也是最常用的方法如\fig?所示。输入 $\boldsymbol x$ 和每个隐藏单元向量 $\boldsymbol h^{(t)}$ 之间的相互作用是通过新引入的权重矩阵 $\boldsymbol R$ 参数化的，这是只包含 $y$ 序列的模型所没有的。同样的乘积 $\boldsymbol x^\top\boldsymbol R$ 在每个时间步作为隐藏单元的一个额外输入。我们可以认为 $\boldsymbol x$ 的选择（确定 $\boldsymbol x^\top\boldsymbol R$ 值），是有效地用于每个隐藏单元的一个新偏置参数。权重与输入保持独立。我们可以认为这种模型采用了非条件模型的 $\boldsymbol \theta$，并将 $\boldsymbol \omega$ 代入 $\boldsymbol \theta$，其中 $\boldsymbol \omega$ 内的偏置参数现在是输入的函数。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/8S9ArommpRQr.png?imageslim">
</p>

> 10.9 将固定长度的向量 $\boldsymbol x$ 映射到序列 $\boldsymbol Y$ 上分布的 RNN。这类 RNN 适用于很多任务如图注，其中单个图像作为模型的输入，然后产生描述图像的词序列。观察到的输出序列的每个元素 $\boldsymbol y^{(t)}$ 同时用作输入（对于当前时间步）和训练期间的目标（对于前一时间步）。



RNN 可以接收向量序列 $\boldsymbol x^{(t)}$ 作为输入，而不是仅接收单个向量 $\boldsymbol x$ 作为输入。$\boldsymbol{a}^{(t)}=\boldsymbol{b}+\boldsymbol{W} \boldsymbol{h}^{(t-1)}+\boldsymbol{U} \boldsymbol{x}^{(t)}$ 描述的 RNN 对应条件分布 $P(\boldsymbol y^{(1)}, \dots, \boldsymbol y^{(\tau)} \mid \boldsymbol x^{(1)}, \dots, \boldsymbol x^{(\tau)})$，并在条件独立的假设下这个分布分解为


$$\begin{aligned}
 \prod_t P(\boldsymbol y^{(t)} \mid \boldsymbol x^{(1)}, \dots, \boldsymbol x^{(t)}).
\end{aligned}$$


为去掉条件独立的假设，我们可以在时刻 $t$ 的输出到时刻 $t+1$ 的隐藏单元添加连接，如\fig?所示。该模型就可以代表关于 $\boldsymbol y$ 序列的任意概率分布。这种给定一个序列表示另一个序列分布的模型的还是有一个限制，就是这两个序列的长度必须是相同的。我们将在 节 基于编码-解码的序列到序列架构 描述如何消除这种限制。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/iJNVdQrlhLm0.png?imageslim">
</p>

> 10.10 将可变长度的 $\boldsymbol x$ 值序列映射到相同长度的 $\boldsymbol y$ 值序列上分布的条件循环神经网络。对比\fig?，此 RNN 包含从前一个输出到当前状态的连接。这些连接允许此 RNN 对给定 $\boldsymbol x$ 的序列后相同长度的 $\boldsymbol y$ 序列上的任意分布建模。\fig?的 RNN 仅能表示在给定 $\boldsymbol x$ 值的情况下，$\boldsymbol y$ 值彼此条件独立的分布。



# 相关

- 《深度学习》花书
