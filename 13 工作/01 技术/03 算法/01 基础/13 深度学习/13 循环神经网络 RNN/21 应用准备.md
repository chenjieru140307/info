

为什么 RNN 训练的时候 Loss 波动很大：

- RNN 特有的 memory 会影响后期其他的 RNN 的特点，梯度时大时小，learning rate 没法个性化的调整，导致 RNN 在 train 的过程中，Loss 会震荡起伏，
- 为了解决 RNN 的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。（怎么进行截断？要设置多少为好？）
