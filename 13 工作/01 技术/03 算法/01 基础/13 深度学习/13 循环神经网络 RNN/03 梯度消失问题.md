## 梯度消失


RNN 梯度消失原因：

- 先看 tanh 函数的函数及导数如图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/ixiOvxKjmRsR.jpg?imageslim">
</p>

- sigmoid函数的函数及导数图如下所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/OEtthch7MiF8.jpg?imageslim">
</p>

- 即：
  - sigmoid函数的导数范围是(0,0.25]
  - tach函数的导数范围是(0,1]
  - 他们的导数最大都不大于 1。

- 由于 $L$ 在 $t$ 时刻对 $W$ 和 $U$ 偏导数为：

    $$
    \frac{\partial L^{(t)}}{\partial W}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial W}
    $$

    $$
    \frac{\partial L^{(t)}}{\partial U}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial U}
    $$

- 如此时选择的激活函数为 $tanh$ 或 $sigmoid$，则中间累乘的部分为：

$$
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{tanh^{'}}\cdot W_{s}
$$

$$
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{sigmoid^{'}}\cdot W_{s}
$$

- 可见：
  - 累乘会导致激活函数导数的累乘，如果取 tanh 或 sigmoid 函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于 $0$，这就是“梯度消失“现象。
- 注意：
  - 实际使用中，会优先选择 tanh 函数，原因是 tanh 函数相对于 sigmoid 函数来说梯度较大，收敛速度更快且引起梯度消失更慢。




解决：

- 选取更好的激活函数，如 Relu 激活函数。
  - ReLU函数的左侧导数为 0，右侧导数恒为 1，这就避免了“梯度消失“的发生。
  - 但恒为 1 的导数容易导致“梯度爆炸“，可以设定合适的阈值来解决这个问题。
- 加入 BN 层
  - 其优点包括可加速收敛、控制过拟合，可以少用或不用 Dropout 和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。（RNN 不是不使用 BN 的吗加了 BN 之后还可以用 dropout 吗？）
- 改变传播结构，LSTM 结构可以有效解决这个问题。
  - LSTM 是为了解决 RNN 在处理长期依赖时的困难。




（下面合并到上面）


## 循环神经网络为什么会出现梯度消失或梯度爆炸？有哪些改进方案？

循环神经网络模型的求解可以采用 BPTT（Back Propagation Through Time，基于时间的反向传播）算法实现，BPTT 实际上是反向传播算法的简单变种。<span style="color:red;">BPTT 算法还是要看下具体是怎么计算的。</span>

如果将循环神经网络按时间展开成 $T$ 层的前馈神经网络来理解，就和普通的反向传播算法没有什么区别了。

循环神经网络的设计初衷之一就是能够捕获长距离输入之间的依赖。从结构上来看，循环神经网络也理应能够做到这一点。

然而实践发现，使用 BPTT 算法学习的循环神经网络并不能成功捕捉到长距离的依赖关系，这一现象主要源于深度神经网络中的梯度消失。传统的循环神经网络梯度可以表示成连乘的形式，


$$
\frac{\partial n e t_{t}}{\partial n e t_{1}}=\frac{\partial n e t_{t}}{\partial n e t_{t-1}} \cdot \frac{\partial n e t_{t-1}}{\partial n e t_{t-2}} \cdots \frac{\partial n e t_{2}}{\partial n e t_{1}}\tag{10.4}
$$

其中：

$$
net_{t}=U x_{t}+W h_{t-1}\tag{10.5}
$$

$$
h_{t}=f\left(\text { net }_{t}\right)\tag{10.6}
$$

$$
y=g\left(V h_{t}\right)\tag{10.7}
$$


$$
\begin{aligned}\frac{\partial n e t_{t}}{\partial n e t_{t-1}}&=\frac{\partial n e t_{t}}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial n e t_{t-1}}=W \cdot \operatorname{diag}\left[f^{\prime}\left(n e t_{t-1}\right)\right]\\&=\left( \begin{array}{ccc}{\mathrm{w}_{11} f^{\prime}\left(n e t_{t-1}^{1}\right)} & {\cdots} & {\mathrm{w}_{1 n} f^{\prime}\left(n e t_{t-1}^{n}\right)} \\ {\vdots} & {\ddots} & {\vdots} \\ {\mathrm{w}_{n 1} f^{\prime}\left(n e t_{t-1}^{1}\right)} & {\cdots} & {\mathrm{w}_{n n} f^{\prime}\left(n e t_{t-1}^{n}\right)}\end{array}\right)\end{aligned}\tag{10.8}
$$



其中 $n$ 为隐含层 $h_{t−1}$ 的维度（即隐含单元的个数），$\frac{\partial n e t_{t}}{\partial n e t_{t-1}}$ 对应的 $n×n$ 维矩阵，又被称为雅可比矩阵。<span style="color:red;">再看下</span>

由于预测的误差是沿着神经网络的每一层反向传播的，因此：

- 当雅克比矩阵的最大特征值大于 1 时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸；
- 反之，若雅克比矩阵的最大特征值小于 1，梯度的大小会呈指数缩小，产生梯度消失。

对于普通的前馈网络来说，梯度消失意味着无法通过加深网络层次来改善神经网络的预测效果，因为无论如何加深网络，只有靠近输出的若干层才真正起到学习的作用。**这使得循环神经网络模型很难学习到输入序列中的长距离依赖关系。** <span style="color:red;">是的。</span>


那么怎么解决这个梯度爆炸和梯度消失问题呢？

- 梯度爆炸的问题可以通过梯度裁剪来缓解，即当梯度的范式大于某个给定值时，对梯度进行等比收缩。<span style="color:red;">怎么进行梯度裁剪？怎么对梯度进行等比收缩？现实场景中真的会这么做吗？</span>
- 而梯度消失问题相对比较棘手，需要对模型本身进行改进：
    - 深度残差网络是对前馈神经网络的改进，通过残差学习的方式缓解了梯度消失的现象，从而使得我们能够学习到更深层的网络表示；<span style="color:red;">是的，之前有介绍过的。的确是不错的。</span>
    - 而对于循环神经网络来说，长短时记忆模型[23]及其变种门控循环单元（Gated recurrent unit，GRU）[24]等模型通过加入门控机制，很大程度上弥补了梯度消失所带来的损失。<span style="color:red;">嗯。好的。</span>




# 相关

- 《百面机器学习》
