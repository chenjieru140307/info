

# 长短期记忆和其他门控 RNN

本文撰写之时，实际应用中最有效的序列模型称为门控 RNN。包括基于长短期记忆和基于门控循环单元的网络。

像渗漏单元一样，门控 RNN 想法也是基于生成通过时间的路径，其中导数既不消失也不发生爆炸。渗漏单元通过手动选择常量的连接权重或参数化的连接权重来达到这一目的。门控 RNN 将其推广为在每个时间步都可能改变的连接权重。

渗漏单元允许网络在较长持续时间内\emph{积累}信息（诸如用于特定特征或类的线索）。然而，一旦该信息被使用，让神经网络\emph{遗忘}旧的状态可能是有用的。例如，如果一个序列是由子序列组成，我们希望渗漏单元能在各子序列内积累线索，我们需要将状态设置为 0 以忘记旧状态的机制。我们希望神经网络学会决定何时清除状态，而不是手动决定。这就是门控 RNN 要做的事。


## LSTM

引入自循环的巧妙构思，以产生梯度长时间持续流动的路径是初始长短期记忆模型的核心贡献。其中一个关键扩展是使自循环的权重视上下文而定，而不是固定的。门控此自循环（由另一个隐藏单元控制）的权重，累积的时间尺度可以动态地改变。在这种情况下，即使是具有固定参数的 LSTM，累积的时间尺度也可以因输入序列而改变，因为时间常数是模型本身的输出。LSTM 已经在许多应用中取得重大成功，如无约束手写识别、语音识别、手写生成、机器翻译、为图像生成标题和解析。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/LiPHCbXs9Q0d.png?imageslim">
</p>

> 10.16 LSTM 循环网络"细胞"的框图。细胞彼此循环连接，代替一般循环网络中普通的隐藏单元。这里使用常规的人工神经元计算输入特征。如果 sigmoid 输入门允许，它的值可以累加到状态。状态单元具有线性自循环，其权重由遗忘门控制。细胞的输出可以被输出门关闭。所有门控单元都具有 sigmoid 非线性，而输入单元可具有任意的压缩非线性。状态单元也可以用作门控单元的额外输入。黑色方块表示单个时间步的延迟。


LSTM 块如图 10.16 所示。在浅循环网络的架构下，相应的前向传播公式如下。更深的架构也被成功应用。LSTM 循环网络除了外部的 RNN 循环外，还具有内部的"LSTM 细胞"循环（自环），因此 LSTM 不是简单地向输入和循环单元的仿射变换之后施加一个逐元素的非线性。与普通的循环网络类似，每个单元有相同的输入和输出，但也有更多的参数和控制信息流动的门控单元系统。最重要的组成部分是状态单元 $s_i^{(t)}$，与前一节讨论的渗漏单元有类似的线性自环。然而，此处自环的权重（或相关联的时间常数）由遗忘门~$f_i^{(t)}$ 控制（时刻 $t$ 和细胞 $i$），由 sigmoid  单元将权重设置为 0 和 1 之间的值：



$$\begin{aligned}
 f_i^{(t)} = \sigma \Big( b_i^f + \sum_j U_{i,j}^f x_j^{(t)} + \sum_j W_{i,j}^f h_j^{(t-1)} \Big),
\end{aligned}$$


其中 $\boldsymbol x^{(t)}$ 是当前输入向量，$\boldsymbol h^{t}$ 是当前隐藏层向量，$\boldsymbol h^{t}$ 包含所有 LSTM 细胞的输出。


$\boldsymbol b^f, \boldsymbol U^f, \boldsymbol W^f$ 分别是偏置、输入权重和遗忘门的循环权重。因此 LSTM 细胞内部状态以如下方式更新，其中有一个条件的自环权重 $f_i^{(t)}$：


$$\begin{aligned}
 s_i^{(t)} = f_i^{(t)}  s_i^{(t-1)} +  g_i^{(t)}
 \sigma \Big( b_i + \sum_j U_{i,j} x_j^{(t)} + \sum_j W_{i,j} h_j^{(t-1)} \Big),
\end{aligned}$$


其中 $\boldsymbol b, \boldsymbol U, \boldsymbol W$ 分别是 LSTM 细胞中的偏置、输入权重和遗忘门的循环权重。**外部输入门**(external input gate)单元 $g_i^{(t)}$ 以类似遗忘门（使用 sigmoid 获得一个 0 和 1 之间的值）的方式更新，但有自身的参数：


$$\begin{aligned}
 g_i^{(t)} = \sigma \Big( b_i^g + \sum_j U_{i,j}^g x_j^{(t)} + \sum_j W_{i,j}^g h_j^{(t-1)} \Big).
\end{aligned}$$


LSTM 细胞的输出 $h_i^{(t)}$ 也可以由**输出门**(output gate)~$q_i^{(t)}$ 关闭（使用 sigmoid 单元作为门控）：


$$\begin{aligned}
 h_i^{(t)} &= \text{tanh}\big( s_i^{(t)} \big) q_i^{(t)}, \\
 q_i^{(t)} &= \sigma \Big( b_i^o + \sum_j U_{i,j}^o x_j^{(t)} + \sum_j W_{i,j}^o h_j^{(t-1)} \Big),
\end{aligned}$$


其中 $\boldsymbol b^o, \boldsymbol U^o, \boldsymbol W^o$ 分别是偏置、输入权重和遗忘门的循环权重。在这些变体中，可以选择使用细胞状态 $s_i^{(t)}$ 作为额外的输入（及其权重），输入到第 $i$ 个单元的三个门，如图 10.16 所示。这将需要三个额外的参数。


LSTM 网络比简单的循环架构更易于学习长期依赖，先是用于测试长期依赖学习能力的人工数据集，然后是在具有挑战性的序列处理任务上获得最先进的表现。LSTM 的变体和替代也已经被研究和使用，这将在下文进行讨论。


## 其他门控 RNN

LSTM 架构中哪些部分是真正必须的？还可以设计哪些其他成功架构允许网络动态地控制时间尺度和不同单元的遗忘行为？

最近关于门控 RNN 的工作给出了这些问题的某些答案，其单元也被称为门控循环单元或~GRU。与 LSTM 的主要区别是，单个门控单元同时控制遗忘因子和更新状态单元的决定。更新公式如下：


$$\begin{aligned}
 h_i^{(t)} = u_i^{(t-1)} h_i^{(t-1)} + (1 - u_i^{(t-1)}) \sigma
 \Big( b_i + \sum_j U_{i,j} x_j^{(t)} + \sum_j W_{i,j} r_j^{(t-1)} h_j^{(t-1)} \Big),
\end{aligned}$$


其中 $\boldsymbol u$ 代表"更新"门，$\boldsymbol r$ 表示"复位"门。它们的值就如通常所定义的：


$$\begin{aligned}
 u_i^{(t)} = \sigma \Big( b_i^u + \sum_j U_{i,j}^u x_j^{(t)} + \sum_j W_{i,j}^u h_j^{(t)} \Big),
\end{aligned}$$


和


$$\begin{aligned}
r_i^{(t)} = \sigma \Big( b_i^r + \sum_j U_{i,j}^r x_j^{(t)} + \sum_j W_{i,j}^r h_j^{(t)} \Big).
\end{aligned}$$


复位和更新门能独立地"忽略"状态向量的一部分。更新门像条件渗漏累积器一样可以线性门控任意维度，从而选择将它复制（在 sigmoid  的一个极端）或完全由新的"目标状态"值（朝向渗漏累积器的收敛方向）替换并完全忽略它（在另一个极端）。复位门控制当前状态中哪些部分用于计算下一个目标状态，在过去状态和未来状态之间引入了附加的非线性效应。


围绕这一主题可以设计更多的变种。例如复位门（或遗忘门）的输出可以在多个隐藏单元间共享。或者，全局门的乘积（覆盖一整组的单元，例如整一层）和一个局部门（每单元）可用于结合全局控制和局部控制。然而，一些调查发现这些 LSTM 和 GRU 架构的变种，在广泛的任务中难以明显地同时击败这两个原始架构。{Greff-et-al-arxiv2015}发现其中的关键因素是遗忘门，而 {Jozefowicz-et-al-ICML2015}发现向 LSTM 遗忘门加入 1 的偏置(由 {Gers-et-al-2000}提倡)能让 LSTM 变得与已探索的最佳变种一样健壮。



# 相关

- 《深度学习》花书
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
