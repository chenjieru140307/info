---
title: 3.12 基于Encoder-Decoder的序列到序列架构
toc: true
date: 2019-06-05
---

# 基于 Encoder-Decoder的序列到序列架构


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/1G80vekC6syt.png?imageslim">
</p>

> 10.5 关于时间展开的循环神经网络，在序列结束时具有单个输出。这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标（如此处所示），或者通过更下游模块的反向传播来获得输出 $\boldsymbol o^{(t)}$ 上的梯度。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/8S9ArommpRQr.png?imageslim">
</p>

> 10.9 将固定长度的向量 $\boldsymbol x$ 映射到序列 $\boldsymbol Y$ 上分布的 RNN。这类 RNN 适用于很多任务如图注，其中单个图像作为模型的输入，然后产生描述图像的词序列。观察到的输出序列的每个元素 $\boldsymbol y^{(t)}$ 同时用作输入（对于当前时间步）和训练期间的目标（对于前一时间步）。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/vbaHrTaPtk2L.png?imageslim">
</p>

> 10.3 计算循环网络(将 $\boldsymbol x$ 值的输入序列映射到输出值 $\boldsymbol o$ 的对应序列)训练损失的计算图。损失 $L$ 衡量每个 $\boldsymbol o$ 与相应的训练目标 $\boldsymbol y$ 的距离。当使用 softmax 输出时，我们假设 $\boldsymbol o$ 是未归一化的对数概率。损失 $L$ 内部计算 $\hat{\boldsymbol y} = \text{softmax}(\boldsymbol o)$，并将其与目标 $\boldsymbol y$ 比较。RNN输入到隐藏的连接由权重矩阵 $\boldsymbol U$ 参数化，隐藏到隐藏的循环连接由权重矩阵 $\boldsymbol W$ 参数化以及隐藏到输出的连接由权重矩阵 $\boldsymbol V$ 参数化。式 10.8 定义了该模型中的前向传播。(左)使用循环连接绘制的 RNN 和它的损失。(右)同一网络被视为展开的计算图，其中每个节点现在与一个特定的时间实例相关联。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/dnSYuK8LS1LA.png?imageslim">
</p>

> 10.4 此类 RNN 的唯一循环是从输出到隐藏层的反馈连接。在每个时间步~$t$，输入为 $\boldsymbol x_t$，隐藏层激活为 $\boldsymbol h^{(t)}$，输出为 $\boldsymbol o^{(t)}$，目标为 $\boldsymbol y^{(t)}$，损失为 $L^{(t)}$。\emph{(左)}回路原理图。\emph{(右)}展开的计算图。这样的 RNN 没有\fig?表示的 RNN 那样强大（只能表示更小的函数集合）。\fig?中的 RNN 可以选择将其想要的关于过去的任何信息放入隐藏表示 $\boldsymbol h$ 中并且将 $\boldsymbol h$ 传播到未来。该图中的 RNN 被训练为将特定输出值放入 $\boldsymbol o$ 中，并且 $\boldsymbol o$ 是允许传播到未来的唯一信息。此处没有从 $\boldsymbol h$ 前向传播的直接连接。之前的 $\boldsymbol h$ 仅通过产生的预测间接地连接到当前。$\boldsymbol o$ 通常缺乏过去的重要信息，除非它非常高维且内容丰富。这使得该图中的 RNN 不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化，如 节 导师驱动过程和输出循环网络 所述。




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/iJNVdQrlhLm0.png?imageslim">
</p>

> 10.10 将可变长度的 $\boldsymbol x$ 值序列映射到相同长度的 $\boldsymbol y$ 值序列上分布的条件循环神经网络。对比\fig?，此 RNN 包含从前一个输出到当前状态的连接。这些连接允许此 RNN 对给定 $\boldsymbol x$ 的序列后相同长度的 $\boldsymbol y$ 序列上的任意分布建模。\fig?的 RNN 仅能表示在给定 $\boldsymbol x$ 值的情况下，$\boldsymbol y$ 值彼此条件独立的分布。





<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/CMPpOBBD4isV.png?imageslim">
</p>


> 10.11 典型的双向循环神经网络中的计算，意图学习将输入序列 $\boldsymbol x$ 映射到目标序列 $\boldsymbol y$（在每个步骤 $t$ 具有损失 $L^{(t)}$）。循环性 $\boldsymbol h$ 在时间上向前传播信息（向右），而循环性 $\boldsymbol g$ 在时间上向后传播信息（向左）。因此在每个点 $t$，输出单元 $\boldsymbol o^{(t)}$ 可以受益于输入 $\boldsymbol h^{(t)}$ 中关于过去的相关概要以及输入 $\boldsymbol g^{(t)}$ 中关于未来的相关概要。




我们已经在图 10.5 看到 RNN 如何将输入序列映射成固定大小的向量，在图 10.9 中看到 RNN 如何将固定大小的向量映射成一个序列，在图 10.3 、图 10.4 、图 10.10 和图 10.11 中看到 RNN 如何将一个输入序列映射到等长的输出序列。

本节我们讨论如何训练 RNN，使其将输入序列映射到不一定等长的输出序列。这在许多场景中都有应用，如语音识别、机器翻译或问答，其中训练集的输入和输出序列的长度通常不相同（虽然它们的长度可能相关）。

我们经常将 RNN 的输入称为"上下文"。我们希望产生此上下文的表示，$C$。这个上下文 $C$ 可能是一个概括输入序列 $\boldsymbol X=(\boldsymbol x^{(1)},\dots,\boldsymbol x^{(n_x)})$ 的向量或者向量序列。

用于映射可变长度序列到另一可变长度序列最简单的 RNN 架构最初由{cho-al-emnlp14}提出，之后不久由{Sutskever-et-al-NIPS2014}独立开发，并且第一个使用这种方法获得翻译的最好结果。前一系统是对另一个机器翻译系统产生的建议进行评分，而后者使用独立的循环网络生成翻译。这些作者分别将该架构称为编码-解码或序列到序列架构，如图 10.12 所示。这个想法非常简单：

- (1) 编码器或**读取器**(reader)或**输入**(input) RNN 处理输入序列。编码器输出上下文 $C$（通常是最终隐藏状态的简单函数）。
- (2) 解码器或**写入器**(writer)或**输出**(output) RNN 则以固定长度的向量（如\fig?）为条件产生输出序列 $\boldsymbol Y=(\boldsymbol y^{(1)}, \dots, \boldsymbol y^{(n_y)})$。

这种架构对比本章前几节提出的架构的创新之处在于长度 $n_x$ 和 $n_y$ 可以彼此不同，而之前的架构约束 $n_x = n_y = \tau$。在序列到序列的架构中，两个 RNN 共同训练以最大化 $\log P( \boldsymbol y^{(1)}, \dots, \boldsymbol y^{(n_y)} \mid \boldsymbol x^{(1)},\dots,\boldsymbol x^{(n_x)} )$(关于训练集中所有 $\boldsymbol x$ 和 $\boldsymbol y$ 对的平均)。编码器 RNN 的最后一个状态 $\boldsymbol h_{n_x}$ 通常被当作输入的表示 $C$ 并作为解码器 RNN 的输入。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/nyN7lX9yvIhE.png?imageslim">
</p>

> 10.12 在给定输入序列 $(\mathbf x^{(1)},\mathbf x^{(2)},\dots,\mathbf x^{(n_x)})$ 的情况下学习生成输出序列 $(\mathbf y^{(1)},\mathbf y^{(2)},\dots,\mathbf y^{(n_y)})$ 的编码器-解码器或序列到序列的 RNN 架构的示例。它由读取输入序列的编码器 RNN 以及生成输出序列（或计算给定输出序列的概率）的解码器 RNN 组成。编码器 RNN 的最终隐藏状态用于计算一般为固定大小的上下文变量 $C$，$C$ 表示输入序列的语义概要并且作为解码器 RNN 的输入。


如果上下文 $C$ 是一个向量，则解码器 RNN 只是在 节 基于上下文的 RNN 序列建模 描述的向量到序列 RNN。正如我们所见，向量到序列 RNN 至少有两种接受输入的方法。输入可以被提供为 RNN 的初始状态，或连接到每个时间步中的隐藏单元。这两种方式也可以结合。

这里并不强制要求编码器与解码器的隐藏层具有相同的大小。

此架构的一个明显不足是，编码器 RNN 输出的上下文 $C$ 的维度太小而难以适当地概括一个长序列。这种现象由{Bahdanau-et-al-ICLR2015-small}在机器翻译中观察到。他们提出让 $C$ 成为可变长度的序列，而不是一个固定大小的向量。此外，他们还引入了将序列 $C$ 的元素和输出序列的元素相关联的注意力机制。<span style="color:red;">注意力机制补充到这里。</span>



# 相关

- 《深度学习》花书

