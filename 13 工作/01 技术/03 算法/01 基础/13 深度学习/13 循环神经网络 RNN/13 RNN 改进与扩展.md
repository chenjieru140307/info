

## Simple RNNs (SRNs)

（没有清楚）

- 介绍：
  - SRNs 是一个三层网络，其在隐藏层增加了上下文单元。
  - 图示：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/1EjDBDq3gjLb.png?imageslim">
    </p>
  - 说明：
    - $y$ 是隐藏层，
    - $u$ 是上下文单元。
      - 上下文单元节点与隐藏层中节点的连接是固定的，并且权值也是固定的。
      - 上下文节点与隐藏层节点一一对应，并且值是确定的。
- 作用：
  - 由于，在每一步中，使用标准的前向反馈进行传播，然后使用学习算法进行学习。上下文每一个节点保存其连接隐藏层节点上一步输出，即保存上文，并作用于当前步对应的隐藏层节点状态，即隐藏层的输入由输入层的输出与上一步的自身状态所决定。因此 SRNs 能够解决标准多层感知机(MLP)无法解决的对序列数据进行预测的问题。




## Bidirectional RNNs (双向网络)

（没有理解）

- 介绍：
  - 将两层 RNNs 叠加在一起，当前时刻输出(第 $t$ 步的输出)不仅仅与之前序列有关，还与之后序列有关。
  - 应用：
    - 为了预测一个语句中的缺失词语，就需要该词汇的上下文信息。
  - 图示
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/esL4wEQgr5yU.png?imageslim">
    </p>

  - 说明：
    - Bidirectional RNNs 由两个 RNNs 上下叠加在一起组成的。输出由前向 RNNs 和后向 RNNs 共同决定。



## Deep RNNs

- 介绍：
  - 与 Bidirectional RNNs 相似，其也是又多层 RNNs 叠加，因此每一步的输入有了多层网络。该网络具有更强大的表达与学习能力，但是复杂性也随之提高，同时需要更多的训练数据。
  - 图示：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/drOUe36O4Vc0.png?imageslim">
    </p>




## Echo State Networks（ESNs）


- 介绍：
  - 使用大规模随机连接的循环网络取代经典神经网络中的中间层，从而简化网络的训练过程。
- 图示：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/8VN3HC6Ug3l1.png?imageslim">
    </p>
- 说明：
  - $W$ 储备池中节点间连接权值矩阵；
  - $W_{in}$ 输入层到储备池之间连接权值矩阵，表明储备池中的神经元之间是相互连接；
  - $W_{back}$ 输出层到储备池之间的反馈连接权值矩阵，表明储备池会有输出层来的反馈；
  - $W_{out}$ 输入层、储备池、输出层到输出层的连接权值矩阵，表明输出层不仅与储备池连接，还与输入层和自己连接。
  - $W_{outbias}$ 输出层的偏置项。
- 特点
  - 它的核心结构为一个随机生成、且保持不变的储备池(Reservoir)。储备池是大规模随机生成稀疏连接(SD通常保持 1%-5% ，SD表示储备池中互相连接的神经元占总神经元个数 N 的比例)的循环结构；<span style="color:red;">什么是储备池？怎么随机生成稀疏连接的？连接固定不变吗？</span>
  - 从储备池到输出层的权值矩阵是唯一需要调整的部分；
  - 简单的线性回归便能够完成网络训练；

## Clockwork RNNs(CW-RNNs)

（没清楚）

- 缘由：
  - CW-RNNs 通过不同隐藏层模块在不同时钟频率下工作来解决长时依赖问题。
    - 将时钟时间进行离散化，不同的隐藏层组将在不同时刻进行工作。
    - 因此，所有的隐藏层组在每一步不会全部同时工作，这样便会加快网络的训练。
    - 并且，时钟周期小组的神经元不会连接到时钟周期大组的神经元，只允许周期大的神经元连接到周期小的(组与组之间的连接以及信息传递是有向的)。周期大的速度慢，周期小的速度快，因此是速度慢的神经元连速度快的神经元，反之则不成立。
- 介绍：
  - CW-RNNs 是 RNNs 的改良版本，其使用时钟频率来驱动。
  - 它将隐藏层分为几个块(组，Group/Module)，每一组按照自己规定的时钟频率对输入进行处理。
  - 为了降低 RNNs 的复杂度，CW-RNNs 减少了参数数量，并且提高了网络性能，加速网络训练。
- 结构：
  - CW-RNNs 与 SRNs 网络结构类似，也包括输入层(Input)、隐藏层(Hidden)、输出层(Output)，它们之间存在前向连接，输入层到隐藏层连接，隐藏层到输出层连接。
  - 但是与 SRN 不同的是，隐藏层中的神经元会被划分为若干个组，设为 $g​$，每一组中的神经元个数相同，设为 $k​$，并为每一个组分配一个时钟周期 $T_i\epsilon\{T_1,T_2,...,T_g\}​$，每一组中的所有神经元都是全连接，但是组 $j​$ 到组 $i​$ 的循环连接则需要满足 $T_j​$ 大于 $T_i​$。
  - 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/QSj5TUI78yJo.png?imageslim">
    </p>
  - 说明：
    - 将这些组按照时钟周期递增从左到右进行排序，即 $T_1<T_2<...<T_g​$，那么连接便是从右到左。
    - 例如：隐藏层共有 $256$ 个节点，分为四组，周期分别是 $[1,2,4,8]$，那么每个隐藏层组 $256/4=64$ 个节点，第一组隐藏层与隐藏层的连接矩阵为 $64\times​64$ 的矩阵，第二层的矩阵则为 $64\times​128$ 矩阵，第三组为 $64\times​(3\times​64)=64\times192$ 矩阵，第四组为 $64\times​(4\times​64)=64\times​256$ 矩阵。这就解释了上一段中速度慢的组连接到速度快的组，反之则不成立。


