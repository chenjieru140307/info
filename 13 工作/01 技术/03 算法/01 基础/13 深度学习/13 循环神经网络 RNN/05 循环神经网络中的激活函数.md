
# 循环神经网络中的激活函数


- ReLU
  - 可以使用 ReLU 作为激活函数，但是需要对矩阵的初值做一定限制，否则十分容易引发数值问题。
    - 让我们回顾一下循环神经网络的前向传播公式：
    $$
    n e t_{t}=U x_{t}+W h_{t-1}\tag{10.9}
    $$

    $$
    h_{t}=f\left(n e t_{t}\right)\tag{10.10}
    $$

    - 根据前向传播公式向前传递一层，可以得到：
    $$
    net_{t}=U x_{t}+W h_{t-1}=U x_{t}+W f\left(U x_{t-1}+W h_{t-2}\right)\tag{10.11}
    $$

    - 此时，如果采用 ReLU 替代公式中的激活函数 $f$，并且假设 ReLU 函数一直处于激活区域（即输入大于 0），则有：
    
    $$f(x)=x$$
    $$n e t_{t}=U x_{t}+W\left(U x_{t-1}+W h_{t-2}\right)$$

    - 则，按照这种情况继续展开，$net_{t}$ 的表达式中最终包含 $t$ 个 $W$ 连乘。
    - 而，这时，如果 $W$ 不是单位矩阵（对角线上的元素为 1，其余元素为 0 的矩阵），最终的结果将会趋于 0 或者无穷，引发严重的数值问题。
    - 为什么在卷积神经网络中不会出现这样的现象呢？
      - 因为在卷积神经网络中，每一层的权重矩阵 $W$ 是不同的，并且在初始化时它们是独立同分布的，因此可以相互抵消，在多层之后一般不会出现严重的数值问题。
  - 而且，使用 ReLU 后，如果 $W$ 不是单位矩阵，那么梯度消失或者爆炸问题还是存在：
    - 回到循环神经网络的梯度计算公式：

    $$
    \begin{aligned}\frac{\partial n e t_{t}}{\partial n e t_{t-1}}&=W \cdot \operatorname{diag}\left[f^{\prime}\left(n e t_{t-1}\right)\right]\\&=\left( \begin{array}{ccc}{W_{11} f^{\prime}\left(n e t_{t-1}^{1}\right)} & {\cdots} & {W_{1 n} f^{\prime}\left(n e t_{t-1}^{n}\right)} \\ {\vdots} & {\ddots} & {\vdots} \\ {W_{n 1} f^{\prime}\left(n e t_{t-1}^{1}\right)} & {\cdots} & {W_{n n} f^{\prime}\left(n e t_{t-1}^{n}\right)}\end{array}\right)\end{aligned}
    $$

    - 假设采用 ReLU 激活函数，且一开始所有的神经元都处于激活中（即输入大于 $0$），则 $\operatorname{diag}\left[f^{\prime}\left(n e t_{t-1}\right)\right]$ 为单位矩阵，有 $\frac{\partial n e t_{t}}{\partial n e t_{t-1}}=W$ 。
    - 在梯度传递经历了 $n$ 层之后，$\frac{\partial n e t_{t}}{\partial n e t_{1}}=W^{n}$ 。可以看到，即使采用了 ReLU 激活函数，只要 $W$ 不是单位矩阵，梯度还是会出现消失或者爆炸的现象。
  - 综上所述，当采用 ReLU 作为循环神经网络中隐含层的激活函数时，只有当 $W$ 的取值在单位矩阵附近时才能取得比较好的效果，因此需要将 $W$ 初始化为单位矩阵。
    - 实验证明，初始化 $W$ 为单位矩阵并使用 ReLU 激活函数在一些应用中取得了与长短期记忆模型相似的结果，并且学习速度比长短期记忆模型更快，是一个值得尝试的小技巧。
