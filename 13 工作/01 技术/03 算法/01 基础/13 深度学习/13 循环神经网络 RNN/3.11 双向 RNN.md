

# 双向 RNN

目前为止我们考虑的所有循环神经网络有一个"因果"结构，意味着在时刻 $t$ 的状态只能从过去的序列 $\boldsymbol x^{(1)},\dots,\boldsymbol x^{(t-1)}$ 以及当前的输入 $\boldsymbol x^{(t)}$ 捕获信息。我们还讨论了某些在 $\boldsymbol y$ 可用时，允许过去的 $\boldsymbol y$ 值信息影响当前状态的模型。

然而，在许多应用中，我们要输出的 $\boldsymbol y^{(t)}$ 的预测可能依赖于整个输入序列。例如，在语音识别中，由于协同发音，当前声音作为音素的正确解释可能取决于未来几个音素，甚至潜在的可能取决于未来的几个词，因为词与附近的词之间的存在语义依赖：如果当前的词有两种声学上合理的解释，我们可能要在更远的未来（和过去）寻找信息区分它们。这在手写识别和许多其他序列到序列学习的任务中也是如此，将会在下一节中描述。

双向循环神经网络（或双向 RNN）为满足这种需要而被发明。他们在需要双向信息的应用中非常成功，如手写识别，语音识别以及生物信息学。

顾名思义，双向 RNN 结合时间上从序列起点开始移动的 RNN 和另一个时间上从序列末尾开始移动的 RNN。图 10.11 展示了典型的双向 RNN，其中 $\boldsymbol h^{(t)}$ 代表通过时间向前移动的子 RNN 的状态，$\boldsymbol g^{(t)}$ 代表通过时间向后移动的子 RNN 的状态。这允许输出单元 $\boldsymbol o^{(t)}$ 能够计算同时依赖于过去和未来且对时刻 $t$ 的输入值最敏感的表示，而不必指定 $t$ 周围固定大小的窗口（这是前馈网络、卷积网络或具有固定大小的先行缓存器的常规 RNN 所必须要做的）。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/CMPpOBBD4isV.png?imageslim">
</p>


> 10.11 典型的双向循环神经网络中的计算，意图学习将输入序列 $\boldsymbol x$ 映射到目标序列 $\boldsymbol y$（在每个步骤 $t$ 具有损失 $L^{(t)}$）。循环性 $\boldsymbol h$ 在时间上向前传播信息（向右），而循环性 $\boldsymbol g$ 在时间上向后传播信息（向左）。因此在每个点 $t$，输出单元 $\boldsymbol o^{(t)}$ 可以受益于输入 $\boldsymbol h^{(t)}$ 中关于过去的相关概要以及输入 $\boldsymbol g^{(t)}$ 中关于未来的相关概要。



这个想法可以自然地扩展到 2 维输入，如图像，由**四个** RNN 组成，每一个沿着四个方向中的一个计算：上、下、左、右。如果 RNN 能够学习到承载长期信息，那在 2 维网格每个点 $(i, j)$ 的输出 $O_{i,j}$ 就能计算一个能捕捉到大多局部信息但仍依赖于长期输入的表示。相比卷积网络，应用于图像的 RNN 计算成本通常更高，但允许同一特征图的特征之间存在长期横向的相互作用。实际上，对于这样的 RNN，前向传播公式可以写成表示使用卷积的形式，计算自底向上到每一层的输入（在整合横向相互作用的特征图的循环传播之前）。



# 相关

- 《深度学习》花书
