# 计算

网络如下：


- 如图：

​<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/v76x6xBxcow8.png?imageslim">
</p>

- 说明：
  - $x$ 表示输入，$h$ 是隐层单元，$o$ 是输出，$L$ 为损失函数，$y$ 为训练集标签。
  - $t$ 表示 $t$ 时刻的状态，
  - $V,U,W$ 是权值，同一类型的连接权值相同。


RNN 前向传播：

- 对于 $t$ 时刻，隐藏层为：
    $$
    h^{(t)}=\phi(Ux^{(t)}+Wh^{(t-1)}+b)
    $$

- 说明：
  - 其中，$\phi()$ 为激活函数，一般会选择 tanh 函数。（为什么使用 tanh 函数？不使用 ReLU 函数？）
  - $b$ 为偏置。

- $t$ 时刻，网络输出为：
    $$
    o^{(t)}=Vh^{(t)}+c
    $$
- 此时，模型的预测输出为：

    $$
    \widehat{y}^{(t)}=\sigma(o^{(t)})
    $$

- 说明：
  - 其中 $\sigma​$ 为激活函数，通常 RNN 用于分类，故这里一般用 softmax 函数。




（这部分合并到上面去）


前向传播：

- 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/vbaHrTaPtk2L.png?imageslim">
    </p>

- 假设：
  - 使用双曲正切激活函数。
  - 假定输出是离散的，如用于预测词或字符的 RNN。
    - 表示离散变量的常规方式是把输出 $\boldsymbol o$ 作为每个离散变量可能值的非标准化对数概率。然后，我们可以应用 softmax 函数后续处理后，获得标准化后概率的输出向量 $\hat \boldsymbol y$。
- RNN 从特定的初始状态 $\boldsymbol h^{(0)}$ 开始前向传播。从 $t= 1$ 到 $t = \tau$ 的每个时间步，我们应用以下更新方程：


$$
a^{(t)}=b+W h^{(t-1)}+U x^{(t)}\tag{10.8}
$$

$$
\boldsymbol{h}^{(t)}=\tanh \left(\boldsymbol{a}^{(t)}\right)\tag{10.9}
$$

$$
\boldsymbol{o}^{(t)}=\boldsymbol{c}+\boldsymbol{V} \boldsymbol{h}^{(t)}\tag{10.10}
$$

$$
\hat{\boldsymbol{y}}^{(t)}=\operatorname{softmax}\left(\boldsymbol{o}^{(t)}\right)\tag{10.11}
$$

- 说明：
  - 其中的参数的偏置向量 $\boldsymbol b$ 和 $\boldsymbol c$ 连同权重矩阵 $\boldsymbol U$、$\boldsymbol V$ 和 $\boldsymbol W$，分别对应于输入到隐藏、隐藏到输出和隐藏到隐藏的连接。
- 这个循环网络将一个输入序列映射到相同长度的输出序列。
- 此时，与 $\boldsymbol x$ 序列配对的 $\boldsymbol y$ 的总损失就是所有时间步的损失之和。
- 例如，$L^{(t)}$ 为给定的 $\boldsymbol x^{(1)}, \dots, \boldsymbol x^{(t)}$ 后 $\boldsymbol y^{(t)}$ 的负对数似然，则

$$\begin{aligned}
L\big( \{ \boldsymbol x^{(1)}, \dots, \boldsymbol x^{(\tau)} \}, \{ \boldsymbol y^{(1)}, \dots, \boldsymbol y^{(\tau)}  \} \big) & = \sum_t L^{(t)} \\
& = - \sum_t \log p_{\text{model}} \big(  y^{(t)} \mid  \{ \boldsymbol x^{(1)}, \dots, \boldsymbol x^{(t)} \} \big) ,
\end{aligned}$$

- 说明：
  - 其中 $p_{\text{model}} \big(  y^{(t)} \mid  \{ \boldsymbol x^{(1)}, \dots, \boldsymbol x^{(t)} \} \big)$ 需要读取模型输出向量 $\hat \boldsymbol y^{(t)}$ 中对应于 $y^{(t)}$ 的项。
  - 关于各个参数计算这个损失函数的梯度是计算成本很高的操作。
    - 梯度计算涉及执行一次前向传播，接着是由右到左的反向传播。
    - 运行时间是 $\mathcal O(\tau)$，并且不能通过并行化来降低，因为前向传播图是固有循序的；每个时间步只能一前一后地计算。
    - 前向传播中的各个状态必须保存，直到它们反向传播中被再次使用，因此内存代价也是 $\mathcal O(\tau)$。
    - 应用于展开图且代价为 $\mathcal O(\tau)$ 的反向传播算法称为通过时间反向传播。






## 反向传播


RNN 反向传播：BPTT back-propagation through time 


- 中心思想和 BP 算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。
- 需要寻优的参数有三个，分别是 $U$、$V$、$W$。
- 与 BP 算法不同的是，
  - 其中 $W$ 和 $U$ 两个参数的寻优过程需要追溯之前的历史数据，
  - 参数 $V$ 相对简单只需关注目前
- 先求解参数 $V$ 的偏导数：
  - $t$ 时刻的损失，对于 $V$ 的偏导为：
    $$
    \frac{\partial L^{(t)}}{\partial V}=\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}
    $$
  - 由于，RNN 的损失随时间累加，即：
    $$
    L=\sum_{t=1}^{n}L^{(t)}
    $$
  - 则，总损失对于$V$ 的偏导为：
    $$
    \frac{\partial L}{\partial V}=\sum_{t=1}^{n}\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}
    $$
- W 和 U 的偏导的求解
  - 由于需要涉及到历史数据，其偏导求起来相对复杂。
  - 为了简化推导过程，我们假设只有三个时刻，
  - 那么在第三个时刻 $L$ 对 $W$，$L$ 对 $U$ 的偏导数分别为：

    $$
    \begin{aligned}
    \frac{\partial L^{(3)}}{\partial W}=&\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial W}
    \\&+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial W}
    \\&+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial W}
    \end{aligned}
    $$

    $$
    \begin{aligned}
    \frac{\partial L^{(3)}}{\partial U}=&\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial U}
    \\&+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial U}
    \\&+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial U}
    \end{aligned}
    $$

  - 可以观察到，在某个时刻的对 $W$ 或是 $U$ 的偏导数，需要追溯这个时刻之前所有时刻的信息。
  - 根据上面两个式子得出 $L$ 在 $t$ 时刻对 $W$ 和 $U$ 偏导数的通式：
    $$
    \frac{\partial L^{(t)}}{\partial W}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial W}
    $$

    $$
    \frac{\partial L^{(t)}}{\partial U}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial U}
    $$
  - 整体的偏导公式就是将其按时刻再一一加起来。






（下面这个合并到上面去）


计算循环神经网络的梯度


为了获得 BPTT 算法行为的一些直观理解，我们举例说明如何通过 BPTT 计算上述 RNN 公式（$\boldsymbol{a}^{(t)}=\boldsymbol{b}+\boldsymbol{W} \boldsymbol{h}^{(t-1)}+\boldsymbol{U} \boldsymbol{x}^{(t)}$ 和 $L\left(\left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(\tau)}\right\},\left\{\boldsymbol{y}^{(1)}, \ldots, \boldsymbol{y}^{(\tau)}\right\}\right)$ ）的梯度。


计算图的节点包括参数 $\boldsymbol U,\boldsymbol V,\boldsymbol W, \boldsymbol b$ 和 $\boldsymbol c$，以及以 $t$ 为索引的节点序列 $\boldsymbol x^{(t)}, \boldsymbol h^{(t)},\boldsymbol o^{(t)}$ 和 $L^{(t)}$。对于每一个节点 $\boldsymbol N$，我们需要基于 $\boldsymbol N$ 后面的节点的梯度，递归地计算梯度 $\nabla_{\boldsymbol N} L$。我们从紧接着最终损失的节点开始递归：





$$\begin{aligned}
\frac{\partial L}{\partial L^{(t)}} = 1.
\end{aligned}$$



在这个导数中，我们假设输出 $\boldsymbol o^{(t)}$ 作为 softmax函数的参数，我们可以从 softmax函数可以获得关于输出概率的向量 $\hat{\boldsymbol y}$。我们也假设损失是迄今为止给定了输入后的真实目标 $y^{(t)}$ 的负对数似然。对于所有 $i,t$，关于时间步 $t$ 输出的梯度 $\nabla_{\boldsymbol o^{(t)}} L$ 如下：



$$\begin{aligned}
 (\nabla_{\boldsymbol o^{(t)}} L)_i =  \frac{\partial L}{\partial o_i^{(t)}}
 =  \frac{\partial L}{\partial L^{(t)}}  \frac{\partial L^{(t)}}{\partial o_i^{(t)}}
 = \hat y_i^{(t)} - \mathbf{1}_{i,y^{(t)}}.
\end{aligned}$$



我们从序列的末尾开始，反向进行计算。在最后的时间步 $\tau$, $\boldsymbol h^{(\tau)}$ 只有 $\boldsymbol o^{(\tau)}$ 作为后续节点，因此这个梯度很简单：


$$\begin{aligned}
 \nabla_{\boldsymbol h^{(\tau)}} L = \boldsymbol V^\top \nabla_{\boldsymbol o^{(\tau)}} L.
\end{aligned}$$


然后，我们可以从时刻 $t=\tau-1$ 到 $t=1$ 反向迭代， 通过时间反向传播梯度，注意 $\boldsymbol h^{(t)}(t < \tau)$ 同时具有 $\boldsymbol o^{(t)}$ 和 $\boldsymbol h^{(t+1)}$ 两个后续节点。因此，它的梯度由下式计算


$$\begin{aligned}
  \nabla_{\boldsymbol h^{(t)}} L = \Big( \frac{\partial \boldsymbol h^{(t+1)}}{ \partial \boldsymbol h^{(t)}}  \Big)^\top(\nabla_{\boldsymbol h^{(t+1)}} L)
  + \Big( \frac{\partial \boldsymbol o^{(t)}}{ \partial \boldsymbol h^{(t)}}  \Big)^\top (\nabla_{\boldsymbol o^{(t)}} L) \\
  = \boldsymbol W^\top (\nabla_{\boldsymbol h^{(t+1)}} L) \text{diag} \Big( 1 - (\boldsymbol h^{(t+1)})^2 \Big)
    + \boldsymbol V^\top ( \nabla_{\boldsymbol o^{(t)}} L ),
\end{aligned}$$



其中 $\text{diag} \Big( 1 - (\boldsymbol h^{(t+1)})^2 \Big)$ 表示包含元素 $1 - (h_i^{(t+1)})^2$ 的对角矩阵。这是关于时刻 $t+1$ 与隐藏单元 $i$ 关联的双曲正切的 Jacobian。


一旦获得了计算图内部节点的梯度，我们就可以得到关于参数节点的梯度。因为参数在许多时间步共享，我们必须在表示这些变量的微积分操作时谨慎对待。我们希望实现的等式使用 节 一般化的反向传播 中的 bprop 方法计算计算图中单一边对梯度的贡献。然而微积分中的 $\nabla_{\boldsymbol W} f$ 算子，计算 $\boldsymbol W$ 对于 $f$ 的贡献时将计算图中的\emph{所有}边都考虑进去了。为了消除这种歧义，我们定义只在 $t$ 时刻使用的虚拟变量 $\boldsymbol W^{(t)}$ 作为 $\boldsymbol W$ 的副本。然后，我们可以使用 $\nabla_{\boldsymbol W^{(t)}}$ 表示权重在时间步 $t$ 对梯度的贡献。

使用这个表示，关于剩下参数的梯度可以由下式给出：



$$\begin{aligned}
 \nabla_{\boldsymbol c} L &=  \sum_t \Big( \frac{\partial \boldsymbol o^{(t)}}{\partial \boldsymbol c} \Big)^\top \nabla_{\boldsymbol o^{(t)}} L
 = \sum_t \nabla_{\boldsymbol o^{(t)}} L ,\\
 \nabla_{\boldsymbol b} L &= \sum_t \Big( \frac{\partial \boldsymbol h^{(t)}}{\partial \boldsymbol b^{(t)}} \Big)^\top \nabla_{\boldsymbol h^{(t)}} L
 = \sum_t \text{diag} \Big( 1 - \big( \boldsymbol h^{(t)} \big)^2 \Big)  \nabla_{\boldsymbol h^{(t)}} L  ,\\
 \nabla_{\boldsymbol V} L &= \sum_t \sum_i \Big( \frac{\partial L} {\partial o_i^{(t)}}\Big) \nabla_{\boldsymbol V} o_i^{(t)}
 = \sum_t (\nabla_{\boldsymbol o^{(t)}} L) \boldsymbol h^{(t)^\top},\\
 \nabla_{\boldsymbol W} L &= \sum_t \sum_i \Big( \frac{\partial L} {\partial h_i^{(t)}}\Big)
 \nabla_{\boldsymbol W^{(t)}} h_i^{(t)} \\
&= \sum_t \text{diag} \Big( 1 - \big( \boldsymbol h^{(t)} \big)^2 \Big) ( \nabla_{\boldsymbol h^{(t)}} L) \boldsymbol h^{(t-1)^\top} ,\\
 \nabla_{\boldsymbol U} L &= \sum_t \sum_i \Big( \frac{\partial L} {\partial h_i^{(t)}}\Big)
 \nabla_{\boldsymbol U^{(t)}} h_i^{(t)} \\
&= \sum_t \text{diag} \Big( 1 - \big( \boldsymbol h^{(t)} \big)^2 \Big) ( \nabla_{\boldsymbol h^{(t)}} L) \boldsymbol x^{(t)^\top} ,
\end{aligned}$$



因为计算图中定义的损失的任何参数都不是训练数据 $\boldsymbol x^{(t)}$ 的父节点，所以我们不需要计算关于它的梯度。



# 相关

- 《深度学习》花书
