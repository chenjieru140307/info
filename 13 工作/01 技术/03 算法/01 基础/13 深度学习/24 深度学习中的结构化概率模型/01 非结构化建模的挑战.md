


# 非结构化建模的挑战




深度学习的目标是使得机器学习能够解决许多人工智能中亟需解决的挑战。这也意味着它们能够理解具有丰富结构的高维数据。举个例子，我们希望 AI 的算法能够理解自然图片(注：自然图片指的是能够在正常的环境下被照相机拍摄的图片，不同于合成的图片，或者一个网页的截图等等。)，表示语音的声音信号和包含许多词和标点的文档。



分类问题可以把这样一个来自高维分布的数据作为输入，然后使用一个类别的标签来概括它——这个标签可以是照片中是什么物品，一段语音中说的是哪个单词，也可以是一段文档描述的是哪个话题。这个分类过程丢弃了输入数据中的大部分信息，然后产生单个值的输出（或者是关于单个输出值的概率分布）。这个分类器通常可以忽略输入数据的很多部分。例如，当我们识别一张照片中的一个物体时，我们通常可以忽略图片的背景。



我们也可以使用概率模型完成许多其他的任务。这些任务通常相比于分类成本更高。其中的一些任务需要产生多个输出。大部分任务需要对输入数据整个结构的完整理解，所以并不能舍弃数据的一部分。这些任务包括以下几个：

+ **估计密度函数**：给定一个输入 $\boldsymbol x$，机器学习系统返回一个对数据生成分布的真实密度函数 $p(\boldsymbol x)$ 的估计。这只需要一个输出，但它需要完全理解整个输入。即使向量中只有一个元素不太正常，系统也会给它赋予很低的概率。
+ **去噪**：给定一个受损的或者观察有误的输入数据 $\tilde{\boldsymbol x}$，机器学习系统返回一个对原始的真实 $\boldsymbol x$ 的估计。举个例子，有时候机器学习系统需要从一张老相片中去除灰尘或者抓痕。这个系统会产生多个输出值（对应着估计的干净样本 $\boldsymbol x$ 的每一个元素），并且需要我们有一个对输入的整体理解（因为即使只有一个损坏的区域，仍然会显示最终估计被损坏）。
+ **缺失值的填补**：给定 $\boldsymbol x$ 的某些元素作为观察值，模型被要求返回一个 $\boldsymbol x$ 一些或者全部未观察值的估计或者概率分布。这个模型返回的也是多个输出。由于这个模型需要恢复 $\boldsymbol x$ 的每一个元素，所以它必须理解整个输入。
+ **采样**： 模型从分布 $p(\boldsymbol x)$ 中抽取新的样本。其应用包括语音合成，即产生一个听起来很像人说话的声音。这个模型也需要多个输出以及对输入整体的良好建模。即使样本只有一个从错误分布中产生的元素，那么采样的过程也是错误的。


\fig?中描述了一个使用较小的自然图片的采样任务。




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/fB3zYnHJXOb5.png?imageslim">
</p>
> 16.1 自然图片的概率建模。\emph{(上)} CIFAR-10数据集中的 $32\times 32$ 像素的样例图片。\emph{(下)}从这个数据集上训练的结构化概率模型中抽出的样本。每一个样本都出现在与其欧氏距离最近的训练样本的格点中。这种比较使得我们发现这个模型确实能够生成新的图片，而不是记住训练样本。为了方便展示，两个集合的图片都经过了微调。

对上千甚至是上百万随机变量的分布建模，无论从计算上还是从统计意义上说，都是一个极具挑战性的任务。假设我们只想对二值的随机变量建模。这是一个最简单的例子，但是我们仍然无能为力。对一个只有 $32\times 32$ 像素的彩色（RGB）图片来说，存在 $2^{3072}$ 种可能的二值图片。这个数量已经超过了 $10^{800}$，比宇宙中的原子总数还要多。

通常意义上讲，如果我们希望对一个包含 $n$ 个离散变量并且每个变量都能取 $k$ 个值的 $\boldsymbol x$ 的分布建模，那么最简单的表示 $P(\boldsymbol x)$ 的方法需要存储一个可以查询的表格。这个表格记录了每一种可能值的概率，则需要 $k^n$ 个参数。

基于下述几个原因，这种方式是不可行的：

+ \emph{内存： 存储参数的开销。}除了极小的 $n$ 和 $k$ 的值，用表格的形式来表示这样一个分布需要太多的存储空间。

+  \emph{统计的高效性}：当模型中的参数个数增加时，使用统计估计器估计这些参数所需要的训练数据数量也需要相应地增加。因为基于查表的模型拥有天文数字级别的参数，为了准确地拟合，相应的训练集的大小也是相同级别的。任何这样的模型都会导致严重的过拟合，除非我们添加一些额外的假设来联系表格中的不同元素（正如 节 n-gram 中所举的回退或者平滑~$n$-gram~模型）。

+ \emph{运行时间：推断的开销。}假设我们需要完成这样一个推断的任务，其中我们需要使用联合分布 $P(\mathbf x)$ 来计算某些其他的分布，比如说边缘分布 $P(\mathrm x_1)$ 或者是条件分布 $P(\mathrm x_2\mid \mathrm x_1)$。计算这样的分布需要对整个表格的某些项进行求和操作，因此这样的操作的运行时间和上述高昂的内存开销是一个级别的。


+ \emph{运行时间： 采样的开销。}类似的，假设我们想要从这样的模型中采样。最简单的方法就是从均匀分布中采样，$u\sim \text{U}(0,1)$，然后把表格中的元素累加起来，直到和大于 $u$，然后返回最后一个加上的元素。最差情况下，这个操作需要读取整个表格，所以和其他操作一样，它也需要指数级别的时间。




基于表格操作的方法的主要问题是我们显式地对每一种可能的变量子集所产生的每一种可能类型的相互作用建模。在实际问题中我们遇到的概率分布远比这个简单。通常，许多变量只是间接地相互作用。


例如，我们想要对接力跑步比赛中一个队伍完成比赛的时间进行建模。假设这个队伍有三名成员：Alice， Bob和 Carol。在比赛开始时，Alice拿着接力棒，开始跑第一段距离。在跑完她的路程以后，她把棒递给了 Bob。然后 Bob 开始跑，再把棒给 Carol，Carol跑最后一棒。我们可以用连续变量来建模他们每个人完成的时间。因为 Alice 第一个跑，所以她的完成时间并不依赖于其他的人。Bob的完成时间依赖于 Alice 的完成时间，因为 Bob 只能在 Alice 跑完以后才能开始跑。如果 Alice 跑得更快，那么 Bob 也会完成得更快。所有其他关系都可以被类似地推出。最后，Carol的完成时间依赖于她的两个队友。如果 Alice 跑得很慢，那么 Bob 也会完成得更慢。结果，Carol将会更晚开始跑步，因此她的完成时间也更有可能要晚。然而，在给定 Bob 完成时间的情况下，Carol的完成时间只是\emph{间接地}依赖于 Alice 的完成时间。如果我们已经知道了 Bob 的完成时间，知道 Alice 的完成时间对估计 Carol 的完成时间并无任何帮助。这意味着我们可以通过仅仅两个相互作用来建模这个接力赛。这两个相互作用分别是 Alice 的完成时间对 Bob 的完成时间的影响和 Bob 的完成时间对 Carol 的完成时间的影响。在这个模型中，我们可以忽略第三种间接的相互作用，即 Alice 的完成时间对 Carol 的完成时间的影响。


结构化概率模型为随机变量之间的直接作用提供了一个正式的建模框架。这种方式大大减少了模型的参数个数以致于模型只需要更少的数据来进行有效的估计。<span style="color:red;">这些更轻便的模型在模型存储，模型推断以及从模型中采样时有着更小的计算开销。 </span>这些更小的模型大大减小了在模型存储、模型推断以及从模型中采样时的计算开销。




# 相关

- 《深度学习》花书
