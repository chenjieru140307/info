
# 线性因子模型介绍

之前总结的方法大部分是在有大量数据情况下的监督学习方法，而假如我们想减小数据量的要求，则需要一些无监督学习及半监督学习方法，虽然有很多无监督学习方法，但是目前还无法达到深度学习在监督学习问题中所达到的精度，这常常是由于我们需要解决的问题的维度过高或计算量过大造成的。

无监督学习常常需要建立一种依赖于观察数据的概率模型 $p_{\text {model}}(x)$ ，但由于实际观察的数据 x 常常比较杂乱没有规律，通常我们会用某种代表了更低维基本特征的隐性变量(latent variables) $h$ 来更好的表征数据，将问题转化为 $p_{\text {model}}(x)=E_{h} p_{\text {model}}(x | h)$ ，这一章主要介绍了最基本的利用隐性变量的概率模型——线性因子模型(Linear Factor model)，即假定 $h$ 取自某种先验分布(prior distribution) $h \sim p(h)$ ，而我们观察到的数据是 $h$ 的线性变换与一些随机噪声的叠加，用式子表示为 $x=W h+b+$noise ，之后讨论的不同方法会选择不同的 $p(h)$ 和 noise 分布。



许多深度学习的研究前沿均涉及构建输入的概率模型 $p_{\text{model}}(\boldsymbol x)$。原则上说，给定任何其他变量的情况下，这样的模型可以使用概率推断来预测其环境中的任何变量。许多这样的模型还具有潜变量 $\boldsymbol h$，其中 $p_{\text{model}}(\boldsymbol x) = \mathbb E_{\boldsymbol h}\, p_{\text{model}}(\boldsymbol x\mid\boldsymbol h)$。这些潜变量提供了表示数据的另一种方式。我们在深度前馈网络和循环网络中已经发现，基于潜变量的分布式表示继承了表示学习的所有优点。


在本章中，我们描述了一些基于潜变量的最简单的概率模型：线性因子模型。这些模型有时被用来作为混合模型的组成模块**或者更大的深度概率模型**。同时，也介绍了构建生成模型所需的许多基本方法，在此基础上更先进的深度模型也将得到进一步扩展。


线性因子模型通过随机线性解码器函数来定义，该函数通过对 $\boldsymbol h$ 的线性变换以及添加噪声来生成 $\boldsymbol x$。


有趣的是，通过这些模型我们能够发现一些符合简单联合分布的解释性因子。<span style="color:red;">这些模型很有趣，因为它们使得我们能够发现一些拥有简单联合分布的解释性因子。</span>线性解码器的简单性使得它们成为了最早被广泛研究的潜变量模型。


线性因子模型描述如下的数据生成过程。首先，我们从一个分布中抽取解释性因子 $\boldsymbol h$


$$\begin{aligned}
\mathbf h \sim p(\boldsymbol h),
\end{aligned}$$


其中 $p(\boldsymbol h)$ 是一个因子分布，满足 $p(\boldsymbol h) = \prod_{i}^{}p(h_i)$，所以易于从中采样。接下来，在给定因子的情况下，我们对实值的可观察变量进行采样


$$\begin{aligned}
\boldsymbol x = \boldsymbol W \boldsymbol h + \boldsymbol b + \text{noise},
\end{aligned}$$


其中噪声通常是对角化的（在维度上是独立的）且服从高斯分布。这在图 13.1 有具体说明。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/C9wpBfC2ccnA.png?imageslim">
</p>



> 13.1 描述线性因子模型族的有向图模型，其中我们假设观察到的数据向量 $\boldsymbol x$ 是通过独立的潜在因子 $\boldsymbol h$ 的线性组合再加上一定噪声获得的。不同的模型，比如概率 PCA，因子分析或者是 ICA，都是选择了不同形式的噪声以及先验 $p(\boldsymbol h)$。}
\end{figure}



# 相关

- 《深度学习》花书
