

# 不同的峰值之间的混合挑战



使用\,MCMC\，方法的主要难点在于马尔可夫链的混合通常不理想。在理想情况下，从设计好的马尔可夫链中采出的连续样本之间是完全独立的，而且在 $\boldsymbol x$ 空间中，马尔可夫链会按概率大小访问许多不同区域。

然而，\,MCMC\，方法采出的样本可能会具有很强的相关性，尤其是在高维的情况下。我们把这种现象称为慢混合甚至混合失败。具有缓慢混合的\,MCMC\，方法可以被视为对能量函数无意地执行类似于带噪声的梯度下降的操作，或者说等价于相对于链的状态（被采样的随机变量）依据概率进行噪声爬坡。（在马尔可夫链的状态空间中）从 $\boldsymbol x^{(t-1)}$ 到 $\boldsymbol x^{(t)}$ 该链倾向于选取很小的步长，其中能量 $E(\boldsymbol x^{(t)})$ 通常低于或者近似等于能量 $E(\boldsymbol x^{(t-1)})$，倾向于向较低能量的区域移动。当从可能性较小的状态（比来自 $p(\boldsymbol x)$ 的典型样本拥有更高的能量）开始时，链趋向于逐渐减少状态的能量，并且仅仅偶尔移动到另一个峰值。一旦该链已经找到低能量的区域（例如，如果变量是图像中的像素，则低能量的区域可以是同一对象所对应图像的一个连通的流形），我们称之为峰值，链将倾向于围绕着这个峰值游走（按某一种形式随机游走）。它时不时会走出该峰值，但是结果通常会返回该峰值或者（如果找到一条离开的路线）移向另一个峰值。问题是对于很多有趣的分布来说成功的离开路线很少，所以马尔可夫链将在一个峰值附近抽取远超过需求的样本。



当我们考虑 Gibbs采样算法时，这种现象格外明显。在这种情况下，我们考虑在一定步数内从一个峰值移动到一个临近峰值的概率。决定这个概率的是两个峰值之间的"能量障碍"的形状。隔着一个巨大"能量障碍" （低概率的区域）的两个峰值之间的转移概率是（随着能量障碍的高度）指数下降的，如\fig?所示。当目标分布有多个高概率峰值并且被低概率区域所分割，尤其当\,Gibbs采样的每一步都只是更新变量的一小部分而这一小部分变量又严重依赖其他的变量时，就会产生问题。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/GQkBAiUVnHvx.png?imageslim">
</p>
> 17.1 对于三种分布使用 Gibbs采样所产生的路径，所有的分布马尔可夫链初始值都设为峰值。\emph{(左)}一个带有两个独立变量的多维正态分布。由于变量之间是相互独立的，Gibbs采样混合得很好。\emph{(中)}变量之间存在高度相关性的一个多维正态分布。变量之间的相关性使得马尔可夫链很难混合。因为每一个变量的更新需要相对其他变量求条件分布，相关性减慢了马尔可夫链远离初始点的速度。\emph{(右)}\，峰值之间间距很大且不在轴上对齐的混合高斯分布。Gibbs采样混合得很慢，因为每次更新仅仅一个变量很难跨越不同的峰值。


举一个简单的例子，考虑两个变量 $\mathrm a$，$\mathrm b$ 的基于能量的模型，这两个变量都是二值的，取值 $+1$ 或者 $-1$。如果对某个较大的正数 $w$，$E(\mathrm a,\mathrm b) = - w \mathrm a \mathrm b$，那么这个模型传达了一个强烈的信息，$\mathrm a$ 和 $\mathrm b$ 有相同的符号。当 $\mathrm a=1$ 时用 Gibbs采样更新 $\mathrm b$。给定 $\mathrm b$ 时的条件分布满足 $p(\mathrm b=1\mid \mathrm a=1) = \sigma(w)$。如果 $w$ 的值很大，sigmoid 函数趋近于饱和，那么 $b$ 也取到 $1$ 的概率趋近于 $1$。同理，如果 $\mathrm a=-1$，那么 $\mathrm b$ 取到 $-1$ 的概率也趋于 $1$。根据模型 $p_{\text{model}}(\mathrm a,\mathrm b)$，两个变量取一样的符号的概率几乎相等。根据 $p_{\text{model}}(\mathrm a\mid \mathrm b)$，两个变量应该有相同的符号。这也意味着 Gibbs采样很难会改变这些变量的符号。

在更实际的问题中，这种挑战更加艰巨因为在实际问题中我们不能仅仅关注在两个峰值之间的转移，更要关注在多个峰值之间的转移。如果由于峰值之间混合困难，而导致某几个这样的转移难以完成，那么得到一些可靠的覆盖大部分峰值的样本集合的计算代价是很高的，同时马尔可夫链收敛到它的平稳分布的过程也会非常缓慢。

通过寻找一些高度依赖变量的组以及分块同时更新块（组）中的变量，这个问题有时候是可以被解决的。然而不幸的是，当依赖关系很复杂时，从这些组中采样的过程从计算角度上说是难以处理的。归根结底，马尔可夫链最初就是被提出来解决这个问题，即从大量变量中采样的问题。


在定义了一个联合分布 $p_{\text{model}}(\boldsymbol x,\boldsymbol h)$ 的潜变量模型中，我们经常通过交替地从 $p_{\text{model}}(\boldsymbol x\mid \boldsymbol h)$ 和 $p_{\text{model}}(\boldsymbol h\mid \boldsymbol x)$ 中采样来达到抽 $\boldsymbol x$ 的目的。从快速混合的角度上说，我们更希望 $p_{\text{model}}(\boldsymbol h\mid \boldsymbol x)$ 有很大的熵。然而，从学习一个 $\boldsymbol h$ 的有用表示的角度上考虑，我们还是希望 $\boldsymbol h$ 能够包含 $\boldsymbol x$ 的足够信息从而能够较完整地重构它，这意味 $\boldsymbol h$ 和 $\boldsymbol x$ 要有非常高的互信息。这两个目标是相互矛盾的。我们经常学习到能够将 $\boldsymbol x$ 精确地编码为 $\boldsymbol h$ 的生成模型，但是无法很好混合。这种情况在玻尔兹曼机中经常出现，一个玻尔兹曼机学到的分布越尖锐，该分布的马尔可夫链采样越难混合得好。这个问题在\fig?中有所描述。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/bLRogPuMNPgm.png?imageslim">
</p>
> 17.2 深度概率模型中一个混合缓慢问题的例证。每张图都是按照从左到右从上到下的顺序的。\emph{(左)} Gibbs采样从 MNIST 数据集训练成的深度玻尔兹曼机中采出的连续样本。这些连续的样本之间非常相似。由于\,Gibbs采样作用于一个深度图模型，相似度更多地是基于语义而非原始视觉特征。但是对于吉布斯链来说从分布的一个峰值转移到另一个仍然是很困难的，比如说改变数字。\emph{(右)}从生成式对抗网络中抽出的连续原始样本。因为原始采样生成的样本之间互相独立，所以不存在混合问题。


当感兴趣的分布对于每个类具有单独的流形结构时，所有这些问题都使\,MCMC\，方法变得不那么有用：分布集中在许多峰值周围，并且这些峰值由大量高能量区域分割。我们在许多分类问题中遇到的是这种类型的分布，由于峰值之间混合缓慢，它将使得 MCMC 方法非常缓慢地收敛。




## 不同峰值之间通过回火来混合


当一个分布有一些陡峭的峰并且被低概率区域包围时，很难在分布的不同峰值之间混合。<span style="color:red;">峰值。</span>一些加速混合的方法是基于构造一个概率分布替代目标分布，这个概率分布的峰值没有那么高，峰值周围的低谷也没有那么低。基于能量的模型为这个想法提供一种简单的做法。目前为止，我们一直将基于能量的模型描述为定义一个概率分布：


$$\begin{aligned}
p(\boldsymbol x) \propto \exp(-E(\boldsymbol x)).
\end{aligned}$$


基于能量的模型可以通过添加一个额外的控制峰值尖锐程度的参数 $\beta$ 来加强：


$$\begin{aligned}
p_{\beta}(\boldsymbol x) \propto \exp(-\beta E(\boldsymbol x)).
\end{aligned}$$


$\beta$ 参数可以被理解为温度的倒数，反映了基于能量的模型的统计物理学起源。当温度趋近于 0 时，$\beta$ 趋近于无穷大，此时的基于能量的模型是确定性的。当温度趋近于无穷大时，$\beta$ 趋近于零，基于能量的模型（对离散的 $\boldsymbol x$）成了均匀分布。


通常情况下，在 $\beta = 1$ 时训练一个模型。但我们也可以利用其他温度，尤其是 $\beta < 1$ 的情况。回火作为一种通用的策略，它通过从 $\beta<1$ 模型中采样来实现在 $p_1$ 的不同峰值之间快速混合。


基于回火转移~的马尔可夫链临时从高温度的分布中采样使其在不同峰值之间混合，然后继续从单位温度的分布中采样。这些技巧被应用在一些模型比如 RBM中 。另一种方法是利用并行回火~。其中马尔可夫链并行地模拟许多不同温度的不同状态。最高温度的状态混合较慢，相比之下最低温度的状态，即温度为 $1$ 时，采出了精确的样本。转移算子包括了两个温度之间的随机跳转，所以一个高温度状态分布槽中的样本有足够大的概率跳转到低温度分布的槽中。这个方法也被应用到了 RBM 中。尽管回火这种方法前景可期，现今它仍然无法让我们在采样复杂的基于能量的模型中更进一步。一个可能的原因是在临界温度时温度转移算子必须设置得非常慢（因为温度需要逐渐下降）来确保回火的有效性。





## 深度也许会有助于混合


当我们从潜变量模型 $p(\boldsymbol h,\boldsymbol x)$ 中采样时，我们可以发现如果 $p(\boldsymbol h\mid \boldsymbol x)$ 将 $\boldsymbol x$ 编码得非常好，那么从 $p(\boldsymbol x \mid \boldsymbol h)$ 中采样时，并不会太大地改变 $\boldsymbol x$，那么混合结果会很糟糕。解决这个问题的一种方法是使得 $\boldsymbol h$ 成为一种将 $\boldsymbol x$ 编码为 $\boldsymbol h$ 的深度表示，从而使得马尔可夫链在 $\boldsymbol h$ 空间中更容易混合。在许多表示学习算法如自编码器和\,RBM\，中，$\boldsymbol h$ 的边缘分布相比于 $\boldsymbol x$ 上的原始数据分布，通常表现为更加均匀、更趋近于单峰值。或许可以说，这是因为利用了所有可用的表示空间并尽量减小重构误差。因为当训练集上的不同样本之间在 $\boldsymbol h$ 空间能够被非常容易地区分时，我们也会很容易地最小化重构误差。{Bengio-et-al-ICML2013-small}观察到这样的现象，堆叠越深的正则化自编码器或者\,RBM，顶端 $\boldsymbol h$ 空间的边缘分布越趋向于均匀和发散，而且不同峰值（比如说实验中的类别）所对应区域之间的间距也会越小。在高层空间中训练\,RBM\，会使得 Gibbs采样在峰值间混合得更快。然而，如何利用这种观察到的现象来辅助训练深度生成模型或者从中采样仍然有待探索。


尽管存在混合的难点，蒙特卡罗技术仍然是一个有用的工具，通常也是最好的可用工具。事实上，在遇到难以处理的无向模型中的配分函数时，蒙特卡罗方法仍然是最主要的工具，这将在下一章详细阐述。





# 相关

- 《深度学习》花书
