


# 变分推断和变分学习




我们已经说明过了为什么证据下界 $\mathcal L(\boldsymbol v,\boldsymbol \theta,q)$ 是 $\log  p(\boldsymbol v;\boldsymbol \theta)$ 的一个下界、如何将推断看作是关于分布 $q$ 最大化 $\mathcal L$ 的过程以及如何将学习看作是关于参数 $\boldsymbol \theta$ 最大化 $\mathcal L$ 的过程。我们也讲到了~EM~算法在给定了分布 $q$ 的条件下能够进行大学习步骤，而基于~MAP~推断的学习算法则是学习一个 $p(\boldsymbol h \mid \boldsymbol v)$ 的点估计而非推断整个完整的分布。在这里我们介绍一些变分学习中更加通用的算法。



变分学习的核心思想就是在一个关于 $q$ 的有约束的分布族上最大化 $\mathcal L$。选择这个分布族时应该考虑到计算 $\mathbb E_q \log p(\boldsymbol h,\boldsymbol v)$ 的难易度。一个典型的方法就是添加分布 $q$ 如何分解的假设。



一种常用的变分学习的方法是加入一些限制使得 $q$ 是一个因子分布：


$$\begin{aligned}
	q(\boldsymbol h\mid\boldsymbol v) = \prod_{i}^{}q(h_i \mid \boldsymbol v).
\end{aligned}$$


这被称为均值场方法。更一般地说，我们可以通过选择分布 $q$ 的形式来选择任何图模型的结构，通过选择变量之间相互作用的多少来灵活地决定近似程度的大小。这种完全通用的图模型方法被称为结构化变分推断 。



变分方法的优点是我们不需要为分布 $q$ 设定一个特定的参数化形式。我们设定它如何分解，之后通过解决优化问题来找出在这些分解限制下最优的概率分布。对离散型潜变量来说，这意味着我们使用传统的优化技巧来优化描述分布 $q$ 的有限个变量。对连续型潜变量来说，这意味着我们使用一个被称为变分法的数学分支工具来解决函数空间上的优化问题。然后决定哪一个函数来表示分布 $q$。变分法是"变分学习"或者"变分推断"这些名字的来因，尽管当潜变量是离散时变分法并没有用武之地。当遇到连续型潜变量时，变分法不需要过多地人工选择模型，是一种很有用的工具。我们只需要设定分布 $q$ 如何分解，而不需要去猜测一个特定的能够精确近似原后验分布的分布 $q$。



因为 $\mathcal L(\boldsymbol v,\boldsymbol \theta,q)$ 被定义成 $\log p(\boldsymbol v;\boldsymbol \theta) - D_{\text{KL}} (q(\boldsymbol h\mid\boldsymbol v) \boldsymbol ert  p(\boldsymbol h\mid\boldsymbol v;\boldsymbol \theta) )$，我们可以认为关于 $q$ 最大化 $\mathcal L$ 的问题等价于（关于 $q$）最小化 $D_{\text{KL}}(q(\boldsymbol h\mid\boldsymbol v)\boldsymbol ert p(\boldsymbol h\mid\boldsymbol v))$。在这种情况下，我们要用 $q$ 来拟合 $p$。然而，与以前方法不同，我们使用~KL散度的相反方向来拟合一个近似。当我们使用最大似然估计来用模型拟合数据时，我们最小化 $D_{\text{KL}}(p_{\text{data}} \boldsymbol ert p_{\text{model}})$。如图 3.6 所示，这意味着最大似然鼓励模型在每一个数据达到高概率的地方达到高概率，而基于优化的推断则鼓励了 $q$ 在每一个真实后验分布概率低的地方概率较小。这两种基于~KL散度的方法都有各自的优点与缺点。选择哪一种方法取决于在具体每一个应用中哪一种性质更受偏好。在基于优化的推断问题中，从计算角度考虑，我们选择使用 $D_{\text{KL}}(q(\boldsymbol h\mid\boldsymbol v)\boldsymbol ert p(\boldsymbol h\mid\boldsymbol v))$。具体地说，计算 $D_{\text{KL}}(q(\boldsymbol h\mid\boldsymbol v)\boldsymbol ert p(\boldsymbol h\mid\boldsymbol v))$ 涉及到了计算分布 $q$ 下的期望。所以通过将分布 $q$ 设计得较为简单，我们可以简化求所需要的期望的计算过程。KL散度的相反方向需要计算真实后验分布下的期望。因为真实后验分布的形式是由模型的选择决定的，所以我们不能设计出一种能够精确计算 $D_{\text{KL}}(p(\boldsymbol h\mid\boldsymbol v) \boldsymbol ert q(\boldsymbol h\mid\boldsymbol v))$ 的开销较小的方法。






## 离散型潜变量



关于离散型潜变量的变分推断相对来说比较直接。我们定义一个分布 $q$，通常分布 $q$ 的每个因子都由一些离散状态的可查询表格定义。在最简单的情况中，$\boldsymbol h$ 是二值的并且我们做了均值场假定，分布 $q$ 可以根据每一个 $h_i$ 分解。在这种情况下，我们可以用一个向量 $\hat{\boldsymbol h}$ 来参数化分布 $q$，$\hat{\boldsymbol h}$ 的每一个元素都代表一个概率，即 $q(h_i = 1\mid \boldsymbol v) = \hat{h}_i$。



在确定了如何表示分布 $q$ 以后，我们只需要优化它的参数。在离散型潜变量模型中，这是一个标准的优化问题。基本上分布 $q$ 的选择可以通过任何优化算法解决，比如梯度下降算法。



因为它在许多学习算法的内循环中出现，所以这个优化问题必须可以很快求解。为了追求速度，我们通常使用特殊设计的优化算法。这些算法通常能够在极少的循环内解决一些小而简单的问题。一个常见的选择是使用不动点方程，换句话说，就是解关于 $\hat{h}_i$ 的方程

$$
\frac{\partial}{\partial \hat{h}_i}\mathcal L = 0.
$$

我们反复地更新 $\hat{\boldsymbol h}$ 不同的元素直到满足收敛准则。



为了具体化这些描述，我们接下来会讲如何将变分推断应用到二值稀疏编码模型（这里我们所描述的模型是 {henniges2010binary} 提出的，但是我们采用了传统、通用的均值场方法，而原文作者采用了一种特殊设计的算法）中。数学推导过程非常详细，为希望完全了解我们描述过的变分推断和变分学习高级概念描述的读者所准备。而对于并不计划推导或者实现变分学习算法的读者来说，可以放心跳过，直接阅读下一节，这并不会遗漏新的高级概念。建议那些从事二值稀疏编码研究的读者可以重新看一下 节 常用函数的有用性质 中描述的一些经常在概率模型中出现的有用的函数性质。我们在推导过程中随意地使用了这些性质，并没有特别强调它们。



在二值稀疏编码模型中，输入 $\boldsymbol v\in\mathbb R^n$，是由模型通过添加高斯噪声到 $m$ 个或有或无的不同成分的和而生成的。每一个成分可以是开或者关的，对应着隐藏单元~$\boldsymbol h \in\{0,1\}^m$:


$$\begin{aligned}
p(h_i = 1) &= \sigma(b_i), \\
p(\boldsymbol v\mid\boldsymbol h) &= \mathcal N(\boldsymbol v;\boldsymbol W \boldsymbol h,{\boldsymbol \beta}^{-1}),
\end{aligned}$$


其中 $\boldsymbol b$ 是一个可以学习的偏置集合，$\boldsymbol W$ 是一个可以学习的权值矩阵，${\boldsymbol \beta}$ 是一个可以学习的对角精度矩阵。



使用最大似然来训练这样一个模型需要对参数进行求导。我们考虑对其中一个偏置进行求导的过程：


$$\begin{aligned}
		& \frac{\partial}{\partial b_i} \log p(\boldsymbol v) \\
		= &  \frac{\frac{\partial}{\partial b_i} p(\boldsymbol v)}{p(\boldsymbol v)}\\
		= & \frac{\frac{\partial}{\partial b_i} \sum_{\boldsymbol h}^{} p(\boldsymbol h,\boldsymbol v)}{p(\boldsymbol v)}\\
		= &  \frac{\frac{\partial}{\partial b_i} \sum_{\boldsymbol h}^{} p(\boldsymbol h) p(\boldsymbol v\mid \boldsymbol h)  }{p(\boldsymbol v)}\\
		= &  \frac{ \sum_{\boldsymbol h}^{}  p(\boldsymbol v\mid \boldsymbol h) \frac{\partial}{\partial b_i} p(\boldsymbol h) }{p(\boldsymbol v)}\\
		= &  \sum_{\boldsymbol h}^{}  p(\boldsymbol h\mid \boldsymbol v) \frac{  \frac{\partial}{\partial b_i} p(\boldsymbol h) }{p(\boldsymbol h)}\\
		= & \mathbb E_{\mathbf h\sim p(\boldsymbol h\mid\boldsymbol v)} \frac{\partial}{\partial b_i}\log p(\boldsymbol h).
\end{aligned}$$



这需要计算 $p(\boldsymbol h\mid\boldsymbol v)$ 下的期望。不幸的是，$p(\boldsymbol h\mid\boldsymbol v)$ 是一个很复杂的分布。关于 $p(\boldsymbol h,\boldsymbol v)$ 和 $p(\boldsymbol h\mid\boldsymbol v)$ 的图结构可以参考图 19.2。隐藏单元的后验分布对应的是关于隐藏单元的完全图，所以相对于暴力算法，变量消去算法并不能有助于提高计算期望的效率。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/aNVnuBmeL7PT.png?imageslim">
</p>

> 19.2 包含四个隐藏单元的二值稀疏编码的图结构。\emph{(左)} $p(\boldsymbol h,\boldsymbol v)$ 的图结构。要注意边是有向的，每两个隐藏单元都是每个可见单元的共父。\emph{(右)} $p(\boldsymbol h,\boldsymbol v)$ 的图结构。为了解释共父之间的活跃路径，后验分布所有隐藏单元之间都有边。



取而代之的是，我们可以应用变分推断和变分学习来解决这个难点。


我们可以做一个均值场近似：

$$
q(\boldsymbol h\mid\boldsymbol v) = \prod_{i}^{}q(h_i\mid\boldsymbol v).
$$



二值稀疏编码中的潜变量是二值的，所以为了表示可分解的 $q$ 我们假设对 $m$ 个~Bernoulli分布 $q(h_i\mid\boldsymbol v)$ 建模。表示~Bernoulli分布的一种很自然的方法是使用一个概率向量 $\hat{\boldsymbol h}$，满足 $q(h_i\mid\boldsymbol v) = \hat{h}_i$。为了避免计算中的误差，比如说计算 $\log \hat{h}_i$ 时，我们对 $\hat{h}_i$ 添加一个约束，即 $\hat{h}_i$ 不等于 $0$ 或者 $1$。



我们将会看到变分推断方程理论上永远不会赋予 $\hat{h}_i\ $ $0$ 或者 $1$。然而在软件实现过程中，机器的舍入误差会导致 $0$ 或者 $1$ 的值。在二值稀疏编码的软件实现中，我们希望使用一个没有限制的变分参数向量 $\boldsymbol z$ 以及通过关系 $\hat{\boldsymbol h} = \sigma(\boldsymbol z)$ 来获得 $\boldsymbol h$。因此通过使用等式 $\log \sigma(z_i) = -\zeta(-z_i)$ 来建立\,sigmoid\，函数和\,softplus\，函数的关系，我们可以放心地在计算机上计算 $\log \hat{h}_i$。



在开始二值稀疏编码模型中变分学习的推导时，我们首先说明了均值场近似的使用可以使得学习过程更加简单。



证据下界可以表示为


$$\begin{aligned}
& \mathcal L(\boldsymbol v,\boldsymbol \theta,q)\\
 = & \mathbb E_{\mathbf h\sim q}[\log p(\boldsymbol h,\boldsymbol v)] + H(q)\\
 = & \mathbb E_{\mathbf h\sim q}[\log p(\boldsymbol h) + \log p(\boldsymbol v\mid\boldsymbol h) - \log q(\boldsymbol h\mid\boldsymbol v)]\\
= & \mathbb E_{\mathbf h\sim q}\Big[\sum_{i=1}^{m}\log p(h_i) + \sum_{i=1}^{n} \log p(v_i\mid\boldsymbol h) - \sum_{i=1}^{m}\log q(h_i\mid\boldsymbol v)\Big]\\
= &  \sum_{i=1}^{m}\Big[\hat{h}_i(\log \sigma(b_i) - \log \hat{h}_i) + (1 - \hat{h}_i)(\log \sigma(-b_i) - \log (1-\hat{h}_i))\Big] \\
& +  \mathbb E_{\mathbf h\sim q} \Bigg[ \sum_{i=1}^{n}\log \sqrt{\frac{\beta_i}{2\pi}}\exp(-\frac{\beta_i}{2}(v_i - \boldsymbol W_{i,:}\boldsymbol h)^2)\Bigg] \\
= &  \sum_{i=1}^{m}\Big[\hat{h}_i(\log \sigma(b_i) - \log \hat{h}_i) + (1 - \hat{h}_i)(\log \sigma(-b_i) - \log (1-\hat{h}_i))\Big] \\
& + \frac{1}{2} \sum_{i=1}^{n} \Bigg[ \log {\frac{\beta_i}{2\pi}} - \beta_i \Bigg(v_i^2 - 2 v_i \boldsymbol W_{i,:}\hat{\boldsymbol h} + \sum_{j}\Big[W^2_{i,j}\hat{h}_j + \sum_{k \neq j}W_{i,j}W_{i,k}\hat{h}_j\hat{h}_k \Big] \Bigg) \Bigg].
\end{aligned}$$




尽管这些方程从美学观点来看有些不尽如人意。他们展示了 $\mathcal L$ 可以被表示为少量简单的代数运算。因此证据下界 $\,\mathcal L$ 是易于处理的。我们可以把 $\mathcal L$ 看作是难以处理的对数似然函数的一个替代。



原则上说，我们可以使用关于 $\boldsymbol v$ 和 $\boldsymbol h$ 的梯度上升。这会成为一个推断和学习算法的完美组合。(注：译者注：推断算法和学习算法的组合)但是，由于两个原因，我们往往不这么做。第一点，对每一个 $\boldsymbol v$ 我们需要存储 $\hat{\boldsymbol h}$。我们通常更加偏向于那些不需要为每一个样本都准备内存的算法。如果我们需要为每一个样本都存储一个动态更新的向量，使得算法很难处理几十亿的样本。第二个原因就是为了能够识别 $\boldsymbol v$ 的内容，我们希望能够有能力快速提取特征 $\hat{\boldsymbol h}$。
在实际应用场景中，我们需要在有限时间内计算出 $\hat{\boldsymbol h}$。



由于以上两个原因，我们通常不会采用梯度下降来计算均值场参数 $\hat{\boldsymbol h}$。取而代之的是，我们使用不动点方程来快速估计。



不动点方程的核心思想是我们寻找一个关于 $\boldsymbol h$ 的局部极大点，满足 $\nabla_{\boldsymbol h}\mathcal L(\boldsymbol v,\boldsymbol \theta,\hat{\boldsymbol h}) = 0$。我们无法同时高效地计算所有 $\hat{\boldsymbol h}$ 的元素。然而，我们可以解决单个变量的问题：

$$
\frac{\partial}{\partial \hat{h}_i} \mathcal L(\boldsymbol v,\boldsymbol \theta,\hat{\boldsymbol h}) = 0 .
$$




我们可以迭代地将这个解应用到 $i = 1,\ldots,m$，然后重复这个循环直到我们满足了收敛准则。常见的收敛准则包含了当整个循环所改进的 $\mathcal L$ 不超过预设的容差量时停止，或者是循环中改变的 $\hat{\boldsymbol h}$ 不超过某个值时停止。


在很多不同的模型中，迭代的均值场不动点方程是一种能够提供快速变分推断的通用算法。为了使它更加具体，我们详细地讲一下如何推导出二值稀疏编码模型的更新过程。



首先，我们给出了对 $\hat{h}_i$ 的导数表达式。为了得到这个表达式，我们将 $+\frac{1}{2} \sum_{i=1}^{n}\left[\log \frac{\beta_{i}}{2 \pi}-\beta_{i}\left(v_{i}^{2}-2 v_{i} \boldsymbol{W}_{i,:} \hat{\boldsymbol{h}}+\sum_{j}\left[W_{i, j}^{2} \hat{h}_{j}+\sum_{k \neq j} W_{i, j} W_{i, k} \hat{h}_{j} \hat{h}_{k}\right]\right)\right]$ 代入到 $\frac{\partial}{\partial \hat{h}_{i}} \mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, \hat{\boldsymbol{h}})=0$ 的左边：


$$\begin{aligned}
& \frac{\partial}{\partial \hat{h}_i} \mathcal L (\boldsymbol v,\boldsymbol \theta,\hat{\boldsymbol h})    \\
= & \frac{\partial}{\partial \hat{h}_i} \Bigg[\sum_{j=1}^{m}\Big[\hat{h}_j (\log \sigma(b_j) - \log \hat{h}_j) + (1 - \hat{h}_j)(\log \sigma(-b_j) - \log (1-\hat{h}_j))\Big] \\
& + \frac{1}{2} \sum_{j=1}^{n} \Bigg[ \log {\frac{\beta_j}{2\pi}} - \beta_j \Bigg(v_j^2 - 2 v_j \boldsymbol W_{j,:}\hat{\boldsymbol h} + \sum_{k}\Bigg[W^2_{j,k}\hat{h}_k + \sum_{l \neq k}W_{j,k}W_{j,l}\hat{h}_k\hat{h}_l \Bigg] \Bigg) \Bigg]\Bigg] \\
= & \log \sigma(b_i) - \log \hat{h}_i - 1 + \log (1 - \hat{h}_i ) + 1 - \log \sigma (- b_i ) \\
 & + \sum_{j=1}^{n} \Bigg[\beta_j \Bigg(v_j W_{j,i} - \frac{1}{2} W_{j,i}^2 - \sum_{k \neq i} \boldsymbol W_{j,k}\boldsymbol W_{j,i} \hat{h}_k\Bigg) \Bigg]\\
 = & b_i - \log \hat{h}_i + \log (1 - \hat{h}_i ) + \boldsymbol v^{\top} {\boldsymbol \beta} \boldsymbol W_{:,i} - \frac{1}{2} \boldsymbol W_{:,i}^{\top} {\boldsymbol \beta}\boldsymbol W_{:,i} -\sum_{j\neq i}\boldsymbol W^{\top}_{:,j}{\boldsymbol \beta}\boldsymbol W_{:,i}\hat{h}_j.
\end{aligned}$$


为了应用固定点更新的推断规则，我们通过令 $b_{i}-\log \hat{h}_{i}+\log \left(1-\hat{h}_{i}\right)+\boldsymbol{v}^{\top} \boldsymbol{\beta} \boldsymbol{W}_{:, i}-\frac{1}{2} \boldsymbol{W}_{:, i}^{\top} \boldsymbol{\beta} \boldsymbol{W}_{:, i}-\sum_{j \neq i} \boldsymbol{W}_{:, j}^{\top} \boldsymbol{\beta} \boldsymbol{W}_{:, i} \hat{h}_{j}$ 等于 $0$ 来解 $\hat{h}_i$：


$$\begin{aligned}
\hat{h}_i = \sigma\Bigg(b_i + \boldsymbol v^{\top} {\boldsymbol \beta} \boldsymbol W_{:,i} - \frac{1}{2} \boldsymbol W_{:,i}^{\top} {\boldsymbol \beta} \boldsymbol W_{:,i} - \sum_{j \neq i }  \boldsymbol W_{:,j}^{\top} {\boldsymbol \beta}  \boldsymbol W_{:,i} \hat{h}_j \Bigg).
\end{aligned}$$


此时，我们可以发现图模型中的推断和循环神经网络之间存在着紧密的联系。具体地说，均值场不动点方程定义了一个循环神经网络。这个神经网络的任务就是完成推断。我们已经从模型描述的角度介绍了如何推导这个网络，但是直接训练这个推断网络也是可行的。有关这种思路的一些想法在 章 深度生成模型 中有所描述。



在二值稀疏编码模型中，我们可以发现 $\hat{h}_{i}=\sigma\left(b_{i}+\boldsymbol{v}^{\top} \boldsymbol{\beta} \boldsymbol{W}_{:, i}-\frac{1}{2} \boldsymbol{W}_{:, i}^{\top} \boldsymbol{\beta} \boldsymbol{W}_{:, i}-\sum_{j \neq i} \boldsymbol{W}_{:, j}^{\top} \boldsymbol{\beta} \boldsymbol{W}_{:, i} \hat{h}_{j}\right)$ 中描述的循环网络连接包含了根据相邻隐藏单元变化值来反复更新当前隐藏单元的操作。<span style="color:red;">连接 换成 （和图模型推断的相关联系）</span>输入层通常给隐藏单元发送一个固定的信息 $\boldsymbol v^{\top}\beta\boldsymbol W$，然而隐藏单元不断地更新互相传送的信息。具体地说，当 $\hat{h}_i$ 和 $\hat{h}_j$ 两个单元的权重向量平行时，它们会互相抑制。这也是一种形式的竞争——两个解释输入的隐藏单元之间，只有一个解释得更好的才被允许继续保持活跃。在二值稀疏编码的后验分布中，均值场近似试图捕获到更多的相消解释相互作用，从而产生了这种竞争。<span style="color:red;">捕获  换成  反应。</span>事实上，相消解释效应会产生一个多峰值的后验分布，以致于如果我们从后验分布中采样，一些样本在一个单元是活跃的，其他的样本在另一个单元活跃，只有很少的样本能够两者都处于活跃状态。不幸的是，相消解释作用无法通过均值场中因子分布 $q$ 来建模，因此建模时均值场近似只能选择一个峰值。这个现象的一个例子可以参考图 3.6。





我们将 $\hat{h}_{i}=\sigma\left(b_{i}+\boldsymbol{v}^{\top} \boldsymbol{\beta} \boldsymbol{W}_{:, i}-\frac{1}{2} \boldsymbol{W}_{:, i}^{\top} \boldsymbol{\beta} \boldsymbol{W}_{:, i}-\sum_{j \neq i} \boldsymbol{W}_{:, j}^{\top} \boldsymbol{\beta} \boldsymbol{W}_{:, i} \hat{h}_{j}\right)$ 重写成等价的形式来揭示一些深层的含义：


$$\begin{aligned}
\hat{h}_i = \sigma\Bigg(b_i + \Big(\boldsymbol v - \sum_{j\neq i} \boldsymbol W_{:,j}\hat{h}_j\Big)^{\top} {\boldsymbol \beta}\boldsymbol W_{:,i} - \frac{1}{2} \boldsymbol W_{:,i}^{\top} {\boldsymbol \beta} \boldsymbol W_{:,i}\Bigg).
\end{aligned}$$


在这种新的形式中，我们可以将 $\boldsymbol v - \sum_{j\neq i} \boldsymbol W_{:,j}\hat{h}_j$ 看作是输入，而不是 $\boldsymbol v$。因此，我们可以把第 $i$ 个单元视作给定其他单元编码时给 $\boldsymbol v$ 中的剩余误差编码。由此我们可以将稀疏编码视作是一个迭代的自编码器，将输入反复地编码解码，试图在每一轮迭代后都能修复重构中的误差。



在这个例子中，我们已经推导出了每一次更新单个结点的更新规则。如果能够同时更新更多的结点，那会更令人满意。某些图模型，比如深度玻尔兹曼机，我们可以同时解出 $\hat{\boldsymbol h}$ 中的许多元素。不幸的是，二值稀疏编码并不适用这种块更新。取而代之的是，我们使用一种被称为衰减的启发式技巧来实现块更新。在衰减方法中，对 $\hat{\boldsymbol h}$ 中的每一个元素我们都可以解出最优值，然后对于所有的值都在这个方向上移动一小步。这个方法不能保证每一步都能增加 $\mathcal L$，但是对于许多模型都很有效。关于在信息传输算法中如何选择同步程度以及使用衰减策略可以参考 {koller-book2009} 。
 % 637 head





## 变分法


在继续介绍变分学习之前，我们有必要简单地介绍一种变分学习中重要的数学工具：变分法。



许多机器学习的技巧是基于寻找一个输入向量 $\boldsymbol \theta\in\mathbb R^n$ 来最小化函数 $J(\boldsymbol \theta)$，使得它取到最小值。这个步骤可以利用多元微积分以及线性代数的知识找到满足 $\nabla_{\boldsymbol \theta} J(\boldsymbol \theta) = 0$ 的临界点来完成。在某些情况下，我们希望能够解一个函数 $f(\boldsymbol x)$，比如当我们希望找到一些随机变量的概率密度函数时。正是变分法能够让我们完成这个目标。




函数 $f$ 的函数被称为泛函 $J[f]$。正如我们许多情况下对一个函数求关于以向量的元素为变量的偏导数一样，我们可以使用泛函导数，即在任意特定的 $\boldsymbol x$ 值，对一个泛函 $J[f]$ 求关于函数 $f(\boldsymbol x)$ 的导数，这也被称为变分导数。泛函 $J$ 的关于函数 $f$ 在点 $\boldsymbol x$ 处的泛函导数被记作 $\frac{\delta}{\delta f(x)}J$。




完整正式的泛函导数的推导不在本书的范围之内。对于我们的目标而言，了解可微分函数 $f(\boldsymbol x)$ 以及带有连续导数的可微分函数 $g(y,\boldsymbol x)$ 就足够了：


$$\begin{aligned}
	\frac{\delta}{\delta f(\boldsymbol x)} \int g(f(\boldsymbol x),\boldsymbol x)d\boldsymbol x = \frac{\partial}{\partial y}g(f(\boldsymbol x),\boldsymbol x).
\end{aligned}$$


为了使上述等式更加直观，我们可以把 $f(\boldsymbol x)$ 看作是一个有着无穷不可数多元素的向量，由一个实数向量 $\boldsymbol x$ 表示。在这里（看作是一个不完全的介绍），这种关系式中描述的泛函导数和向量 $\boldsymbol \theta\in\mathbb R^n$ 的导数相同：


$$\begin{aligned}
	\frac{\partial}{\partial \theta_i}\sum_{j}^{}g(\theta_j,j) = \frac{\partial}{\partial \theta_i}g(\theta_i,i).
\end{aligned}$$


在其他机器学习文献中的许多结果则使用了更为通用的欧拉-拉格朗日方程，它能够使得 $g$ 不仅依赖于 $f$ 的值，还依赖于 $f$ 的导数。但是在本书中我们不需要这个通用版本。



为了关于一个向量优化某个函数，我们求出了这个函数关于这个向量的梯度，然后找这个梯度中每一个元素都为 $0$ 的点。类似地，我们可以通过寻找一个函数使得泛函导数的每个点都等于 $0$ 从而来优化一个泛函。



下面介绍一个该过程如何运行的例子，我们考虑寻找一个定义在 $x\in\mathbb R$ 上的有最大微分熵的概率密度函数。我们回过头来看一下一个概率分布 $p(x)$ 的熵，定义如下：


$$\begin{aligned}
	H[p] = - \mathbb E_x \log p(x).
\end{aligned}$$


对于连续的值，这个期望可以被看作一个积分：


$$\begin{aligned}
	H[p] = - \int p(x) \log p(x) dx.
\end{aligned}$$



我们不能简单地仅仅关于函数 $p(x)$ 最大化 $H[p]$，因为那样的话结果可能不是一个概率分布。为了解决这个问题，我们需要使用一个拉格朗日乘子来添加一个分布 $p(x)$ 积分值为 $1$ 的约束。同样地，当方差增大时，熵也会无限制地增加。因此，寻找哪一个分布有最大熵这个问题是没有意义的。但是，在给定固定的方差 $\sigma^2$ 时，我们可以寻找一个最大熵的分布。最后，这个问题还是欠定的，因为在不改变熵的条件下一个分布可以被随意地改变。为了获得一个唯一的解，我们再加一个约束：分布的均值必须为 $\mu$。那么这个问题的拉格朗日泛函如下：


$$\begin{aligned}
&		\mathcal L[p] =  \lambda_1 \Big(\int p(x)dx - 1\Big)  + \lambda_2 (\mathbb E[x] - \mu) +  \lambda_3 (\mathbb E[(x-\mu)^2] - \sigma^2)  + H[p]\\
& =  \int \Big(\lambda_1 p(x) + \lambda_2 p(x)x + \lambda_3 p(x)(x-\mu)^2 - p(x)\log p(x) \Big)dx - \lambda_1 - \mu \lambda_2 - \sigma^2\lambda_3.
\end{aligned}$$



为了关于 $p$ 最小化拉格朗日乘子，我们令泛函导数等于 $0$：


$$\begin{aligned}
	\forall x,\ \  \frac{\delta}{\delta p(x)} \mathcal L = \lambda_1 + \lambda_2 x + \lambda_3(x-\mu)^2 - 1 - \log p(x) = 0 .
\end{aligned}$$



这个条件告诉我们 $p(x)$ 的泛函形式。通过代数运算重组上述方程，我们可以得到


$$\begin{aligned}
	p(x) = \exp\big(\lambda_1 + \lambda_2 x + \lambda_3 (x-\mu)^2  - 1\big).
\end{aligned}$$



我们并没有直接假设 $p(x)$ 取这种形式，而是通过最小化泛函从理论上得到了这个 $p(x)$ 的表达式。为了解决这个最小化问题，我们需要选择 $\lambda$ 的值来确保所有的约束都能够满足。我们有很大的自由去选择 $\lambda$。因为只要满足约束，拉格朗日关于 $\lambda$ 这个变量的梯度就为 $0$。为了满足所有的约束，我们可以令 $\lambda_1 = 1 - \log \sigma\sqrt{2\pi}$,$\lambda_2 = 0$, $\lambda_3 = - \frac{1}{2\sigma^2}$，从而得到


$$\begin{aligned}
	p(x) = \mathcal N(x;\mu,\sigma^2).
\end{aligned}$$


这也是当我们不知道真实的分布时总是使用正态分布的一个原因。因为正态分布拥有最大的熵，我们通过这个假定来保证了最小可能量的结构。




当寻找熵的拉格朗日泛函的临界点并且给定一个固定的方差时，我们只能找到一个对应最大熵的临界点。那最小化熵的概率密度函数是什么样的呢？为什么我们无法发现对应着极小点的第二个临界点呢？原因是没有一个特定的函数能够达到最小的熵值。当函数把越多的概率密度加到 $x = \mu + \sigma$ 和 $x = \mu - \sigma$ 两个点上，越少的概率密度到其他点上时，它们的熵值会减少，而方差却不变。然而任何把所有的权重都放在这两点的函数的积分都不为 $1$，不是一个有效的概率分布。所以不存在一个最小熵的概率密度函数，就像不存在一个最小的正实数一样。然而，我们发现存在一个收敛的概率分布的序列，收敛到权重都在两个点上。这种情况能够退化为混合 Dirac 分布。因为\,Dirac分布并不是一个单独的概率密度函数，所以\,Dirac分布或者混合\,Dirac分布并不能对应函数空间的一个点。所以对我们来说，当寻找一个泛函导数为 $0$ 的函数空间的点时，这些分布是不可见的。这就是这种方法的局限之处。诸如\,Dirac分布这样的分布可以通过其他方法被找到，比如可以先猜测一个解，然后证明它是满足条件的。





## 连续型潜变量



当我们的图模型包含连续型潜变量时，我们仍然可以通过最大化 $\mathcal L$ 进行变分推断和变分学习。然而，我们需要使用变分法来实现关于 $q(\boldsymbol h\mid\boldsymbol v)$ 最大化 $\mathcal L$。




在大多数情况下，研究者并不需要解决任何变分法的问题。取而代之的是，均值场固定点迭代更新有一个通用的方程。如果我们做了均值场近似：


$$\begin{aligned}
	q(\boldsymbol h\mid\boldsymbol v) = \prod_i q(h_i \mid\boldsymbol v),
\end{aligned}$$



并且对任何的 $j\neq i$ 固定 $q(h_j\mid\boldsymbol v)$，那么只需要满足分布 $p$ 中任何联合分布变量的概率值不为 $0$，我们就可以通过归一化下面这个未归一的分布


$$\begin{aligned}
	\tilde{q}(h_i \mid\boldsymbol v) = \exp \big(\mathbb E_{\mathbf h_{-i}\sim q(\mathbf h_{-i}\mid\boldsymbol v)}	\log \tilde{p}(\boldsymbol v,\boldsymbol h)\big)
\end{aligned}$$



来得到最优的 $q(h_i\mid\boldsymbol v)$。在这个方程中计算期望就能得到正确的 $q(h_i\mid\boldsymbol v)$ 的表达式。我们只有在希望提出一种新形式的变分学习算法时才需要使用变分法来直接推导 $q$ 的函数形式。$\tilde{q}\left(h_{i} | \boldsymbol{v}\right)=\exp \left(\mathbb{E}_{\mathbf{h}_{-i} \sim q\left(\mathbf{h}_{-i} | v\right)} \log \tilde{p}(\boldsymbol{v}, \boldsymbol{h})\right)$ 给出了适用于任何概率模型的均值场近似。


$\tilde{q}\left(h_{i} | \boldsymbol{v}\right)=\exp \left(\mathbb{E}_{\mathbf{h}_{-i} \sim q\left(\mathbf{h}_{-i} | v\right)} \log \tilde{p}(\boldsymbol{v}, \boldsymbol{h})\right)$ 是一个不动点方程，对每一个 $i$ 它都被迭代地反复使用直到收敛。然而，它还包含着更多的信息。它还包含了最优解取到的泛函形式，无论我们是否能够通过不动点方程来解出它。这意味着我们可以利用方程中的泛函形式，把其中一些值当成参数，然后通过任何我们想用的优化算法来解决这个问题。



我们拿一个简单的概率模型作为例子，其中潜变量满足 $\boldsymbol h\in\mathbb R^2$，可见变量只有一个 $v$。假设 $p(\boldsymbol h) = \mathcal N(\boldsymbol h;0,\boldsymbol I)$ 以及 $p(v\mid\boldsymbol h) = \mathcal N(v;\boldsymbol w^{\top}\boldsymbol h;1)$，我们可以积掉 $\boldsymbol h$ 来简化这个模型，结果是关于 $v$ 的高斯分布。这个模型本身并不有趣。只是为了说明变分法如何应用在概率建模之中，我们才构造了这个模型。




忽略归一化常数时，真实的后验分布如下：


$$\begin{aligned}
   & p(\boldsymbol h\mid\boldsymbol v)\\
 \propto & p(\boldsymbol h, \boldsymbol v)\\
 = & p(h_1) p(h_2) p(\boldsymbol v\mid\boldsymbol h)\\
 \propto & \exp \big(-\frac{1}{2} [h_1^2 + h_2^2 + (v-h_1w_1 - h_2w_2)^2]\big)\\
 = & \exp \big( - \frac{1}{2} [h_1^2 + h_2^2 + v^2 + h_1^2w_1^2 + h_2^2w_2^2 - 2vh_1w_1 - 2vh_2w_2 + 2h_1w_1h_2w_2] \big).
\end{aligned}$$


在上式中，我们发现由于带有 $h_1,h_2$ 乘积项的存在，真实的后验并不能关于 $h_1,h_2$ 分解。




应用 $\tilde{q}\left(h_{i} | \boldsymbol{v}\right)=\exp \left(\mathbb{E}_{\mathbf{h}_{-i} \sim q\left(\mathbf{h}_{-i} | v\right)} \log \tilde{p}(\boldsymbol{v}, \boldsymbol{h})\right)$ ，我们可以得到


$$\begin{aligned}
& \tilde{q}(h_1\mid\boldsymbol v)\\
= & \exp\big(\mathbb E_{\mathrm h_2\sim q(\mathrm h_2\mid\boldsymbol v)} \log \tilde{p}(\boldsymbol v,\boldsymbol h)\big) \\
= & \exp\Big( -\frac{1}{2} \mathbb E_{\mathrm h_2\sim q(\mathrm h_2\mid\boldsymbol v)} [h_1^2 + h_2^2 + v^2 + h_1^2w_1^2 + h_2^2w_2^2 \\
& -2vh_1w_1 - 2vh_2w_2 + 2h_1w_1h_2w_2]\Big).
\end{aligned}$$



从这里，我们可以发现其中我们只需要从 $q(h_2\mid \boldsymbol v)$ 中获得两个有效值：$\mathbb E_{\mathrm h_2\sim q(\mathrm h\mid\boldsymbol v)}[h_2]$ 和 $\mathbb E_{\mathrm h_2\sim q(\mathrm h\mid\boldsymbol v)}[h_2^2]$。把这两项记作 $\langle h_2 \rangle$ 和 $\langle h_2^2 \rangle$，我们可以得到：


$$\begin{aligned}
\tilde{q}(h_1\mid\boldsymbol v) = & \exp(-\frac{1}{2} [h_1^2 + \langle h_2^2 \rangle  + v^2 + h_1^2w_1^2 + \langle h_2^2 \rangle w_2^2
\\ &	-2vh_1w_1 - 2v\langle h_2 \rangle w_2 + 2h_1w_1\langle h_2 \rangle w_2]).
\end{aligned}$$



从这里，我们可以发现 $\tilde{q}$ 的泛函形式满足高斯分布。因此，我们可以得到 $q(\boldsymbol h\mid\boldsymbol v) = \mathcal N(\boldsymbol h;\boldsymbol mu,\boldsymbol \beta^{-1})$，其中 $\boldsymbol mu$ 和对角的 $\boldsymbol \beta$ 是变分参数，我们可以使用任何方法来优化它。<!-- %有必要再强调一下，我们并没有假设 $q$ 是一个高斯分布，这个高斯的形式是使用变分法来最大化关于 $\mathcal L$ 的分布 $q$\footnote{此处似乎有笔误。}推导出的。     什么笔误啊。。。  最大化 L 关于 q 还是 最大化 q 关于 L -->有必要再强调一下，我们并没有假设 $q$ 是一个高斯分布，这个高斯的形式是使用变分法来关于分布 $q$ 最大化 $\mathcal L$ 而推导出来的。在不同的模型上应用相同的方法可能会得到不同泛函形式的分布 $q$。


当然，上述模型只是为了说明情况的一个简单例子。深度学习中关于变分学习中连续型变量的实际应用可以参考 {Goodfeli-et-al-TPAMI-Deep-PrePrint-2013-small}。





## 学习和推断之间的相互作用




在学习算法中使用近似推断会影响学习的过程，反过来学习的过程也会影响推断算法的准确性。



具体来说，训练算法倾向于朝使得近似推断算法中的近似假设变得更加真实的方向来适应模型。当训练参数时，变分学习增加


$$\begin{aligned}
\mathbb E_{\mathbf h \sim q}\log p(\boldsymbol v,\boldsymbol h).
\end{aligned}$$

对于一个特定的 $\boldsymbol v$，对于 $q(\boldsymbol h \mid \boldsymbol v)$ 中概率很大的 $\boldsymbol h$ 它增加了 $p(\boldsymbol h\mid\boldsymbol v)$；对于 $q(\boldsymbol h \mid \boldsymbol v)$ 中概率很小的 $\boldsymbol h$ 它减小了 $p(\boldsymbol h\mid\boldsymbol v)$。

这种行为使得我们做的近似假设变得合理。 %这种行为使我们的近似假设成为自我实现。如果我们用单峰值近似后验来训练模型，那么所得具有真实后验的模型会比我们使用精确推断训练模型获得的模型更接近单峰值。


因此，估计变分近似对模型的破坏程度是很困难的。存在几种估计 $\log p(\boldsymbol v)$ 的方式。通常我们在训练模型之后估计 $\log p(\boldsymbol v;\boldsymbol \theta)$，然后发现它和 $\mathcal L(\boldsymbol v,\boldsymbol \theta,q)$ 的差距是很小的。从这里我们可以得出结论，对于特定的从学习过程中获得的 $\boldsymbol \theta$ 来说，变分近似是很准确的。然而我们无法直接得到变分近似普遍很准确或者变分近似几乎不会对学习过程产生任何负面影响这样的结论。为了准确衡量变分近似带来的危害，我们需要知道 $\boldsymbol \theta^* = \max_{\boldsymbol \theta} \log p(\boldsymbol v;\boldsymbol \theta)$。$\mathcal L(\boldsymbol v,\boldsymbol \theta,q)\approx \log p(\boldsymbol v;\boldsymbol \theta)$ 和 $\log p(\boldsymbol v;\boldsymbol \theta)\ll \log p(\boldsymbol v;\boldsymbol \theta^*)$ 同时成立是有可能的。如果存在 $\max_q \mathcal L(\boldsymbol v,\boldsymbol \theta^*,q)\ll \log p(\boldsymbol v;\boldsymbol \theta^*)$，即在 $\boldsymbol \theta^*$ 点处后验分布太过复杂使得 $q$ 分布族无法准确描述，那么学习过程永远无法到达 $\boldsymbol \theta^*$。这样的一类问题是很难发现的，因为只有在我们有一个能够找到 $\boldsymbol \theta^*$ 的较好的学习算法时，才能确定地进行上述的比较。




# 相关

- 《深度学习》花书
