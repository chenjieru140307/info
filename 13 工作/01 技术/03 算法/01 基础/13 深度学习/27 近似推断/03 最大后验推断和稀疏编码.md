

# 最大后验推断和稀疏编码




我们通常使用推断这个术语来指代给定一些其他变量的情况下计算某些变量概率分布的过程。当训练带有潜变量的概率模型时，我们通常关注于计算 $p(\boldsymbol h\mid\boldsymbol v)$。另一种可选的推断形式是计算一个缺失变量的最可能值来代替在所有可能值的完整分布上的推断。
在潜变量模型中，这意味着计算


$$\begin{aligned}
\boldsymbol h^* = \underset{\boldsymbol h}{\arg\max} \ \  p(\boldsymbol h\mid\boldsymbol v).
\end{aligned}$$


这被称作最大后验推断，简称~MAP~推断。




MAP~推断并不被视作是一种近似推断，它只是精确地计算了最有可能的一个 $\boldsymbol h^*$。然而，如果我们希望设计一个最大化 $\mathcal L(\boldsymbol v,\boldsymbol h,q)$ 的学习过程，那么把~MAP~推断视作是输出一个 $q$ 值的学习过程是很有帮助的。在这种情况下，我们可以将~MAP~推断视作是近似推断，因为它并不能提供一个最优的 $q$。




我们回过头来看看 节 把推断视作优化问题 中所描述的精确推断，它指的是关于一个在无限制的概率分布族中的分布 $q$ 使用精确的优化算法来最大化


$$\begin{aligned}
\mathcal L(\boldsymbol v,{\boldsymbol \theta},q)
 = \mathbb E_{\mathbf h\sim q}[\log p(\boldsymbol h , \boldsymbol v)] + H(q).
\end{aligned}$$


我们通过限定分布 $q$ 属于某个分布族，能够使得~MAP~推断成为一种形式的近似推断。具体地说，我们令分布 $q$ 满足一个\,Dirac分布：


$$\begin{aligned}
q(\boldsymbol h\mid\boldsymbol v) = \delta(\boldsymbol h - {\boldsymbol mu}).
\end{aligned}$$


这也意味着现在我们可以通过 $\boldsymbol mu$ 来完全控制分布 $q$。将 $\mathcal L$ 中不随 $\boldsymbol mu$ 变化的项丢弃，我们只需解决一个优化问题：


$$\begin{aligned}
\boldsymbol mu^*  =  \underset{\boldsymbol mu}{\arg\max}\ \log p(\boldsymbol h = \boldsymbol mu,\boldsymbol v),
\end{aligned}$$


这等价于~MAP~推断问题


$$\begin{aligned}
\boldsymbol h^* = \underset{\boldsymbol h}{\arg\max}\  p(\boldsymbol h\mid\boldsymbol v).
\end{aligned}$$





因此我们能够证明一种类似于~EM~算法的学习算法，其中我们轮流迭代两步，一步是用~MAP~推断估计出 $\boldsymbol h^*$，另一步是更新 $\boldsymbol \theta$ 来增大 $\log p(\boldsymbol h^*,\boldsymbol v)$。从~EM~算法角度看，这也是对 $\mathcal L$ 的一种形式的坐标上升，交替迭代时通过推断来优化关于 $q$ 的 $\mathcal L$ 以及通过参数更新来优化关于 $\boldsymbol \theta$ 的 $\mathcal L$。作为一个整体，这个算法的正确性可以得到保证，因为 $\mathcal L$ 是 $\log p(\boldsymbol v)$ 的下界。在~MAP~推断中，这个保证是无效的，因为~Dirac分布的微分熵趋近于负无穷，使得这个界会无限地松。然而，人为加入一些 $\boldsymbol mu$ 的噪声会使得这个界又有了意义。



MAP~推断作为特征提取器以及一种学习机制被广泛地应用在了深度学习中。它主要用于稀疏编码模型中。



我们回过头来看 节 稀疏编码 中的稀疏编码，稀疏编码是一种在隐藏单元上加上了诱导稀疏性的先验知识的线性因子模型。一个常用的选择是可分解的 Laplace 先验，表示为


$$\begin{aligned}
	p(h_i) = \frac{\lambda}{2}  \exp(-\lambda \vert h_i \vert).
\end{aligned}$$

可见的节点是由一个线性变化加上噪声生成的：


$$\begin{aligned}
p(\boldsymbol v\mid\boldsymbol h) = \mathcal N(\boldsymbol v;{\boldsymbol W}\boldsymbol h + \boldsymbol b,\beta^{-1}{\boldsymbol I}).
\end{aligned}$$



分布 $p(\boldsymbol h\mid\boldsymbol v)$ 难以计算，甚至难以表达。每一对 $h_i$， $h_j$ 变量都是 $\boldsymbol v$ 的母节点。这也意味着当 $\boldsymbol v$ 可被观察时，图模型包含了一条连接 $h_i$ 和 $h_j$ 的活跃路径。因此 $p(\boldsymbol h \mid\boldsymbol v)$ 中所有的隐藏单元都包含在了一个巨大的团中。如果是高斯模型，那么这些相互作用关系可以通过协方差矩阵来高效地建模。然而稀疏型先验使得这些相互作用关系并不服从高斯分布。



分布 $p(\boldsymbol x\mid\boldsymbol h)$ 的难处理性导致了对数似然及其梯度也很难得到。因此我们不能使用精确的最大似然估计来进行学习。取而代之的是，我们通过~MAP~推断以及最大化由以 $\boldsymbol h$ 为中心的\,Dirac分布所定义而成的~ELBO~来学习模型参数。



如果我们将训练集中所有的向量 $\boldsymbol h$ 拼成矩阵 $\boldsymbol H$，并将所有的向量 $\boldsymbol v$ 拼起来组成矩阵 $\boldsymbol V$，那么稀疏编码问题意味着最小化


$$\begin{aligned}
	J(\boldsymbol H,\boldsymbol W) = \sum_{i,j}^{}\vert H_{i,j}\vert + \sum_{i,j}^{}\Big(\boldsymbol V - \boldsymbol H \boldsymbol W^{\top}\Big)^2_{i,j}.
\end{aligned}$$


为了避免如极端小的 $\boldsymbol H$ 和极端大的 $\boldsymbol W$ 这样的病态的解，大多数稀疏编码的应用包含了权重衰减或者对 $\boldsymbol H$ 列范数的限制。



我们可以通过交替迭代，分别关于 $\boldsymbol H$ 和 $\boldsymbol W$ 最小化 $J$ 的方式来最小化 $J$。且两个子问题都是凸的。事实上，关于 $\boldsymbol W$ 的最小化问题就是一个线性回归问题。然而关于这两个变量同时最小化 $J$ 的问题通常并不是凸的。



关于 $\boldsymbol H$ 的最小化问题需要某些特别设计的算法，例如特征符号搜索方法~。





# 相关

- 《深度学习》花书
