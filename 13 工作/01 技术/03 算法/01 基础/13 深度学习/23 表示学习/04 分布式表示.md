

# 分布式表示



分布式表示的概念（由很多元素组合的表示，这些元素之间可以设置成可分离的）是表示学习最重要的工具之一。分布式表示非常强大，因为他们能用具有 $k$ 个值的 $n$ 个特征去描述 $k^n$ 个不同的概念。正如我们在本书中看到的，具有多个隐藏单元的神经网络和具有多个潜变量的概率模型都利用了分布式表示的策略。我们现在再介绍一个观察结果。许多深度学习算法基于的假设是，隐藏单元能够学习表示出解释数据的潜在因果因子，就像 节 半监督解释因果关系 中讨论的一样。这种方法在分布式表示上是自然的，因为表示空间中的每个方向都对应着一个不同的潜在配置变量的值。



$n$ 维二元向量是一个分布式表示的示例，有 $2^n$ 种配置，每一种都对应输入空间中的一个不同区域，如\fig?所示。这可以与\emph{符号表示}相比较，其中输入关联到单一符号或类别。如果字典中有 $n$ 个符号，那么可以想象有 $n$ 个特征监测器，每个特征探测器监测相关类别的存在。在这种情况下，只有表示空间中 $n$ 个不同配置才有可能在输入空间中刻画 $n$ 个不同的区域，如\fig?所示。这样的符号表示也被称为~one-hot~表示，因为它可以表示成相互排斥的 $n$ 维二元向量（其中只有一位是激活的）。符号表示是更广泛的非分布式表示类中的一个具体示例，它可以包含很多条目，但是每个条目没有显著意义的单独控制作用。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/b3mBIxGOpCoX.png?imageslim">
</p>
> 15.7 基于分布式表示的学习算法如何将输入空间分割成多个区域的图示。这个例子具有二元变量 $h_1$，$h_2$，$h_3$。每个特征通过为学成的线性变换设定输出阈值而定义。每个特征将 $\mathbb R^2$ 分成两个半平面。令 $h_i^+$ 表示输入点 $h_i=1$ 的集合；$h_i^-$ 表示输入点 $h_i=0$ 的集合。在这个图示中，每条线代表着一个 $h_i$ 的决策边界，对应的箭头指向边界的 $h_i^+$ 区域。整个表示在这些半平面的每个相交区域都指定一个唯一值。例如，表示值为 $[1,1,1]^\top$ 对应着区域 $h_1^+ \cap h_2^+ \cap h_3^+$。可以将以上表示和\fig?中的非分布式表示进行比较。在输入维度是 $d$ 的一般情况下，分布式表示通过半空间（而不是半平面）的交叉分割 $\mathbb R^d$。具有 $n$ 个特征的分布式表示给 $O(n^d)$ 个不同区域分配唯一的编码，而具有 $n$ 个样本的最近邻算法只能给 $n$ 个不同区域分配唯一的编码。因此，分布式表示能够比非分布式表示多分配指数级的区域。注意并非所有的 $\boldsymbol h$ 值都是可取的（这个例子中没有 $\boldsymbol h=\mathbf{0}$），在分布式表示上的线性分类器不能向每个相邻区域分配不同的类别标识；甚至深度线性阈值网络的~VC~维只有 $O(w\log w)$（其中 $w$ 是权重数目）。强表示层和弱分类器层的组合是一个强正则化项。试图学习"人"和"非人"概念的分类器不需要给表示为"戴眼镜的女人"和"没有戴眼镜的男人"的输入分配不同的类别。容量限制鼓励每个分类器关注少数几个 $h_i$，鼓励 $\boldsymbol h$ 以线性可分的方式学习表示这些类别。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/1pXcVNuQQDbF.png?imageslim">
</p>

> 15.8 最近邻算法如何将输入空间分成不同区域的图示。最近邻算法是一个基于非分布式表示的学习算法的示例。不同的非分布式算法可以具有不同的几何形状，但是它们通常将输入空间分成区域，\emph{每个区域具有不同的参数}。非分布式方法的优点是，给定足够的参数，它能够拟合一个训练集，而不需要复杂的优化算法。因为它直接为每个区域\emph{独立地}设置不同的参数。缺点是，非分布式表示的模型只能通过平滑先验来局部地泛化，因此学习波峰波谷多于样本的复杂函数时，该方法是不可行的。和分布式表示的对比，可以参照\fig?。


以下是基于非分布式表示的学习算法的示例：

+ 聚类算法，包含 $k$-means算法：每个输入点恰好分配到一个类别。

+ $k$-最近邻算法：给定一个输入，一个或几个模板或原型样本与之关联。
	在 $k >  1$ 的情况下，每个输入都使用多个值来描述，但是它们不能彼此分开控制，因此这不能算真正的分布式表示。

+ 决策树：给定输入时，只有一个叶节点（和从根到该叶节点路径上的点）是被激活的。

+ 高斯混合体和专家混合体：模板（聚类中心）或专家关联一个激活的\emph{程度}。
	和 $k$-最近邻算法一样，每个输入用多个值表示，但是这些值不能轻易地彼此分开控制。

+ 具有高斯核 （或其他类似的局部核）的核机器： 尽管每个"支持向量"或模板样本的激活程度是连续值，但仍然会出现和高斯混合体相同的问题。

+ 基于~$n$-gram~的语言或翻译模型：根据后缀的树结构划分上下文集合（符号序列）。
	例如，一个叶节点可能对应于最后两个单词 $w_1$ 和 $w_2$。
	树上的每个叶节点分别估计单独的参数（有些共享也是可能的）。




对于部分非分布式算法而言，有些输出并非是恒定的，而是在相邻区域之间内插。
参数（或样本）的数量和它们能够定义区域的数量之间仍保持线性关系。



将分布式表示和符号表示区分开来的一个重要概念是，由不同概念之间的 **共享属性而产生的泛化**。作为纯符号，"猫"和"狗"之间的距离和任意其他两种符号的距离一样。然而，如果将它们与有意义的分布式表示相关联，那么关于猫的很多特点可以推广到狗，反之亦然。例如，我们的分布式表示可能会包含诸如"具有皮毛"或"腿的数目"这类在"猫"和"狗"的嵌入上具有相同值的项。正如 节 神经语言模型 所讨论的，作用于单词分布式表示的神经语言模型比其他直接对单词~one-hot~表示进行操作的模型泛化得更好。分布式表示具有丰富的 **相似性空间**，语义上相近的概念（或输入）在距离上接近，这是纯粹的符号表示所缺少的特点。



在学习算法中使用分布式表示何时以及为什么具有统计优势？当一个明显复杂的结构可以用较少参数紧致地表示时，分布式表示具有统计上的优点。一些传统的非分布式学习算法仅仅在平滑假设的情况下能够泛化，也就是说如果 $u\approx v$，那么学习到的目标函数 $f$ 通常具有 $f(u) \approx f(v)$ 的性质。有许多方法来形式化这样一个假设，但其结果是如果我们有一个样本 $(x,y)$，并且我们知道 $f(x) \approx y$，那么我们可以选取一个估计 $\hat{f}$ 近似地满足这些限制，并且当我们移动到附近的输入 $x + \epsilon$ 时，$\hat{f}$ 尽可能少地发生改变。显然这个假设是非常有用的，但是它会遭受维数灾难：学习出一个能够在很多不同区域上增加或减少很多次的目标函数，我们可能需要至少和可区分区域数量一样多的样本。我们可以将每一个区域视为一个类别或符号：通过让每个符号（或区域）具有单独的自由度，我们可以学习出从符号映射到值的任意解码器。然而，这不能推广到新区域的新符号上。

> 一般来说，我们可能会想要学习一个函数，这个函数在指数级数量区域的表现都是不同的：在 $d$-维空间中，为了区分每一维，至少有两个不同的值。我们想要函数 $f$ 区分这 $2^d$ 个不同的区域，需要 $O(2^d)$ 量级的训练样本



如果我们幸运的话，除了平滑之外，目标函数可能还有一些其他规律。例如，具有最大池化的卷积网络可以在不考虑对象在图像中位置（即使对象的空间变换不对应输入空间的平滑变换）的情况下识别出对象。



让我们检查分布式表示学习算法的一个特殊情况，它通过对输入的线性函数进行阈值处理来提取二元特征。该表示中的每个二元特征将 $\mathbb R^d$ 分成一对半空间，如\fig?所示。$n$ 个相应半空间的指数级数量的交集确定了该分布式表示学习器能够区分多少区域。空间 $\mathbb R^d$ 中的 $n$ 个超平面的排列组合能够生成多少区间？通过应用关于超平面交集的一般结果~，我们发现~这个二元特征表示能够区分的空间数量是

$$
	\sum_{j=0}^d \binom{n}{j} = O(n^d).
$$

因此，我们会发现关于输入大小呈指数级增长，关于隐藏单元的数量呈多项式级增长。



这提供了分布式表示泛化能力的一种几何解释：$O(nd)$ 个参数（空间 $\mathbb R^d$ 中的 $n$ 个线性阈值特征）能够明确表示输入空间中 $O(n^d)$ 个不同区域。如果我们没有对数据做任何假设，并且每个区域使用唯一的符号来表示，每个符号使用单独的参数去识别 $\mathbb R^d$ 中的对应区域，那么指定 $O(n^d)$ 个区域需要 $O(n^d)$ 个样本。更一般地，分布式表示的优势还可以体现在我们对分布式表示中的每个特征使用非线性的、可能连续的特征提取器，而不是线性阈值单元的情况。在这种情况下，如果具有 $k$ 个参数的参数变换可以学习输入空间中的 $r$ 个区域（$k\ll r$），并且如果学习这样的表示有助于关注的任务，那么这种方式会比非分布式情景（我们需要 $O(r)$ 个样本来获得相同的特征，将输入空间相关联地划分成 $r$ 个区域。）泛化得更好。使用较少的参数来表示模型意味着我们只需拟合较少的参数，因此只需要更少的训练样本去获得良好的泛化。



另一个解释基于分布式表示的模型泛化能力更好的说法是，尽管能够明确地编码这么多不同的区域，但它们的容量仍然是很有限的。例如，线性阈值单元神经网络的~VC~维仅为 $O(w\log w)$， 其中 $w$ 是权重的数目~。这种限制出现的原因是，虽然我们可以为表示空间分配非常多的唯一码，但是我们不能完全使用所有的码空间，也不能使用线性分类器学习出从表示空间 $\boldsymbol h$ 到输出 $\boldsymbol y$ 的任意函数映射。因此使用与线性分类器相结合的分布式表示传达了一种先验信念，待识别的类在 $\boldsymbol h$ 代表的潜在因果因子的函数下是线性可分的。我们通常想要学习类别，例如所有绿色对象的图像集合，或是所有汽车图像集合，但不会是需要非线性~XOR~逻辑的类别。例如，我们通常不会将数据划分成所有红色汽车和绿色卡车作为一个集合，所有绿色汽车和红色卡车作为另一个集合。



到目前为止讨论的想法都是抽象的，但是它们可以通过实验验证。{Zhou-et-al-ICLR2015}发现，在~ImageNet~和~Places~基准数据集上训练的深度卷积网络中的隐藏单元学成的特征通常是可以解释的，对应人类自然分配的标签。在实践中，隐藏单元并不能总是学习出具有简单语言学名称的事物，但有趣的是，这些事物会在那些最好的计算机视觉深度网络的顶层附近出现。这些特征的共同之处在于，我们可以设想\emph{学习其中的每个特征不需要知道所有其他特征的所有配置}。{radford2015unsupervised}发现生成模型可以学习人脸图像的表示，在表示空间中的不同方向捕获不同的潜在变差因素。\fig?展示表示空间中的一个方向对应着该人是男性还是女性，而另一个方向对应着该人是否戴着眼镜。这些特征都是自动发现的，而非先验固定的。我们没有必要为隐藏单元分类器提供标签：只要该任务需要这样的特征，梯度下降就能在感兴趣的目标函数上自然地学习出语义上有趣的特征。我们可以学习出男性和女性之间的区别，或者是眼镜的存在与否，而不必通过涵盖所有这些值组合的样本来表征其他 $n-1$ 个特征的所有配置。这种形式的统计可分离性质能够泛化到训练期间从未见过的新特征上。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/pHN1hLAzu8RV.png?imageslim">
</p>
> 15.9 生成模型学到了分布式表示，能够从戴眼镜的概念中区分性别的概念。如果我们从一个戴眼镜的男人的概念表示向量开始，然后减去一个没戴眼镜的男人的概念表示向量，最后加上一个没戴眼镜的女人的概念表示向量，那么我们会得到一个戴眼镜的女人的概念表示向量。生成模型将所有这些表示向量正确地解码为可被识别为正确类别的图像。




# 相关

- 《深度学习》花书
