

# 提供发现潜在原因的线索




我们回到最初的问题之一来结束本章：什么原因能够使一个表示比另一个表示更好？首先在 节 半监督解释因果关系 中介绍的一个答案是，一个理想的表示能够区分生成数据变化的潜在因果因子，特别是那些与我们的应用相关的因素。表示学习的大多数策略都会引入一些有助于学习潜在变差因素的线索。这些线索可以帮助学习器将这些观察到的因素与其他因素分开。监督学习提供了非常强的线索：每个观察向量 $\boldsymbol x$ 的标签 $\boldsymbol y$，它通常直接指定了至少一个变差因素。更一般地，为了利用丰富的未标注数据，表示学习会使用关于潜在因素的其他不太直接的提示。这些提示包含一些我们（学习算法的设计者）为了引导学习器而强加的隐式先验信息。诸如没有免费午餐定理的这些结果表明，正则化策略对于获得良好泛化是很有必要的。当不可能找到一个普遍良好的正则化策略时，深度学习的一个目标是找到一套相当通用的正则化策略，使其能够适用于各种各样的~AI~任务（类似于人和动物能够解决的任务）。



在此，我们提供了一些通用正则化策略的列表。该列表显然是不详尽的，但是给出了一些学习算法是如何发现对应潜在因素的特征的具体示例。该列表在 {Bengio-Courville-Vincent-TPAMI-2012}的第 3.1节中提出，这里进行了部分拓展。

+ \emph{平滑}：假设对于单位 $\boldsymbol d$ 和小量 $\epsilon$ 有 $f(\boldsymbol x + \epsilon \boldsymbol d) \approx f(\boldsymbol x)$。
	这个假设允许学习器从训练样本泛化到输入空间中附近的点。
	许多机器学习算法都利用了这个想法，但它不能克服维数灾难难题。
+ \emph{线性}：很多学习算法假定一些变量之间的关系是线性的。
	这使得算法能够预测远离观测数据的点，但有时可能会导致一些极端的预测。
	大多数简单的学习算法不会做平滑假设，而会做线性假设。
	这些假设实际上是不同的，具有很大权重的线性函数在高维空间中可能不是非常平滑的。
	参看 {Goodfellow-2015-adversarial}了解关于线性假设局限性的进一步讨论。
+ \emph{多个解释因子}：许多表示学习算法受以下假设的启发，数据是由多个潜在解释因子生成的，并且给定每一个因子的状态，大多数任务都能轻易解决。
	节 半监督解释因果关系 描述了这种观点如何通过表示学习来启发半监督学习的。
	学习 $p(\boldsymbol x)$ 的结构要求学习出一些对建模 $p(\boldsymbol y\mid\boldsymbol x)$ 同样有用的特征，因为它们都涉及到相同的潜在解释因子。
	节 分布式表示 介绍了这种观点如何启发分布式表示的使用，表示空间中分离的方向对应着分离的变差因素。
+ \emph{因果因子}：该模型认为学成表示所描述的变差因素是观察数据 $\boldsymbol x$ 的成因，而并非反过来。
	正如 节 半监督解释因果关系 中讨论的，这对于半监督学习是有利的，当潜在成因上的分布发生改变，或者我们应用模型到一个新的任务上时，学成的模型都会更加鲁棒。
+ \emph{深度，或者解释因子的层次组织}：高级抽象概念能够通过将简单概念层次化来定义。
	从另一个角度来看，深度架构表达了我们认为任务应该由多个程序步骤完成的观念，其中每一个步骤回溯到先前步骤处理之后的输出。
+ \emph{任务间共享因素}：
	当多个对应到不同变量 $\mathrm y_i$ 的任务共享相同的输入 $\mathbf x$ 时，或者当每个任务关联到全局输入 $\mathbf x$ 的子集或者函数 $f^{(i)}(\mathbf x)$ 时，我们会假设每个变量 $\mathrm y_i$ 关联到来自相关因素 $\mathbf h$ 公共池的不同子集。
	因为这些子集有重叠，所以通过共享的中间表示 $ P(\mathbf h \mid \mathbf x)$ 来学习所有的 $P(\mathrm y_i \mid \mathbf x)$ 能够使任务间共享统计强度。
+ \emph{流形}：概率质量集中，并且集中区域是局部连通的，且占据很小的体积。
	在连续情况下，这些区域可以用比数据所在原始空间低很多维的低维流形来近似。
	很多机器学习算法只在这些流形上有效。
	一些机器学习算法，特别是自编码器，会试图显式地学习流形的结构。
+ \emph{自然聚类}：很多机器学习算法假设输入空间中每个连通流形可以被分配一个单独的类。
	数据分布在许多个不连通的流形上，但相同流形上数据的类别是相同的。
	这个假设激励了各种学习算法，包括正切传播、双反向传播、流形正切分类器和对抗训练。
+ \emph{时间和空间相干性}：慢特征分析和相关的算法假设，最重要的解释因子随时间变化很缓慢，或者至少假设预测真实的潜在解释因子比预测诸如像素值这类原始观察会更容易些。
	读者可以参考 节 慢特征分析 ，进一步了解这个方法。
+ \emph{稀疏性}：假设大部分特征和大部分输入不相关，如在表示猫的图像时，没有必要使用象鼻的特征。
	因此，我们可以强加一个先验，任何可以解释为"存在"或"不存在"的特征在大多数时间都是不存在的。
+ \emph{简化因子依赖}：在良好的高级表示中，因子会通过简单的依赖相互关联。
	最简单的可能是边缘独立，即 $P(\mathbf h) = \prod_i P(\mathbf h_i)$。
	但是线性依赖或浅层自编码器所能表示的依赖关系也是合理的假设。
	这可以从许多物理定律中看出来，并且假设在学成表示的顶层插入线性预测器或分解的先验。



表示学习的概念将许多深度学习形式联系在了一起。前馈网络和循环网络，自编码器和深度概率模型都在学习和使用表示。学习最佳表示仍然是一个令人兴奋的研究方向。






# 相关

- 《深度学习》花书
