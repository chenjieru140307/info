

# Bagging 和其他集成方法

Bagging：

- Bagging 是通过结合几个模型降低泛化误差的技术。
- 主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。
- 这是机器学习中常规策略的一个例子，被称为模型平均。采用这种策略的技术被称为集成方法。


奏效的原因：

- 模型平均奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。

举例：

- 假设我们有 $k$ 个回归模型。
- 假设每个模型在每个例子上的误差是 $\epsilon_i$，这个误差服从零均值方差为 $\mathbb E[\epsilon_i^2] = v$ 且协方差为 $\mathbb E[\epsilon_i \epsilon_j] = c$ 的多维正态分布。
- 通过所有集成模型的平均预测所得误差是 $\frac{1}{k} \sum_i \epsilon_i$。
- 集成预测器平方误差的期望是


$$\begin{aligned}
 \mathbb E \Bigg[\Bigg(\frac{1}{k} \sum_i \epsilon_i \Bigg)^2\Bigg] &= \frac{1}{k^2}
 \mathbb E \Bigg[\sum_i \Bigg(\epsilon_i^2 + \sum_{j \neq i} \epsilon_i \epsilon_j\Bigg)\Bigg], \\
&= \frac{1}{k} v + \frac{k-1}{k} c .
\end{aligned}$$

- 即：
  - 在误差完全相关即 $c=v$ 的情况下，均方误差减少到 $v$，所以模型平均没有任何帮助。
  - 在错误完全不相关即 $c =0$ 的情况下，该集成平方误差的期望仅为 $\frac{1}{k}v$。这意味着集成平方误差的期望会随着集成规模增大而线性减小。
  - 换言之，平均上，集成至少与它的任何成员表现得一样好，并且如果成员的误差是独立的，集成将显著地比其成员表现得更好。


Bagging：

- Bagging是一种允许重复多次使用同一种模型、训练算法和目标函数的方法。
- 具体来说，Bagging涉及构造 $k$ 个不同的数据集。
  - 每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例。这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子（如果所得训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 $2/3$ 的实例）。
- 模型 $i$ 在数据集 $i$ 上训练。每个数据集所含样本的差异导致了训练模型之间的差异。

举例：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/GwrqDDoqWUDo.png?imageslim">
</p>

说明:

- 上图描述了 Bagging 如何工作。
- 假设我们在上述数据集（包含一个 8、一个 6 和一个 9）上训练数字 8 的检测器。
  - 假设我们制作了两个不同的重采样数据集。Bagging训练程序通过有放回采样构建这些数据集。
  - 第一个数据集忽略 9 并重复 8。在这个数据集上，检测器得知数字顶部有一个环就对应于一个 8。
  - 第二个数据集中，我们忽略 6 并重复 9。在这种情况下，检测器得知数字底部有一个环就对应于一个 8。
  - 这些单独的分类规则中的每一个都是不可靠的，但如果我们平均它们的输出，就能得到鲁棒的检测器，只有当 8 的两个环都存在时才能实现最大置信度。


对于神经网络：

- 神经网络能找到足够多的不同的解，意味着他们可以从模型平均中受益(即使所有模型都在同一数据集上训练)。
- 神经网络中随机初始化的差异、小批量的随机选择、超参数的差异或不同输出的非确定性实现往往足以使得集成中的不同成员具有部分独立的误差。

注意：

- 模型平均是一个减少泛化误差的非常强大可靠的方法。在作为科学论文算法的基准时，它通常是不鼓励使用的，因为任何机器学习算法都可以从模型平均中大幅获益（以增加计算和存储为代价）。
- 不过，在实际中，或者比赛中，是可以使用的，效果很好。机器学习比赛中的取胜算法通常是使用超过几十种模型平均的方法。
- 不是所有构建集成的技术都是为了让集成模型比单一模型更加正则化。
  - 例如，一种被称为 Boosting 的技术构建比单个模型容量更高的集成模型。通过向集成逐步添加神经网络，Boosting 已经被应用于构建神经网络的集成。通过逐渐增加神经网络的隐藏单元，Boosting也可以将单个神经网络解释为一个集成。（没有明白，Boosting 怎么逐渐增加隐藏单元的？）
