



# 提前终止为何具有正则化效果



目前为止，我们已经声明提前终止是一种正则化策略，但我们只通过展示验证集误差的学习曲线是一个 U 型曲线来支持这种说法。

提前终止正则化模型的真正机制是什么呢？

Bishop1995 和 Sjoberg95 认为提前终止可以将优化过程的参数空间限制在初始参数值 $\boldsymbol \theta_0$ 的小邻域内。更具体地，想象用学习率 $\epsilon$ 进行 $\tau$ 个优化步骤（对应于 $\tau$ 个训练迭代）。我们可以将 $\epsilon \tau$ 作为有效容量的度量。假设梯度有界，限制迭代的次数和学习速率能够限制从 $\boldsymbol \theta_0$ 到达的参数空间的大小，如图所示。在这个意义上，$\epsilon \tau$ 的效果就好像是权重衰减系数的倒数。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/toTARYy61pYq.png?imageslim">
</p>

> 图 7.4 提前终止效果的示意图。**(左)** 实线轮廓线表示负对数似然的轮廓。虚线表示从原点开始的 SGD 所经过的轨迹。提前终止的轨迹在较早的点 $\tilde \boldsymbol w$ 处停止，而不是停止在最小化代价的点 $\boldsymbol w^*$ 处。**(右)** 为了对比，使用 $L^2$ 正则化效果的示意图。虚线圆圈表示 $L^2$ 惩罚的轮廓，$L^2$ 惩罚使得总代价的最小值比非正则化代价的最小值更靠近原点。





事实上，在二次误差的简单线性模型和简单的梯度下降情况下，我们可以展示提前终止相当于 $L^2$ 正则化。

为了与经典 $L^2$ 正则化比较，我们只考察唯一的参数是线性权重（$\boldsymbol \theta = \boldsymbol w$）的简单情形。我们在权重 $\boldsymbol w$ 的经验最佳值 $\boldsymbol w^*$ 附近以二次近似建模代价函数 $J$：


$$\begin{aligned}
 \hat J(\boldsymbol \theta) = J(\boldsymbol w^*) + \frac{1}{2}  (\boldsymbol w- \boldsymbol w^*)^\top \boldsymbol H  (\boldsymbol w - \boldsymbol w^*),
\end{aligned}$$


其中 $\boldsymbol H$ 是 $J$ 关于 $\boldsymbol w$ 在 $\boldsymbol w^*$ 点的 Hessian。鉴于假设 $\boldsymbol w^*$ 是 $J(\boldsymbol w)$ 的最小点，我们知道 $\boldsymbol H$ 为半正定。在局部泰勒级数逼近下，梯度由下式给出：


$$\begin{aligned}
 \nabla_{\boldsymbol w} \hat J (\boldsymbol w) = \boldsymbol H (\boldsymbol w - \boldsymbol w^*).
\end{aligned}$$



接下来我们研究训练时参数向量的轨迹。为简化起见，我们将参数向量初始化为原点(注：对于神经网络，我们需要打破隐藏单元间的对称平衡因此不能将所有参数都初始化为 $\mathbf{0}$（如 节 基于梯度的学习 所讨论的）。然而，对于其他任何初始值 $\boldsymbol w_{(0)}$ 该论证都成立)，也就是 $\boldsymbol w^{(0)} = 0$。我们通过分析 $\hat{J}$ 上的梯度下降来研究 $J$ 上近似的梯度下降的效果：


$$\begin{aligned}
\boldsymbol w^{(\tau)} &= \boldsymbol w^{(\tau-1)} -\epsilon \nabla_{\boldsymbol w} \hat{J}( \boldsymbol w^{(\tau-1)} ) \\
&=  \boldsymbol w^{(\tau-1)}  - \epsilon  \boldsymbol H ( \boldsymbol w^{(\tau-1)} -  \boldsymbol w^* ), \\
\boldsymbol w^{(\tau)}  -  \boldsymbol w^* &= (\boldsymbol I - \epsilon  \boldsymbol H) ( \boldsymbol w^{(\tau-1)} -  \boldsymbol w^* ).
 \end{aligned}$$


现在让我们在 $\boldsymbol H$ 特征向量的空间中改写表达式，利用 $\boldsymbol H$ 的特征分解：$\boldsymbol H = \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top$，其中 $\boldsymbol \Lambda$ 是对角矩阵，$\boldsymbol Q$ 是特征向量的一组标准正交基。


$$\begin{aligned}
\boldsymbol w^{(\tau)}  -  \boldsymbol w^* &= (\boldsymbol I - \epsilon \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top) ( \boldsymbol w^{(\tau-1)} -  \boldsymbol w^* ) \\
\boldsymbol Q^\top (\boldsymbol w^{(\tau)}  -  \boldsymbol w^*) &= (\boldsymbol I - \epsilon \boldsymbol \Lambda)\boldsymbol Q^\top ( \boldsymbol w^{(\tau-1)} -  \boldsymbol w^* )
\end{aligned}$$


假定 $\boldsymbol w^{(0)} = 0$ 并且 $\epsilon$ 选择得足够小以保证 $|1 - \epsilon \lambda_i |<1$，经过 $\tau$ 次参数更新后轨迹如下：

$$\begin{aligned}
\boldsymbol Q^\top  \boldsymbol w^{(\tau)} = [\boldsymbol I - (\boldsymbol I - \epsilon \boldsymbol \Lambda)^\tau] \boldsymbol Q^\top  \boldsymbol w^* .
\end{aligned}$$


现在，$\begin{aligned} \tilde{w} &=\left(Q \Lambda Q^{\top}+\alpha I\right)^{-1} Q \Lambda Q^{\top} w^{*} \\ &=\left[Q(\Lambda+\alpha I) Q^{\top}\right]^{-1} Q \Lambda Q^{\top} w^{*} \\ &=Q(\Lambda+\alpha I)^{-1} \Lambda Q^{\top} w^{*} \end{aligned}$ 中 $\boldsymbol Q^\top \tilde \boldsymbol w$ 的表达式能被重写为：
$$\begin{aligned}
\boldsymbol Q^\top  \tilde \boldsymbol w &= (\boldsymbol \Lambda + \alpha \boldsymbol I)^{-1} \boldsymbol \Lambda \boldsymbol Q^\top  \boldsymbol w^*, \\
\boldsymbol Q^\top  \tilde \boldsymbol w &= [\boldsymbol I - (\boldsymbol \Lambda + \alpha \boldsymbol I)^{-1} \alpha] \boldsymbol Q^\top  \boldsymbol w^*.
\end{aligned}$$


比较 $Q^{\top} w^{(\tau)}=\left[I-(I-\epsilon \Lambda)^{\tau}\right] Q^{\top} w^{*}$ 和 $\boldsymbol{Q}^{\top} \tilde{\boldsymbol{w}}=\left[\boldsymbol{I}-(\boldsymbol{\Lambda}+\alpha \boldsymbol{I})^{-1} \alpha\right] \boldsymbol{Q}^{\top} \boldsymbol{w}^{*}$ ，我们能够发现，如果超参数 $\epsilon,\alpha$ 和 $\tau$ 满足如下：


$$\begin{aligned}
(\boldsymbol I - \epsilon \boldsymbol \Lambda)^\tau =  (\boldsymbol \Lambda + \alpha \boldsymbol I)^{-1} \alpha,
\end{aligned}$$


那么 $L^2~$ 正则化和提前终止可以被看作是等价的（至少在目标函数的二次近似下）。进一步取对数，使用 $\log~(1+x)$ 的级数展开，我们可以得出结论：如果所有 $\lambda_i$ 是小的（即 $\epsilon \lambda_i \ll 1$ 且 $\lambda_i / \alpha \ll 1$），那么


$$\begin{aligned}
\tau \approx \frac{1}{\epsilon \alpha}, \\
\alpha \approx \frac{1}{\tau \epsilon}.
\end{aligned}$$


也就是说，在这些假设下，训练迭代次数 $\tau$ 起着与 $L^2$ 参数成反比的作用，$\tau \epsilon$ 的倒数与权重衰减系数的作用类似。

在大曲率（目标函数）方向上的参数值受正则化影响小于小曲率方向。当然，在提前终止的情况下，这实际上意味着在大曲率方向的参数比较小曲率方向的参数更早地学习到。

本节中的推导表明长度为 $\tau$ 的轨迹结束于 $L^2$ 正则化目标的极小点。当然，提前终止比简单的轨迹长度限制更丰富；取而代之，提前终止通常涉及监控验证集误差，以便在空间特别好的点处终止轨迹。因此提前终止比权重衰减更具有优势，提前终止能自动确定正则化的正确量，而权重衰减需要进行多个不同超参数值的训练实验。






# 相关

- 《深度学习》花书
