

# 作为约束的范数惩罚

预备知识：

- 约束优化


考虑经过参数范数正则化的代价函数：

$$
\begin{aligned}
\tilde{J}(\boldsymbol \theta;\boldsymbol X, \boldsymbol y) = J(\boldsymbol \theta;\boldsymbol X, \boldsymbol y) + \alpha \Omega(\boldsymbol \theta) .
\end{aligned}\tag{7.25}
$$

回顾约束优化内容，我们可以构造一个广义 Lagrange 函数来最小化带约束的函数，即在原始目标函数上添加一系列惩罚项。每个惩罚是一个被称为 Karush–Kuhn–Tucker乘子的系数以及一个表示约束是否满足的函数之间的乘积。如果我们想约束 $\Omega(\boldsymbol \theta)$ 小于某个常数 $k$，我们可以构建广义 Lagrange 函数

$$\begin{aligned}
\mathcal L(\boldsymbol \theta, \alpha; \boldsymbol X, \boldsymbol y) = J(\boldsymbol \theta; \boldsymbol X, \boldsymbol y) + \alpha (\Omega(\boldsymbol \theta) - k).
\end{aligned}\tag{7.26}
$$

这个约束问题的解由下式给出

$$
\begin{aligned}
\boldsymbol \theta^* = \underset{\boldsymbol \theta}{ \arg \min } \underset{\alpha, \alpha \geq 0}{\max} \mathcal L(\boldsymbol \theta, \alpha).
\end{aligned}\tag{7.27}
$$

如 节 约束优化 中描述的，解决这个问题我们需要对 $\boldsymbol \theta$ 和 $\alpha$ 都做出调整。节 实例：线性最小二乘 给出了一个带 $L^2$ 约束的线性回归实例。还有许多不同的优化方法，有些可能会使用梯度下降而其他可能会使用梯度为 0 的解析解，但在所有过程中 $\alpha$ 在 $\Omega(\boldsymbol \theta) > k$ 时必须增加，在 $\Omega(\boldsymbol \theta) < k$ 时必须减小。所有正值的 $\alpha$ 都鼓励 $\Omega(\boldsymbol \theta)$ 收缩。最优值 $\alpha^*$ 也将鼓励 $\Omega(\boldsymbol \theta)$ 收缩，但不会强到使得 $\Omega(\boldsymbol \theta)$ 小于 $k$。

为了洞察约束的影响，我们可以固定 $\alpha^*$，把这个问题看成只跟 $\boldsymbol \theta$ 有关的函数：<span style="color:red;">嗯。</span>


$$\begin{aligned}
\boldsymbol \theta^* =  \underset{\boldsymbol \theta}{ \arg \min } ~\mathcal L(\boldsymbol \theta, \alpha^*) =
\underset{\boldsymbol \theta}{ \arg \min }
J(\boldsymbol \theta; \boldsymbol X, \boldsymbol y) + \alpha^* \Omega(\boldsymbol \theta).
\end{aligned}\tag{7.28}
$$


这和最小化 $\tilde J$ 的正则化训练问题是完全一样的。因此，我们可以把参数范数惩罚看作对权重强加的约束。如果 $\Omega$ 是 $L^2$ 范数，那么权重就是被约束在一个 $L^2$ 球中。如果 $\Omega$ 是 $L^1$ 范数，那么权重就是被约束在一个 $L^1$ 范数限制的区域中。通常我们不知道权重衰减系数 $\alpha^*$ 约束的区域大小，因为 $\alpha^*$ 的值不直接告诉我们 $k$ 的值。原则上我们可以解得 $k$，但 $k$ 和 $\alpha^*$ 之间的关系取决于 $J$ 的形式。虽然我们不知道约束区域的确切大小，但我们可以通过增加或者减小 $\alpha$ 来大致扩大或收缩约束区域。较大的 $\alpha$，将得到一个较小的约束区域。较小的 $\alpha$，将得到一个较大的约束区域。<span style="color:red;">嗯。</span>


有时候，我们希望使用显式的限制，而不是惩罚。如第 4.4 节所述，我们可以修改下降算法（如随机梯度下降算法），使其先计算 $J(\boldsymbol \theta)$ 的下降步，然后将 $\boldsymbol \theta$ 投影到满足 $\Omega(\boldsymbol \theta) < k$ 的最近点。如果我们知道什么样的 $k$ 是合适的，而不想花时间寻找对应于此 $k$ 处的 $\alpha$ 值，这会非常有用。<span style="color:red;">没明白。</span>

另一个使用显式约束和重投影而不是使用惩罚强加约束的原因是惩罚可能会导致目标函数非凸而使算法陷入局部极小(对应于小的 $\boldsymbol \theta$）。当训练神经网络时，这通常表现为训练带有几个"死亡单元"的神经网络。<span style="color:red;">嗯。</span>这些单元不会对网络学到的函数有太大影响，因为进入或离开它们的权重都非常小。当使用权重范数的惩罚训练时，即使可以通过增加权重以显著减少 $J$，这些配置也可能是局部最优的。因为重投影实现的显式约束不鼓励权重接近原点，所以在这些情况下效果更好。通过重投影实现的显式约束只在权重变大并试图离开限制区域时产生作用。<span style="color:red;">没明白。</span>

最后，因为重投影的显式约束还对优化过程增加了一定的稳定性，所以这是另一个好处。当使用较高的学习率时，很可能进入正反馈，即大的权重诱导大梯度，然后使得权重获得较大更新。如果这些更新持续增加权重的大小，$\boldsymbol \theta$ 就会迅速增大，直到离原点很远而发生溢出。重投影的显式约束可以防止这种反馈环引起权重无限制地持续增加。{Hinton-et-al-arxiv2012} 建议结合使用约束和高学习速率，这样能更快地探索参数空间，并保持一定的稳定性。<span style="color:red;">为什么可以保持稳定性？</span>



{Hinton-et-al-arxiv2012} 尤其推荐由 {Srebro05} 引入的策略：约束神经网络层的权重矩阵每列的范数，而不是限制整个权重矩阵的~Frobenius~范数。分别限制每一列的范数可以防止某一隐藏单元有非常大的权重。如果我们将此约束转换成~Lagrange~函数中的一个惩罚，这将与 $L^2$ 权重衰减类似但每个隐藏单元的权重都具有单独的~KKT~乘子。每个~KKT~乘子分别会被动态更新，以使每个隐藏单元服从约束。在实践中，列范数的限制总是通过重投影的显式约束来实现。



# 相关

- 《深度学习》花书
