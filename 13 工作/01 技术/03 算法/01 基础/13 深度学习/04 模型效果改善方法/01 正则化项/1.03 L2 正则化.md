

## $L^2$ 参数正则化

$L^2$ 参数正则化

亦称为岭回归



$L^2$ 参数正则化：

- 是最简单而又最常见的参数范数惩罚，通常被称为权重衰减。


介绍：

- 正则项为：

$$
\Omega(\boldsymbol \theta) = \frac{1}{2} \|{\boldsymbol w}\|_2^2
$$

- 效果：
  - 使权重更加接近原点
    - 注：更一般地，我们可以将参数正则化为接近空间中的任意特定点，令人惊讶的是这样也仍有正则化效果，但是特定点越接近真实值结果越好。当我们不知道正确的值应该是正还是负时，零是有意义的默认值。由于模型参数正则化为零的情况更为常见，我们将只探讨这种特殊情况。

查看正则化后的梯度表现：

- 为了简单起见，我们假定其中没有偏置参数，因此 $\boldsymbol \theta$ 就是 $\boldsymbol w$。
- 目标函数：

$$\begin{aligned}
\tilde{J}(\boldsymbol w;\boldsymbol X, \boldsymbol y) =& J(\boldsymbol w;\boldsymbol X, \boldsymbol y)+\alpha\cdot \frac{1}{2} \|{\boldsymbol w}\|_2^2
\\=&J(\boldsymbol w;\boldsymbol X, \boldsymbol y)+\frac{\alpha}{2} \boldsymbol w^\top \boldsymbol w,
\end{aligned}$$

- 梯度为

$$\begin{aligned}
\nabla_{\boldsymbol w} \tilde{J}(\boldsymbol w;\boldsymbol X,\boldsymbol y) =\nabla_{\boldsymbol w} J(\boldsymbol w;\boldsymbol X, \boldsymbol y)+\alpha \boldsymbol w .
\end{aligned}$$

- 使用单步梯度下降更新权重，即执行以下更新：

$$
\begin{aligned}
&\boldsymbol w \leftarrow \boldsymbol w - \epsilon(\alpha \boldsymbol w + \nabla_{\boldsymbol w} J(\boldsymbol w;\boldsymbol X, \boldsymbol y)).
\\\Rightarrow &\boldsymbol w \leftarrow (1-\epsilon \alpha)\boldsymbol w - \epsilon \nabla_{\boldsymbol w} J(\boldsymbol w;\boldsymbol X, \boldsymbol y).
\end{aligned}
$$

- 说明：
  - 可见，加入权重衰减后引起了学习规则的修改，即相对于原来的 $\boldsymbol w \leftarrow \boldsymbol w - \epsilon \nabla_{\boldsymbol w} J(\boldsymbol w;\boldsymbol X, \boldsymbol y)$ ，在每步执行梯度更新之前，先收缩了权重向量（将权重向量乘以一个常数因子）
- 在整个训练过程中的含义：
  - 没有正则化时：
    - 令 $\boldsymbol w^*$ 为未正则化的目标函数取得最小训练误差时的权重向量，即 $\boldsymbol w^* = \arg \min _{\boldsymbol w} J(\boldsymbol w)$。
    - 我们在 $\boldsymbol w^*$ 的邻域对目标函数做二次近似，即泰勒展开。
    - 此时，如果目标函数确实是二次的(如以均方误差拟合线性回归模型的情况)，则该近似是完美的。近似的 $\hat J(\boldsymbol \theta)$ 如下：

    $$\begin{aligned}
    \hat J(\boldsymbol \theta) = J(\boldsymbol w^*) + \frac{1}{2}(\boldsymbol w - \boldsymbol w^*)^\top \boldsymbol H (\boldsymbol w - \boldsymbol w^*),\tag{7.6}
    \end{aligned}$$

    - 说明：
        - 其中 $\boldsymbol H$ 是 $J$ 在 $\boldsymbol w^*$ 处计算的 Hessian 矩阵(关于 $\boldsymbol w$)。
        - 因为 $\boldsymbol w^*$ 被定义为最优，即梯度消失为 $0$，所以该二次近似中没有一阶项。
        - 同样地，因为 $\boldsymbol w^*$ 是 $J$ 的一个最优点，我们可以得出 $\boldsymbol H$ 是半正定的结论。（这个半正定怎么得出的，还没有很熟悉泰勒公式与矩阵）
    - 当 $\hat J$ 取得最小时，其梯度为 $0$。

    $$\begin{aligned}
    \nabla_{\boldsymbol w} \hat{J}(\boldsymbol w) = \boldsymbol H (\boldsymbol w - \boldsymbol w^*)=0\tag{7.7}
    \end{aligned}$$

  - 有正则化时：
    - 我们使用变量 $\tilde{\boldsymbol w}$ 表示此时的最优点。
    - 将此时的 $\tilde{\boldsymbol w}$ 带入上式：

    $$
    \begin{aligned}
    \alpha \tilde{\boldsymbol{w}}+\boldsymbol{H}\left(\tilde{\boldsymbol{w}}-\boldsymbol{w}^{*}\right)=&0
    \\\Rightarrow (\boldsymbol{H}+\alpha \boldsymbol{I}) \tilde{\boldsymbol{w}}=&\boldsymbol{H} \boldsymbol{w}^{*}
    \\\Rightarrow \tilde{\boldsymbol{w}}=&(\boldsymbol{H}+\alpha \boldsymbol{I})^{-1} \boldsymbol{H} \boldsymbol{w}^{*}
    \end{aligned}
    $$

    - 说明：
      - 当 $\alpha$ 趋向于 $0$ 时，正则化的解 $\tilde{\boldsymbol w}$ 会趋向 $\boldsymbol w^*$。
      - 那么当 $\alpha$ 增加时会发生什么呢？
        - 因为 $\boldsymbol H$ 是实对称的，所以我们可以将其分解为一个对角矩阵 $\boldsymbol \Lambda$ 和一组特征向量的标准正交基 $\boldsymbol Q$，并且有 $\boldsymbol H = \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top$。
        - 将其应用于 $\tilde{\boldsymbol{w}}=(\boldsymbol{H}+\alpha \boldsymbol{I})^{-1} \boldsymbol{H} \boldsymbol{w}^{*}$，
        - 可得：$\begin{aligned}\tilde \boldsymbol w &= ( \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top + \alpha \boldsymbol I)^{-1} \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top \boldsymbol w^* \\&=  [ \boldsymbol Q( \boldsymbol \Lambda+ \alpha \boldsymbol I)  \boldsymbol Q^\top ]^{-1} \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top \boldsymbol w^*\\\end{aligned}$
        - 说明：
          - 可以看到，权重衰减的效果是沿着由 $\boldsymbol H$ 的特征向量所定义的轴缩放 $\boldsymbol w^*$。
          - 具体来说，我们会根据 $\frac{\lambda_i}{\lambda_i + \alpha}$ 因子缩放与 $\boldsymbol H$ 第 $i$ 个特征向量对齐的 $\boldsymbol w^*$ 的分量。
            - 缩放原理如下图所示
              - 下图为特征向量和特征值的作用效果的一个实例。在这里，矩阵 $\boldsymbol{A}$ 有两个标准正交的特征向量，对应特征值为 $\lambda_{1}$ 的 $\boldsymbol{v}^{(1)}$ 以及对应特征值为 $\lambda_{2}$ 的 $\boldsymbol{v}^{(2)}$。
                - (左)我们画出了所有单位向量 $\boldsymbol{u} \in \mathbb{R}^{2}$ 的集合，构成一个单位圆。
                - (右)我们画出了所有 $\boldsymbol{A} \boldsymbol{u}$ 点的集合。
              - 通过观察 $\boldsymbol{A}$ 拉伸单位圆的方式，我们可以看到它将 $\boldsymbol{v}^{(2)}$ 方向的空间拉伸了 $\lambda_{i}$ 倍。

            <p align="center">
                <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190721/mUVsTJNE2NTH.png?imageslim">
            </p>

          - 此时，沿着 $\boldsymbol H$ 特征值较大的方向(如 $\lambda_i \gg \alpha$)正则化的影响较小。而 $\lambda_i \ll \alpha$ 的分量将会收缩到几乎为零。如下图所示。
            <p align="center">
                <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20190713/jiUcnJ82NwwQ.png?imageslim">
            </p>

            - 说明：$L^2$（或权重衰减）正则化对最佳 $\boldsymbol w$ 值的影响。实线椭圆表示没有正则化目标的等值线。虚线圆圈表示 $L^2$ 正则化项的等值线。在 $\tilde{\boldsymbol w}$ 点，这两个竞争目标达到平衡。目标函数 $J$ 的 Hessian 的第一维特征值很小。当从 $\boldsymbol w^*$ 水平移动时，目标函数不会增加得太多。因为目标函数对这个方向没有强烈的偏好，所以正则化项对该轴具有强烈的影响。正则化项将 $w_1$ 拉向零。而目标函数对沿着第二维远离 $\boldsymbol w^*$ 的移动非常敏感。对应的特征值较大，表示高曲率。因此，权重衰减对 $w_2$ 的位置影响相对较小。


只有在显著减小目标函数方向上的参数会保留得相对完好。在无助于目标函数减小的方向（对应 Hessian 矩阵较小的特征值）上改变参数不会显著增加梯度。这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。


目前为止，我们讨论了权重衰减对优化一个抽象通用的二次代价函数的影响。这些影响具体是怎么和机器学习关联的呢？我们可以研究线性回归，它的真实代价函数是二次的，因此我们可以使用相同的方法分析。再次应用分析，我们会在这种情况下得到相同的结果，但这次我们使用训练数据的术语表述。线性回归的代价函数是平方误差之和：

$$
\begin{aligned}
(\boldsymbol X \boldsymbol w - \boldsymbol y)^\top (\boldsymbol X \boldsymbol w - \boldsymbol y).
\end{aligned}\tag{7.14}
$$

我们添加 $L^2$ 正则项后，目标函数变为

$$
\begin{aligned}
(\boldsymbol X \boldsymbol w - \boldsymbol y)^\top (\boldsymbol X \boldsymbol w - \boldsymbol y) + \frac{1}{2}\alpha \boldsymbol w^\top \boldsymbol w.
\end{aligned}\tag{7.15}
$$

这将普通方程的解从

$$
\begin{aligned}
\boldsymbol w = (\boldsymbol X^\top \boldsymbol X)^{-1} \boldsymbol X^\top \boldsymbol y
\end{aligned}\tag{7.16}
$$

变为

$$
\begin{aligned}
\boldsymbol w = (\boldsymbol X^\top \boldsymbol X + \alpha \boldsymbol I)^{-1} \boldsymbol X^\top \boldsymbol y .
\end{aligned}\tag{7.17}
$$

式 7.16 中的矩阵 $\boldsymbol X^\top\boldsymbol X$ 与协方差矩阵 $\frac{1}{m}\boldsymbol X^\top\boldsymbol X$ 成正比。$L^2$ 正则项将这个矩阵替换为式 7.17 中的 $(\boldsymbol X^\top \boldsymbol X + \alpha \boldsymbol I)^{-1}$ 这个新矩阵与原来的是一样的，不同的仅仅是在对角加了 $\alpha$。这个矩阵的对角项对应每个输入特征的方差。<span style="color:red;">这个矩阵的对角项对应每个输入特征的方差 是什么意思？</span>我们可以看到，$L^2$ 正则化能让学习算法"感知"到具有较高方差的输入 $\boldsymbol x$，因此与输出目标的协方差较小（相对增加方差）的特征的权重将会收缩。<span style="color:red;">什么是协方差较小的特征的权重？</span>


# 相关

- 《深度学习》花书
