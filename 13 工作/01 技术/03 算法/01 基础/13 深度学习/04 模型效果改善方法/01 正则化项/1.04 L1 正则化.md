
# 可以补充进来的

- 惩罚系数 $\alpha$ 要怎么设定呢？
- 那么有没有每层使用不同的 $\alpha$ 系数的？
- 为什么叫做岭回归这个名字？










# $L^1$ 参数正则化

介绍：

- 对参数 $\boldsymbol w$ 的 $L^1$ 正则化被定义为：

$$
\begin{aligned}
\Omega(\boldsymbol \theta) = \|{ \boldsymbol w }\|_1 = \sum_i | w_i |,
\end{aligned}\tag{7.18}
$$

- 即各个参数的绝对值之和
  - 注：如同 $L^2$ 正则化，我们能将参数正则化到其他非零值 $\boldsymbol w^{(o)}$。在这种情况下，$L^1$ 正则化将会引入不同的项 $\Omega(\boldsymbol \theta)=\|\boldsymbol w - \boldsymbol w^{(o)} \|_1 = \sum_i | w_i - w_i^{(o)} |$。)。


- 此时，正则化的目标函数 $\tilde{J}(\boldsymbol w;\boldsymbol X, \boldsymbol y)$ 为：


$$
\begin{aligned}
\tilde{J}(\boldsymbol w;\boldsymbol X, \boldsymbol y) = \alpha \| \boldsymbol w \|_1 +  J(\boldsymbol w;\boldsymbol X, \boldsymbol y) ,
\end{aligned}\tag{7.19}
$$

- 对应的梯度(实际上是次梯度)：

$$
\nabla_{w} \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha \operatorname{sign}(\boldsymbol{w})+\nabla_{w} J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})\tag{7.20}
$$

- 说明：
  - 其中 $\text{sign}(\boldsymbol w)$ 只是简单地取 $\boldsymbol w$ 各个元素的正负号。

与 L2 对比：

- L1 正则化目标函数的梯度为：

$$
\nabla_{w} \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha \operatorname{sign}(\boldsymbol{w})+\nabla_{w} J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})\tag{7.20}
$$

- L2 正则化目标函数的梯度为：

$$\begin{aligned}
\nabla_{\boldsymbol w} \tilde{J}(\boldsymbol w;\boldsymbol X,\boldsymbol y) =\nabla_{\boldsymbol w} J(\boldsymbol w;\boldsymbol X, \boldsymbol y)+\alpha \boldsymbol w .
\end{aligned}$$

- 说明：
  - 可见，$L^1$ 的正则化对梯度的影响不再是线性地缩放每个 $w_i$；而是添加了一项与 $\text{sign}(w_i)$ 同号的常数。
  - 使用这种形式的梯度之后，我们不一定能得到 $J(\boldsymbol X, \boldsymbol y;\boldsymbol w)$ 二次近似的直接算术解（$L^2$ 正则化时可以）。


（下面的没看懂）

简单线性模型具有二次代价函数，我们可以通过泰勒级数表示。或者我们可以设想，这是逼近更复杂模型的代价函数的截断泰勒级数。

在这个设定下，梯度由下式给出：（为什么是这样的表达？）

$$\begin{aligned}
\nabla_{\boldsymbol w} \hat{J}(\boldsymbol w) = \boldsymbol H (\boldsymbol w - \boldsymbol w^*),
\end{aligned}$$

同样，$\boldsymbol H$ 是 $J$ 在 $\boldsymbol w^*$ 处的 Hessian 矩阵(关于 $\boldsymbol w$)。

由于 $L^1$ 惩罚项在完全一般化的 Hessian 的情况下，无法得到直接清晰的代数表达式，因此我们将进一步简化假设 Hessian 是对角的，即 $\boldsymbol H = \text{diag}([H_{1,1},\dots, H_{n,n}])$，其中每个 $H_{i,i}>0$。如果线性回归问题中的数据已被预处理（如可以使用 PCA），去除了输入特征之间的相关性，那么这一假设成立。<span style="color:red;">嗯。</span>

我们可以将 $L^1$ 正则化目标函数的二次近似分解成关于参数的求和：

$$
\hat{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=J\left(\boldsymbol{w}^{*} ; \boldsymbol{X}, \boldsymbol{y}\right)+\sum_{i}\left[\frac{1}{2} H_{i, i}\left(w_{i}-w_{i}^{*}\right)^{2}+\alpha\left|w_{i}\right|\right]\tag{7.22}
$$

如下列形式的解析解（对每一维 $i$）可以最小化这个近似代价函数：

$$
w_{i}=\operatorname{sign}\left(w_{i}^{*}\right) \max \left\{\left|w_{i}^{*}\right|-\frac{\alpha}{H_{i, i}}, 0\right\}\tag{7.23}
$$

对每个 $i$，考虑 $w_i^* > 0$ 的情形，会有两种可能结果：



- $w_i^* \leq \frac{\alpha}{H_{i,i}}$ 的情况。正则化后目标中的 $w_i$ 最优值是 $w_i = 0$。这是因为在方向 $i$ 上 $J(\boldsymbol w; \boldsymbol X, \boldsymbol y)$ 对 $\hat J(\boldsymbol w; \boldsymbol X, \boldsymbol y)$ 的贡献被抵消，$L^1$ 正则化项将 $w_i$ 推至 $0$。
- $w_i^* > \frac{\alpha}{H_{i,i}}$ 的情况。在这种情况下，正则化不会将 $w_i$ 的最优值推至 0，而仅仅在那个方向上移动 $\frac{\alpha}{H_{i,i}}$ 的距离。


$w_i^* < 0$ 的情况与之类似，但是 $L^1$ 惩罚项使 $w_i$ 更接近 0(增加 $\frac{\alpha}{H_{i,i}}$)或者为 0。

相比 $L^2$ 正则化，$L^1$ 正则化会产生更稀疏的解。此处稀疏性指的是最优值中的一些参数为 $0$。和 $L^2$ 正则化相比，$L^1$ 正则化的稀疏性具有本质的不同。式 7.13 给出了 $L^2$ 正则化的解 $\tilde \boldsymbol w$。如果我们使用 Hessian 矩阵 $\boldsymbol H$ 为对角正定矩阵的假设（与 $L^1$ 正则化分析时一样），重新考虑这个等式，我们发现 $\tilde{w_i} = \frac{H_{i,i}}{H_{i,i} + \alpha} w_i^*$。如果 $w_i^*$ 不是零，那么 $\tilde{w_i}$ 也会保持非零。这表明 $L^2$ 正则化不会使参数变得稀疏，而 $L^1$ 正则化有可能通过足够大的 $\alpha$ 实现稀疏。

由 $L^1$ 正则化导出的稀疏性质已经被广泛地用于**特征选择机制**。特征选择从可用的特征子集选择出有意义的特征，化简机器学习问题。著名的 LASSO~（Least Absolute Shrinkage and Selection Operator）模型将 $L^1$ 惩罚和线性模型结合，并使用最小二乘代价函数。$L^1$ 惩罚使部分子集的权重为零，表明相应的特征可以被安全地忽略。<span style="color:red;">太帅了。</span>

在第 5.6.1 节，我们看到许多正则化策略可以被解释为 MAP 贝叶斯推断，特别是 $L^2$ 正则化相当于权重是高斯先验的 MAP 贝叶斯推断。对于 $L^1$ 正则化，用于正则化代价函数的惩罚项 $\alpha \Omega(\boldsymbol w) =  \alpha \sum_i |w_i |$ 与通过 MAP 贝叶斯推断最大化的对数先验项是等价的（$\boldsymbol w \in \mathbb R^n$ 并且权重先验是各向同性的拉普拉斯分布（式 3.26 ））：<span style="color:red;">没懂。</span>


$$
\begin{aligned}
\log p(\boldsymbol w) = \sum_i \log \text{Laplace}(w_i;0,\frac{1}{\alpha}) =
-\alpha \|{\boldsymbol w}\|_1 + n \log \alpha - n \log 2.
\end{aligned}\tag{7.24}
$$


因为是关于 $\boldsymbol w$ 最大化进行学习，我们可以忽略 $\log \alpha - \log 2$ 项，因为它们与 $\boldsymbol w$ 无关。


# 相关

- 《深度学习》花书
