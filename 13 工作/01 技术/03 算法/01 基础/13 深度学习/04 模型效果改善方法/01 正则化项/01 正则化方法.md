# 正则化方法

## 介绍

- 许多正则化方法通过对目标函数 $J$ 添加一个参数范数惩罚 $\Omega(\boldsymbol \theta)$ 来限制模型的学习能力。
  - 如神经网络、线性回归或逻辑回归

描述：

- 我们将正则化后的目标函数记为 $\tilde{J}$：


$$
\begin{aligned}
\tilde{J}(\boldsymbol \theta;\boldsymbol X, \boldsymbol y) = J(\boldsymbol \theta;\boldsymbol X, \boldsymbol y) + \alpha \Omega(\boldsymbol \theta),
\end{aligned}
$$

- 其中
  - $\alpha \in [0, \infty)$ 是权衡范数惩罚项 $\Omega$ 和标准目标函数 $J(\boldsymbol X;\boldsymbol \theta)$ 相对贡献的超参数。
    - 将 $\alpha$ 设为 0 表示没有正则化。
    - $\alpha$ 越大，对应正则化惩罚越大。（要怎么设定好？）
- 当我们的训练算法最小化正则化后的目标函数 $\tilde{J}$ 时，
  - 它会：
    - 降低原始目标 $J$ 关于训练数据的误差
    - 并同时减小在某些衡量标准下参数 $\boldsymbol \theta$（或参数子集）的规模。
  - 选择不同的参数范数 $\Omega$ 会偏好不同的解。


使用：

- 在神经网络的情况下，有时希望对网络的每一层使用单独的惩罚，并分配不同的 $\alpha$ 系数。
- 但是，由于寻找合适的多个超参数的代价很大，因此为了减少搜索空间，我们会在所有层使用相同的权重衰减。


正则化惩罚对应的参数：

- 在神经网络中，我们通常只对权重 做惩罚而不对偏置做正则惩罚。
  - 因为：
    - 精确拟合偏置所需的数据通常比拟合权重少得多。每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。
    - 另外，正则化偏置参数可能会导致明显的欠拟合。
  - 因此，我们使用向量 $\boldsymbol w$ 表示所有应受范数惩罚影响的权重，而向量 $\boldsymbol \theta$ 表示所有参数(包括 $\boldsymbol w$ 和无需正则化的参数)。



下面，我们会讨论各种范数惩罚对模型的影响。

