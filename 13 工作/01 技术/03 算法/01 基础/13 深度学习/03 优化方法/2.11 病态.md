

## 病态

在优化凸函数时，会遇到一些挑战。这其中最突出的是 Hessian 矩阵 $\boldsymbol H$ 的病态。这是数值优化、凸优化或其他形式的优化中普遍存在的问题，更多细节请回顾 《梯度之上：Jacobian 和 Hessian 矩阵》。

病态问题一般被认为存在于神经网络训练过程中。病态体现在随机梯度下降会"卡"在某些情况，此时即使很小的更新步长也会增加代价函数。

回顾 $f\left(x^{(0)}-\epsilon \boldsymbol{g}\right) \approx f\left(\boldsymbol{x}^{(0)}\right)-\epsilon \boldsymbol{g}^{\top} \boldsymbol{g}+\frac{1}{2} \epsilon^{2} \boldsymbol{g}^{\top} \boldsymbol{H} \boldsymbol{g}$ ，代价函数的二阶泰勒级数展开预测梯度下降中的 $-\epsilon\boldsymbol g$ 会增加

$$
    \frac{1}{2} \epsilon^2 \boldsymbol g^\top \boldsymbol H\boldsymbol g - \epsilon\boldsymbol g^\top\boldsymbol g
$$

到代价中。当 $\frac{1}{2} \epsilon^2 \boldsymbol g^\top\boldsymbol H\boldsymbol g$ 超过 $\epsilon\boldsymbol g^\top\boldsymbol g$ 时，梯度的病态会成为问题。我们可以通过监测平方梯度范数 $\boldsymbol g^\top\boldsymbol g$ 和 $\boldsymbol g^\top \boldsymbol H\boldsymbol g$，来判断病态是否不利于神经网络训练任务。在很多情况中，梯度范数不会在训练过程中显著缩小，但是 $\boldsymbol g^\top\boldsymbol H\boldsymbol g$ 的增长会超过一个数量级。其结果是尽管梯度很强，学习会变得非常缓慢，因为学习率必须收缩以弥补更强的曲率。如图所示，成功训练的神经网络中，梯度显著增加。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/7i1262hftEE8.png?imageslim">
</p>

> 图 8.1 梯度下降通常不会到达任何类型的临界点。此示例中，在用于对象检测的卷积网络的整个训练期间， 梯度范数持续增加。(左)各个梯度计算的范数如何随时间分布的散点图。为了方便作图，每轮仅绘制一个梯度范数。我们将所有梯度范数的移动平均绘制为实曲线。梯度范数明显随时间增加，而不是如我们所期望的那样随训练过程收敛到临界点而减小。(右)尽管梯度递增，训练过程却相当成功。验证集上的分类误差可以降低到较低水平。







尽管病态还存在于除了神经网络训练的其他情况中，有些适用于其他情况的解决病态的技术并不适用于神经网络。例如，牛顿法在解决带有病态条件的 Hessian 矩阵的凸优化问题时，是一个非常优秀的工具，但是我们将会在以下小节中说明牛顿法运用到神经网络时需要很大的改动。




# 相关

- 《深度学习》花书
