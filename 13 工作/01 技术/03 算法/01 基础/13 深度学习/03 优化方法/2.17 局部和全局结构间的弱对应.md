


## 局部和全局结构间的弱对应

迄今为止，我们讨论的许多问题都是关于损失函数在单个点的性质——若 $J(\boldsymbol \theta)$ 是当前点 $\boldsymbol \theta$ 的病态条件，或者 $\boldsymbol \theta$ 在悬崖中，或者 $\boldsymbol \theta$ 是一个下降方向不明显的鞍点，那么会很难更新当前步。


如果该方向在局部改进很大，但并没有指向代价低得多的遥远区域，那么我们有可能在单点处克服以上所有困难，但仍然表现不佳。


{GoodfellowOptimization15}认为大部分训练的运行时间取决于到达解决方案的轨迹长度。如下图所示，学习轨迹将花费大量的时间探寻一个围绕山形结构的宽弧。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200616/HETIrxErfabQ.png?imageslim">
</p>

> 图8.2: 神经网络代价函数的可视化。这些可视化对应用于真实对象识别和自然语言处理任务的前 贵神经网络、卷积网络和循环网络而言是类似的。令人惊许的是，这些可视化通常不会显示出很多 明显的障碍。大约 2012 年，在随机梯度下降开始成功训练非常大的模型之前，相比这些投影所显示的神经网络代价函数的表面通常被认为有更多的非见结构。该投影所显示的主要障碍是初始参 数附近的高代价鞍点，但如由蓝色路径所示，SGD训练轨迹能轻易地逃脱该鞍点。大多数训练时，SGD 间花费在横穿代价函数中相对平坦的峡谷，可能由于梯度中的高噪声、或该区域中 Hessian 矩阵 的病态条件，或者需要经过间接的弧路径绕过图中可见的高“山”。

大多数优化研究的难点集中于训练是否找到了全局最小点、局部极小点或是鞍点，但在实践中神经网络不会到达任何一种临界点。下图表明神经网络通常不会到达梯度很小的区域。甚至，这些临界点不一定存在。例如，损失函数 $-\log p(y\mid\boldsymbol x;\boldsymbol \theta)$ 可以没有全局最小点，而是当随着训练模型逐渐稳定后，渐近地收敛于某个值。对于具有离散的 $y$ 和~softmax~分布 $p(y\mid\boldsymbol x)$ 的分类器而言，若模型能够正确分类训练集上的每个样本，则负对数似然可以无限趋近但不会等于零。同样地，实值模型 $p(y\mid\boldsymbol x) = \mathcal{N}(y;f(\boldsymbol \theta),\beta^{-1})$ 的负对数似然会趋向于负无穷——如果 $f(\boldsymbol \theta)$ 能够正确预测所有训练集中的目标 $y$，学习算法会无限制地增加 $\beta$。下图给出了一个失败的例子，即使没有局部极小值和鞍点，该例还是不能从局部优化中找到一个良好的代价函数值。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/WFI4TOwtaRXB.png?imageslim">
</p>

> 图 8.4 如果局部表面没有指向全局解，基于局部下坡移动的优化可能就会失败。这里我们提供一个例子，说明即使在没有鞍点或局部极小值的情况下，优化过程会如何失败。此例中的代价函数仅包含朝向低值而不是极小值的渐近线。在这种情况下，造成这种困难的主要原因是初始化在"山"的错误一侧，并且无法遍历。在高维空间中，学习算法通常可以环绕过这样的高山，但是相关的轨迹可能会很长，并且导致过长的训练时间，如图 8.2 所示。



未来的研究需要进一步探索影响学习轨迹长度和更好地表征训练过程的结果。


许多现有研究方法在求解具有困难全局结构的问题时，旨在寻求良好的初始点，而不是开发非局部范围更新的算法。


梯度下降和基本上所有的可以有效训练神经网络的学习算法，都是基于局部较小更新。之前的小节主要集中于为何这些局部范围更新的正确方向难以计算。我们也许能计算目标函数的一些性质，如近似的有偏梯度或正确方向估计的方差。在这些情况下，难以确定局部下降能否定义通向有效解的足够短的路径，但我们并不能真的遵循局部下降的路径。目标函数可能有诸如病态条件或不连续梯度的问题，使得梯度为目标函数提供较好近似的区间非常小。在这些情况下，步长为 $\epsilon$ 的局部下降可能定义了到达解的合理的短路经，但是我们只能计算步长为 $\delta \ll \epsilon$ 的局部下降方向。在这些情况下，局部下降或许能定义通向解的路径，但是该路径包含很多次更新，因此遵循该路径会带来很高的计算代价。有时，比如说当目标函数有一个宽而平的区域，或是我们试图寻求精确的临界点（通常来说后一种情况只发生于显式求解临界点的方法，如牛顿法）时，局部信息不能为我们提供任何指导。在这些情况下，局部下降完全无法定义通向解的路径。在其他情况下，局部移动可能太过贪心，朝着下坡方向移动，却和所有可行解南辕北辙，如 图 8.4 所示，或者是用舍近求远的方法来求解问题，如图 8.2 所示。目前，我们还不了解这些问题中的哪一个与神经网络优化中的难点最相关，这是研究领域的热点方向。


不管哪个问题最重要，如果存在一个区域，我们遵循局部下降便能合理地直接到达某个解，并且我们能够在该良好区域上初始化学习，那么这些问题都可以避免。最终的观点还是建议在传统优化算法上研究怎样选择更佳的初始化点，以此来实现目标更切实可行。




# 相关

- 《深度学习》花书
