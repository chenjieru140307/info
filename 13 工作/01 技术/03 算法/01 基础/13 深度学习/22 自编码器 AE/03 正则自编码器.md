

# 正则自编码器

正则自编码器：

- 正则自编码器使用的损失函数可以鼓励模型学习其他特性（除了将输入复制到输出），而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量。
  - 这些特性包括稀疏表示、表示的小导数、以及对噪声或输入缺失的鲁棒性。即使模型容量大到足以学习一个无意义的恒等函数，非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布的有用信息。<span style="color:red;">什么是正则自编码器？</span>
- 除了这里所描述的方法（正则化自编码器最自然的解释），几乎任何带有潜变量并配有一个推断过程（计算给定输入的潜在表示）的生成模型，都可以看作是自编码器的一种特殊形式。
  - 强调与自编码器联系的两个生成式建模方法是 Helmholtz 机的衍生模型，
    - 如变分自编码器和生成随机网络。
  - 这些变种（或衍生）自编码器能够学习出高容量且过完备的模型，进而发现输入数据中有用的结构信息，并且也无需对模型进行正则化。
  - 这些编码显然是有用的，因为这些模型被训练为近似训练数据的概率分布而不是将输入复制到输出。



## 稀疏自编码器

稀疏自编码器：

- 稀疏自编码器简单地在训练时结合编码层的稀疏惩罚 $\Omega(\boldsymbol h)$ 和重构误差：


$$\begin{aligned}
    L(\boldsymbol x, g(f(\boldsymbol x))) + \Omega(\boldsymbol h),
\end{aligned}$$

- 说明：
  - $g(\boldsymbol h)$ 是解码器的输出
  - 通常 $\boldsymbol h$ 是编码器的输出，即 $\boldsymbol h = f(\boldsymbol x)$。

作用：

- 稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。
  - 稀疏正则化的自编码器能反映训练数据集的独特统计特征。

理解：（应该组织的不好，重新整理）

- 我们可以简单地将惩罚项 $\Omega(\boldsymbol h)$ 视为加到前馈网络的正则项，这个前馈网络的主要任务是将输入复制到输出（无监督学习的目标），并尽可能地根据这些稀疏特征执行一些监督学习任务（根据监督学习的目标）。
- 不像其它正则项如权重衰减——没有直观的贝叶斯解释。
  - 如最大后验 (MAP) 估计描述，权重衰减和其他正则惩罚可以被解释为一个 MAP 近似贝叶斯推断，正则化的惩罚对应于模型参数的先验概率分布。
    - 这种观点认为，正则化的最大似然对应最大化 $p(\boldsymbol \theta \mid \boldsymbol x)$， 相当于最大化 $\log p(\boldsymbol x \mid \boldsymbol \theta) + \log p(\boldsymbol \theta)$。 $\log p(\boldsymbol x \mid \boldsymbol \theta)$ 即通常的数据似然项，参数的对数先验项 $\log p(\boldsymbol \theta)$ 则包含了对 $\boldsymbol \theta$ 特定值的偏好。这种观点在贝叶斯统计有所描述。
  - 正则自编码器不适用这样的解释是因为正则项取决于数据，因此根据定义上（从文字的正式意义）来说，它不是一个先验。
  - 如此，我们仍可以认为这些正则项隐式地表达了对函数的偏好。
- 我们可以认为整个稀疏自编码器框架是对带有潜变量的生成模型的近似最大似然训练，而不将稀疏惩罚视为复制任务的正则化。
  - 假如我们有一个带有可见变量 $\boldsymbol x$ 和潜变量 $\boldsymbol h$ 的模型，且具有明确的联合分布 $p_{\text{model}}(\boldsymbol x,\boldsymbol h)=p_{\text{model}}(\boldsymbol h)p_{\text{model}} (\boldsymbol x \mid \boldsymbol h)$。我们将 $p_{\text{model}}(\boldsymbol h)$ 视为模型关于潜变量的先验分布，表示模型看到 $\boldsymbol x$ 的信念先验。这与我们之前使用"先验"的方式不同，之前指分布 $p(\boldsymbol \theta)$ 在我们看到数据前就对模型参数的先验进行编码。
  - 对数似然函数可分解为
    $$\begin{aligned}
    \log p_{\text{model}}(\boldsymbol x)=\log \sum_{\boldsymbol h} p_{\text{model}}(\boldsymbol h, \boldsymbol x) .
    \end{aligned}$$
  - 我们可以认为自编码器使用一个高似然值 $\boldsymbol h$ 的点估计近似这个总和。这类似于稀疏编码生成模型，但 $\boldsymbol h$ 是参数编码器的输出，而不是从优化结果推断出的最可能的 $\boldsymbol h$。
  - 从这个角度看，我们根据这个选择的 $\boldsymbol h$，最大化如下

    $$\begin{aligned}
    \log p_{\text{model}}(\boldsymbol h, \boldsymbol x)=\log p_{\text{model}}(\boldsymbol h) + \log p_{\text{model}}(\boldsymbol x \mid \boldsymbol h) .
    \end{aligned}$$

  - $\log p_{\text{model}}(\boldsymbol h)$ 项能被稀疏诱导。如 Laplace 先验，

    $$\begin{aligned}
    p_{\text{model}}(h_i) = \frac{\lambda}{2} e^{-\lambda | h_i |},
    \end{aligned}$$

  - 对应于绝对值稀疏惩罚。将对数先验表示为绝对值惩罚，我们得到

    $$\begin{aligned}
    \Omega(\boldsymbol h) &= \lambda \sum_{i} | h_i  |,\\
    -\log p_{\text{model}}(\boldsymbol h) &=
    \sum_i (\lambda | h_i | - \log \frac{\lambda}{2}) = \Omega(\boldsymbol h) + \text{const},
    \end{aligned}$$

  - 说明：
    - 这里的常数项只跟 $\lambda$ 有关。
  - 通常我们将 $\lambda$ 视为超参数，因此可以丢弃不影响参数学习的常数项。其他如 Student-t 先验也能诱导稀疏性。
  - 从稀疏性导致 $p_{\text{model}}(\boldsymbol h)$ 学习成近似最大似然的结果看，稀疏惩罚完全不是一个正则项。这仅仅影响模型关于潜变量的分布。
  - 这个观点提供了训练自编码器的另一个动机：这是近似训练生成模型的一种途径。这也给出了为什么自编码器学到的特征是有用的另一个解释：它们描述的潜变量可以解释输入。
- 发展：
  - 稀疏自编码器的早期工作 探讨了各种形式的稀疏性，并提出了稀疏惩罚和 $\log  Z$ 项（将最大似然应用到无向概率模型 $p(\boldsymbol x)=\frac{1}{Z}\tilde{p}(\boldsymbol x)$ 时产生）之间的联系。这个想法是最小化 $\log Z$ 防止概率模型处处具有高概率，同理强制稀疏可以防止自编码器处处具有低的重构误差 。这种情况下，这种联系是对通用机制的直观理解而不是数学上的对应。在数学上更容易解释稀疏惩罚对应于有向模型 $p_{\text{model}}(\boldsymbol h)p_{\text{model}}(\boldsymbol x \mid \boldsymbol h)$ 中的 $\log p_{\text{model}}(\boldsymbol h)$。
  - Glorot 提出了一种在稀疏（和去噪）自编码器的 $\boldsymbol h$ 中实现\emph{真正为零}的方式。该想法是使用整流线性单元产生编码层。基于将表示真正推向零（如绝对值惩罚）的先验，可以间接控制表示中零的平均数量。




## 去噪自编码器


去噪自编码器：

- 除了向代价函数增加一个惩罚项，我们也可以通过改变重构误差项来获得一个能学到有用信息的自编码器。
- 传统的自编码器最小化以下目标
    $$\begin{aligned}
    L(\boldsymbol x, g(f(\boldsymbol x))),
    \end{aligned}$$
- 其中：
  - $L$ 是一个损失函数，惩罚 $g(f(\boldsymbol x))$ 与 $\boldsymbol x$ 的差异，如平方误差。
  - 此时，如果模型被赋予过大的容量，那么，$L$ 仅仅使得 $g \circ  f$ 学成一个恒等函数。
- 而，去噪自编码器最小化
    $$\begin{aligned}
    L(\boldsymbol x, g(f(\tilde \boldsymbol x))),
    \end{aligned}$$
- 说明：
  - $\tilde \boldsymbol x$ 是被某种噪声损坏的 $\boldsymbol x$ 的副本。
  - 因此去噪自编码器必须撤消这些损坏，而不是简单地复制输入。
- 去噪训练过程强制 $f$ 和 $g$ 隐式地学习 $p_{\text{data}} (\boldsymbol x)$ 的结构。
  - 因此去噪自编码器也是一个通过最小化重构误差获取有用特性的例子。这也是将过完备、高容量的模型用作自编码器的一个例子，只要小心防止这些模型仅仅学习一个恒等函数。



## 收缩自编码器

收缩自编码器：

- 另一正则化自编码器的策略是使用一个类似稀疏自编码器中的惩罚项 $\Omega$，
    $$\begin{aligned}
        L(\boldsymbol x, g(f(\boldsymbol x))) + \Omega(\boldsymbol h, \boldsymbol x),
    \end{aligned}$$
- 但 $\Omega$ 的形式不同：
    $$\begin{aligned}
    \Omega(\boldsymbol h, \boldsymbol x) = \lambda \sum_i \| \nabla_{\boldsymbol x}h_i \|^2.
    \end{aligned}$$
- 即，以惩罚导数作为正则。
- 这迫使模型学习一个在 $\boldsymbol x$ 变化小时目标也没有太大变化的函数。
- 因为这个惩罚只对训练数据适用，它迫使自编码器学习可以反映训练数据分布信息的特征。
- 理解：
  - 这种方法与去噪自编码器、流形学习和概率模型存在一定理论联系。

