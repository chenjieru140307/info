
# 自编码器


自编码器 AutoEncoders AE

自编码器：

- 自编码器是神经网络的一种，经过训练后能尝试将输入复制到输出。
  - 可以将自编码器看做一种数据压缩算法，其中数据的压缩和解压缩函数是数据相关的、有损的、从样本中自动学习的。
- 自编码器内部有一个隐藏层 $\boldsymbol h$，可以产生编码表示输入。




介绍：

- 该网络可以看作由两部分组成：
  - 一个由函数 $\boldsymbol h = f(\boldsymbol x)$ 表示的编码器
  - 一个生成重构的解码器 $\boldsymbol r=g(\boldsymbol h)$。
- 如图：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190525/6T9y1LhnJfCA.png?imageslim">
    </p>
- 如图：
    <p align="center">
        <img width="40%" height="70%" src="http://images.iterate.site/blog/image/20190718/3rXLEg7owIC2.png?imageslim">
    </p>
- 说明：
  - 上图为自编码器的一般结构
  - 自编码器具有两个组件：
    - 编码器 $f$（将 $\boldsymbol x$ 映射到 $\boldsymbol h$）
    - 解码器 $g$（将 $\boldsymbol h$ 映射到 $\boldsymbol r$）。

- 作用：
  - 用来学习一些有用的特性
    - 如果一个自编码器只是简单地学会将处处设置为 $g(f(\boldsymbol x)) =\boldsymbol x$，那么这个自编码器就没什么特别的用处。
    - 相反，我们不应该将自编码器设计成输入到输出完全相等。
      - 这通常需要向自编码器强加一些约束，使它只能近似地复制，并只能复制与训练数据相似的输入。
      - 这些约束强制模型考虑输入数据的哪些部分需要被优先复制，因此它往往能学习到数据的有用特性。

- 特点：
  - 自编码器是数据相关的（data-specific 或 data-dependent），这意味着自编码器只能压缩那些与训练数据类似的数据。
    - 比如，使用人脸训练出来的自编码器在压缩别的图片，比如树木时性能很差，因为它学习到的特征是与人脸相关的。
  - 自编码器是有损的，意思是解压缩的输出与原来的输入相比是退化的，MP3，JPEG等压缩算法也是如此。这与无损压缩算法不同。
  - 自编码器是从数据样本中自动学习的，这意味着很容易对指定类的输入训练出一种特定的编码器，而不需要完成任何新工作。
- 搭建一个自编码器：
  - 搭建编码器
  - 搭建解码器
  - 设定一个损失函数，用以衡量由于压缩而损失掉的信息。


- 应用：
  - 传统自编码器被用于降维或特征学习。
  - 近年来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿，我们将在深度生成模型中揭示更多细节。
  - 数据去噪
  - 降维
    - 生成的编码比 PCA 可能会更好一些，产生更少的重构误差，所学到的表示更容易定性解释，并能联系基础类别，类别表现为分离良好的集群。
    - 而且，低维表示可以提高许多任务的性能，例如分类。小空间的模型消耗更少的内存和运行时间。据观察，许多降维的形式会将语义上相关的样本置于彼此邻近的位置。映射到低维空间所提供的线索有助于泛化。
    - 为了进行可视化而降维。
      - 配合适当的维度和稀疏约束，自编码器可以学习到比 PCA 等技术更有意思的数据投影。
      - 对于 2D 的数据可视化，t-SNE（读作 tee-snee）或许是目前最好的算法，但通常还是需要原数据的维度相对低一些。所以，可视化高维数据的一个好办法是首先使用自编码器将维度降低到较低的水平（如 32 维），然后再使用 t-SNE将其投影在 2D 平面上。
  - 信息检索
    - 信息检索从降维中获益更多，此任务需要找到数据库中类似查询的条目。
    - 此任务不仅和其他任务一样从降维中获得一般益处，还使某些低维空间中的搜索变得极为高效。
    - 特别的，如果我们训练降维算法生成一个低维且 二值的编码，那么我们就可以将所有数据库条目在哈希表映射为二值编码向量。
    - 这个哈希表允许我们返回具有相同二值编码的数据库条目作为查询结果进行信息检索。
    - 我们也可以非常高效地搜索稍有不同条目，只需反转查询编码的各个位。
    - 这种通过降维和二值化的信息检索方法被称为语义哈希，已经被用于文本输入和图像。（厉害，语义哈希）
    - 通常在最终层上使用 sigmoid 编码函数产生语义哈希的二值编码。sigmoid 单元必须被训练为到达饱和，对所有输入值都接近 0 或接近 1。
      - 能做到这一点的窍门就是训练时在 sigmoid 非线性单元前简单地注入加性噪声。噪声的大小应该随时间增加。要对抗这种噪音并且保存尽可能多的信息，网络必须加大输入到 sigmoid 函数的幅度，直到饱和。
    - 学习哈希函数的思想已在其他多个方向进一步探讨，包括改变损失训练表示的想法，其中所需优化的损失与哈希表中查找附近样本的任务有更直接的联系。


- 理解：
  - 自编码器可以被看作是前馈网络的一个特例，并且可以使用完全相同的技术进行训练，通常使用小批量梯度下降法（其中梯度基于反向传播计算）。
    - 不同于一般的前馈网络，自编码器也可以使用再循环训练，这种学习算法基于比较原始输入的激活和重构输入的激活。相比反向传播算法，再循环算法更具生物学意义，但很少用于机器学习应用。（什么是再循环算法？）
  - 自编码器并不是一个真正的无监督学习的算法，而是一个自监督的算法。
    - 自监督学习是监督学习的一个实例，其标签产生自输入数据。
  - 现代自编码器将编码器和解码器的概念推而广之，将其中的确定函数推广为随机映射 $p_{\text{encoder}} (\boldsymbol h \mid \boldsymbol x)$ 和 $p_{\text{decoder}}(\boldsymbol x \mid \boldsymbol h)$。


- 深度自编码器：
  - 优点：
    - 比相应的浅层或线性自编码器产生更好的压缩效率。
  - 训练
    - 训练深度自编码器的普遍策略是：训练一堆浅层的自编码器来贪心地预训练相应的深度架构。
    - 所以即使最终目标是训练深度自编码器，我们也经常会遇到浅层自编码器。

