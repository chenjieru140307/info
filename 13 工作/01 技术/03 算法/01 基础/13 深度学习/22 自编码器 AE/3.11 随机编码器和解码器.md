

# 随机编码器和解码器

自编码器本质上是一个前馈网络，可以使用与传统前馈网络相同的损失函数和输出单元。


如\sec?中描述，设计前馈网络的输出单元和损失函数普遍策略是定义一个输出分布 $p(\boldsymbol y \mid \boldsymbol x)$ 并最小化负对数似然 $-\log p(\boldsymbol y \mid \boldsymbol x)$。在这种情况下，$\boldsymbol y$ 是关于目标的向量（如类标）。


在自编码器中，$\boldsymbol x$ 既是输入也是目标。然而，我们仍然可以使用与之前相同的架构。给定一个隐藏编码 $\boldsymbol h$，我们可以认为解码器提供了一个条件分布 $p_{\text{model}}(\boldsymbol x \mid \boldsymbol h)$。接着我们根据最小化 $-\log p_{\text{decoder}}(\boldsymbol x \mid \boldsymbol h)$ 来训练自编码器。损失函数的具体形式视 $p_{\text{decoder}}$ 的形式而定。就传统的前馈网络来说，如果 $\boldsymbol x$ 是实值的，那么我们通常使用线性输出单元参数化高斯分布的均值。在这种情况下，负对数似然对应均方误差准则。类似地，二值 $\boldsymbol x$ 对应于一个~Bernoulli分布，其参数由 sigmoid 输出单元确定的。而离散的 $\boldsymbol x$ 对应~softmax~分布，以此类推。在给定 $\boldsymbol h$ 的情况下，为了便于计算概率分布，输出变量通常被视为是条件独立的，但一些技术（如混合密度输出）可以解决输出相关的建模。


为了更彻底地与我们之前了解到的前馈网络相区别，我们也可以将 **编码函数**(encoding function) $f(\boldsymbol x)$ 的概念推广为 **编码分布**(encoding distribution) $p_{\text{encoder}}(\boldsymbol h \mid \boldsymbol x)$， 如 \fig? 中所示。




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/uKtbLStyvKlk.png?imageslim">
</p>

> 14.2 随机自编码器的结构，其中编码器和解码器包括一些噪声注入，而不是简单的函数。这意味着可以将它们的输出视为来自分布的采样（对于编码器是 $p_{\text{encoder}}(\boldsymbol h \mid \boldsymbol x)$，对于解码器是 $p_{\text{decoder}}(\boldsymbol x\mid \boldsymbol h)$）。



任何潜变量模型 $p_{\text{model}}(\boldsymbol h, \boldsymbol x)$ 定义一个随机编码器



$$\begin{aligned}
p_{\text{encoder}}(\boldsymbol h \mid \boldsymbol x) = p_{\text{model}}(\boldsymbol h\mid\boldsymbol x)
\end{aligned}$$


以及一个随机解码器


$$\begin{aligned}
p_{\text{decoder}}(\boldsymbol x \mid \boldsymbol h) = p_{\text{model}}(\boldsymbol x\mid\boldsymbol h).
\end{aligned}$$


通常情况下，编码器和解码器的分布没有必要是与唯一一个联合分布 $p_{\text{model}}(\boldsymbol x, \boldsymbol h)$ 相容的条件分布。{Alain-et-al-arxiv2015}指出，在保证足够的容量和样本的情况下，将编码器和解码器作为去噪自编码器训练，能使它们渐近地相容。



# 相关

- 《深度学习》花书
