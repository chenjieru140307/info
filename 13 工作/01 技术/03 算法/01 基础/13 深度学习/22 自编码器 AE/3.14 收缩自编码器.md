

# 收缩自编码器

收缩自编码器~在编码 $\boldsymbol h = f(\boldsymbol x)$ 的基础上添加了显式的正则项，鼓励 $f$ 的导数尽可能小：


$$\begin{aligned}
\Omega(\boldsymbol h) = \lambda \Bigg\| \frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x} \Bigg\|_F^2 .
\end{aligned}$$


惩罚项 $\Omega(\boldsymbol h)$ 为平方~Frobenius~范数（元素平方之和），作用于与编码器的函数相关偏导数的~Jacobian~矩阵。


去噪自编码器和收缩自编码器之间存在一定联系：{Alain+Bengio-ICLR2013-small}指出在小高斯噪声的限制下，当重构函数将 $\boldsymbol x$ 映射到 $\boldsymbol r = g(f(\boldsymbol x))$ 时，去噪重构误差与收缩惩罚项是等价的。换句话说，去噪自编码器能抵抗小且有限的输入扰动，而收缩自编码器使特征提取函数能抵抗极小的输入扰动。

分类任务中，基于~Jacobian~的收缩惩罚预训练特征函数 $f(\boldsymbol x)$，将收缩惩罚应用在 $f(\boldsymbol x)$ 而不是 $g(f(\boldsymbol x))$ 可以产生最好的分类精度。如 节 得分估计 所讨论，应用于 $f(\boldsymbol x)$ 的收缩惩罚与得分匹配也有紧密的联系。

收缩源于~CAE~弯曲空间的方式。具体来说，由于~CAE~训练为抵抗输入扰动，鼓励将输入点邻域映射到输出点处更小的邻域。我们能认为这是将输入的邻域收缩到更小的输出邻域。


说得更清楚一点，CAE~只在局部收缩——一个训练样本 $\boldsymbol x$ 的所有扰动都映射到 $f(\boldsymbol x)$ 的附近。全局来看，两个不同的点 $\boldsymbol x$ 和 $\boldsymbol x'$ 会分别被映射到远离原点的两个点 $f(\boldsymbol x)$ 和 $f(\boldsymbol x')$。$f$ 扩展到数据流形的中间或远处是合理的（见图 14.7 中小例子的情况）。当 $\Omega(\boldsymbol h)$ 惩罚应用于 sigmoid 单元时，收缩~Jacobian~的简单方式是令 sigmoid 趋向饱和的 0 或 1。这鼓励~CAE~使用 sigmoid 的极值编码输入点，或许可以解释为二进制编码。它也保证了~CAE~可以穿过大部分 sigmoid 隐藏单元能张成的超立方体，进而扩散其编码值。

我们可以认为点 $\boldsymbol x$ 处的~Jacobian~矩阵 $\boldsymbol J$ 能将非线性编码器近似为线性算子。这允许我们更形式地使用"收缩"这个词。在线性理论中，当 $\boldsymbol J\boldsymbol x$ 的范数对于所有单位 $\boldsymbol x$ 都小于等于 1 时，$\boldsymbol J$ 被称为收缩的。换句话说，如果 $\boldsymbol J$ 收缩了单位球，他就是收缩的。我们可以认为~CAE~为鼓励每个局部线性算子具有收缩性，而在每个训练数据点处将~Frobenius~范数作为 $f(\boldsymbol x)$ 的局部线性近似的惩罚。


如 节 使用自编码器学习流形 中描述，正则自编码器基于两种相反的推动力学习流形。在~CAE~的情况下，这两种推动力是重构误差和收缩惩罚 $\Omega(\boldsymbol h)$。单独的重构误差鼓励~CAE~学习一个恒等函数。单独的收缩惩罚将鼓励~CAE~学习关于 $\boldsymbol x$ 是恒定的特征。这两种推动力的折衷产生导数 $\frac{\partial f(\boldsymbol x)}{\partial \boldsymbol x}$ 大多是微小的自编码器。只有少数隐藏单元，对应于一小部分输入数据的方向，可能有显著的导数。


CAE~的目标是学习数据的流形结构。使 $\boldsymbol J\boldsymbol x$ 很大的方向 $\boldsymbol x$，会快速改变 $\boldsymbol h$，因此很可能是近似流形切平面的方向。{Rifai+al-2011-small,Salah+al-2011-small}的实验显示训练~CAE~会导致 $\boldsymbol J$ 中大部分奇异值（幅值）比 1 小，因此是收缩的。然而，有些奇异值仍然比 1 大，因为重构误差的惩罚鼓励~CAE~对最大局部变化的方向进行编码。对应于最大奇异值的方向被解释为收缩自编码器学到的切方向。理想情况下，这些切方向应对应于数据的真实变化。比如，一个应用于图像的~CAE~应该能学到显示图像改变的切向量，如图 14.6 图中物体渐渐改变状态。如下图所示，实验获得的奇异向量的可视化似乎真的对应于输入图象有意义的变换。





<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/VKDpv83pj7Em.png?imageslim">
</p>

> 14.10 通过局部 PCA 和收缩自编码器估计的流形切向量的图示。流形的位置由来自 CIFAR-10数据集中狗的输入图像定义。切向量通过输入到代码映射的~Jacobian~矩阵 $\frac{\partial \boldsymbol h}{\partial \boldsymbol x}$ 的前导奇异向量估计。虽然局部 PCA 和~CAE~都可以捕获局部切方向，但~CAE~能够从有限训练数据形成更准确的估计，因为它利用了不同位置的参数共享（共享激活的隐藏单元子集）。CAE切方向通常对应于物体的移动或改变部分（例如头或腿）。





收缩自编码器正则化准则的一个实际问题是，尽管它在单一隐藏层的自编码器情况下是容易计算的，但在更深的自编码器情况下会变的难以计算。根据 {Rifai+al-2011-small}的策略，分别训练一系列单层的自编码器，并且每个被训练为重构前一个自编码器的隐藏层。这些自编码器的组合就组成了一个深度自编码器。因为每个层分别训练成局部收缩，深度自编码器自然也是收缩的。这个结果与联合训练深度模型完整架构（带有关于 Jacobian 的惩罚项）获得的结果是不同的，但它抓住了许多理想的定性特征。


另一个实际问题是，如果我们不对解码器强加一些约束，收缩惩罚可能导致无用的结果。例如，编码器将输入乘一个小常数 $\epsilon$，解码器将编码除以一个小常数 $\epsilon$。随着 $\epsilon$ 趋向于 0，编码器会使收缩惩罚项 $\Omega(\boldsymbol h)$ 趋向于 0 而学不到任何关于分布的信息。同时，解码器保持完美的重构。{Rifai+al-2011-small}通过绑定 $f$ 和 $g$ 的权重来防止这种情况。$f$ 和 $g$ 都是由线性仿射变换后进行逐元素非线性变换的标准神经网络层组成，因此将 $g$ 的权重矩阵设成 $f$ 权重矩阵的转置是很直观的。



# 相关

- 《深度学习》花书
