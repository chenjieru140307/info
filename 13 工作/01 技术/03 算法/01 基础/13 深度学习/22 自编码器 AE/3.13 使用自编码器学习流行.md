


# 使用自编码器学习流形


如 节 流形学习 描述，自编码器跟其他很多机器学习算法一样，也利用了数据集中在一个低维流形或者一小组这样的流形的思想。其中一些机器学习算法仅能学习到在流形上表现良好但给定不在流形上的输入会导致异常的函数。自编码器进一步借此想法，旨在学习流形的结构。


要了解自编码器如何做到这一点，我们必须介绍流形的一些重要特性。


流形的一个重要特征是切平面的集合。$d$ 维流形上的一点 $\boldsymbol x$，切平面由能张成流形上允许变动的局部方向的 $d$ 维基向量给出。如下图所示，这些局部方向决定了我们能如何微小地变动 $\boldsymbol x$ 而保持于流形上。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/Y3PtJBrPqE4D.png?imageslim">
</p>

> 14.6 正切超平面概念的图示。我们在 $784$ 维空间中创建了 $1$ 维流形。我们使用一张 784 像素的 MNIST 图像，并通过垂直平移来转换它。垂直平移的量定义沿着 1 维流形的坐标，轨迹为通过图像空间的弯曲路径。该图显示了沿着该流形的几个点。为了可视化，我们使用 PCA 将流形投影到 $2$ 维空间中。$n$ 维流形在每个点处都具有 $n$ 维切平面。该切平面恰好在该点接触流形，并且在该点处平行于流形表面。它定义了为保持在流形上可以移动的方向空间。该 $1$ 维流形具有单个切线。我们在图中示出了一个点处的示例切线，其中图像表示该切线方向在图像空间中是怎样的。灰色像素表示沿着切线移动时不改变的像素，白色像素表示变亮的像素，黑色像素表示变暗的像素。


所有自编码器的训练过程涉及两种推动力的折衷：


- 学习训练样本 $\boldsymbol x$ 的表示 $\boldsymbol h$ 使得 $\boldsymbol x$ 能通过解码器近似地从 $\boldsymbol h$ 中恢复。$\boldsymbol x$ 是从训练数据挑出的这一事实很关键，因为这意味着自编码器不需要成功重构不属于数据生成分布下的输入。
- 满足约束或正则惩罚。这可以是限制自编码器容量的架构约束，也可以是加入到重构代价的一个正则项。这些技术一般倾向那些对输入较不敏感的解。



显然，单一的推动力是无用的——从它本身将输入复制到输出是无用的，同样忽略输入也是没用的。相反，两种推动力结合是有用的，因为它们驱使隐藏的表示能捕获有关数据分布结构的信息。重要的原则是，自编码器必须有能力表示**重构训练实例所需的变化**。如果该数据生成分布集中靠近一个低维流形，自编码器能隐式产生捕捉这个流形局部坐标系的表示：仅在 $\boldsymbol x$ 周围关于流形的相切变化需要对应于 $\boldsymbol h=f(\boldsymbol x)$ 中的变化。因此，编码器学习从输入空间 $\boldsymbol x$ 到表示空间的映射，映射仅对沿着流形方向的变化敏感，并且对流形正交方向的变化不敏感。


下图中一维的例子说明，我们可以通过构建对数据点周围的输入扰动不敏感的重构函数，使得自编码器恢复流形结构。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/VoFhEbp2YsyG.png?imageslim">
</p>

> 14.7 如果自编码器学习到对数据点附近的小扰动不变的重构函数，它就能捕获数据的流形结构。这里，流形结构是 0 维流形的集合。虚线对角线表示重构的恒等函数目标。最佳重构函数会在存在数据点的任意处穿过恒等函数。图底部的水平箭头表示在输入空间中基于箭头的 $r(\boldsymbol x)-\boldsymbol x$ 重建方向向量，总是指向最近的"流形"（1维情况下的单个数据点 ）。在数据点周围，去噪自编码器明确地尝试将重构函数 $r(\boldsymbol x)$ 的导数限制为很小。收缩自编码器的编码器执行相同操作。虽然在数据点周围，$r(\boldsymbol x)$ 的导数被要求很小，但在数据点之间它可能会很大。数据点之间的空间对应于流形之间的区域，为将损坏点映射回流形，重构函数必须具有大的导数。



为了理解自编码器可用于流形学习的原因，我们可以将自编码器和其他方法进行对比。学习表征流形最常见的是流形上（或附近）数据点的表示。对于特定的实例，这样的表示也被称为嵌入。它通常由一个低维向量给出，具有比这个流形的"外围"空间更少的维数。有些算法（下面讨论的非参数流形学习算法）直接学习每个训练样例的嵌入，而其他算法学习更一般的映射（有时被称为编码器或表示函数），将周围空间（输入空间）的任意点映射到它的嵌入。


流形学习大多专注于试图捕捉到这些流形的无监督学习过程。最初始的学习非线性流形的机器学习研究专注基于最近邻图的非参数方法。该图中每个训练样例对应一个节点，它的边连接近邻点对。如下图所示，这些方法将每个节点与张成实例和近邻之间的差向量变化方向的切平面相关联。




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/NCssbE4YKsvl.png?imageslim">
</p>

> 14.8 非参数流形学习过程构建的最近邻图，其中节点表示训练样本，有向边指示最近邻关系。因此，各种过程可以获得与图的邻域相关联的切平面以及将每个训练样本与实值向量位置或嵌入相关联的坐标系。我们可以通过插值将这种表示概括为新的样本。只要样本的数量大到足以覆盖流形的弯曲和扭转，这些方法工作良好。图片来自 QMUL 多角度人脸数据集。


全局坐标系则可以通过优化或求解线性系统获得。下图展示了如何通过大量局部线性的类高斯样平铺（或"薄煎饼"，因为高斯块在切平面方向是扁平的）得到一个流形。




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/PXGyx8Pm8dvr.png?imageslim">
</p>

> 14.9 如果每个位置处的切平面（见图 14.6 ）是已知的，则它们可以平铺后形成全局坐标系或密度函数。每个局部块可以被认为是局部欧几里德坐标系或者是局部平面高斯或"薄饼"，在与薄饼正交的方向上具有非常小的方差而在定义坐标系的方向上具有非常大的方差。这些高斯的混合提供了估计的密度函数，如流形中的~Parzen~窗口算法 或其非局部的基于神经网络的变体。


然而，{Bengio+Monperrus-2005}指出了这些局部非参数方法应用于流形学习的根本困难：如果流形不是很光滑（它们有许多波峰、波谷和曲折），为覆盖其中的每一个变化，我们可能需要非常多的训练样本，导致没有能力泛化到没见过的变化。实际上，这些方法只能通过内插，概括相邻实例之间流形的形状。不幸的是，AI~问题中涉及的流形可能具有非常复杂的结构，难以仅从局部插值捕获特征。考虑图 14.6 转换所得的流形样例。如果我们只观察输入向量内的一个坐标 $x_i$，当平移图像，我们可以观察到当这个坐标遇到波峰或波谷时，图像的亮度也会经历一个波峰或波谷。换句话说，底层图像模板亮度的模式复杂性决定执行简单的图像变换所产生的流形的复杂性。这是采用分布式表示和深度学习捕获流形结构的动机。



# 相关

- 《深度学习》花书
