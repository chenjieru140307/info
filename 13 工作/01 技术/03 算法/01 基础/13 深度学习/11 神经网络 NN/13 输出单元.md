

# 输出单元


输出层的作用：

- 我们假设前馈网络提供了一组定义为 $\boldsymbol h=f(\boldsymbol x;\boldsymbol \theta)$ 的隐藏特征。输出层的作用是随后对这些特征进行一些额外的变换来完成整个网络必须完成的任务。

输出单元：

- 代价函数的选择与输出单元的选择紧密相关。
- 大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式。



## 用于高斯输出分布的线性单元

线性单元：

- 一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。
- 这些单元往往被直接称为线性单元。

介绍：

- 给定特征 $\boldsymbol h$
- 线性输出单元层产生一个向量 $\hat{\boldsymbol y} = \boldsymbol W^\top \boldsymbol h+\boldsymbol b$。

作用：

- 线性输出层经常被用来产生条件高斯分布的均值：（没明白）

$$
p(\boldsymbol y\mid\boldsymbol x) = \mathcal N(\boldsymbol y; \hat{\boldsymbol y}, \boldsymbol I ).
$$

- 此时：
  - 最大化其对数似然此时等价于最小化均方误差。


使用：

- 最大似然框架也使得学习高斯分布的协方差矩阵更加容易，或更容易地使高斯分布的协方差矩阵作为输入的函数。然而，对于所有输入，协方差矩阵都必须被限定成一个正定矩阵。线性输出层很难满足这种限定，所以通常使用其他的输出单元来对协方差参数化。对协方差建模的方法将在节 其他的输出类型 中简要介绍。
- 因为线性模型不会饱和，所以它们易于采用基于梯度的优化算法，甚至可以使用其他多种优化算法。


疑问：

- 必须限定为一个正定矩阵是什么意思？
- 对协方差建模是什么意思？
- 什么是高斯分布的协方差矩阵？


## 用于 Bernoulli 输出分布的 sigmoid 单元

缘由：

- 许多任务需要预测二值型变量 $y$ 的值。
  - 具有两个类的分类问题可以归结为这种形式。

由最大似然开始：

- 此时最大似然的方法是定义 $y$ 在 $\boldsymbol x$ 条件下的 Bernoulli 分布。
- Bernoulli 分布仅需单个参数来定义。神经网络只需要预测 $P(y =1\mid\boldsymbol x)$ 即可。为了使这个数是有效的概率，它必须处在区间 $[0, 1]$ 中。
- 为满足该约束条件需要一些细致的设计工作。假设我们打算使用线性单元，并且通过阈值来限制它成为一个有效的概率：

$$
P(y=1 \mid \boldsymbol x) = \max \left \{ 0, \min \{1, \boldsymbol w^\top \boldsymbol h+b \} \right \}.
$$

- 这的确定义了一个有效的条件概率分布，但我们无法使用梯度下降来高效地训练它。
  - 当 $\boldsymbol w^\top \boldsymbol h+b$ 处于单位区间外时，模型的输出对其参数的梯度都将为 $\bm{0}$。梯度为 $\bm{0}$ 通常是有问题的，因为学习算法对于如何改善相应的参数不再具有指导意义。
- 此时，最好是使用一种新的方法来保证无论何时模型给出了错误的答案时，总能有一个较大的梯度。这种方法是基于使用 sigmoid  输出单元结合最大似然来实现的。

sigmoid 输出单元：

- 定义：

$$
\hat{y} = \sigma \left (\boldsymbol w^\top \boldsymbol h + b \right ),
$$

- 说明：
  - $\sigma$ 为 logistic sigmoid 函数。
  - 可以认为 sigmoid  输出单元具有两个部分：
    - 首先，它使用一个线性层来计算 $z=\boldsymbol w^\top \boldsymbol h+b$
    - 接着，它使用 sigmoid  激活函数将 $z$ 转化成概率。

我们暂时忽略对于 $\boldsymbol x$ 的依赖性，只讨论如何用 $z$ 的值来定义 $y$ 的概率分布。sigmoid 可以通过构造一个非归一化（和不为 1）的概率分布 $\tilde{P}(y)$ 来得到。<span style="color:red;">这句话没懂，sigmoid 可以通过构造一个非归一化（和不为 1）的概率分布 $\tilde{P}(y)$ 来得到 是什么意思？</span>我们可以随后除以一个合适的常数来得到有效的概率分布。如果我们假定非归一化的对数概率对 $y$ 和 $z$ 是线性的，<span style="color:red;">什么是 非归一化的对数概率对 $y$ 和 $z$ 是线性的？</span>可以对它取指数来得到非归一化的概率。我们然后对它归一化，可以发现这服从 Bernoulli 分布，该分布受 $z$ 的 sigmoid 变换控制：


$$
\begin{aligned}
\log \tilde{P}(y) &= yz,\\
\tilde{P}(y) &= \exp(yz),\\
P(y) &= \frac{\exp(yz)}{\sum_{y' = 0}^1 \exp(y' z)},\\
P(y) &= \sigma((2y-1)z).
\end{aligned}
$$

<span style="color:red;">没看懂。</span>

基于指数和归一化的概率分布在统计建模的文献中很常见。用于定义这种二值型变量分布的变量 $z$ 被称为分对数。<span style="color:red;">什么是分对数？</span>


这种在对数空间里预测概率的方法可以很自然地使用最大似然学习。因为用于最大似然的代价函数是 $-\log P(y\mid\boldsymbol x)$，代价函数中的 log 抵消了 sigmoid  中的 exp。如果没有这个效果，sigmoid 的饱和性会阻止基于梯度的学习做出好的改进。我们使用最大似然来学习一个由 sigmoid 参数化的 Bernoulli 分布，它的损失函数为

$$\begin{aligned}
J(\boldsymbol \theta) &= -\log P(y\mid\boldsymbol x)\\
&= -\log \sigma ((2y-1)z)\\
&= \zeta((1-2y)z).
\end{aligned}$$

<span style="color:red;">没看懂。</span>

这个推导使用了第 3.10节中的一些性质。通过将损失函数写成 softplus 函数的形式，我们可以看到它仅仅在 $(1-2y)z$ 取绝对值非常大的负值时才会饱和。因此饱和只会出现在模型已经得到正确答案时 - 当 $y=1$ 且 $z$ 取非常大的正值时，或者 $y=0$ 且 $z$ 取非常小的负值时。当 $z$ 的符号错误时，softplus函数的变量 $(1-2y)z$ 可以简化为 $|z|$。当 $|z|$ 变得很大并且 $z$ 的符号错误时，softplus 函数渐近地趋向于它的变量 $|z|$。对 $z$ 求导则渐近地趋向于 $\text{sign}(z)$，所以，对于极限情况下极度不正确的 $z$，softplus 函数完全不会收缩梯度。这个性质很有用，因为它意味着基于梯度的学习可以很快地改正错误的 $z$。<span style="color:red;">没明白。</span>

当我们使用其他的损失函数，例如均方误差之类的，损失函数会在 $\sigma(z)$ 饱和时饱和。sigmoid 激活函数在 $z$ 取非常小的负值时会饱和到 0，当 $z$ 取非常大的正值时会饱和到 1。这种情况一旦发生，梯度会变得非常小以至于不能用来学习，无论此时模型给出的是正确还是错误的答案。因此，最大似然几乎总是训练 sigmoid  输出单元的优选方法。<span style="color:red;">有点理解又有点不清楚。</span>

理论上，sigmoid 的对数总是确定和有限的，因为 sigmoid  的返回值总是被限制在开区间 $(0, 1)$ 上，而不是使用整个闭区间 $[0, 1]$ 的有效概率。在软件实现时，为了避免数值问题，最好将负的对数似然写作 $z$ 的函数，而不是 $\hat{y}=\sigma(z)$ 的函数。如果 sigmoid  函数下溢到零，那么之后对 $\hat{y}$ 取对数会得到负无穷。<span style="color:red;">没懂。</span>







# 相关

- 《深度学习》花书

## 用于 Multinoulli 输出分布的 softmax 单元

任何时候当我们想要表示一个具有 $n$ 个可能取值的离散型随机变量的分布时，我们都可以使用 softmax 函数。它可以看作是 sigmoid  函数的扩展，其中 sigmoid  函数用来表示二值型变量的分布。


softmax 函数最常用作分类器的输出，来表示 $n$ 个不同类上的概率分布。比较少见的是，softmax 函数可以在模型内部使用，例如如果我们想要在某个内部变量的 $n$ 个不同选项中进行选择。<span style="color:red;">内部变量的 $n$ 个不同选项中进行选择，这个什么时候会用到？嗯，想知道，真的会吧 softmax 用在模型内部吗？ </span>

在二值型变量的情况下，我们希望计算一个单独的数

$$
\hat{y} = P(y=1\mid\boldsymbol x).
$$

因为这个数需要处在 0 和 1 之间，并且我们想要让这个数的对数可以很好地用于对数似然的基于梯度的优化，我们选择去预测另外一个数 $z=\log \hat{P}(y=1\mid\boldsymbol x)$。对其指数化和归一化，我们就得到了一个由 sigmoid  函数控制的 Bernoulli 分布。

为了推广到具有 $n$ 个值的离散型变量的情况，我们现在需要创造一个向量 $\hat{\boldsymbol y}$，它的每个元素是 $\hat{y}_i = P(y=i\mid\boldsymbol x)$。我们不仅要求每个 $\hat{y}_i$ 元素介于 0 和 1 之间，还要使得整个向量的和为 1，使得它表示一个有效的概率分布。用于 Bernoulli 分布的方法同样可以推广到 Multinoulli 分布。首先，线性层预测了未归一化的对数概率：<span style="color:red;">嗯，预测未归一化的对数概率这个知道了。但是是怎么控制使线性层预测未归一化的对数概率的？</span>

$$
\boldsymbol z = \boldsymbol W^\top \boldsymbol h+\boldsymbol b,
$$

其中 $z_i=\log \hat{P}(y=i\mid\boldsymbol x)$。softmax函数然后可以对 $z$ 指数化和归一化来获得需要的 $\hat{\boldsymbol y}$。最终，softmax函数的形式为：<span style="color:red;">怎么进行指数化的？</span>

$$
\text{softmax}(\boldsymbol z)_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}.
$$


和 logistic sigmoid 一样，当使用最大化对数似然训练 softmax 来输出目标值 $\mathrm y$ 时，使用指数函数工作地非常好。这种情况下，我们想要最大化 $\log P(\mathrm y =i; \boldsymbol z)=\log \text{softmax}(\boldsymbol z)_i$。将 softmax 定义成指数的形式是很自然的因为对数似然中的 log 可以抵消 softmax 中的 exp：<span style="color:red;">什么叫把 softmax 定义成指数的形式？</span>

$$
\log \text{softmax}(\boldsymbol z)_i = z_i - \log \sum_j \exp(z_j).
$$



式 6.30 中的第一项表示输入 $z_i$ 总是对代价函数有直接的贡献。因为这一项不会饱和，所以即使 $z_i$ 对式 6.30 的第二项的贡献很小，学习依然可以进行。当最大化对数似然时，第一项鼓励 $z_i$ 被推高，而第二项则鼓励所有的 $\boldsymbol z$ 被压低。<span style="color:red;">嗯。</span>为了对第二项 $\log \sum_j \exp(z_j)$ 有一个直观的理解，注意到这一项可以大致近似为 $\max_j z_j$。<span style="color:red;">为什么可以这样近似。</span><span style="color:blue;">哦，后面这句有说的。</span>这种近似是基于对任何明显小于 $\max_j z_j$ 的 $z_k$，$\exp(z_k)$ 都是不重要的。我们能从这种近似中得到的直觉是，负对数似然代价函数总是强烈地惩罚最活跃的不正确预测。<span style="color:red;">是的，负对数似然是在惩罚最活跃的不正确预测的样本。</span>如果正确答案已经具有了 softmax 的最大输入，那么 $-z_i$ 项和 $\log\sum_j \exp(z_j) \approx \max_j z_j = z_i$ 项将大致抵消。<span style="color:red;">是的。</span>这个样本对于整体训练代价贡献很小，这个代价主要由其他未被正确分类的样本产生。<span style="color:red;">是的。挺厉害的。</span>

到目前为止我们只讨论了一个例子。总体来说，未正则化的最大似然会驱动模型去学习一些参数，而这些参数会驱动 softmax 函数来预测在训练集中观察到的每个结果的比率：

$$
\text{softmax}(\boldsymbol z(\boldsymbol x; \boldsymbol \theta))_i \approx \frac{\sum_{j=1}^m \bm{1}_{y^{(j)}=i, \boldsymbol x^{(j)} = \boldsymbol x}  }{ \sum_{j=1}^{m} \bm{1}_{\boldsymbol x^{(j)} = \boldsymbol x} }.
$$

<span style="color:red;">这个式子有点厉害，感觉有点清楚又有点模糊。</span>

因为最大似然是一致的估计量，所以只要模型族能够表示训练的分布，这就能保证发生。在实践中，有限的模型能力和不完美的优化将意味着模型只能近似这些比率。<span style="color:red;">嗯，只能近似这些比率。</span>

除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习。特别是，平方误差对于 softmax 单元来说是一个很差的损失函数，即使模型做出高度可信的不正确预测，也不能训练模型改变其输出。要理解为什么这些损失函数可能失败，我们需要检查 softmax 函数本身。

像 sigmoid 一样，softmax 激活函数可能会饱和。sigmoid 函数具有单个输出，当它的输入极端负或者极端正时会饱和。对于 softmax 的情况，它有多个输出值。当输入值之间的差异变得极端时，这些输出值可能饱和。当 softmax 饱和时，基于 softmax 的许多代价函数也饱和，除非它们能够转化饱和的激活函数。


为了说明 softmax 函数对于输入之间差异的响应，观察到当对所有的输入都加上一个相同常数时 softmax 的输出不变：<span style="color:red;">嗯。</span>

$$
\text{softmax}(\boldsymbol z) = \text{softmax}(\boldsymbol z+c).
$$

使用这个性质，我们可以导出一个数值方法稳定的 softmax 函数的变体：

$$
\text{softmax}(\boldsymbol z) = \text{softmax}(\boldsymbol z- \max_i z_i).
$$

变换后的形式允许我们在对 softmax 函数求值时只有很小的数值误差，即使是当 $z$ 包含极正或者极负的数时。观察 softmax 数值稳定的变体，可以看到 softmax 函数由它的变量偏离 $\max_i z_i$ 的量来驱动。<span style="color:red;">没懂。</span>

当其中一个输入是最大（$z_i = \max_i z_i$）并且 $z_i$ 远大于其他的输入时，相应的输出 $\text{softmax}(\boldsymbol z)_i$ 会饱和到 1。当 $z_i$ 不是最大值并且最大值非常大时，相应的输出 $\text{softmax}(\boldsymbol z)_i$ 也会饱和到 0。这是 sigmoid  单元饱和方式的一般化，并且如果损失函数不被设计成对其进行补偿，那么也会造成类似的学习困难。

softmax 函数的变量 $\boldsymbol z$ 可以通过两种方式产生。最常见的是简单地使神经网络较早的层输出 $\boldsymbol z$ 的每个元素，就像先前描述的使用线性层 $\boldsymbol z={W}^\top\boldsymbol h+\boldsymbol b$。虽然很直观，但这种方法是对分布的过度参数化。$n$ 个输出总和必须为 1 的约束意味着只有 $n-1$ 个参数是必要的；第 $n$ 个概率值可以通过 1 减去前面 $n-1$ 个概率来获得。因此，我们可以强制要求 $\boldsymbol z$ 的一个元素是固定的。例如，我们可以要求 $z_n=0$。事实上，这正是 sigmoid  单元所做的。定义 $P(y=1\mid\boldsymbol x)=\sigma(z)$ 等价于用二维的 $\boldsymbol z$ 以及 $z_1=0$ 来定义 $P(y=1\mid\boldsymbol x)=\text{softmax}(\boldsymbol z)_1$。无论是 $n-1$ 个变量还是 $n$ 个变量的方法，都描述了相同的概率分布，但会产生不同的学习机制。在实践中，无论是过度参数化的版本还是限制的版本都很少有差别，并且实现过度参数化的版本更为简单。<span style="color:red;">看到这一段，平时有用过限制的版本吗？</span>

从神经科学的角度看，有趣的是认为 softmax 是一种在参与其中的单元之间形成竞争的方式：softmax 输出总是和为 1，所以一个单元的值增加必然对应着其他单元值的减少。这与被认为存在于皮质中相邻神经元间的侧抑制类似。在极端情况下（当最大的 $a_i$ 和其他的在幅度上差异很大时），它变成了赢者通吃的形式（其中一个输出接近 1，其他的接近 0）。<span style="color:red;">嗯。</span>

"softmax" 的名称可能会让人产生困惑。这个函数更接近于 argmax 函数而不是 max 函数。"soft"这个术语来源于 softmax 函数是连续可微的。"argmax"函数的结果表示为一个 one-hot 向量（只有一个元素为 1，其余元素都为 0 的向量），不是连续和可微的。<span style="color:red;">"argmax"函数的结果为什么是表示为一个 one-hot 向量？</span>softmax 函数因此提供了 argmax 的"软化"版本。max 函数相应的软化版本是 $\text{softmax}(\boldsymbol z)^\top \boldsymbol z$。可能最好是把 softmax 函数称为 "softargmax"，但当前名称已经是一个根深蒂固的习惯了。<span style="color:red;">没明白。</span>







# 相关

- 《深度学习》花书


## 其他的输出类型

<span style="color:red;">基本没看懂。</span>

之前描述的线性、sigmoid 和 softmax 输出单元是最常见的。神经网络可以推广到我们希望的几乎任何种类的输出层。最大似然原则给如何为几乎任何种类的输出层设计一个好的代价函数提供了指导。<span style="color:red;">嗯。</span>

一般的，如果我们定义了一个条件分布 $p(\boldsymbol y\mid\boldsymbol x; \boldsymbol \theta)$，最大似然原则建议我们使用 $-\log p(\boldsymbol y\mid \boldsymbol x;\boldsymbol \theta)$ 作为代价函数。

一般来说，我们可以认为神经网络表示函数 $f(\boldsymbol x;\boldsymbol \theta)$。这个函数的输出不是对 $\boldsymbol y$ 值的直接预测。相反，$f(\boldsymbol x;\boldsymbol \theta)=\boldsymbol \omega$ 提供了 $y$ 分布的参数。<span style="color:red;">提供了 $y$ 分布的参数是什么意思？</span>我们的损失函数就可以表示成 $-\log p(\mathbf y; \boldsymbol \omega(\boldsymbol x))$。<span style="color:red;">为什么可以表示为这个？</span>

例如，我们想要学习在给定 $\mathbf x$ 时，$\mathbf y$ 的条件高斯分布的方差。<span style="color:red;">为什么要学习 $\mathbf y$ 的条件高斯分布的方差？</span>简单情况下，方差 $\sigma^2$ 是一个常数，此时有一个解析表达式，这是因为方差的最大似然估计量仅仅是观测值 $\mathbf y$ 与它们的期望值的差值的平方平均。一种计算上代价更加高但是不需要写特殊情况代码的方法是简单地将方差作为分布 $p(\mathbf y\mid\boldsymbol x)$ 的其中一个属性，这个分布由 $\boldsymbol \omega=f(\boldsymbol x;\boldsymbol \theta)$ 控制。<span style="color:red;">为什么可以吧方差作为它的一个属性？</span>负对数似然 $-\log p(\boldsymbol y;\boldsymbol \omega(\boldsymbol x))$ 将为代价函数提供一个必要的合适项来使我们的优化过程可以逐渐地学到方差。在标准差不依赖于输入的简单情况下，我们可以在网络中创建一个直接复制到 $\boldsymbol \omega$ 中的新参数。<span style="color:red;">怎么创建？</span>这个新参数可以是 $\sigma$ 本身，或者可以是表示 $\sigma^2$ 的参数 $v$，或者可以是表示 $\frac{1}{\sigma^2}$ 的参数 $\beta$，取决于我们怎样对分布参数化。我们可能希望模型对不同的 $\mathbf x$ 值预测出 $\mathbf y$ 不同的方差。这被称为异方差模型。在异方差情况下，我们简单地把方差指定为 $f(\mathbf x;\boldsymbol \theta)$ 其中一个输出值。实现它的典型方法是使用精度而不是方差来表示高斯分布，就像式 3.22 所描述的。在多维变量的情况下，最常见的是使用一个对角精度矩阵：

<span style="color:red;">上面整段都没看懂。</span>

$$
\text{diag}(\boldsymbol \beta).\tag{6.34}
$$

这个公式适用于梯度下降，因为由 $\boldsymbol \beta$ 参数化的高斯分布的对数似然的公式仅涉及 $\beta_i$ 的乘法和 $\log \beta_i$ 的加法。乘法、加法和对数运算的梯度表现良好。相比之下，如果我们用方差来参数化输出，我们需要用到除法。除法函数在零附近会变得任意陡峭。虽然大梯度可以帮助学习，但任意大的梯度通常导致不稳定。如果我们用标准差来参数化输出，对数似然仍然会涉及除法，并且还将涉及平方。通过平方运算的梯度可能在零附近消失，这使得学习被平方的参数变得困难。无论我们使用的是标准差，方差还是精度，我们必须确保高斯分布的协方差矩阵是正定的。因为精度矩阵的特征值是协方差矩阵特征值的倒数，所以这等价于确保精度矩阵是正定的。如果我们使用对角矩阵，或者是一个常数乘以单位矩阵(注：译者注：这里原文是"If we use a diagonal matrix, or a scalar times the diagonal matrix..."即"如果我们使用对角矩阵，或者是一个标量乘以对角矩阵..."，但一个标量乘以对角矩阵和对角矩阵没区别，结合上下文可以看出，这里原作者误把"identity"写成了"diagonal matrix"，因此这里采用"常数乘以单位矩阵"的译法。)，那么我们需要对模型输出强加的唯一条件是它的元素都为正。如果我们假设 $\boldsymbol a$ 是用于确定对角精度的模型的原始激活，那么可以用 softplus 函数来获得正的精度向量：$\boldsymbol \beta=\boldsymbol zeta(\boldsymbol a)$。这种相同的策略对于方差或标准差同样适用，也适用于常数乘以单位阵的情况。

<span style="color:red;">上面这段没看懂。</span>

学习一个比对角矩阵具有更丰富结构的协方差或者精度矩阵是很少见的。如果协方差矩阵是满的和有条件的，那么参数化的选择就必须要保证预测的协方差矩阵是正定的。这可以通过写成 $\boldsymbol Sigma(\boldsymbol x)=\boldsymbol B(\boldsymbol x)\boldsymbol B^\top (\boldsymbol x)$ 来实现，这里 $\boldsymbol B$ 是一个无约束的方阵。如果矩阵是满秩的，那么一个实际问题是计算似然的代价是很高的，计算一个 $d\times d$ 的矩阵的行列式或者 $\boldsymbol Sigma(\boldsymbol x)$ 的逆（或者等价地并且更常用地，对它特征值分解或者 $\boldsymbol B(\boldsymbol x)$ 的特征值分解）需要 $O(d^3)$ 的计算量。


我们经常想要执行多峰回归(multimodal regression)，即预测条件分布 $p(\boldsymbol y\mid\boldsymbol x)$ 的实值，该条件分布对于相同的 $\boldsymbol x$ 值在 $\boldsymbol y$ 空间中有多个不同的峰值。在这种情况下，高斯混合是输出的自然表示。将高斯混合作为其输出的神经网络通常被称为混合密度网络。具有 $n$ 个分量的高斯混合输出由下面的条件分布定义：

$$
p(\boldsymbol y\mid\boldsymbol x) = \sum_{i=1}^n p(\mathrm c = i \mid \boldsymbol x) \mathcal N(\boldsymbol y; \boldsymbol mu^{(i)}(\boldsymbol x), \boldsymbol Sigma^{(i)}(\boldsymbol x)).
$$

神经网络必须有三个输出：定义 $p(\mathrm c=i\mid\boldsymbol x)$ 的向量，对所有的 $i$ 给出 $\boldsymbol mu^{(i)}(\boldsymbol x)$ 的矩阵，以及对所有的 $i$ 给出 $\boldsymbol Sigma^{(i)}(\boldsymbol x)$ 的张量。这些输出必须满足不同的约束：

- 混合组件 $p(\mathrm c=i\mid\boldsymbol x)$：它们由潜变量(注：我们之所以认为 $\mathrm c$ 是潜在的，是因为我们不能直接在数据中观测到它：给定输入 $\mathbf x$ 和目标 $\mathbf y$，不可能确切地知道是哪个高斯组件产生 $\mathbf y$，但我们可以想象 $\mathbf y$ 是通过选择其中一个来产生的，并且将那个未被观测到的选择作为随机变量。) $\mathrm c$ 关联着，在 $n$ 个不同组件上形成 Multinoulli 分布。这个分布通常可以由 $n$ 维向量的 softmax 来获得，以确保这些输出是正的并且和为 1。
- 均值 $\boldsymbol mu^{(i)}(\boldsymbol x)$ ：它们指明了与第 $i$ 个高斯组件相关联的中心或者均值，并且是无约束的（通常对于这些输出单元完全没有非线性）。如果 $\mathbf y$ 是个 $d$ 维向量，那么网络必须输出一个由 $n$ 个这种 $d$ 维向量组成的 $n\times d$ 的矩阵。用最大似然来学习这些均值要比学习只有一个输出模式的分布的均值稍稍复杂一些。我们只想更新那个真正产生观测数据的组件的均值。在实践中，我们并不知道是哪个组件产生了观测数据。负对数似然表达式将每个样本对每个组件的贡献进行赋权，权重的大小由相应的组件产生这个样本的概率来决定。
- 协方差 $\boldsymbol Sigma^{(i)}(\boldsymbol x)$：它们指明了每个组件 $i$ 的协方差矩阵。和学习单个高斯组件时一样，我们通常使用对角矩阵来避免计算行列式。和学习混合均值时一样，最大似然是很复杂的，它需要将每个点的部分责任分配给每个混合组件。如果给定了混合模型的正确的负对数似然，梯度下降将自动地遵循正确的过程。


有报告说基于梯度的优化方法对于混合条件高斯（作为神经网络的输出）可能是不可靠的，部分是因为涉及到除法（除以方差）可能是数值不稳定的（当某个方差对于特定的实例变得非常小时，会导致非常大的梯度）。一种解决方法是梯度截断（见第 10.11.1 节），另外一种是启发式缩放梯度。<span style="color:red;">哇塞，什么是梯度截断和启发式缩放梯度？</span>


高斯混合输出在语音生成模型和物理运动中特别有效。<span style="color:red;">这么厉害吗？</span>混合密度策略为网络提供了一种方法来表示多种输出模式，并且控制输出的方差，这对于在这些实数域中获得高质量的结果是至关重要的。混合密度网络的一个实例如图 6.4 所示。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190710/zBdECkIoSeVV.png?imageslim">
</p>

> 从具有混合密度输出层的神经网络中抽取的样本。输入 $x$ 从均匀分布中采样，输出 $y$ 从 $p_{\text{model}}(y\mid x)$ 中采样。 神经网络能够学习从输入到输出分布的参数的非线性映射。这些参数包括控制三个组件中的哪一个将产生输出的概率，以及每个组件各自的参数。每个混合组件都是高斯分布，具有预测的均值和方差。 输出分布的这些方面都能够相对输入 $x$ 变化，并且以非线性的方式改变。


一般的，我们可能希望继续对包含更多变量的、更大的向量 $\boldsymbol y$ 来建模，并在这些输出变量上施加更多更丰富的结构。例如，我们可能希望神经网络输出字符序列形成一个句子。在这些情况下，我们可以继续使用最大似然原理应用到我们的模型 $p(\boldsymbol y;\boldsymbol \omega(\boldsymbol x))$ 上，但我们用来描述 $\boldsymbol y$ 的模型会变得非常复杂，超出了本章的范畴。第十章描述了如何使用循环神经网络来定义这种序列上的模型，第三部分描述了对任意概率分布进行建模的高级技术。







# 相关

- 《深度学习》花书
