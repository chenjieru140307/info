# 随机梯度下降

## 神经网络的训练过程

神经网络的训练过程：

- 神经网络的训练过程可看作一个参数寻优过程，即在参数空间中，寻找一组最优参数使得 $E$ 最小。
  - $E$ 表示神经网络在训练集上的误差，它是关于连接权 $\boldsymbol{w}$ 和阈值 $\theta$ 的函数。


基于梯度的搜索：

- 是使用最为广泛的参数寻优方法。
- 过程：
  - 我们从 某些初始解出发，迭代寻找最优参数值。
  - 每次迭代中，我们先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。
    - 例如，由于负梯度方向是函数值下降 最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。
  - 若误差函数在当前点的梯度为零，则已达到局部极小，更新量将为零，这意味着参数的迭代更新将在此停止。
    - 显然，如果误差函数仅有一个局部极小，那么此时找到的局部极小就是全局最小；然而，如果误差函数具有多个局部极小，则不能保证找到的解 是全局最小。对后一种情形，我们称参数寻优陷入了局部极小，这显然不是我们所希望的。
  - 在现实任务中，人们常采用以下策略来试图“跳出”局部极小，从而进一 步接近全局最小：
    - 以多组不同参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数。
      - 这相当于从多个不同的初始点开始搜索，这样就可能陷入不同的局部极小，从中进行选择有可能获得更接近全局最小的结果.
    - 使用模拟退火 (simulated annealing)技术
      - 模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于“跳出”局部极小。在每步迭代过程中，接受 “次优解” 的概率要随着时间的推移而逐渐降低，从而保证算法稳定。（怎么实现的）
    - 使用随机梯度下降。
      - 与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机因素。于是，即便陷入局部极小点，它计算出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索.
    - 注意：上述用于跳出局部极小的技术大多是启发式，理论上尚缺乏保障。


此外，遗传算法也常用来训练神经网络以更好地逼近全局最小。（补充）




## 随机梯度下降

随机梯度下降是梯度下降算法的一个扩展。


梯度下降：

- 代价函数通常可以分解成每个样本的代价函数的总和。例如，训练数据的负条件对数似然可以写成

    $$
    J(\boldsymbol{\theta})=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim \hat{p}_{\mathrm{data}}} L(\boldsymbol{x}, y, \boldsymbol{\theta})=\frac{1}{m} \sum_{i=1}^{m} L\left(\boldsymbol{x}^{(i)}, y^{(i)}, \boldsymbol{\theta}\right)
    $$

- 其中 $L$ 是每个样本的损失 $L(\boldsymbol{x}, y, \boldsymbol{\theta})=-\log p(y | \boldsymbol{x} ; \boldsymbol{\theta})$。
- 对于这些相加的代价函数，梯度下降需要计算

$$
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta})=\frac{1}{m} \sum_{i=1}^{m} \nabla_{\boldsymbol{\theta}} L\left(\boldsymbol{x}^{(i)}, y^{(i)}, \boldsymbol{\theta}\right)
$$

- 这个运算的计算代价是 $O(m)$。随着训练集规模增长为数十亿的样本，计算一步梯度也会消耗相当长的时间。

随机梯度下降：

- 核心是，梯度是期望。
  - 期望可使用小规模的样本近似估计。
- 具体而言：
  - 在算法的每一步，我们从训练集中均匀抽出一小批量样本 $\mathbb{B}=\left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{\left(m^{\prime}\right)}\right\}$。小批量的数目 $m'$ 通常是一个相对较小的数，从一到几百。
    - 重要的是，当训练集大小 $m$ 增长时，$m'$ 通常是固定的。我们可能在拟合几十亿的样本时，每次更新计算只用到几百个样本。
  - 梯度的估计可以表示成
    $$
    \boldsymbol{g}=\frac{1}{m^{\prime}} \nabla_{\boldsymbol{\theta}} \sum_{i=1}^{m^{\prime}} L\left(\boldsymbol{x}^{(i)}, y^{(i)}, \boldsymbol{\theta}\right)
    $$

  - 使用来自小批量 $\mathbb{B}$ 的样本。然后，随机梯度下降算法使用如下的梯度下降估计：

    $$
    \boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\epsilon \boldsymbol{g}
    $$

    - 其中，$\epsilon$ 是学习率。
  - 注意：
    - 用于非凸损失函数的随机梯度下降并没有收敛到极小值的保证，并且对参数的初始值很敏感。
    - 当训练集的大小 $m$ 趋向于无穷大时，该模型最终会在随机梯度下降抽样完训练集上的所有样本之前收敛到可能的最优测试误差。继续增加 $m$ 不会延长达到模型可能的最优测试误差的时间。从这点来看，我们可以认为用 SGD 训练模型的渐近代价是关于 $m$ 的函数的 $O(1)$ 级别。


