# 神经网络


缘由：


- 非线性的分类问题：
  - 如果用 LR 或者 SVM，可以引入一些非线性的 feature。
    - 比如构造一些平方项或者立方项作为特征：$x_1x_2$，$x_1^2$，$x_2^2$ ，$x_1^2x_2$ 等。
  - 如果用 SVM ，可以加一个 kernal，把特征做一个映射，然后再做分割。
  - 可以做成 多个分类器的组合。
    - 比如用多个 weak learner 去组成一个 GBDT ，这样就可以组合成一个决策增强树这样一个分类器。
- 此时，如果特征较多，没法可视化：
  - 对于第一个方法：
    - 此时，无法知道是 $x_1^2$ 有用 还是 $x_2^2$ 有用，还是两个的乘积有用。
    - 假如有 100 万个特征，你去做特征组合，那么最后的维度是非常高的，而且你不知道那个是有用的。
  - 对于第二个方法，效果也不是很好。
  - 对于第三个方法。
    - 工业界比较常用，但是，构造起来也是比较麻烦的。
- 而这些，对于神经网络来说，这些都不是问题，它可以完成任何的一种分布的划分。

介绍:


- 神经网络 neural networks：
  - 神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。
- 神经元 neuron ：
  - 即上述定义中的 “简单单元”。
- 在生物神经网络中：
  - 每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；
  - 如果某神经元的电位超过了一个“阈值” (threshold)，那么它就会被激活，即 “兴奋” 起来，向其他神经元发送化学物质。


## 深层神经网络

神经网络在一定范围内更深的好处：

- 在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。
  - 如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。
- 隐藏层增加，意味着由激活函数带来的非线性变换的嵌套层数更多，能构造更复杂的映射关系。
  - 浅层的神经元从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而深层的神经元可以浅层特征继续学习更高级的特征。



深层神经网络难以训练的原因：

- 梯度消失
  - 在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。
  - 后果：
    - 梯度消失时，此时，前面层的学习会显著慢于后面层的学习，学习会卡住。
  - 原因：
    - 受到多种因素影响，例如
      - 学习率的大小
      - 网络参数的初始化
      - 激活函数的边缘效应等
  - 示意：下图是不同隐含层的学习速率：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/Ntc6QPR3Rbaa.png?imageslim">
    </p>
- 梯度爆炸
  - 在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；
    - 在极端情况下，权重值甚至会溢出，变为 $NaN$ 值，再也无法更新。
- 权重矩阵的退化导致模型的有效自由度减少。（没明白）
  - 参数空间中学习的退化速度减慢，导致减少了模型的有效维数，网络的可用自由度对学习中梯度范数的贡献不均衡，随着相乘矩阵的数量（即网络深度）的增加，矩阵的乘积变得越来越退化。
  - 在有硬饱和边界的非线性网络中（例如 ReLU 网络），随着深度增加，退化过程会变得越来越快。Duvenaud等人 2014 年的论文里展示了关于该退化过程的可视化：

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/HCw9Gq8BPCXB.jpg?imageslim">
    </p>

  - 随着深度的增加，输入空间（左上角所示）会在输入空间中的每个点处被扭曲成越来越细的单丝，只有一个与细丝正交的方向影响网络的响应。沿着这个方向，网络实际上对变化变得非常敏感。


## 深度学习



深度学习和机器学习对比：

- 机器学习：
  - 利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。
  - 机器学习的过程，就是训练数据去优化目标函数。
  - 传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。
- 深度学习：
  - 是一种特殊的机器学习，具有强大的能力和灵活性。
  - 它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。
  - 深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程。

## 计算



计算神经网络的输出：

（后续整合到 反馈的计算里）

<p align="center">
    <img width="80%" height="70%" src="http://images.iterate.site/blog/image/20190722/6nKj2eWXtdhj.png?imageslim">
</p>

过程：

- 上图中，
  - 输入层有三个节点，我们将其依次编号为 1、2、3；
  - 隐藏层的 4 个节点，编号依次为 4、5、6、7；
  - 最后输出层的两个节点编号为 8、9。
  - 比如，隐藏层的节点 4，它和输入层的三个节点 1、2、3 之间都有连接，其连接上的权重分别为是 $w_{41}, w_{42}, w_{43}$。
- 计算节点 4 的输出值：
  - 我们须先得到其所有上游节点（也就是节点 1、2、3）的输出值。节点 1、2、3 是输入层的节点，所以，他们的输出值就是输入向量本身。
  - 按照上图画出的对应关系，可以看到节点 1、2、3 的输出值分别是 $x_1, x_2, x_3$。
  - 则：$a_4 = \sigma(w^T \cdot a) = \sigma(w_{41}x_4 + w_{42}x_2 + w_{43}a_3 + w_{4b})$
  - 其中：
    - $w_{4b}$ 是节点 4 的偏置项。
- 计算输出层的节点 8 的输出值 $y_1​$：
  - $y_1 = \sigma(w^T \cdot a) = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b})$
  - 其中：
    - $w_{8b}​$ 是节点 8 的偏置项。

