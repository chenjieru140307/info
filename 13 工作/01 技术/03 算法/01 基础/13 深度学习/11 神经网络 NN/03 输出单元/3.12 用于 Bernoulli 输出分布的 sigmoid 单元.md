
# 用于 Bernoulli 输出分布的 sigmoid 单元

缘由：

- 许多任务需要预测二值型变量 $y$ 的值。
  - 具有两个类的分类问题可以归结为这种形式。

由最大似然开始：

- 此时最大似然的方法是定义 $y$ 在 $\boldsymbol x$ 条件下的 Bernoulli 分布。
- Bernoulli 分布仅需单个参数来定义。神经网络只需要预测 $P(y =1\mid\boldsymbol x)$ 即可。为了使这个数是有效的概率，它必须处在区间 $[0, 1]$ 中。
- 为满足该约束条件需要一些细致的设计工作。假设我们打算使用线性单元，并且通过阈值来限制它成为一个有效的概率：

$$
P(y=1 \mid \boldsymbol x) = \max \left \{ 0, \min \{1, \boldsymbol w^\top \boldsymbol h+b \} \right \}.
$$

- 这的确定义了一个有效的条件概率分布，但我们无法使用梯度下降来高效地训练它。
  - 当 $\boldsymbol w^\top \boldsymbol h+b$ 处于单位区间外时，模型的输出对其参数的梯度都将为 $\bm{0}$。梯度为 $\bm{0}$ 通常是有问题的，因为学习算法对于如何改善相应的参数不再具有指导意义。
- 此时，最好是使用一种新的方法来保证无论何时模型给出了错误的答案时，总能有一个较大的梯度。这种方法是基于使用 sigmoid  输出单元结合最大似然来实现的。

sigmoid 输出单元：

- 定义：

$$
\hat{y} = \sigma \left (\boldsymbol w^\top \boldsymbol h + b \right ),
$$

- 说明：
  - $\sigma$ 为 logistic sigmoid 函数。
  - 可以认为 sigmoid  输出单元具有两个部分：
    - 首先，它使用一个线性层来计算 $z=\boldsymbol w^\top \boldsymbol h+b$
    - 接着，它使用 sigmoid  激活函数将 $z$ 转化成概率。

我们暂时忽略对于 $\boldsymbol x$ 的依赖性，只讨论如何用 $z$ 的值来定义 $y$ 的概率分布。sigmoid 可以通过构造一个非归一化（和不为 1）的概率分布 $\tilde{P}(y)$ 来得到。<span style="color:red;">这句话没懂，sigmoid 可以通过构造一个非归一化（和不为 1）的概率分布 $\tilde{P}(y)$ 来得到 是什么意思？</span>我们可以随后除以一个合适的常数来得到有效的概率分布。如果我们假定非归一化的对数概率对 $y$ 和 $z$ 是线性的，<span style="color:red;">什么是 非归一化的对数概率对 $y$ 和 $z$ 是线性的？</span>可以对它取指数来得到非归一化的概率。我们然后对它归一化，可以发现这服从 Bernoulli 分布，该分布受 $z$ 的 sigmoid 变换控制：


$$
\begin{aligned}
\log \tilde{P}(y) &= yz,\\
\tilde{P}(y) &= \exp(yz),\\
P(y) &= \frac{\exp(yz)}{\sum_{y' = 0}^1 \exp(y' z)},\\
P(y) &= \sigma((2y-1)z).
\end{aligned}
$$

<span style="color:red;">没看懂。</span>

基于指数和归一化的概率分布在统计建模的文献中很常见。用于定义这种二值型变量分布的变量 $z$ 被称为分对数。<span style="color:red;">什么是分对数？</span>


这种在对数空间里预测概率的方法可以很自然地使用最大似然学习。因为用于最大似然的代价函数是 $-\log P(y\mid\boldsymbol x)$，代价函数中的 log 抵消了 sigmoid  中的 exp。如果没有这个效果，sigmoid 的饱和性会阻止基于梯度的学习做出好的改进。我们使用最大似然来学习一个由 sigmoid 参数化的 Bernoulli 分布，它的损失函数为

$$\begin{aligned}
J(\boldsymbol \theta) &= -\log P(y\mid\boldsymbol x)\\
&= -\log \sigma ((2y-1)z)\\
&= \zeta((1-2y)z).
\end{aligned}$$

<span style="color:red;">没看懂。</span>

这个推导使用了第 3.10节中的一些性质。通过将损失函数写成 softplus 函数的形式，我们可以看到它仅仅在 $(1-2y)z$ 取绝对值非常大的负值时才会饱和。因此饱和只会出现在模型已经得到正确答案时 - 当 $y=1$ 且 $z$ 取非常大的正值时，或者 $y=0$ 且 $z$ 取非常小的负值时。当 $z$ 的符号错误时，softplus函数的变量 $(1-2y)z$ 可以简化为 $|z|$。当 $|z|$ 变得很大并且 $z$ 的符号错误时，softplus 函数渐近地趋向于它的变量 $|z|$。对 $z$ 求导则渐近地趋向于 $\text{sign}(z)$，所以，对于极限情况下极度不正确的 $z$，softplus 函数完全不会收缩梯度。这个性质很有用，因为它意味着基于梯度的学习可以很快地改正错误的 $z$。<span style="color:red;">没明白。</span>

当我们使用其他的损失函数，例如均方误差之类的，损失函数会在 $\sigma(z)$ 饱和时饱和。sigmoid 激活函数在 $z$ 取非常小的负值时会饱和到 0，当 $z$ 取非常大的正值时会饱和到 1。这种情况一旦发生，梯度会变得非常小以至于不能用来学习，无论此时模型给出的是正确还是错误的答案。因此，最大似然几乎总是训练 sigmoid  输出单元的优选方法。<span style="color:red;">有点理解又有点不清楚。</span>

理论上，sigmoid 的对数总是确定和有限的，因为 sigmoid  的返回值总是被限制在开区间 $(0, 1)$ 上，而不是使用整个闭区间 $[0, 1]$ 的有效概率。在软件实现时，为了避免数值问题，最好将负的对数似然写作 $z$ 的函数，而不是 $\hat{y}=\sigma(z)$ 的函数。如果 sigmoid  函数下溢到零，那么之后对 $\hat{y}$ 取对数会得到负无穷。<span style="color:red;">没懂。</span>







# 相关

- 《深度学习》花书
