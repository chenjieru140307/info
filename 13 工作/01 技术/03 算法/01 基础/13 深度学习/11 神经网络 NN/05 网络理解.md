# 网络理解

## 万能近似性质和深度

万能近似定理：

- 一个前馈神经网络如果具有线性输出层和至少一层具有任何一种"挤压"性质的激活函数（例如 logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的 Borel 可测函数。
  - 定义在 $\mathbb R^n$ 的有界闭集上的任意连续函数是 Borel 可测的，因此可以用神经网络来近似。

理解：

- 万能逼近定理意味着无论我们试图学习什么函数，我们知道一个大的 MLP 一定能够**表示**这个函数。
- 然而，我们不能保证训练算法能够**学得**这个函数。
  - 即使 MLP 能够表示该函数，学习也可能因两个不同的原因而失败：
    - 首先，用于训练的优化算法可能找不到用于期望函数的参数值。
    - 其次，训练算法可能由于过拟合而选择了错误的函数。
  - 没有免费的午餐 定理，说明了没有普遍优越的机器学习算法。
    - 前馈网络提供了表示函数的万能系统，在这种意义上，给定一个函数，存在一个前馈网络能够近似该函数。不存在万能的过程既能够验证训练集上的特殊样本，又能够选择一个函数来扩展到训练集上没有的点。
- 万能逼近定理说明了，存在一个足够大的网络能够达到我们所希望的任意精度，但是定理并没有说这个网络有多大。Barron93 提供了单层网络近似一大类函数所需大小的一些界。不幸的是，在最坏情况下，可能需要指数数量的隐藏单元（可能一个隐藏单元对应着一个需要区分的输入配置）。
  - 这在二进制值的情况下很容易看到：向量 $\boldsymbol v \in \{0,1\}^n$ 上的可能的二值型函数的数量是 $2^{2^n}$，并且选择一个这样的函数需要 $2^n$ 位，这通常需要 $O(2^n)$ 的自由度。（没理解）
- 总之，具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。在很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。

存在一些函数族能够在网络的深度大于某个值 $d$ 时被高效地近似，而当深度被限制到小于或等于 $d$ 时需要一个远远大于之前的模型。在很多情况下，浅层模型所需的隐藏单元的数量是 $n$ 的指数级。这个结果最初被证明是在那些不与连续可微的神经网络类似的机器学习模型中出现，但现在已经扩展到了这些模型。第一个结果是关于逻辑门电路的。后来的工作将这些结果扩展到了具有非负权重的线性阈值单元，然后扩展到了具有连续值激活的网络。许多现代神经网络使用整流线性单元。{Leshno-et-al-1993} 证明带有一大类非多项式激活函数族的浅层网络，包括整流线性单元，具有万能的近似性质，但是这些结果并没有强调深度或效率的问题——它们仅指出足够宽的整流网络能够表示任意函数。{Montufar-et-al-NIPS2014} 指出一些用深度整流网络表示的函数可能需要浅层网络（一个隐藏层）指数级的隐藏单元才能表示。更确切的说，他们说明分段线性网络（可以通过整流非线性或 maxout单元获得）可以表示区域的数量是网络深度的指数级的函数。<span style="color:red;">这里为什么是非线性？</span>图 6.5 解释了带有绝对值整流的网络是如何创建函数的镜像图像的，这些函数在某些隐藏单元的顶部计算，作用于隐藏单元的输入。<span style="color:red;">这句没明白。不过 绝对值整流 是 </span>每个隐藏单元指定在哪里折叠输入空间，来创造镜像响应（在绝对值非线性的两侧）。通过组合这些折叠操作，我们获得指数级的分段线性区域，他们可以概括所有种类的规则模式（例如，重复）。<span style="color:red;">哈哈，有些厉害。</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190712/VtPcch6VpP0D.png?imageslim">
</p>

> 关于更深的整流网络具有指数优势的一个直观的几何解释，来自 {Montufar-et-al-NIPS2014}。
>
> - (左) 绝对值整流单元对其输入中的每对镜像点有相同的输出。镜像的对称轴由单元的权重和偏置定义的超平面给出。 在该单元顶部计算的函数（绿色决策面）将是横跨该对称轴的更简单模式的一个镜像。
> - (中) 该函数可以通过折叠对称轴周围的空间来得到。
> - (右) 另一个重复模式可以在第一个的顶部折叠（由另一个下游单元）以获得另外的对称性（现在重复四次，使用了两个隐藏层）。

<span style="color:red;">太酷炫了，有点没有很深理解。</span>


{Montufar-et-al-NIPS2014} 的主要定理指出，具有 $d$ 个输入、深度为 $l$、每个隐藏层具有 $n$ 个单元的深度整流网络可以描述的线性区域的数量是

$$
O \left ( {n \choose d}^{d(l-1)} n^d \right ),
$$

<span style="color:red;">没懂</span>

意味着，这是深度 $l$ 的指数级。

在每个单元具有 $k$ 个过滤器的 maxout 网络中，线性区域的数量是

$$
O \left ( k^{(l-1)+d} \right ).
$$

<span style="color:red;">没懂。</span>

当然，我们不能保证在机器学习（特别是 AI）的应用中我们想要学得的函数类型享有这样的属性。

我们还可能出于统计原因来选择深度模型。任何时候，当我们选择一个特定的机器学习算法时，我们隐含地陈述了一些先验，这些先验是关于算法应该学得什么样的函数的。选择深度模型默许了一个非常普遍的信念，那就是我们想要学得的函数应该涉及几个更加简单的函数的组合。<span style="color:red;">嗯。</span>这可以从表示学习的观点来解释， 我们相信学习的问题包含发现一组潜在的变差因素，<span style="color:red;">什么是变差因素？</span>它们可以根据其他更简单的潜在的变差因素来描述。或者，我们可以将深度结构的使用解释为另一种信念，那就是我们想要学得的函数是包含多个步骤的计算机程序，其中每个步骤使用前一步骤的输出。这些中间输出不一定是变差因素，而是可以类似于网络用来组织其内部处理的计数器或指针。<span style="color:red;">什么意思？</span>根据经验，更深的模型似乎确实在广泛的任务中泛化得更好。图 6.6 和图 6.7展示了一些实验结果的例子。这表明使用深层架构确实在模型学习的函数空间上表示了一个有用的先验。<span style="color:red;">更深的模型似乎确实在广泛的任务中泛化得更好 为什么？深度架构到底表征了一个什么先验？</span>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190712/oAUPHUJLthnx.png?imageslim">
</p>

> 图 6.6 深度的影响。实验结果表明，当从地址照片转录多位数字时，更深层的网络能够更好地泛化。数据来自 {Goodfellow+et+al-ICLR2014a}。测试集上的准确率随着深度的增加而不断增加。图 6.7 给出了一个对照实验，它说明了对模型尺寸其他方面的增加并不能产生相同的效果。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190712/txhRBcxMtp8G.png?imageslim">
</p>

> 图 6.7 参数数量的影响。更深的模型往往表现更好。这不仅仅是因为模型更大。{Goodfellow+et+al-ICLR2014a} 的这项实验表明，增加卷积网络层中参数的数量，但是不增加它们的深度，在提升测试集性能方面几乎没有效果，如此图所示。图例标明了用于画出每条曲线的网络深度，以及曲线表示的是卷积层还是全连接层的大小变化。我们可以观察到，在这种情况下，浅层模型在参数数量达到 2000 万时就过拟合，而深层模型在参数数量超过 6000 万时仍然表现良好。这表明，使用深层模型表达出了对模型可以学习的函数空间的有用偏好。具体来说，它表达了一种信念，即该函数应该由许多更简单的函数复合在一起而得到。这可能导致学习由更简单的表示所组成的表示（例如，由边所定义的角）或者学习具有顺序依赖步骤的程序（例如，首先定位一组对象，然后分割它们，之后识别它们）。<span style="color:red;">嗯嗯，厉害，感觉这个揭示了很多东西，也就是说深度本身暗合了一些道理，嗯，也就是说，自然界的数据集是存在一些道理的，浅层的网络揭示不出这些道理，深层的网络暗合了这些道理。嗯，感觉有很多可以理解的。 </span>







# 相关

- 《深度学习》花书
