
# 感知机

感知机 Perceptron

## 单层感知机


两个输入神经元的感知机网络结构：

<p align="center">
    <img width="60%" height="70%" src="http://images.iterate.site/blog/image/20200617/UcIDphi0zFzC.png?imageslim">
</p>


实现逻辑与、或、非：


- $y=f(\sum_i w_ix_i+b)$
- 假定 $f$ 是阶跃函数
- 有：
    - 与 ($x_1\wedge x_2$): 
      - 令 $w_1=w_2=1$，$b=-2$ ，
      - 则 $y=f(1\cdot x_1+1\cdot x_2 -2)$ ，仅在 $x_1=x_2=1$ 时，$y=1$ 。
    - 或 ($x_1\vee x_2$): 
      - 令 $w_1=w_2=1$，$b=-0.5$
      - 则 $y=f(1\cdot x_1+1\cdot x_2 -0.5)$ ，当 $x_1=1$ 或 $x_2=1$ 时，$y=1$ 。
    - 非 ($\neg x_1$): 
      - 令 $w_1=-0.6$，$w_2=0$，$b=0.5$
      - 则 $y=f(-0.6\cdot x_1+0\cdot x_2 +0.5)$ ，当 $x_1=1$ 时，y=0；当 x_1=0 时，$y=1$ 。



线性可分的与或非，与 非线性可分的 异或：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200617/127cj5OWq3eY.png?imageslim">
</p>


感知机的权重更新推导：

- 感知机的模型为：

$$y=f(\sum_{i} w_i x_i +b)$$

- 将 $b$ 偏置看作一个固定输入为 1 的 “哑结点”
- 则模型可化简为：

$$y=f(\sum_{i} w_i x_i)=f(\boldsymbol w^T \boldsymbol x)$$

- 其中:
  - $f$ 为阶跃函数。
- 假设：
  - 误分类点集合为 $M$
  - $\boldsymbol x_i \in M$ 为误分类点
  - $\boldsymbol x_i$ 的真实标签为 $y_i$，模型的预测值为 $\hat{y_i}$
- 对于误分类点 $\boldsymbol x_i$ 来说，此时
  - $\boldsymbol w^T \boldsymbol x_i \gt 0,\hat{y_i}=1,y_i=0$
  - 或 $\boldsymbol w^T \boldsymbol x_i \lt 0,\hat{y_i}=0,y_i=1$
- 综合考虑两种情形可得：
    - $(\hat{y_i}-y_i)\boldsymbol w \boldsymbol x_i>0$
- 则，此时损失函数为：

$$L(\boldsymbol w)=\sum_{\boldsymbol x_i \in M} (\hat{y_i}-y_i)\boldsymbol w \boldsymbol x_i$$

- 损失函数的梯度为：
$$\nabla_w L(\boldsymbol w)=\sum_{\boldsymbol x_i \in M} (\hat{y_i}-y_i)\boldsymbol x_i$$

- 随机选取一个误分类点 $(\boldsymbol x_i,y_i)$，对 $\boldsymbol w$ 进行更新：

$$
\begin{aligned}
&\boldsymbol w \leftarrow \boldsymbol w+\Delta \boldsymbol w
\\\Rightarrow &\boldsymbol w \leftarrow \boldsymbol w-\eta(\hat{y_i}-y_i)\boldsymbol x_i
\\\Rightarrow &\boldsymbol w \leftarrow \boldsymbol w+\eta(y_i-\hat{y_i})\boldsymbol x_i
\end{aligned}
$$

- 其中:
  - $\eta$ 称为学习率(learning rate)
- 可见：
  - 若感知机对训练样例 $(\boldsymbol{x}, y)$ 预测正确，即 $\hat{y}=y$，则感知机不发生变化，否则将根据错误的程度进行权重调整。


## 双层感知机

举例：

- 搭建一个可以学习 XOR 的网络。
- 异或：
  - 是两个二进制值 $x_1$ 和 $x_2$ 的运算：
    - 当这些二进制值中恰好有一个为 1 时，XOR 函数返回值为 1。
    - 其余情况下返回值为 0。
- 异或函数为 $y = f^*(\boldsymbol x)$
- 模型函数为 $y=f(\boldsymbol x; \boldsymbol \theta)$
- 我们使用算法不断调整参数 $\boldsymbol \theta$ 来使得 $f$ 尽可能接近 $f^*$。
- 样本：
  - 四个点 $\mathbb X =\{[0, 0]^\top, [0, 1]^\top, [1, 0]^\top, [1, 1]^\top\}$
- 损失函数：
  - 均方误差
- 则，此时，整个训练集上的 MSE 损失函数为

$$
J(\boldsymbol{\theta})=\frac{1}{4} \sum_{\boldsymbol{x} \in \mathbb{X}}\left(f^{*}(\boldsymbol{x})-f(\boldsymbol{x} ; \boldsymbol{\theta})\right)^{2}
$$

- 此时，我们需要选择我们模型 $f(\boldsymbol x; \boldsymbol \theta)$ 的形式。
  - 假设我们选择一个线性模型
    - $\boldsymbol \theta$ 包含 $\boldsymbol w$ 和 $b$
    - 那么我们的模型被定义成 $f(\boldsymbol x; \boldsymbol w, b) = \boldsymbol x^\top \boldsymbol w + b.$
    - 可以使用正规方程关于 $\boldsymbol w$ 和 $b$ 最小化 $J(\boldsymbol \theta)$，来得到一个闭式解。
    - 解正规方程以后，我们得到 $\boldsymbol w = 0$ 以及 $b = \frac{1}{2}$。
    - 即：
      - 线性模型仅仅是在任意一点都输出 0.5
    - 为什么会发生这种事？

    <p align="center">
        <img width="40%" height="70%" src="http://images.iterate.site/blog/image/20200617/zyiLYJYBx0JQ.png?imageslim">
    </p>
    
    - 说明：
      - 当 $x_1 = 0$ 时，模型的输出必须随着 $x_2$ 的增大而增大。
      - 当 $x_1 = 1$ 时，模型的输出必须随着 $x_2$ 的增大而减小。
      - 而线性模型必须对 $x_2$ 使用固定的系数 $w_2$。因此，线性模型不能使用 $x_1$ 的值来改变 $x_2$ 的系数，从而不能解决这个问题。

  - 引入一个简单的前馈神经网络。它有一层隐藏层并且隐藏层中包含两个单元。
    - 如下图：

    <p align="center">
        <img width="50%" height="70%" src="http://images.iterate.site/blog/image/20190710/juwopylK47P1.png?imageslim">
    </p>

    - 说明：
      - 这个前馈网络有一个通过函数 $f^{(1)}(\boldsymbol x;\boldsymbol W, \boldsymbol c)$ 计算得到的隐藏单元的向量 $\boldsymbol h$。这些隐藏单元的值随后被用作第二层的输入。
      - 第二层就是这个网络的输出层。输出层仍然只是一个线性回归模型，只不过现在它作用于 $\boldsymbol h$ 而不是 $\boldsymbol x$。
      - 网络现在包含链接在一起的两个函数：$\boldsymbol h=f^{(1)}(\boldsymbol x; \boldsymbol W, \boldsymbol c)$ 和 $y = f^{(2)}(\boldsymbol h; \boldsymbol w, b)$
      - 则，完整的模型是 $f(\boldsymbol x; \boldsymbol W, \boldsymbol c, \boldsymbol w, b) = f^{(2)}(f^{(1)}(\boldsymbol x))$。
    - 加入非线性
      - 如果 $f^{(1)}$ 是线性的，那么前馈网络作为一个整体对于输入仍然是线性的：
        - 暂时忽略截距项，假设 $f^{(1)}(\boldsymbol x)= \boldsymbol W^\top \boldsymbol x$ 并且 $f^{(2)}(\boldsymbol h)=\boldsymbol h^\top \boldsymbol w$，那么 $f(\boldsymbol x) = \boldsymbol w^\top\boldsymbol W^\top \boldsymbol x$。
        - 我们可以将这个函数重新表示成 $f(\boldsymbol x) = \boldsymbol x^\top\boldsymbol w'$ 其中 $\boldsymbol w' = \boldsymbol W\boldsymbol w$。
      - 显然，我们必须用非线性函数来描述这些特征。
        - 大多数神经网络通过仿射变换之后紧跟着一个被称为激活函数的固定非线性函数来实现这个目标，其中仿射变换由学得的参数控制。
        - 我们这里使用这种策略，定义 $\boldsymbol h=g(\boldsymbol W^\top \boldsymbol x+\boldsymbol c)$
        - 其中：
          - $\boldsymbol W$ 是线性变换的权重矩阵
          - $\boldsymbol c$ 是偏置。
            - 此前，为了描述线性回归模型，我们使用权重向量和一个标量的偏置参数来描述从输入向量到输出标量的仿射变换。现在，因为我们描述的是向量 $\boldsymbol x$ 到向量 $\boldsymbol h$ 的仿射变换，所以我们需要一整个向量的偏置参数。
          - $g$ 为 激活函数。通常选择对每个元素分别起作用的函数，有 $h_i =g(\boldsymbol x^\top \boldsymbol W_{:, i} + c_i)$。在现代神经网络中，默认的推荐是使用由激活函数 $g(z)=\boldsymbol ax\{0, z\}$ 定义的整流线性单元或者称为 ReLU，如图所示。

            <p align="center">
                <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190710/tsQaM8wTlmYM.png?imageslim">
            </p>
      - 此时，可以指明我们的整个网络是：
    $$
    f(\boldsymbol x; \boldsymbol W, \boldsymbol c, \boldsymbol w, b) = \boldsymbol w^\top \boldsymbol ax\{ 0, \boldsymbol W^\top \boldsymbol x + \boldsymbol c \} +b.
    $$
    - 给出一个解
      - 我们现在可以给出 XOR 问题的一个解。令
        - $\boldsymbol W = \begin{bmatrix} 1 & 1\\1 & 1 \end{bmatrix}$
        - $\boldsymbol c = \begin{bmatrix} 0\\-1 \end{bmatrix}$
        - $\boldsymbol w = \begin{bmatrix} 1\\ -2\end{bmatrix}$
        - $b=0$
      - 测试，令 $\boldsymbol X$ 表示样本矩阵
        $$
        \begin{aligned}
        \boldsymbol X &= \begin{bmatrix}
        0 & 0\\
        0 & 1\\
        1 & 0\\
        1 & 1
        \end{bmatrix}.
        \\\Rightarrow 
        \boldsymbol X\boldsymbol W &= \begin{bmatrix}
        0 & 0\\
        1 & 1\\
        1 & 1\\
        2 & 2
        \end{bmatrix}.
        \\\Rightarrow 
        \boldsymbol X\boldsymbol W+\boldsymbol{c} &= \begin{bmatrix}
        0 & -1\\
        1 & 0\\
        1 & 0\\
        2 & 1
        \end{bmatrix}.
        \\\Rightarrow 
        \text{ReLU}(\boldsymbol X\boldsymbol W+\boldsymbol{c}) &= \begin{bmatrix}
        0 & 0\\
        1 & 0\\
        1 & 0\\
        2 & 1
        \end{bmatrix}.
        \\\Rightarrow 
        \left(\text{ReLU}(\boldsymbol X\boldsymbol W+\boldsymbol{c})\right)\boldsymbol w &= \begin{bmatrix}
        0\\
        1\\
        1\\
        0
        \end{bmatrix}.
        \end{aligned}
        $$

      - 说明:
        - 第一行到第二行：将输入矩阵乘以第一层的权重矩阵：
        - 第二行到第三行：加上偏置向量 $\boldsymbol c$
        - 第三行：此时，可见，在这个空间中，所有的样本都处在一条斜率为 $1$ 的直线上。当我们沿着这条直线移动时，输出需要从 $0$ 升到 $1$，然后再降回 $0$。线性模型不能实现这样一种函数。
        - 第三行到第四行：使用整流线性变换。
        - 第四行：整流线性变换改变了样本间的关系。它们不再处于同一条直线上了。现在,它们处在一个可以用线性模型解决的空间上。
          - 如图：
            <p align="center">
            <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200617/KlpUUqQkgMke.png?imageslim">
            </p>
        - 第四行到第五行：乘以一个权重向量 $\boldsymbol w$。
    - 此时，神经网络对这一批次中的每个样本都给出了正确的结果。





