

# 从线性模型理解前馈网络

理解前馈网络：

- 一种理解前馈网络的方式是从线性模型开始，并考虑如何克服它的局限性。
  - 线性模型，例如逻辑回归和线性回归，是非常吸引人的，因为无论是通过闭解形式还是使用凸优化<span style="color:red;">什么是闭解形式？</span>，它们都能高效且可靠地拟合。线性模型也有明显的缺陷，那就是该模型的能力被局限在线性函数里，所以它无法理解任何两个输入变量间的相互作用。<span style="color:red;">什么叫无法理解任何两个输入变量之间的相互作用？</span>
  - 为了扩展线性模型来表示 $\boldsymbol x$ 的非线性函数，我们可以不把线性模型用于 $\boldsymbol x$ 本身，而是用在一个变换后的输入 $\phi(\boldsymbol x)$ 上，这里 $\phi$ 是一个非线性变换。同样，我们可以使用节 支持向量机 中描述的核技巧，来得到一个基于隐含地使用 $\phi$ 映射的非线性学习算法。我们可以认为 $\phi$ 提供了一组描述 $\boldsymbol x$ 的特征，或者认为它提供了 $\boldsymbol x$ 的一个新的表示。<span style="color:red;">新的表示，嗯嗯，这样理解有意思，可以的。每种映射都是一种新的表示。</span>
  - 剩下的问题就是如何选择映射 $\phi$。
    - 其中一种选择是使用一个通用的 $\phi$，例如无限维的 $\phi$，它隐含地用在基于 RBF 核的核机器上。如果 $\phi(\boldsymbol x)$ 具有足够高的维数，我们总是有足够的能力来拟合训练集，但是对于测试集的泛化往往不佳。非常通用的特征映射通常只基于局部光滑的原则，并且没有将足够的先验信息进行编码来解决高级问题。<span style="color:red;">隐含的用在基于 RBF 核的核机器上是什么意思？什么无限维的 $\phi$？</span>
    - 另一种选择是手动地设计 $\phi$。在深度学习出现以前，这一直是主流的方法。这种方法对于每个单独的任务都需要人们数十年的努力，从业者各自擅长特定的领域（如语音识别或计算机视觉），并且不同领域之间很难迁移(transfer)。
  - 深度学习的策略是去学习 $\phi$。在这种方法中，我们有一个模型 $y = f(\boldsymbol x;\theta, \boldsymbol w) = \phi(\boldsymbol x; \theta)^\top \boldsymbol w$。我们现在有两种参数：用于从一大类函数中学习 $\phi$ 的参数 $\boldsymbol \theta$，以及用于将 $\phi(\boldsymbol x)$ 映射到所需的输出的参数 $\boldsymbol w$。
  - 这是深度前馈网络的一个例子，其中 $\phi$ 定义了一个隐藏层。这是三种方法中唯一一种放弃了训练问题的凸性的，但是利大于弊。在这种方法中，我们将表示参数化为 $\phi(\boldsymbol x; \boldsymbol \theta)$，并且使用优化算法来寻找 $\boldsymbol \theta$，使它能够得到一个好的表示。<span style="color:red;">有寻找 $\boldsymbol \theta$ 吗？</span>如果我们想要的话，这种方法也可以通过使它变得高度通用以获得第一种方法的优点——我们只需使用一个非常广泛的函数族 $\phi(\boldsymbol x; \boldsymbol \theta)$。<span style="color:red;">什么？</span>这种方法也可以获得第二种方法的优点。人类专家可以将他们的知识编码进网络来帮助泛化，他们只需要设计那些他们期望能够表现优异的函数族 $\phi(\boldsymbol x; \boldsymbol \theta)$ 即可。这种方法的优点是人类设计者只需要寻找正确的函数族即可，而不需要去寻找精确的函数。
  - 这种通过学习特征来改善模型的一般化原则不仅仅适用于本章描述的前馈神经网络。它是深度学习中反复出现的主题，适用于全书描述的所有种类的模型。前馈神经网络是这个原则的应用，它学习从 $\boldsymbol x$ 到 $\boldsymbol y$ 的确定性映射并且没有反馈连接。后面出现的其他模型会把这些原则应用到学习随机映射、学习带有反馈的函数以及学习单个向量的概率分布。<span style="color:red;">嗯，看到之后补充下。</span>


