


ReLU 激活函数：


- $f(z)=\max (0, z)$
  - 值域为 $[0,+\infty)$；
- $f^{\prime}(z)=\left\{\begin{array}{l}{1, z>0} \\ {0, z \leqslant 0}\end{array}\right.$
- 函数图像如下：
  <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/JXUQebXcMFfq.png?imageslim">
  </p>

- 特点：
  - 单侧抑制；
  - 相对宽阔的兴奋边界
  - 稀疏激活性；
    - ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。
    - 因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。
    - **稀疏激活性**：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $x<0$ 时，ReLU 硬饱和，而当 $x>0$ 时，则不存在饱和问题。ReLU 能够在 $x>0$ 时保持梯度不衰减，从而缓解梯度消失问题。
  - 收敛速度比 sigmoid / tanh 函数快，这个是在实践中证明了的。
  - 计算高效简单
- 优点：
  - 使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快：
    - 只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数处处为 1。在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。
  - sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和 Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。
  - 需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。（什么时稀疏性？）
- 缺陷：
  - 整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活为零的样本。
- ReLU 的一个问题：
  - 非饱和性（saturation）：
    - 饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。
      - 最经典的例子是 Sigmoid，它的导数在 $x$ 为比较大的正值和比较小的负值时都会接近于 0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为 0，因此处处饱和，无法作为激活函数。
    - ReLU在 $x>0$ 时导数恒为 1，因此对于再大的正值也不会饱和。但同时对于 $x<0$，其梯度恒为 0，这时候它也会出现饱和的现象（在这种情况下通常称为 dying ReLU）。
    - 所以，这时，如果你的输入不巧全部在 ReLU 的左侧，即 Dead Area，那么这个神经元就挂掉了，再也没法激活，在后向传播时，相应的神经元的参数都不会更新。但是呢，这个发生的概率很小，因为是一批一批训练的。所以虽然可能很脆弱，但是大家都在用。
    - Leaky ReLU 和 PReLU 的提出正是为了解决这一问题。


- 为什么 ReLU 在 0 处不可微还可以用作激活函数呢：
  - 整流线性单元  $g(z)=\max\{0, z\}$  并不是在所有的输入点上都是可微的，在 $z=0$ 处不可微。
  - 这似乎使得 $g$ 对于基于梯度的学习算法无效。
  - 在实践中，梯度下降对这些机器学习模型仍然表现得足够好。部分原因是神经网络训练算法通常不会达到代价函数的局部最小值，而是仅仅显著地减小它的值，
  - 因为我们不再期望训练能够实际到达梯度为 $\bm{0}$ 的点，所以代价函数的最小值对应于梯度未定义的点是可以接受的。
  - 不可微的激活函数通常只在少数点上不可微。一般来说，函数 $g(z)$ 具有左导数和右导数，左导数定义为紧邻在 $z$ 左边的函数的斜率，右导数定义为紧邻在 $z$ 右边的函数的斜率。只有当函数在 $z$ 处的左导数和右导数都有定义并且相等时，函数在 $z$ 点处才是可微的。
  - 神经网络中用到的函数通常对左导数和右导数都有定义。在 $g(z)=\max\{0,z\}$ 的情况下，在 $z=0$ 处的左导数是 0，右导数是 1。神经网络训练的软件实现通常返回左导数或右导数的其中一个，而不是报告导数未定义或产生一个错误。
    - 这可以通过观察到在数字计算机上基于梯度的优化总是会受到数值误差的影响来启发式地给出理由。
    - 当一个函数被要求计算 $g(0)$ 时，底层值真正为 0 是不太可能的。相对的，它可能是被舍入为 0 的一个小量 $\epsilon$。
    - 在某些情况下，理论上有更好的理由，但这些通常对神经网络训练并不适用。重要的是，在实践中，我们可以放心地忽略下面描述的激活函数激活函数的不可微性。

Leaky ReLU 激活函数

- $f(x) =  \left\{\begin{aligned}
ax, \quad x<0 \\x, \quad x>0\end{aligned}\right.$
  - 值域为 $(-\infty,+\infty)$。
- 图像如下（$a = 0.5$）：常为 0.01

  <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/NvHfkJ7nPi7X.png?imageslim">
  </p>

- 与 ReLU 比较：
  - 在小于 0 的时候，斜率为很小的一个比如 0.1，这样在反向传播的时候也能少许的改变一下权重。

  <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/Hm8c961KBG.png?imageslim">
  </p>



maxout 激活函数：

- maxout 进一步扩展了 ReLU。
- maxout 单元将 $\boldsymbol z$ 划分为每组具有 $k$ 个值的组，而不是使用作用于每个元素的函数 $g(z)$ 。
- 每个 maxout 单元则输出每组中的最大元素：

$$
g(\boldsymbol z)_i = \underset{j\in \mathbb G^{(i)}}{\max} z_j
$$

- 这里 $\mathbb G^{(i)}$ 是组 $i$ 的输入索引集 $\{(i-1)k+1, \ldots, ik\}$。
- 这提供了一种方法来学习对输入 $\boldsymbol x$ 空间中多个方向响应的分段线性函数。

maxout 单元可以学习具有多达 $k$ 段的分段线性的凸函数。maxout 单元因此可以视为学习激活函数本身而不仅仅是单元之间的关系。使用足够大的 $k$，maxout单元可以以任意的精确度来近似任何凸函数。特别地，具有两块的 maxout 层可以学习实现和传统层相同的输入 $\boldsymbol x$ 的函数，这些传统层可以使用整流线性激活函数、绝对值整流、渗漏整流线性单元 或参数化整流线性单元，或者可以学习实现与这些都不同的函数。maxout 层的参数化当然也将与这些层不同，所以即使是 maxout 学习去实现和其他种类的层相同的 $\boldsymbol x$ 的函数这种情况下，学习的机理也是不一样的。<span style="color:red;">感觉非常厉害，但是还是理解的不够。</span>

每个 maxout 单元现在由 $k$ 个权重向量来参数化，而不仅仅是一个，所以 maxout 单元通常比整流线性单元需要更多的正则化。<span style="color:red;">是呀。</span>如果训练集很大并且每个单元的块数保持很低的话，它们可以在没有正则化的情况下工作得不错。

<span style="color:red;">这个 maxout 单元有使用过吗？</span>

maxout 单元还有一些其他的优点。在某些情况下，要求更少的参数可以获得一些统计和计算上的优点。具体来说，如果由 $n$ 个不同的线性过滤器描述的特征可以在不损失信息的情况下，用每一组 $k$ 个特征的最大值来概括的话，那么下一层可以获得 $k$ 倍更少的权重数 。<span style="color:red;">？没明白，什么是线性过滤器？什么是用每一组 $k$ 个特征的最大值来概括？</span>

因为每个单元由多个过滤器驱动，maxout 单元具有一些冗余来帮助它们抵抗一种被称为灾难遗忘的现象，这个现象是说神经网络忘记了如何执行它们过去训练的任务。<span style="color:red;">什么是灾难遗忘？</span>

整流线性单元和它们的这些扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。使用线性行为更容易优化的一般性原则同样也适用于除深度线性网络以外的情景。循环网络可以从序列中学习并产生状态和输出的序列。当训练它们时，需要通过一些时间步来传播信息，当其中包含一些线性计算（具有大小接近 1 的某些方向导数）时，这会更容易。作为性能最好的循环网络结构之一，LSTM 通过求和在时间上传播信息，这是一种特别直观的线性激活。它将在第 10.10 节中进一步讨论。

<span style="color:red;">maxout 真的有在使用吗？要怎么用？</span>


# 相关

- 《百面机器学习》
- 《深度学习》花书
