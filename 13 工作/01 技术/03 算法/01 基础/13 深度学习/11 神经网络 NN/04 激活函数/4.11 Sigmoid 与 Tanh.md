

Sigmoid 激活函数：

- $f(z)=\frac{1}{1+e^{-z}}$
  - 值域为 $(0,1)$。
    - 因此可以把输出的值看作是概率值。而概率值可以作为一个判别的标准。比如有了概率之后，我们通常可以给定一个分界线，比如 0.5 ，这样就可以来判定这个样本是正样本还是负样本。
- $f^{\prime}(z)=f(z)(1-f(z))$
- 函数图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/URVjdLLrU1FD.png?imageslim">
    </p>
- 对两个不同点的梯度的说明
  - 如图：

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/4hydKl8lFODO.jpg?imageslim">
    </p>

  - 说明:
    - 上图中，对 0.88 和 0.98 两个点进行比较：
      - 假设目标是收敛到 1.0。
        - 0.88 离目标 1.0 比较远，梯度比较大，权值调整比较大。
        - 0.98 离目标 1.0 比较近，梯度比较小，权值调整比较小。调整方案合理。
      - 假如目标是收敛到 0。
        - 0.88 离目标 0 比较近，梯度比较大，权值调整比较大。
        - 0.98 离目标 0 比较远，梯度比较小，权值调整比较小。调整方案不合理。
    - 即：在使用 sigmoid 函数的情况下, 初始的代价（误差）越大，导致训练越慢。
- 使用：
  - 以前这个是最常用的激活函数，但是现在一般只用在输出层，中间层很少使用。
  - 这是因为它有几个缺点：
    - sigmoid 函数的两头过于平坦，导致如果 $x$ 大于 5 时候，对 sigmoid 函数求导的话基本是 0，这样在应用链式法则进行求导的时候，导数非常容易是 0，会导致梯度消失。
      - 因为这个原因，现在不鼓励将它们用作前馈网络中的激活函数。当使用一个合适的代价函数来抵消 sigmoid 的饱和性时，它们作为输出单元可以与基于梯度的学习相兼容。
    - 而且，它的输出值是 0~1 ，没有负数，这样是不对称的，因此这会导致比如，输入的时候我还是有负值的，但是我下一层的输出就全是大于 0 的了。
    - 输出范围有限。
      - 有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如 Sigmoid、TanH。
      - 但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时 loss 函数中的 log 操作能够抵消其梯度消失的影响）、LSTM里的 gate 函数。

Tanh 激活函数：


- $f(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$
  - 值域为 $(-1,1)$
- $f^{\prime}(z)=1-(f(z))^{2}$
- 函数图像如下：

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/ViAgM2OV8sqr.png?imageslim">
    </p>
 
- 使用：
  - 与 sigmoid 类似，两头过于平坦，很容易导致梯度消失。
  - 当必须要使用 sigmoid  激活函数时，双曲正切激活函数通常要比 logistic sigmoid 函数表现更好。
- Tanh 与 Sigmoid 对比：
  - 为什么 Tanh 收敛速度比 Sigmoid 快？
    - 首先看如下两个函数的求导：
    $$
    \tanh ^{\prime}(x)=1-\tanh (x)^{2} \in(0,1)
    $$

    $$
    s^{\prime}(x)=s(x) *(1-s(x)) \in\left(0, \frac{1}{4}\right]
    $$
    - 可见，$\tanh(x)$ 梯度消失的问题比 $\text{sigmoid}$ 轻，所以 Tanh 收敛速度比 Sigmoid 快。
