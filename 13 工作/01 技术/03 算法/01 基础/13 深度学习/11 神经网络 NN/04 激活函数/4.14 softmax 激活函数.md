
 softmax 函数

函数定义为：

$$
\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}
$$

Softmax 多用于多分类神经网络输出。



### 3.4.9 Softmax 定义及作用

<span style="color:red;">softmax 的公式重新整理下。</span>

Softmax 是一种形如下式的函数：

$$
P(i) = \frac{exp(\theta_i^T x)}{\sum_{k=1}^{K} exp(\theta_i^T x)}
$$

其中，$\theta_i$ 和 $x$ 是列向量，$\theta_i^T x$ 可能被换成函数关于 $x$ 的函数 $f_i(x)$

通过 softmax 函数，可以使得 $P(i)$ 的范围在 $[0,1]$ 之间。在回归和分类问题中，通常 $\theta$ 是待求参数，通过寻找使得 $P(i)$ 最大的 $\theta_i$ 作为最佳参数。

但是，使得范围在 $[0,1]$  之间的方法有很多，为啥要在前面加上以 $e$ 的幂函数的形式呢？参考 logistic 函数：

$$
P(i) = \frac{1}{1+exp(-\theta_i^T x)}
$$

这个函数的作用就是使得 $P(i)$ 在负无穷到 $0$ 的区间趋向于 $0$， 在 $0$ 到正无穷的区间趋向 $1$。

同样 softmax 函数加入了 $e$ 的幂函数正是为了两极化：

- 正样本的结果将趋近于 1，
- 而负样本的结果趋近于 0。

这样为多类别提供了方便（可以把 $P(i)$ 看做是样本属于类别的概率）。可以说，Softmax 函数是 logistic 函数的一种泛化。

Softmax 函数可以把它的输入，通常被称为 logits 或者 logit scores，处理成 0 到 1 之间，并且能够把输出归一化到和为 1。这意味着 softmax 函数与分类的概率分布等价。它是一个网络预测多分类问题的最佳输出激活函数。<span style="color:red;">嗯，是最佳的吗？</span>




### 3.4.10 Softmax 函数如何应用于多分类？

​softmax 用于多分类过程中，它将多个神经元的输出，映射到 $(0,1)$ 区间内，可以看成概率来理解，从而来进行多分类！

​假设我们有一个数组，$V_i$ 表示 $V$  中的第 $i$ 个元素，那么这个元素的 softmax 值就是

$$
S_i = \frac{e^{V_i}}{\sum_j e^{V_j}}
$$

从下图看，神经网络中包含了输入层，然后通过两个特征层处理，最后通过 softmax 分析器就能得到不同条件下的概率，这里需要分成三个类别，最终会得到 $y=0, y=1, y=2$ 的概率值。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/j5uziqVDMLas.png?imageslim">
</p>


继续看下面的图，三个输入通过 softmax 后得到一个数组 $[0.05 , 0.10 , 0.85]​$，这就是 soft 的功能。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/J9YkaGnuOgU0.png?imageslim">
</p>


更形象的映射过程如下图所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/tpiMHpc7MX4g.png?imageslim">
</p>

softmax 直白来说就是将原来输出是 $3,1,-3​$ 通过 softmax 函数一作用，就映射成为 $(0,1)$ 的值，而这些值的累和为 $1​$（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！<span style="color:red;">嗯。</span>
