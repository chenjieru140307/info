
# 激活函数


激活函数：

- 对于深度神经网络，我们在每一层线性变换后叠加一个非线性激活函数，以避免多层网络等效于单层线性函数，从而获得更强大的学习与拟合能力。


激活函数的设计：

- 暂时还没有许多明确的指导性理论原则。


描述：

- 除非另有说明，大多数的激活函数都可以描述为：
- 接受输入向量 $\boldsymbol x$，计算仿射变换 $\boldsymbol z=\boldsymbol W^\top \boldsymbol x+\boldsymbol b$，然后使用一个逐元素的非线性函数 $g(\boldsymbol z)$。大多数激活函数的区别仅仅在于激活函数 $g(\boldsymbol z)$ 的形式。

激活函数：

- Sigmoid
- Tanh(双曲正切)
- ReLU  修正线性单元   用的最多的
  - Leaky ReLU 对 ReLU 做一些修正
  - ELU  对 Leaky ReLU 做一些修正
  - Maxout 基于 ReLU 的衍生


一些激活函数可能并不是在所有的输入点上都是可微的。例如，整流线性单元 $g(z)=\max\{0, z\}$ 在 $z=0$ 处不可微。




为什么需要激活函数：

- 激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。
- 激活函数可以引入非线性因素。
  - 如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。
- 激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。

为什么激活函数需要用非线性函数：

- 假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。
- 使用非线性激活函数，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。




激活函数一般有哪些性质：

- 非线性：
  - 当激活函数是线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $f(x)=x$，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；
- 可微性：
  - 当优化方法是基于梯度的时候，就体现了该性质；
- 单调性：
  - 当激活函数是单调的时候，单层网络能够保证是凸函数；
- $f(x) \approx x$：
  - 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；<span style="color:red;">是这样吗？之前没有注意过，为什么呢？</span>
- 输出值的范围：
  - 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著
  - 当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。





如何选择激活函数：

- 通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好。
- 常见的情况：
  - 如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。
  - 如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。
  - sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。<span style="color:red;">嗯，是的。</span>
  - tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。<span style="color:red;">是吗？</span>
  - ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。
  - 如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。



## 常见激活函数

- sigmoid
- tanh
- relu
- leakyrelu


Sigmoid 激活函数：

- $f(z)=\frac{1}{1+e^{-z}}$
  - 值域为 $(0,1)$。
    - 因此可以把输出的值看作是概率值。而概率值可以作为一个判别的标准。比如有了概率之后，我们通常可以给定一个分界线，比如 0.5 ，这样就可以来判定这个样本是正样本还是负样本。
- $f^{\prime}(z)=f(z)(1-f(z))$
- 函数图像：
    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/URVjdLLrU1FD.png?imageslim">
    </p>
- 对两个不同点的梯度的说明
  - 如图：

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/4hydKl8lFODO.jpg?imageslim">
    </p>

  - 说明:
    - 上图中，对 0.88 和 0.98 两个点进行比较：
      - 假设目标是收敛到 1.0。
        - 0.88 离目标 1.0 比较远，梯度比较大，权值调整比较大。
        - 0.98 离目标 1.0 比较近，梯度比较小，权值调整比较小。调整方案合理。
      - 假如目标是收敛到 0。
        - 0.88 离目标 0 比较近，梯度比较大，权值调整比较大。
        - 0.98 离目标 0 比较远，梯度比较小，权值调整比较小。调整方案不合理。
    - 即：在使用 sigmoid 函数的情况下, 初始的代价（误差）越大，导致训练越慢。
- 使用：
  - 以前这个是最常用的激活函数，但是现在一般只用在输出层，中间层很少使用。
  - 这是因为它有几个缺点：
    - 因为：$f^{\prime}(z)=f(z)(1-f(z))$，所以，$f(z)$ 趋近于 $0$ 或者 趋近于 $1$ 时，导数均为 $0$，这样在应用链式法则进行求导的时候，导数非常容易是 0，会导致梯度消失。
      - 因为这个原因，现在不鼓励将它们用作前馈网络中的激活函数。当使用一个合适的代价函数来抵消 sigmoid 的饱和性时，它们作为输出单元可以与基于梯度的学习相兼容。
    - 而且，它的输出值是 0~1 ，没有负数，这样是不对称的，因此这会导致比如，输入的时候我还是有负值的，但是我下一层的输出就全是大于 0 的了。
    - 输出范围有限。
      - 有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如 Sigmoid、TanH。
      - 但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时 loss 函数中的 log 操作能够抵消其梯度消失的影响）、LSTM里的 gate 函数。

Tanh 激活函数：


- $f(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$
  - 值域为 $(-1,1)$
- $f^{\prime}(z)=1-(f(z))^{2}$
- 函数图像如下：

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/ViAgM2OV8sqr.png?imageslim">
    </p>
 
- 使用：
  - 因为 $f^{\prime}(z)=1-(f(z))^{2}$，所以，$f(z)$ 趋近于 $-1$ 或者 趋近于 $1$ 时，导数均为 $0$，这样在应用链式法则进行求导的时候，导数非常容易是 0，会导致梯度消失。
  - 当必须要使用 sigmoid  激活函数时，双曲正切激活函数通常要比 logistic sigmoid 函数表现更好。
- Tanh 与 Sigmoid 对比：
  - 为什么 Tanh 收敛速度比 Sigmoid 快？
    - 首先看如下两个函数的求导：
    $$
    \tanh ^{\prime}(x)=1-\tanh (x)^{2} \in(0,1)
    $$

    $$
    s^{\prime}(x)=s(x) *(1-s(x)) \in\left(0, \frac{1}{4}\right]
    $$
    - 可见，$\tanh(x)$ 梯度消失的问题比 $\text{sigmoid}$ 轻，所以 Tanh 收敛速度比 Sigmoid 快。





ReLU 激活函数：


- $f(z)=\max (0, z)$
  - 值域为 $[0,+\infty)$；
- $f^{\prime}(z)=\left\{\begin{array}{l}{1, z>0} \\ {0, z \leqslant 0}\end{array}\right.$
- 函数图像如下：
  <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/JXUQebXcMFfq.png?imageslim">
  </p>




- 特点：
  - ReLU 的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活边界。
  - ReLU 的单侧抑制提供了网络的稀疏表达能力。
    - ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。
    - 因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。
    - **稀疏激活性**：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $x<0$ 时，ReLU 硬饱和，而当 $x>0$ 时，则不存在饱和问题。ReLU 能够在 $x>0$ 时保持梯度不衰减，从而缓解梯度消失问题。
      - Leaky ReLu 不会产生这个问题。
  - 收敛速度比 sigmoid / tanh 函数快，这个是在实践中证明了的。
  - 从计算的角度上，Sigmoid 和 Tanh 激活函数均需要计算指数，复杂度高，而 ReLU 只需要一个阈值即可得到激活值。计算高效简单
  - 使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快：
    - 只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数处处为 1。在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。
  - sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和 Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。
- ReLU 的局限性：
  - ReLU 的局限性在于其训练过程中会导致神经元死亡的问题。
    - 这是由于函数 $f(z)=\max (0, z)$ 导致负梯度在经过该 ReLU 单元时被置为 0，且在之后也不被任何数据激活，即流经该神经元的梯度永远为 0，不对任何数据产生响应。
    - 在实际训练中，如果学习率（Learning Rate）设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。
    - 为解决这一问题，人们设计了 ReLU 的变种 Leaky ReLU（LReLU）。
- 缺陷：
  - 整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活为零的样本。
- ReLU 的一个问题：
  - 非饱和性（saturation）：
    - 饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。
      - 最经典的例子是 Sigmoid，它的导数在 $x$ 为比较大的正值和比较小的负值时都会接近于 0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为 0，因此处处饱和，无法作为激活函数。
    - ReLU在 $x>0$ 时导数恒为 1，因此对于再大的正值也不会饱和。但同时对于 $x<0$，其梯度恒为 0，这时候它也会出现饱和的现象（在这种情况下通常称为 dying ReLU）。
    - 所以，这时，如果你的输入不巧全部在 ReLU 的左侧，即 Dead Area，那么这个神经元就挂掉了，再也没法激活，在后向传播时，相应的神经元的参数都不会更新。但是呢，这个发生的概率很小，因为是一批一批训练的。所以虽然可能很脆弱，但是大家都在用。
    - Leaky ReLU 和 PReLU 的提出正是为了解决这一问题。


- 为什么 ReLU 在 0 处不可微还可以用作激活函数呢：
  - 整流线性单元  $g(z)=\max\{0, z\}$  并不是在所有的输入点上都是可微的，在 $z=0$ 处不可微。
  - 这似乎使得 $g$ 对于基于梯度的学习算法无效。
  - 在实践中，梯度下降对这些机器学习模型仍然表现得足够好。部分原因是神经网络训练算法通常不会达到代价函数的局部最小值，而是仅仅显著地减小它的值，
  - 因为我们不再期望训练能够实际到达梯度为 $\bm{0}$ 的点，所以代价函数的最小值对应于梯度未定义的点是可以接受的。
  - 不可微的激活函数通常只在少数点上不可微。一般来说，函数 $g(z)$ 具有左导数和右导数，左导数定义为紧邻在 $z$ 左边的函数的斜率，右导数定义为紧邻在 $z$ 右边的函数的斜率。只有当函数在 $z$ 处的左导数和右导数都有定义并且相等时，函数在 $z$ 点处才是可微的。
  - 神经网络中用到的函数通常对左导数和右导数都有定义。在 $g(z)=\max\{0,z\}$ 的情况下，在 $z=0$ 处的左导数是 0，右导数是 1。神经网络训练的软件实现通常返回左导数或右导数的其中一个，而不是报告导数未定义或产生一个错误。
    - 这可以通过观察到在数字计算机上基于梯度的优化总是会受到数值误差的影响来启发式地给出理由。
    - 当一个函数被要求计算 $g(0)$ 时，底层值真正为 0 是不太可能的。相对的，它可能是被舍入为 0 的一个小量 $\epsilon$。
    - 在某些情况下，理论上有更好的理由，但这些通常对神经网络训练并不适用。重要的是，在实践中，我们可以放心地忽略下面描述的激活函数激活函数的不可微性。

Leaky ReLU 激活函数

- $f(x) =  \left\{\begin{aligned}
ax, \quad x<0 \\x, \quad x>0\end{aligned}\right.$
  - 值域为 $(-\infty,+\infty)$。
- 图像如下（$a = 0.5$）：常为 0.01

  <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/NvHfkJ7nPi7X.png?imageslim">
  </p>

- LReLU 与 ReLU 的比较：
  - 当 $z<0$ 时 LReLU 不为 $0$，而是一个斜率为 $a$ 的线性函数，一般 $a$ 为一个很小的正常数，这样既实现了单侧抑制，又保留了部分负梯度信息以致不完全丢失。
    - 但另一方面，$a$ 值的选择增加了问题难度，需要较强的人工先验或多次重复训练以确定合适的参数值。（这个 $a$ 要怎么定？）

  <p align="center">
      <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190413/gtJ0WK6T3kOc.png?imageslim">
  </p>




SoftPlus 激活函数：

- $f(x) = ln( 1 + e^x)$
  - 值域为 $(0,+\infty)$。
- 函数图像如下:

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/xP0IjrNWaI0o.png?imageslim">
</p>

- 这是整流线性单元的平滑版本，通常不鼓励使用 softplus函数，因为整流线性单元的效果结果更好。
  - softplus表明激活函数类型的性能可能是非常反直觉的——因为它处处可导或者因为它不完全饱和，人们可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。


其他激活函数：

- 径向基函数：
  - $h_i = \exp \left (-\frac{1}{\sigma_i^2}|| \boldsymbol W_{:,i}-\boldsymbol x||^2 \right )$。这个函数在 $\boldsymbol x$ 接近模板 $\boldsymbol W_{:,i}$ 时更加活跃。因为它对大部分 $\boldsymbol x$ 都饱和到 0，因此很难优化。
- 硬双曲正切函数：
  - 它的形状和 $\text{tanh}$ 以及整流线性单元类似，但是不同于后者，它是有界的，$g(a)=\max(-1, \min(1,a))$。

