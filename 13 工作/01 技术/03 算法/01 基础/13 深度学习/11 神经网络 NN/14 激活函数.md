
# 激活函数


激活函数：

- 对于深度神经网络，我们在每一层线性变换后叠加一个非线性激活函数，以避免多层网络等效于单层线性函数，从而获得更强大的学习与拟合能力。


激活函数的设计：

- 暂时还没有许多明确的指导性理论原则。




描述：

- 除非另有说明，大多数的激活函数都可以描述为：
- 接受输入向量 $\boldsymbol x$，计算仿射变换 $\boldsymbol z=\boldsymbol W^\top \boldsymbol x+\boldsymbol b$，然后使用一个逐元素的非线性函数 $g(\boldsymbol z)$。大多数激活函数的区别仅仅在于激活函数 $g(\boldsymbol z)$ 的形式。

激活函数：

- Sigmoid
- Tanh(双曲正切)
- ReLU  修正线性单元   用的最多的
- Leaky ReLU 对 ReLU 做一些修正
- ELU  对 Leaky ReLU 做一些修正
- Maxout


一些激活函数可能并不是在所有的输入点上都是可微的。例如，整流线性单元 $g(z)=\max\{0, z\}$ 在 $z=0$ 处不可微。




为什么需要激活函数：

- 激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。
- 激活函数可以引入非线性因素。
  - 如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。
- 激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。

为什么激活函数需要用非线性函数：

- 假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。
- 使用非线性激活函数，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。




激活函数一般有哪些性质：

- 非线性：
  - 当激活函数是线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $f(x)=x$，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；
- 可微性：
  - 当优化方法是基于梯度的时候，就体现了该性质；
- 单调性：
  - 当激活函数是单调的时候，单层网络能够保证是凸函数；
- $f(x) \approx x$：
  - 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；<span style="color:red;">是这样吗？之前没有注意过，为什么呢？</span>
- 输出值的范围：
  - 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著
  - 当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。





如何选择激活函数：

- 通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好。
- 常见的情况：
  - 如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。
  - 如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。
  - sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。<span style="color:red;">嗯，是的。</span>
  - tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。<span style="color:red;">是吗？</span>
  - ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。
  - 如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。



## 常见激活函数

Sigmoid激活函数：


- $f(x)=\frac{1}{1+e^{-x}}$
- $f^{'}(x)=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))$
- 当 $x=10$，或 $x=-10​$，$f^{'}(x) \approx0​$，
- 当 $x=0​$ 时，$f^{'}(x) =0.25​$


Tanh激活函数

- $f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$
- $f^{'}(x)=-(tanh(x))^2$
- 当 $x=10$，或 $x=-10$，$f^{'}(x) \approx0$
- 当 $x=0$ 时，$f^{`}(x) =1$


Relu激活函数

- $f(x)=max(0,x)$
- $c(u)=\begin{cases} 0,x<0 \\ 1,x>0 \\ undefined,x=0\end{cases}$
- 通常 $x=0$ 时，给定其导数为 1 和 0 <span style="color:red;">是这样吗？之前看花书上好像是说给一个极小值，嗯确认下。