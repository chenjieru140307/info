## 交叉熵损失函数

交叉熵损失函数定义及其求导：


- 设：
  - $i_j$ 为输入
  - $z=\sum w_{j}i_{j}+b​$ 是输入的带权和。
  - 输出为 $\mathrm{a}=\sigma(\mathrm{z})$
  - $\sigma(z)=\frac{1}{1+e^{-z}}$
- 则，交叉熵为：

    $$
    C=-\frac{1}{n}\sum[y\ln a+(1-y)\ln (1-a)]
    $$

- 说明：
  - $n$ 是训练数据的总数
  - 求和是在所有的训练输入 $x$ 上进行的， $y$ 是对应的目标输出。






交叉熵为何能够解释成⼀个代价函数：

- 它是非负的。
  - 由于对数函数的定义域是 $(0，1)$，所以，$\ln a$ 和 $\ln (1-a)$ 均为负数。
- 如果对于所有的训练输入 $x$，神经元实际的输出接近目标值，那么交叉熵将接近 $0$。
  - 因为 $a$ 是 $(0,1)$ ，因此，$ln(a)$ 就是 $(-\infty,0)$ ，而 $y$ 的值是 $0$,$1$ ，所以 $C$ 接近 $0$。
- 综上所述：
  - 交叉熵是非负的，在神经元达到很好的正确率的时候会接近 $0$。这些其实就是我们想要的代价函数的特性。



交叉熵是否能够解决学习缓慢的问题：

- 计算交叉熵函数关于权重的偏导数：

$$
\begin{aligned}\frac{\partial C}{\partial w_{j}}&=-\frac{1}{n}\sum \frac{\partial }{\partial w_{j}}[ylna+(1-y)ln(1-a)]
\\&=-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]*\frac{\partial a}{\partial w_{j}}
\\&=-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})*\frac{\partial a}{\partial w_{j}}
\\&=-\frac{1}{n}\sum (\frac{y}{\sigma(z)}-\frac{1-y}{1-\sigma(z)})\frac{\partial \sigma(z)}{\partial w_{j}}
\\&=-\frac{1}{n}\sum (\frac{y}{\sigma(z)}-\frac{1-y}{1-\sigma(z)}){\sigma}'(z)x_{j}
\\&=\frac{1}{n}\sum x_{j}({\sigma}(z)-y)
\end{aligned}
$$

- 说明：
  - 第三行到第四行：将 $a={\sigma}(z)$ 代入
  - 第五行到第六行：根据 $\sigma(z)=\frac{1}{1+e^{-z}}$ 的定义，有： ${\sigma}'(z)=\sigma(z)(1-\sigma(z))$。带入。

- 类似的，可以计算出关于偏置的偏导数：

$$
\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\sigma}(z)-y)​
$$

- 可见：
  - 权重学习的速度受到 $\sigma(z)-y$，也就是输出中的误差的控制。
  - 也就是：越大的误差就会有越快的学习速度。这是我们直觉上期待的结果。
  - 特别地，与二次代价函数相比，这个代价函数还避免了像 ${\sigma}'(z)$ 导致的学习缓慢。当我们使用交叉熵的时候，${\sigma}'(z)$ 被约掉了，所以我们不再需要关心它是不是变得很小。也即：它避免了学习速度下降的问题。
    - 均方误差在 20 世纪 80 年代和 90 年代流行，但逐渐被交叉熵损失替代，并且最大似然原理的想法在统计学界和机器学习界之间广泛传播。
    - 使用交叉熵损失大大提高了具有 sigmoid  和 softmax 输出的模型的性能，而当使用均方误差损失时会存在饱和和学习缓慢的问题。
    - 实际上，这也并不是非常奇迹的事情。我们在后面可以看到，交叉熵其实只是满足这种特性的⼀种选择罢了。
