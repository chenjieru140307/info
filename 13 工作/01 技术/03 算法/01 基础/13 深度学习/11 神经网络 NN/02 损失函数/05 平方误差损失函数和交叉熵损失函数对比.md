

平方误差损失函数和交叉熵损失函数对比：

- 平方损失函数更适合输出为连续，并且最后一层不含 Sigmoid 或 Softmax 激活函数的神经网络；
  - 原因：
    - 平方误差损失函数：
      - 权值 $w$ 和偏置 $b$ 的偏导数为：
        - $\frac{\partial J}{\partial w}=(a-y)\sigma'(z)x$
        - $\frac{\partial J}{\partial b}=(a-y)\sigma'(z)$
      - 可见：
        - 偏导数受激活函数的导数影响，sigmoid函数导数在输出接近 0 和 1 时非常小，会导致一些实例在刚开始训练时学习得非常慢。
    - 交叉熵损失函数：
      - 权值 $w$ 和偏置 $b$ 的梯度推导为：
        - $\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)$
        - $\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)$
      - 可见：
        - 权重学习的速度受到 $\sigma{(z)}-y$ 影响，更大的误差，就有更快的学习速度，避免了二次代价函数方程中因 $\sigma'{(z)}$ 导致的学习缓慢的情况。
- 交叉熵损失则更适合二分类或多分类的场景。





