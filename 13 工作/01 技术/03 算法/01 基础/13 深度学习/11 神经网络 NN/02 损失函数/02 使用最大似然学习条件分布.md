


# 使用最大似然学习条件分布

代价函数由来：

- 大多数现代的神经网络使用最大似然来训练。
- 这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。
- 这个代价函数表示为

$$
J(\boldsymbol \theta) = -\mathbb E_{\mathbf x, \mathbf y \sim \hat{p}_\text{data}} \log p_\text{model} (\boldsymbol y \mid \boldsymbol x).
$$

- 使用最大似然来导出代价函数的一个优势是：
  - 它减轻了为每个模型设计代价函数的负担。明确一个模型 $p(\boldsymbol y\mid\boldsymbol x)$ 则自动地确定了一个代价函数 $\log p(\boldsymbol y\mid\boldsymbol x)$。

随着模型的变化：

- 代价函数的具体形式随着模型而改变，取决于 $\log p_\text{model}$ 的具体形式。
- 上述方程的展开形式通常会有一些项不依赖于模型的参数，我们可以舍去。
  - 例如：
    - 如果 $p_\text{model}(\boldsymbol y\mid\boldsymbol x) = \mathcal N(\boldsymbol y;f(\boldsymbol x;\boldsymbol \theta), \boldsymbol I)$
    - 那么我们就重新得到了均方误差代价 $J(\theta) = \frac{1}{2} \mathbb E_{\mathbf x, \mathbf y \sim  \hat{p}_\text{data}} || \boldsymbol y - f(\boldsymbol x; \boldsymbol \theta) ||^2 + \text{const},$
    - 至少系数 $\frac{1}{2}$ 和常数项不依赖于 $\boldsymbol \theta$。舍弃的常数是基于高斯分布的方差，在这种情况下我们选择不把它参数化。
  - 之前，我们看到了对输出分布的最大似然估计和对线性模型均方误差的最小化之间的等价性<span style="color:red;">等价性这部分再看下。</span>，但事实上，这种等价性并不要求 $f(\boldsymbol x; \boldsymbol \theta)$ 用于预测高斯分布的均值。


对于代价函数的设计：

- 贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须足够的大和具有足够的预测性，来为学习算法提供一个好的指引。
- 饱和（变得非常平）的函数破坏了这一目标，因为它们把梯度变得非常小。这在很多情况下都会发生，因为用于产生隐藏单元或者输出单元的输出的激活函数会饱和。负的对数似然帮助我们在很多模型中避免这个问题。很多输出单元都会包含一个指数函数，这在它的变量取绝对值非常大的负值时会造成饱和。<span style="color:red;">输出单元包含一个指数函数，是指的 softmax 吗？还是 sigmoid？还是什么？</span>负对数似然代价函数中的对数函数消除了某些输出单元中的指数效果。我们将会在 节 输出单元 中讨论代价函数和输出单元的选择间的相互作用。


用于实现最大似然估计的交叉熵代价函数：

（没有很明白）

- 用于实现最大似然估计的交叉熵代价函数有一个不同寻常的特性，那就是：
  - 当它被应用于实践中经常遇到的模型时，它通常没有最小值。
  - 对于离散型输出变量，大多数模型以一种特殊的形式来参数化，即它们不能表示概率零和一，但是可以无限接近。逻辑回归是其中一个例子。对于实值的输出变量，如果模型可以控制输出分布的密度（例如，通过学习高斯输出分布的方差参数），<span style="color:red;">控制输出分布的密度是什么意思？</span>那么它可能对正确的训练集输出赋予极其高的密度，这将导致交叉熵趋向负无穷。<span style="color:red;">对正确的训练集输出赋予极其高的密度是什么意思？为什么会导致交叉熵趋向于负无穷？</span>第七章中描述的正则化技术提供了一些不同的方法来修正学习问题，使得模型不会通过这种方式来获得无限制的收益。<span style="color:red;">有哪些正则化技术来消除这个问题？</span>

