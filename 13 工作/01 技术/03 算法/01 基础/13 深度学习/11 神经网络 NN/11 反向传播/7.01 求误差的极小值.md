# 神经网络的训练过程

神经网络的训练过程：

- 神经网络的训练过程可看作一个参数寻优过程，即在参数空间中，寻找一组最优参数使得 $E$ 最小。
  - $E$ 表示神经网络在训练集上的误差，它是关于连接权 $\boldsymbol{w}$ 和阈值 $\theta$ 的函数。


基于梯度的搜索：

- 是使用最为广泛的参数寻优方法。
- 过程：
  - 我们从 某些初始解出发，迭代寻找最优参数值。
  - 每次迭代中，我们先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。
    - 例如，由于负梯度方向是函数值下降 最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。
  - 若误差函数在当前点的梯度为零，则已达到局部极小，更新量将为零，这意味着参数的迭代更新将在此停止。
    - 显然，如果误差函数仅有一个局部极小，那么此时找到的局部极小就是全局最小；然而，如果误差函数具有多个局部极小，则不能保证找到的解 是全局最小。对后一种情形，我们称参数寻优陷入了局部极小，这显然不是我们所希望的。
  - 在现实任务中，人们常采用以下策略来试图“跳出”局部极小，从而进一 步接近全局最小：
    - 以多组不同参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数。
      - 这相当于从多个不同的初始点开始搜索，这样就可能陷入不同的局部极小，从中进行选择有可能获得更接近全局最小的结果.
    - 使用模拟退火 (simulated annealing)技术
      - 模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于“跳出”局部极小。在每步迭代过程中，接受 “次优解” 的概率要随着时间的推移而逐渐降低，从而保证算法稳定。（怎么实现的）
    - 使用随机梯度下降。
      - 与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机因素。于是，即便陷入局部极小点，它计算出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索.
    - 注意：上述用于跳出局部极小的技术大多是启发式，理论上尚缺乏保障。


此外，遗传算法也常用来训练神经网络以更好地逼近全局最小。（补充）


