# KNN

## 介绍

K近邻 K-NearestNeighbor KNN 

特质：

- 非参数学习算法
- 非概率算法
- 监督学习算法
- 可用于分类和回归
- 不具有显式的学习过程

过程：

- 假设我们已经给定一个训练数据集，其中的实例类别已定。
- 此时，对于一个新的实例，基于某种距离度量找出训练集中与其最靠近的  $k$ 个训练样本
  - 也可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大。
- 然后，基于这 $k$ 个“邻居”的信息来进行预测。
  - 一般，这 K 个实例的多数属于某个类，就把该输入实例分类到这个类中。




优点：

- 精度高、
- 对异常值不敏感、
- 无数据输入假定

缺点：

- 计算复杂度高、
- 空间复杂度高
- 它的计算成本很高，在训练集较小时泛化能力很差。
- 不能学习出哪一个特征比其他更具识别力。

适用数据范围：

- 数值型和标称型 **什么是标称型？**




涉及到：

- 距离度量：什么叫最近？
- K值的选择 ：到底选几个？
- 分类决策规则：知道它周边的 k 个样本了，怎么来决定这个测试样本的类别？
  - 分类任务，可使用投票法
    - 即选择这 $k$ 个样本中出现最多的类别标记作为预测结果；
  - 回归任务，可使用平均法，
    - 即将这 $k$ 个样本的实值输出标记的平均值作为预测结果；



## K 值的选择

- K 值较小。
  - 此时，只有与输入实例较近或相似的训练实例才会对预测结果起作用，容易发生过拟合
- K 值较大。
  - 此时，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远训练实例也会对预测器作用，使预测发生错误。


选取：

- 可以通过交叉验证来进行尝试。


## KNN 的最大误差 在二分类问题上

在二分类问题上，若 $k=1$


- 给定测试样本 $\boldsymbol{x}$，若其最近样本为 $\boldsymbol{z}$
- 则 KNN 出错的概率就是 $\boldsymbol{x}$ 与 $\boldsymbol{z}$ 类别标记不同的概率，即


$$
\begin{aligned} P(e r r) &=1-\sum_{c \in \mathcal{Y}} P(c | \boldsymbol{x}) P(c | \boldsymbol{z}) \\ & \simeq 1-\sum_{c \in \mathcal{Y}} P^{2}(c | \boldsymbol{x}) \\ & \leqslant 1-P^{2}\left(c^{*} | \boldsymbol{x}\right) \\ &=\left(1+P\left(c^{*} | \boldsymbol{x}\right)\right)\left(1-P\left(c^{*} | \boldsymbol{x}\right)\right) \\ & \leqslant 2 \times\left(1-P\left(c^{*} | \boldsymbol{x}\right)\right) \end{aligned}
$$


说明：（没有很明白）

- 第一行到第二行：假设样本独立同分布，且对任意 $\boldsymbol{x}$ 和任意小正数 $\delta$ ，在 $\boldsymbol{x}$ 附近 $\delta$ 距离范围内总能找到一个训练样本；换言之，对任意测试样本，总能在任意近的范围内找到训练样本 $\boldsymbol{z}$ 。
- 第二行到第三行：令 $c^{*}=\arg \max _{c \in \mathcal{Y}} P(c | \boldsymbol{x})$ 表示贝叶斯最优分类器的结果

得到：

- 由第五行得知，最近邻分类器虽简单，但它的泛化错误率不超过贝叶斯最优分类器的错误率的两倍！


即：K 近邻的最大误差：

- 例如，假设我们有一个用 0-1误差度量性能的多分类任务。那么，当训练样本数目趋向于无穷大时，1-最近邻的误差将收敛到两倍贝叶斯误差。
  - 超出贝叶斯误差的原因是它会随机从等距离的临近点中随机挑一个。而存在无限的训练数据时，所有测试点 $\boldsymbol{x}$ 周围距离为零的邻近点有无限多个。
  - 如果我们使用所有这些临近点投票的决策方式，而不是随机挑选一个，那么该过程将会收敛到贝叶斯错误率。

