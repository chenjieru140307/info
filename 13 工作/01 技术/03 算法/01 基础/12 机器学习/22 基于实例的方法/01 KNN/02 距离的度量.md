

## 距离的度量

**欧氏距离**
  
$$d(x, y)=\sqrt{\sum_{i=1}^{n}\left(x_{i}-y_{i}\right)^{2}}$$

**曼哈顿距离**

$$\mathrm{d(x, y)}=\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|$$

**切比雪夫距离**

$$d_{\text {Chebyshev }}(x, y)=\max _{i}\left(\left|x_{i}-y_{i}\right|\right)$$

等价于：

$$d_{\text {Chebyshev }}(x, y)=\lim _{k \rightarrow \infty}\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{k}\right)^{1 / k}$$

也即 P 范数的极值。


**闵可夫斯基距离 Minkowski Distance**

$$d(x, y)=\sqrt[p]{\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p}}$$

也即 p 范数距离。

其中：

- 当 $p=1$ 时，就是曼哈顿距离
- 当 $p=2$ 时，就是欧氏距离
- 当 $p \rightarrow \infty$ 时，就是切比雪夫距离



**标准化欧氏距离  Standardized Euclidean distance** 

（具体怎么标准化）

- 对于欧氏距离，既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。

过程：

- 标准化：$X^{*}=\frac{X-m}{s}$
- 然后使用欧氏距离：

$$d(x, y)=\sqrt{\sum_{i=1}^{n}\left(x_{i}-y_{i}\right)^{2}}$$

注：

- 如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。

**马氏距离 Mahalanobis Distance**

- 有 M 个样本向量 $X_1,\ldots X_m$，协方差矩阵记为 $S$，均值记为向量 $\mu$，则其中样本向量 $X$ 到 $\mu$ 的马氏距离表示为：（没有很清楚，为什么要与平均值计算距离？）

$$\mathrm{D}(\mathrm{X})=\sqrt{(X-\mu)^{T} S^{-1}(X-\mu)}$$

说明：

- 协方差矩阵中每个元素是各个矢量元素之间的协方差 Cov(X,Y)

样本 $X_i$ 与 $X_j$ 之间的马氏距离为：

$$\mathrm{D}\left(X_{i}, X_{j}\right)=\sqrt{\left(X_{i}-X_{j}\right)^{T} S^{-1}\left(X_{i}-X_{j}\right)}$$


若协方差矩阵是单位矩阵（各个样本向量之间独立同分布），则为：

$$\mathrm{D}\left(X_{i}, X_{j}\right)=\sqrt{\left(X_{i}-X_{j}\right)^{T}\left(X_{i}-X_{j}\right)}$$


若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。

优缺点：

- 马氏距离是量纲无关距离，排除了变量之间的相关性的干扰。


**巴氏距离 Bhattacharyya Distance**


- 用于测量两个离散或连续概率分布的相似性。

对于离散概率分布：

$$D_{B}(p, q)=-\ln (B C(p, q))$$

说明：

- 其中，$B C(p, q)$ 为 Bhattacharyya 系数
  - 离散概率时：$B C(p, q)=\sum_{x \in X} \sqrt{p(x) q(x)}$
  - 连续概率时：$B C(p, q)=\int \sqrt{p(x) q(x)} d x$



**汉明距离 Hamming distance**


- 两个等长字符串 s1 与 s2 之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为 2。
- 应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。


**夹角余弦 Cosine**

- 可用来衡量两个向量方向的差异。

$$\cos (\theta)=\frac{a \cdot b}{|a||b|}$$

- 夹角余弦取值范围为 $[-1,1]$。
  - 余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大
  - 当两个向量的方向重合时夹角余弦取最大值 1，当两个向量的方向完全相反夹角余弦取最小值-1。

**杰卡德相似系数 Jaccard similarity coefficient**

- 杰卡德相似系数
  - 集合 A 和 B 的交集在 A，B 的并集中所占的比例。
  - 可以用来衡量两个集合的相似度

$$\mathrm{J}(\mathrm{A}, \mathrm{B})=\frac{|A \cap B|}{|A \cup B|}$$

- 杰卡德距离：
  - 用来衡量两个集合的区分度

$$\mathrm{J}_{\delta}(\mathrm{A}, \mathrm{B})=1-J(A, B)=\frac{|A \cup B|-|A \cap B|}{|A \cup B|}$$


举例：

- 样本 A 与 B 是两个 n 维向量，而且所有维度的取值都是 0 或 1，如：A 为 0111，B 为 1011。
- 则，可以用如下来表示一些维度的个数：
  - M11：样本 A 与 B 在那个维度都是 1 的维度的个数
  - M01：样本 A 是 0，样本 B 是 1 的维度的个数
  - M10：样本 A 是 1，样本 B 是 0 的维度的个数
  - M00：样本 A 与 B 都是 0 的维度的个数
- 则，

$$J=\frac{M_{11}}{M_{01}+M_{10}+M_{11}}$$


**皮尔逊系数 Pearson Correlation Coefficient**


相关系数：

$$\rho_{X Y}=\frac{\operatorname{Cov}(\mathrm{X}, \mathrm{Y})}{\sqrt{D(X)} \sqrt{D(Y)}}=\frac{E((\mathrm{X}-\mathrm{EX})(Y-E Y))}{\sqrt{D(X)} \sqrt{D(Y)}}$$

说明：

- 相关系数是衡量随机变量 $X$ 与 $Y$ 相关程度的一种方法，相关系数的取值范围是$[-1,1]$。
- 相关系数的绝对值越大，则表明 $X$ 与 $Y$ 相关度越高。当 $X$ 与 $Y$ 线性相关时，相关系数取值为 $1$（正线性相关）或 $-1$（负线性相关）。


相关距离：

$$D_{x y}=1-\rho_{X Y}$$


皮尔逊系数：

皮尔逊积矩相关系数（Pearson product-moment correlation coefficient，PPMCC，PCCs, 用 r 表示）

$$\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{E\left(\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right)}{\sigma_{X} \sigma_{Y}}=\frac{E(X Y)-E(X) E(Y)}{\sqrt{E\left(X^{2}\right)-E^{2}(X)} \sqrt{E\left(Y^{2}\right)-E^{2}(Y)}}$$

说明：

- 为两个变量之间的协方差和标准差的商



使用 rho 进行表示：

$$\begin{aligned}
r=&\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}
\\=&\frac{1}{n-1} \sum_{i=1}^{n}\left(\frac{X_{i}-\bar{X}}{s_{X}}\right)\left(\frac{Y_{i}-\bar{Y}}{s_{Y}}\right)
\end{aligned}
$$

说明：

- 其中 $\frac{X_{i}-\bar{X}}{s_{X}}$ 、$\bar{X}$ 及 $s_{X}$，分别是标准分、样本平均值和样本标准差。

应用：

- 当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于：
  - 两个变量之间是线性关系，都是连续数据。
  - 两个变量的总体是正态分布，或接近正态的单峰分布。
  - 两个变量的观测值是成对的，每对观测值之间相互独立。
- 皮尔逊积矩相关系数用于度量两个变量 X 和 Y 之间的相关（线性相关），其值介于-1与 1 之间。
  - 0.8-1.0     极强相关
  - 0.6-0.8     强相关
  - 0.4-0.6     中等程度相关
  - 0.2-0.4     弱相关
  - 0.0-0.2     极弱相关或无相关


理解：角度一

按照高中数学水平来理解，可以看做将两组数据首先做 Z 分数处理之后，然后两组数据的乘积和除以样本数，Z 分数一般代表正态分布中，数据偏离中心点的距离。等于变量减掉平均数再除以标准差。

样本标准差则等于变量减掉平均数的平方和，再除以样本数，最后再开方，也就是说，方差开方即为标准差，样本标准差计算公式为：

$$\sqrt{\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}-\bar{x}\right)^{2}}$$

所以, 根据这个最朴素的理解，我们可以将公式依次精简为:

$$\begin{aligned}
r_{x y}=&\frac{\sum Z x Z y}{N} 
\\=&\frac{\sum\left(\frac{X-\bar{X}}{S x}\right)\left(\frac{Y-\bar{Y}}{S y}\right)}{N} 
\\=&\frac{\sum(X-\bar{X})(Y-\bar{Y})}{N \bullet S x S y} 
\\=&\frac{\sum(X-\bar{X})(Y-\bar{Y})}{N \bullet(\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}})(\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}})} 
\\=&\frac{\sum(X-\bar{X})(Y-\bar{Y})}{(\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}})(\sqrt{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}})}
\end{aligned}$$

理解：角度二


可以看做是两组数据的向量夹角的余弦。

<p align="center">
    <img width="60%" height="70%" src="http://images.iterate.site/blog/image/180708/lch7hJEjDH.png?imageslim">
</p>

说明：

- 回归直线： y=gx(x) [红色] 和 x=gy(y) [蓝色]
- 上图，对于没有中心化的数据, 相关系数与两条可能的回归线 y=gx(x) 和 x=gy(y) 夹角的余弦值一致。
- 对于没有中心化的数据 (也就是说, 数据移动一个样本平均值以使其均值为 0), 相关系数也可以被视作由两个随机变量 向量 夹角 的 余弦值（见下方）。
- 举例：5 个国家的国民生产总值分别为 10, 20, 30, 50 和 80 亿美元。 假设这 5 个国家 (顺序相同) 的贫困百分比分别为 11%, 12%, 13%, 15%, and 18% 。 令 $x$ 和 $y$ 分别为包含上述 5 个数据的向量: $x = (1, 2, 3, 5, 8)$ 和 $y = (0.11, 0.12, 0.13, 0.15, 0.18)$。
  - 利用通常的方法计算两个向量之间的夹角  (参见 数量积), 未中心化 的相关系数是:$\cos \theta=\frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}=\frac{2.93}{\sqrt{103} \sqrt{0.0983}}=0.920814711$
  - 我们发现以上的数据特意选定为完全相关: $y = 0.10 + 0.01 x$。 于是，皮尔逊相关系数应该等于 $1$。将数据中心化 (通过 $E(x) = 3.8$ 移动 $x$ 和通过 $E(y) = 0.138$ 移动 y ) 得到 $x = (−2.8, −1.8, −0.8, 1.2, 4.2)$ 和 $y = (−0.028, −0.018, −0.008, 0.012, 0.042)$, 从中 $\cos \theta=\frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}=\frac{0.308}{\sqrt{30.8} \sqrt{0.00308}}=1=\rho_{x y}$

皮尔逊相关的约束条件：

- 两个变量间有线性关系
- 变量是连续变量
- 变量均符合正态分布，且二元分布也符合正态分布
- 两变量独立

在实践统计中，一般只输出两个系数，一个是相关系数，也就是计算出来的相关系数大小，在-1到 1 之间；另一个是独立样本检验系数，用来检验样本一致性。

## 总结

距离的应用场景：

- 空间：欧氏距离
- 路径：曼哈顿距离
- 国际象棋国王：切比雪夫距离，以上三种的统一形式:闵可夫斯基距离
- 加权：标准化欧氏距离
- 排除量纲和依存：马氏距离
- 向量差距：夹角余弦
- 编码差别：汉明距离
- 集合近似度：杰卡德类似系数与距离
- 相关：相关系数与相关距离。