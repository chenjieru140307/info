

# 度量学习

距离度量学习 distance metric learning


缘由：

- 我们对高维数据进行降维的主要目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。
- 而此时，每个空间对应了在样本属性上定义的一个距离度量，也就是，寻找合适的空间，实质上就是在寻找一个合适的距离度量。
- 那么，为何不直接尝试“学习”出一个合适的距离度量呢？




## 对距离的度量进行推广

欲对距离度量进行学习，必须有一个便于学习的距离度量表达形式。之前我们知道很多种距离度量的表达式，但它们都是“固定的”、没有可调节的参数，因此不能通过对数据样本的学习来加以改善。为此，我们先来做一个推广。


对两个 $d$ 维样本 $\boldsymbol{x}_{i}$ 和 $\boldsymbol{x}_{j}$ ，它们之间的平方氏距离可写为

$$
\operatorname{dist}_{\mathrm{ed}}^{2}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{2}^{2}=\operatorname{dist}_{i j, 1}^{2}+\operatorname{dist}_{i j, 2}^{2}+\ldots+\operatorname{dist}_{i j, d}^{2}
$$

其中 $d i s t_{i j, k}$ 表示 $\boldsymbol{x}_{i}$ 和 $\boldsymbol{x}_{j}$ 在第 $k$ 维上的距离。若假定不同属性的重要性不同，则可引入属性权重 $\mathbf{W}$ ，得到

$$
\begin{aligned} \operatorname{dist}_{\mathrm{wed}}^{2}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) &=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{2}^{2}=w_{1} \cdot \operatorname{dist}_{i j, 1}^{2}+w_{2} \cdot \operatorname{dis} t_{i j, 2}^{2}+\ldots+w_{d} \cdot \operatorname{dis} t_{i j, d}^{2} \\ &=\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)^{\mathrm{T}} \mathbf{W}\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right) \end{aligned}\tag{10.33}
$$


其中 $w_{i} \geqslant 0$ ，$\mathbf{W}=\operatorname{diag}(\boldsymbol{w})$ 是一个对角矩阵，$(\mathbf{W})_{i i}=w_{i}$ 。


式(10.33)中的 $\mathbf{W}$ 可通过学习确定，但我们还能再往前走一步:

$\mathbf{W}$ 的非对角元素均为零，这意味着坐标轴是正交的，即属性之间无关；但现实问题中往往不是这样，例如考虑西瓜的“重量”和“体积”这两个属性，它们显然是正相关的，其对应的坐标轴不再正交。为此，将式(10.33)中的 $\mathbf{W}$ 替换为一个普通的半正定对称矩阵 $\mathbf{M}$，于是就得到了马氏距离(Mahalanobis distance)

$$
\operatorname{dist}_{\operatorname{mah}}^{2}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)^{\mathrm{T}} \mathbf{M}\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{\mathbf{M}}^{2}
$$

其中  $\mathbf{M}$ 亦称“度量矩阵”，而度量学习则是对 $\mathbf{M}$ 进行学习。

注意到为了保持距离非负且对称， $\mathbf{M}$ 必须是(半)正定对称矩阵，即必有正交基  $\mathbf{P}$ 使得  $\mathbf{M}$ 能写为 $\mathbf{M}=\mathbf{P} \mathbf{P}^{T}$ 。




## 设置一个目标

对 $\mathbf{M}$ 进行学习当然要设置一个目标。假定我们是希望提高近邻分类器的性能，则可将 $\mathbf{M}$ 直接嵌入到近邻分类器的评价指标中去，通过优化该性能指标相应地求得 $\mathbf{M}$ 。

下面我们以近邻成分分析(Neighbourhood Component Analysis，简称 NCA) 为例进行讨论。

近邻分类器在进行判别时通常使用多数投票法，邻域中的每个样本投 1 票, 邻域外的样本投 0 票。不妨将其替换为概率投票法。对于任意样本 $\boldsymbol{x}_{j}$ ，它对 $\boldsymbol{x}_{i}$ 分类结果影响的概率为

$$
p_{i j}=\frac{\exp \left(-\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{\mathbf{M}}^{2}\right)}{\sum_{l} \exp \left(-\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\right\|_{\mathbf{M}}^{2}\right)}
$$



当 $i=j$ 时，$p_{ij}$ 最大。显然，$\boldsymbol{x}_{j}$ 对 $\boldsymbol{x}_{i}$ 的影响随着它们之间距离的增大而减小。 若以留一法 (LOO) 正确率的最大化为目标，则可计算 $\boldsymbol{x}_{i}$ 的留一法正确率，即它被自身之外的所有样本正确分类的概率为

$$
p_{i}=\sum_{j \in \Omega_{i}} p_{i j}
$$

其中 $\Omega_i$ 表示与 $\boldsymbol{x}_{i}$ 属于相同类别的样本的下标集合。于是，整个样本集上的留一法正确率为

$$
\sum_{i=1}^{m} p_{i}=\sum_{i=1}^{m} \sum_{j \in \Omega_{i}} p_{i j}
$$


将式(10.35)代入(10.37)，再考虑到 $\mathbf{M}=\mathbf{P} \mathbf{P}^{\mathrm{T}}$，则 NCA 的优化目标为

$$
\min _{\mathbf{P}} 1-\sum_{i=1}^{m} \sum_{j \in \Omega_{i}} \frac{\exp \left(-\left\|\mathbf{P}^{\mathrm{T}} \boldsymbol{x}_{i}-\mathbf{P}^{\mathrm{T}} \boldsymbol{x}_{j}\right\|_{2}^{2}\right)}{\sum_{l} \exp \left(-\left\|\mathbf{P}^{\mathrm{T}} \boldsymbol{x}_{i}-\mathbf{P}^{\mathrm{T}} \boldsymbol{x}_{l}\right\|_{2}^{2}\right)}
$$

求解式(10.38)即可得到最大化近邻分类器 LOO 正确率的距离度量矩阵 $\mathbf{M}$。









实际上，我们不仅能把错误率这样的监督学习目标作为度量学习的优化目标，还能在度量学习中引入领域知识。

例如，若已知某些样本相似、某些样本不相似，则可定义“必连”(must-link)约束集合 $\mathcal{M}$ 与“勿连”(caimot-link)约束集合 $\mathcal{C}$ ，$\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \in \mathcal{M}$  表示 $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{j}$ 相似，$\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{k}\right) \in \mathcal{C}$ 表示 $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{k}$ 不相似。 显然，我们希望相似的样本之间距离较小，不相似的样本之间距离较大，于是可通过求解下面这个凸优化问题获得适当的度量矩阵 $\mathbf{M}$ [Xing et al., 2003]:

$$
\begin{array}{c}{\min _{\mathbf{M}} \sum_{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \in \mathcal{M}}\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{\mathrm{M}}^{2}} \\ {\text { s.t. } \sum_{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{k}\right) \in \mathcal{M}}\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\right\|_{\mathbf{M}}^{2} \geqslant 1} \\ {\mathbf{M} \succeq 0}\end{array}
$$

其中约束 $\mathbf{M} \succeq 0$ 表明 $\mathbf{M}$ 必须是半正定的。式(10.39)要求在不相似样本间的距离不小于 1 的前提下，使相似样本间的距离尽可能小。


不同的度量学习方法针对不同目标获得“好”的半正定对称距离度量矩阵 $\mathbf{M}$ ，若 $\mathbf{M}$ 是一个低秩矩阵，则通过对 $\mathbf{M}$ 进行特征值分解，总能找到一组正交基，其正交基数目为矩阵 $\mathbf{M}$ 的秩 $\operatorname{rank}(\mathbf{M})$ , 小于原属性数 $d$ 。于是，度量学习学得的结果可衍生出一个降维矩阵 $P\in\mathbb{R}^{d\times rank(M)}$ ，能用于降维之目的。









# 相关

- 《机器学习》周志华
