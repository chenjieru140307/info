
# 图半监督学习

给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图中一个结点，若两个样本之间的相似度很高(或相关性很强)，则对应的结点之间存在一条边，边的“强度”(strength)正比于样本之间的相似度(或相关性)。我们可将有标记样本所对应的结点想象为染过色，而未标记样本所对应的结点尚未染色。于是，半监督学习就对应于“颜色”在图上扩散或传播的过程。由于一个图对应了一个矩阵，这就使得我们能基于矩阵运算来进行半监督学习算法的推导与分析.

给定 $D_{l}=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{l}, y_{l}\right)\right\}$ 和 $D_{u}=\left\{\boldsymbol{x}_{l+1}, \boldsymbol{x}_{l+2}, \dots, \boldsymbol{x}_{l+u}\right\}$ ， $l\ll u$ ，$l+u=m$ 我们先基于 $D_{l} \cup D_{u}$  构建一个图 $G = (V,E)$，其中结点集 $V=\left\{\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{l}, \boldsymbol{x}_{l+1}, \ldots, \boldsymbol{x}_{l+u}\right\}$，边集 $E$ 可表示为一个亲和矩阵(affinity matrix)，常基于高斯函数定义为

$$
(\mathbf{W})_{i j}=\left\{\begin{array}{ll}{\exp \left(\frac{-\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{2}^{2}}{2 \sigma^{2}}\right),} & {\text { if } i \neq j} \\ {0,} & {\text { otherwise }}\end{array}\right.
$$


其中 $i,j\in\{1,2,\cdots,m\}$,$\rho>0$ 是用户指定的高斯函数带宽参数.

假定从图 $G = (V,E)$ 将学得一个实值函数 $f : V \rightarrow \mathbb{R}$，其对应的分类规则为 $y_{i}=\operatorname{sign}\left(f\left(\boldsymbol{x}_{i}\right)\right)$,$y_i\in\{-1,+1\}$ 。直观上看，相似的样本应具有相似的标记， 于是可定义关于 $f$ 的“能量函数”(energy function) :

$$
\begin{aligned} E(f) &=\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m}(\mathbf{W})_{i j}\left(f\left(\boldsymbol{x}_{i}\right)-f\left(\boldsymbol{x}_{j}\right)\right)^{2} \\ &=\frac{1}{2}\left(\sum_{i=1}^{m} d_{i} f^{2}\left(\boldsymbol{x}_{i}\right)+\sum_{j=1}^{m} d_{j} f^{2}\left(\boldsymbol{x}_{j}\right)-2 \sum_{i=1}^{m} \sum_{j=1}^{m}(\mathbf{W})_{i j} f\left(\boldsymbol{x}_{i}\right) f\left(\boldsymbol{x}_{j}\right)\right) \\ &=\sum_{i=1}^{m} d_{i} f^{2}\left(\boldsymbol{x}_{i}\right)-\sum_{i=1}^{m} \sum_{j=1}^{m}(\mathbf{W})_{i j} f\left(\boldsymbol{x}_{i}\right) f\left(\boldsymbol{x}_{j}\right) \\ &=\boldsymbol{f}^{\mathrm{T}}(\mathbf{D}-\mathbf{W}) \boldsymbol{f} \end{aligned}
$$



其中 $\boldsymbol{f}=\left(\boldsymbol{f}_{l}^{\mathrm{T}} \boldsymbol{f}_{u}^{\mathrm{T}}\right)^{\mathrm{T}}$ ,$\boldsymbol{f}_{l}=\left(f\left(\boldsymbol{x}_{1}\right) ; f\left(\boldsymbol{x}_{2}\right) ; \ldots ; f\left(\boldsymbol{x}_{l}\right)\right)$, $\boldsymbol{f}_{u}=\left(f\left(\boldsymbol{x}_{l+1}\right)\right.f\left(\boldsymbol{x}_{l+2}\right) ; \ldots ; f\left(\boldsymbol{x}_{l+u}\right) )$ 分别为函数 $f$ 在有标记样本与未标记样本上的预测结果,$\mathbf{D}=\operatorname{diag}\left(d_{1}, d_{2}, \dots, d_{l+u}\right)$ 是一个对角矩阵，其对角元素 $d_{i}=\sum_{j=1}^{l+u}(\mathbf{W})_{i j}$ 为矩 阵 $\mathbf{W}$ 的第 $i$ 行元素之和.

> $\mathbf{W}$  为对称矩阵，因此 $d_i$ 亦为 $\mathbf{W}$ 第 $i$ 列元素之和。

具有最小能量的函数 $f$ 在有标记样本上满足 $f\left(\boldsymbol{x}_{i}\right)=y_{i}(i=1,2, \dots, l)$ ，在未标记样本上满足 $\mathbf{\Delta} \mathbf{f}=\mathbf{0}$，其中 $\mathbf{\Delta}=\mathbf{D}-\mathbf{W}$ 为拉普拉斯矩阵(Laplacian matrix)。以第 $l$ 行与第 $l$ 列为界，采用分块矩阵表示方式：$\mathbf{W}=\left[\begin{array}{ll}{\mathbf{W}_{l l}} & {\mathbf{W}_{l u}} \\ {\mathbf{W}_{u l}} & {\mathbf{W}_{u u}}\end{array}\right]$ ，$\mathbf{D}=\left[\begin{array}{cc}{\mathbf{D}_{l l}} & {\mathbf{0}_{l u}} \\ {\mathbf{0}_{u l}} & {\mathbf{D}_{u u}}\end{array}\right]$，则式(13.12)可重写为

$$
\begin{aligned} E(f) &=\left(\boldsymbol{f}_{l}^{\mathrm{T}} \boldsymbol{f}_{u}^{\mathrm{T}}\right)\left(\left[\begin{array}{cc}{\mathbf{D}_{l l}} & {\mathbf{0}_{l u}} \\ {\mathbf{0}_{u l}} & {\mathbf{D}_{u u}}\end{array}\right]-\left[\begin{array}{cc}{\mathbf{w}_{l l}} & {\mathbf{W}_{l u}} \\ {\mathbf{W}_{u l}} & {\mathbf{W}_{u u}}\end{array}\right]\right)\left[\begin{array}{c}{\boldsymbol{f}_{l}} \\ {\boldsymbol{f}_{u}}\end{array}\right] \\ &=\boldsymbol{f}_{l}^{\mathrm{T}}\left(\mathbf{D}_{l l}-\mathbf{W}_{l l}\right) \boldsymbol{f}_{l}-2 \boldsymbol{f}_{u}^{\mathrm{T}} \mathbf{W}_{u l} \boldsymbol{f}_{l}+\boldsymbol{f}_{u}^{\mathrm{T}}\left(\mathbf{D}_{u u}-\mathbf{W}_{u u}\right) \boldsymbol{f}_{u} \end{aligned}
$$


由 $\frac{\partial{E(f)} }{\partial f_u}=0$ 可得

$$
\boldsymbol{f}_{u}=\left(\mathbf{D}_{u u}-\mathbf{W}_{u u}\right)^{-1} \mathbf{W}_{u l} \boldsymbol{f}_{l}
$$

令

$$
\begin{aligned} \mathbf{P} &=\mathbf{D}^{-1} \mathbf{W}=\left[\begin{array}{cc}{\mathbf{D}_{l l}^{-1}} & {\mathbf{0}_{l u}} \\ {\mathbf{0}_{u l}} & {\mathbf{D}_{u u}^{-1}}\end{array}\right]\left[\begin{array}{ll}{\mathbf{W}_{l l}} & {\mathbf{W}_{l u}} \\ {\mathbf{W}_{u l}} & {\mathbf{W}_{u u}}\end{array}\right] \\ &=\left[\begin{array}{cc}{\mathbf{D}_{l l}^{-1} \mathbf{W}_{l l}} & {\mathbf{D}_{l l}^{-1} \mathbf{W}_{l u}} \\ {\mathbf{D}_{u u}^{-1} \mathbf{W}_{u l}} & {\mathbf{D}_{u u}^{-1} \mathbf{W}_{u u}}\end{array}\right] \end{aligned}
$$


即 $\mathbf{P}_{u u}=\mathbf{D}_{u u}^{-1} \mathbf{W}_{u u}$,$\mathbf{P}_{u l}=\mathbf{D}_{u u}^{-1} \mathbf{W}_{u l}$，则式(13.15)可重写为

$$
\begin{aligned} \boldsymbol{f}_{u} &=\left(\mathbf{D}_{u u}\left(\mathbf{I}-\mathbf{D}_{u u}^{-1} \mathbf{W}_{u u}\right)\right)^{-1} \mathbf{W}_{u l} \boldsymbol{f}_{l} \\ &=\left(\mathbf{I}-\mathbf{D}_{u u}^{-1} \mathbf{W}_{u u}\right)^{-1} \mathbf{D}_{u u}^{-1} \mathbf{W}_{u l} \boldsymbol{f}_{l} \\ &=\left(\mathbf{I}-\mathbf{P}_{u u}\right)^{-1} \mathbf{P}_{u l} \boldsymbol{f}_{l} \end{aligned}
$$


于是，将 $D_l$ 上的标记信息作为 $\boldsymbol{f}_{l}=\left(y_{1} ; y_{2} ; \ldots ; y_{l}\right)$ 代入式(13.17)，即可利用求得的 $\boldsymbol{f}_{u}$ 对未标记样本进行预测.

上面描述的是一个针对二分类问题的标记传播(label propagation)方法，下 面来看一个适用于多分类问题的标记传播方法[Zhou et al., 2004].

假定 $y_{i} \in \mathcal{Y}$ 仍基于 $D_{l} \cup D_{u}$ ，构建一个图 $G=(V,E)$ ，其中结点集 $V=\left\{\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{l}, \ldots, \boldsymbol{x}_{l+u}\right\}$ ，边集 $E$ 所对应的 $\mathbf{W}$ 仍使用式(13,11)，对角矩阵 $\mathbf{D}=\operatorname{diag}\left(d_{1}, d_{2}, \dots, d_{l+u}\right)$ 的对角元素 $d_{i}=\sum_{j=1}^{l+u}(\mathbf{W})_{i j}$ 。定义一个 $(l+u) \times|\mathcal{Y}|$ 的非负标记矩阵 $\mathbf{F}=\left(\mathbf{F}_{1}^{\mathrm{T}}, \mathbf{F}_{2}^{\mathrm{T}}, \ldots, \mathbf{F}_{l+u}^{\mathrm{T}}\right)^{\mathrm{T}}$ ，其第 $i$ 行元素 $\mathbf{F}_{i}=\left((\mathbf{F})_{i 1},(\mathbf{F})_{i 2}, \ldots,(\mathbf{F})_{i|\mathcal{Y}|}\right)$ 为示例 $\boldsymbol{x}_{i}$ 的标记向量，相应的分类规则为: $y_{i}=\arg \max _{1 \leqslant j \leqslant|\mathcal{Y}|}(\mathbf{F})_{i j}$ 。




对 $i=1,2, \ldots, m, j=1,2, \ldots,|\mathcal{Y}|$，将 $\mathbf{F}$ 初始化为

$$
\mathbf{F}(0)=(\mathbf{Y})_{i j}=\left\{\begin{array}{ll}{1,} & {\text { if }(1 \leqslant i \leqslant l) \wedge\left(y_{i}=j\right)} \\ {0,} & {\text { otherwise }}\end{array}\right.
$$


显然，$\mathbf{Y}$ 的前 $l$ 行就是 $l$ 个有标记样本的标记向量.

基于 $\mathbf{W}$ 构造一个标记传播矩阵 $\mathbf{S}=\mathbf{D}^{-\frac{1}{2}} \mathbf{W} \mathbf{D}^{-\frac{1}{2}}$ ，其中 $\mathbf{D}^{-\frac{1}{2}}=\operatorname{diag}\left(\frac{1}{\sqrt{d_{1}}}, \frac{1}{\sqrt{d_{2}}}, \ldots, \frac{1}{\sqrt{d_{l+u}}}\right)$ ，于是有迭代计算式

$$
\mathbf{F}(t+1)=\alpha \mathbf{S F}(t)+(1-\alpha) \mathbf{Y}
$$

其中 $\alpha\in(0,1)$ 为用户指定的参数，用于对标记传播项 $\mathbf{S F}(t)$ 与初始化项 $\mathbf{Y}$ 的重要性进行折中。基于式(13.19)迭代至收敛可得

$$
\mathbf{F}^{*}=\lim _{t \rightarrow \infty} \mathbf{F}(t)=(1-\alpha)(\mathbf{I}-\alpha \mathbf{S})^{-1} \mathbf{Y}
$$


由 $\mathbf{F}^{*}$ 可获得 $D_u$ 中样本的标记 $\left(\hat{y}_{l+1}, \hat{y}_{l+2}, \dots, \hat{y}_{l+u}\right)$ 。算法描述如图 13.5所示.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180701/5hGhkeb6a0.png?imageslim">
</p>

事实上，图 13.5的算法对应于正则化框架[Zhou et al., 2004]

$$
\min _{\mathbf{F}} \frac{1}{2}\left(\sum_{i, j=1}^{l+u}(\mathbf{W})_{i j}\left\|\frac{1}{\sqrt{d_{i}}} \mathbf{F}_{i}-\frac{1}{\sqrt{d_{j}}} \mathbf{F}_{j}\right\|^{2}\right)+\mu \sum_{i=1}^{l}\left\|\mathbf{F}_{i}-\mathbf{Y}_{i}\right\|^{2}
$$


其中 $\mu>0$ 为正则化参数。当 $\mu=\frac{1-\alpha}{\alpha}$ 时，式(13.21)的最优解恰为图 13.5算法的迭代收敛解 $\mathbf{F}^{*}$。

式(13.21)右边第二项是迫使学得结果在有标记样本上的预测与真实标记 尽可能相同，而第一项则迫使相近样本具有相似的标记，显然，它与式(13.12)都 是基于半监督学习的基本假设，不同的是式(13.21)考虑离散的类别标记，而 式(13.12)则是考虑输出连续值.

图半监督学习方法在概念上相当清晰，且易于通过对所涉矩阵运算的分析来探索算法性质。但此类算法的缺陷也相当明显。首先是在存储开销上，若样本数为 $O(m)$，则算法中所涉及的矩阵规模为 $O(m^2)$ ，这使得此类算法很难直接 处理大规模数据；另一方面，由于构图过程仅能考虑训练样本集，难以判知新样本在图中的位置，因此，在接收到新样本时，或是将其加入原数据集对图进行重 构并重新进行标记传播，或是需引入额外的预测机制，例如将 $D_l$ 和经标记传播后得到标记的 $D_u$ 合并作为训练集，另外训练一个学习器例如支持向量机来对新样本进行预测.



# 相关

- 《机器学习》周志华
