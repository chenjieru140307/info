
# 多变量决策树


决策树形成的分类边界：

- 若我们把每个属性视为坐标空间中的一个坐标轴，则 $d$ 个属性描述的样本就对应了 $d$ 维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界。
- 决策树所形成的分类边界有一个明显的特点: 轴平行 (axis-parallel) ，即它的分类边界由若干个与坐标轴平行的分段组成。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200607/WSbk9Rm7Xpt1.png?imageslim">
</p>

- 这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值。但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似。
  - 此时的决策树会相当复杂，由于要进行大量的属性测试，预测 时间开销会很大.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200607/ss3GiYGIIusf.png?imageslim">
</p>


解决：

- 如果能使用斜的划分边界，如上图中红色线段所示，则决策树模型将大为简化。


多变量决策树 (multivariate decision tree)


- 可以实现这样的 “斜划分” 甚至更复杂划分的决策树。
- 在此类决策树中，非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试。换言之，每个非叶结点是一个形如 $\sum_{i=1}^{d} w_{i} a_{i}=t$ 的线性分类器，其中 $w_i$ 是属性 $a_i$  的权重, $w_i$ 和 $t$ 可在该结点所含的样本集和属性集上学得。
- 于是，与传统的“单变量决策树”(univariate decision tree)不同，在多变量决策树的学习过程中， 不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。


类似如下：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200607/rp10szzI2M8a.png?imageslim">
</p>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200607/DxMmA0ij9ywp.png?imageslim">
</p>


疑问：

- 怎么实现的。补充下。

