# 决策树




决策树 Decision Tree

介绍：

- 一个困难的预测问题，通过树的分支节点，被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。
- 将该过程不断递归下去（Recursive Partitioning）。
- 随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。
- 当分支节点的深度或者问题的简单程度满足一定的停止规则（Stopping Rule）时, 该分支节点会停止分裂，此为自上而下的停止阈值（Cutoff Threshold）法；
  - 有些决策树也使用自下而上的剪枝（Pruning）法。


说明：

- 每次决策，其考虑的范围，是在上次决策结果的限定范围之内的。
- 树由结点和有向边组成。
  - 结点分为内部结点和叶结点，
    - 内部结点表示一个特征或属性
    - 叶结点表示类别
    - 从顶部根结点开始，所有样本聚在一起。经过根结点的划分，样本被分到不同的子结点中。再根据子结点的特征进一步划分，直至所有样本都被归到某一个类别（即叶结点）中。


优点：

- 构造简单，判别计算快速
- 易理解，机理解释起来简单。
- 可以用于小数据集。
- 时间复杂度较小，为用于训练决策树的数据点的对数。
- 可以处理数字和数据的类别。
- 能够处理多输出的问题。（怎么处理？）
- 对缺失值不敏感。
- 可以处理不相关特征数据。<span style="color:red;">是的，没有提取特征之间的相关性。</span>
- 效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。
- 对数据不需要任何加工，数据不需要任何加工，让本既可以是离散性也可以是连续型，而且数据也是允许缺失的。
- 对不均衡的数据效果也很好。



缺点：

- 对连续性的字段比较难预测。
- 泛化能力差，容易出现过拟合
- 当类别太多时，错误可能就会增加的比较快。
- 在处理特征关联性比较强的数据时表现得不是太好。<span style="color:red;">什么样的特征关联性比较强的数据？是线性关联吗？还是逻辑关联？</span>
- 对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。
- 对新增加的样本，需要重新调整树的结构。如果新增加的一个样本，如果分错了，那么这个树就不是一个最优树了，那么就需要重新调整。（怎么进行调整？）


应用：

- 可以用于分类和回归问题，
- 在市场营销和生物医药等领域尤其受欢迎
  - 主要因为树形结构与销售、诊断等场景下的决策过程十分相似。


在决策树中应用集成学习：

- 随机森林
- 梯度提升决策树




过程：

- 特征选择：
  - 从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准。
    - 如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。
- 决策树生成：
  - 根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。
- 剪枝：
  - 决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。
    - 剪枝技术有预剪枝和后剪枝两种。
