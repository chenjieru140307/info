


# 决策树中缺失值的处理

现实任务中常会遇到不完整样本，即样本的某些属性值缺失。例如由于诊测成本、隐私保护等因素，患者的医疗数据在某些属性上的取值（如 HIV 测试结果）未知；尤其是在属性数目较多的情况下，往往会有大量样本出现缺失值。如果简单地放弃不完整样本，仅使用无缺失值的样本来进行学习，显然是对数据信息极大的浪费。
例如，表 4.4是表 4.1中的西瓜数据集 2.0出现缺失值的版 本，如果放弃不完整样本，则仅有编号{4, 7, 14, 16} 的 4 个样本能被使用。显然，有必要考虑利用有缺失属性值的训练样例来进行学习.<span style="color:red;">嗯</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180626/60DCkg8cBK.png?imageslim">
</p>

我们需解决两个问题：

1. 如何在属性值缺失的情况下进行划分属性选择?
2. 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

给定训练集 D 和属性 a ，令 $\tilde{D}$ 表示 $D$ 中在属性 a 上没有缺失值的样本子集。对问题(1)，显然我们仅可根据 $\tilde{D}$ 来判断属性 a 的优劣。假定属性 a 有 V 个可取值 $\{a^1,a^2,\cdots ,a^V\}$ 。令 $\tilde{D}^v$ 表示  $\tilde{D}$  中在属性 a 上取值为 $a^v$ 的样本子集，$\tilde{D}_k$ 表示 $\tilde{D}$ 中属于第 k 类 $(k=1,2,\cdots,|\mathcal{Y}|)$ 的样本子集，则显然有 $\tilde{D}=\bigcup_{k=1}^{|\mathcal{Y}|}\tilde{D}_k$ ,$\tilde{D}=\bigcup_{v=1}^{|\mathcal{V}|}\tilde{D}_v$ 。 假定我们为每个样本 x 赋予一个权重 $w_x$ ，并定义


$$
\rho=\frac{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in D} w_{\boldsymbol{x}}}\tag{4.9}
$$
$$
\tilde{p}_{k}=\frac{\sum_{\boldsymbol{x} \in \tilde{D}_{k}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}} \quad(1 \leqslant k \leqslant|\mathcal{Y}|)\tag{4.10}
$$
$$
\tilde{r}_{v}=\frac{\sum_{\boldsymbol{x} \in \tilde{D}^{v}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}} \quad(1 \leqslant v \leqslant V)\tag{4.11}
$$

直观地看，对属性 a，$\rho$ 表示无缺失值样本所占的比例，$\tilde{p}_k$ 表示无缺失值样本中第 k 类所占的比例，$\tilde{r}_v$ 则表示无缺失值样本中在属性 a 上取值 $a^v$ 的样本所占的比例。显然，$\sum_{k=1}^{|\mathcal{Y}|}\tilde{p}_k=1$ ,$\sum_{v=1}^{V}\tilde{r}_v=1$ 。

基于上述定义，我们可将信息增益的计算式(4.2)推广为

$$
\begin{aligned} \operatorname{Gain}(D, a) &=\rho \times \operatorname{Gain}(\tilde{D}, a) \\ &=\rho \times\left(\operatorname{Ent}(\tilde{D})-\sum_{v=1}^{V} \tilde{r}_{v} \operatorname{Ent}\left(\tilde{D}^{v}\right)\right) \end{aligned}\tag{4.12}
$$

其中由式(4.1)，有

$$
\operatorname{Ent}(\tilde{D})=-\sum_{k=1}^{ | \mathcal{Y |}} \tilde{p}_{k} \log _{2} \tilde{p}_{k}
$$

对问题(2)，若样本 $\boldsymbol{x}$ 在划分属性 $a$ 上的取值已知，则将 $\boldsymbol{x}$ 划入与其取值对 应的子结点，且样本权值在子结点中保持为 $w_\boldsymbol{x}$ 。若样本 $\boldsymbol{x}$ 在划分属性 a 上的取值未知，则将 $\boldsymbol{x}$ 同时划入所有子结点，且样本权值在与属性值 $a^v$ 对应的子结点中调整为 $\tilde{r}_v\cdot w_x$ ；直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去.

C4.5算法使用了上述解决方案[Quinlan, 1993]。下面我们以表 4.4的数据集为例来生成一棵决策树.

在学习开始时，根结点包含样本集 D 中全部 17 个样例，各样例的权值均为 1.以属性“色泽”为例，该属性上无缺失值的样例子集 $\tilde{D}$ 乃包含编号为 {2,3,4,6,7,8,9,10,11，12,14,15,16,17} 的 14 个样例。显然，$\tilde{D}$ 的信息熵为

$$
\begin{aligned} \operatorname{Ent}(\tilde{D}) &=-\sum_{k=1}^{2} \tilde{p}_{k} \log _{2} \tilde{p}_{k} \\ &=-\left(\frac{6}{14} \log _{2} \frac{6}{14}+\frac{8}{14} \log _{2} \frac{8}{14}\right)=0.985 \end{aligned}
$$

令 $\tilde{D}^1$ ，$\tilde{D}^2$ 与 $\tilde{D}^3$ 分别表示在属性“色泽”上取值为“青绿” “乌黑”以及“浅白”的样本子集，有

$$
\operatorname{Ent}\left(\tilde{D}^{1}\right)=-\left(\frac{2}{4} \log _{2} \frac{2}{4}+\frac{2}{4} \log _{2} \frac{2}{4}\right)=1.000
$$

$$
\operatorname{Ent}\left(\tilde{D}^{2}\right)=-\left(\frac{4}{6} \log _{2} \frac{4}{6}+\frac{2}{6} \log _{2} \frac{2}{6}\right)=0.918
$$
$$
\operatorname{Ent}\left(\tilde{D}^{3}\right)=-\left(\frac{0}{4} \log _{2} \frac{0}{4}+\frac{4}{4} \log _{2} \frac{4}{4}\right)=0.000
$$

因此，样本子集 $\tilde{D}$ 上属性 “色泽” 的信息増益为

$$
\begin{aligned} \operatorname{Gain}(\tilde{D}, 色泽) &=\operatorname{Ent}(\tilde{D})-\sum_{v=1}^{3} \tilde{r}_{v} \operatorname{Ent}\left(\tilde{D}^{v}\right) \\ &=0.985-\left(\frac{4}{14} \times 1.000+\frac{6}{14} \times 0.918+\frac{4}{14} \times 0.000\right) \\ &=0.306 \end{aligned}
$$

于是，样本集 D 上属性 “色泽” 的信息增益为：


$$
\begin{aligned}
\operatorname{Gain}(\tilde{D}，色泽)&=\rho \times \operatorname(Gain)(\tilde{D}，色泽)\\&=\frac{14}{17} \times 0.306=0.252
\end{aligned}
$$

类似地可计算出所有属性在 D 上的信息增益：

- Gain（D，色泽）=0.252; Gain（D，根蒂）=0.171;
- Gain（D，敲声）=0.145; Gain（D，纹理）=0.424;
- Gain（D，脐部）=0.289; Gain（D，触感）=0.006;

“纹理” 在所有属性中取得了最大的信息增益，被用于对根结点进行划分. 划分结果是使编号为 {1,2,3,4,5,6,15} 的样本进入“纹理=清晰”分支，编号为 {7,9,13,14,17} 的样本进入 “纹理=稍糊” 分支，而编号为 {11,12,16} 的样 本进入 “纹理=模糊” 分支，且样本在各子结点中的权重保持为 1。需注意的 是，编号为 {8} 的样本在属性 “纹理” 上出现了缺失值，因此它将同时进入三个分支中，但权重在三个子结点中分别调整为 $\frac{7}{15}$、$\frac{5}{15}$ 和 $\frac{3}{15}$ 编号为 {10} 的样本有类似划分结果。

上述结点划分过程递归执行，最终生成的决策树如图 4.9所示.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180626/HeJ1B0dgBb.png?imageslim">
</p>






# 相关


- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
