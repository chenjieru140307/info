

# 节点划分

## 划分标准

对于树，希望：

- 能拟合训练数据，达到良好的分类效果
- 控制其复杂度，使得具有一定的泛化能力。


常用：

- 从若干不同的决策树中选取最优的决策树是一个 NP 完全问题，在实际中我们通常会采用启发式学习的方法去构建一棵满足启发式条件的决策树。（什么是 NP 完全问题？为什么是一个 NP 完全问题？）


对于划分：

- 关键：如何选择最优划分属性。
  - 即，判断什么样的划分是优的
- 希望：随着划分，分支结点所包含的样本尽可能属于同一类别，即结点的 "纯度" (purity) 越来越高。
- 衡量：衡量划分的好坏：
  - 信息增益       对应 ID3 决策树学习算法
  - 信息增益率     对应 C4.5 决策树算法
  - 基尼系数       对应 CART 决策树



## 基于信息增益来划分 ID3



信息熵 information entropy：

- 用来度量样本集合纯度。
- 代表着一个系统中蕴含多少信息量，信息量越大表明一个系统不确定性就越大，就存在越多的可能性。

定义：

- 假定当前样本集合 $D$  中第 $k$  类样本所占的比例为  $p_{k}(k=1,2, \ldots,|\mathcal{Y}|)$
- 则 $D$ 的信息熵定义为


$$
\operatorname{Ent}(D)=-\sum_{k=1}^{ | \mathcal{Y |}} p_{k} \log _{2} p_{k}
$$

说明：

- 此时，若，$D$ 中 $|y|$ 类样本均匀分布，即此时样本 $D$ 的纯度最小，信息熵最大，为


$$
\begin{aligned}
\operatorname{Ent}(D) =&-\sum_{k=1}^{|y|}\frac{1}{|y|}log_{2}{\frac{1}{|y|}} 
\\=& \sum_{k=1}^{|y|}\frac{1}{|y|}log_{2}{|y|} 
\\=& log_{2}{|y|}
\end{aligned}
$$

- 此时，若，样本 $D$ 中只有一类样本，此时样本的纯度最大，信息熵最小，其值为

$$
\begin{aligned}
Ent(D) =&-\sum_{k=1}^{|y|}\frac{1}{|y|}log_{2}{\frac{1}{|y|}} 
\\=& -1log_21-0log_20-...-0log_20 
\\=& 0
\end{aligned}
$$

注意：

- 在计算信息熵的时候，我们约定：若 $p=0$，则 $p \log _{2} p=0$。



**划分：**

未划分时，信息熵为：

$$
\operatorname{Ent}(D)=-\sum_{k=1}^{ | \mathcal{Y |}} p_{k} \log _{2} p_{k}
$$

划分：

- 离散属性 $a$ 有 $V$ 个可能的取值 $\left\{a^{1}, a^{2}, \ldots, a^{V}\right\}$，如果我们使用属性 $a$ 来对整个样本集 $D$ 进行划分，就会产生 $V$ 个分支结点，每个分支的样本数为 $D^v$ 。
- 此时，我们单看某个 $D^v$ ，它其实也可以计算出一个信息熵的 $Ent(D^v)$ 。
- 则，划分后的总的样本集的信息熵如下，
  - 其中，我们给每个分支样本集的信息熵赋予权重 $\left|D^{v}\right| /|D|$。即样本数越多的分支结点的影响越大。

$$\sum_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v)$$

引起的信息增益：

$$
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)
$$

说明：

- 信息增益越大，表示使用特征 $a$ 来对样本集进行划分所获得的纯度提升越大。
  - 因为，信息增益越大，说明划分后的信息熵越小，即说明样本集越纯。最小为 0。

注意：

- 由于在计算信息增益中倾向于特征值越多的特征进行优先划分，那么，如果，某个特征值的离散值个数与样本集 $D$ 个数相同，此时，虽然使用这个特征进行划分使得样本纯度提升最高，但是并不具有泛化能力。

即：

- 我们可以使用信息增益来进行决策树的划分属性选择
- 选择能够使信息增益最大的属性：

$$
a_{*}=\underset{a \in A}{\arg \max } \operatorname{Gain}(D, a)
$$

**举例：**

样本：

| 编号    | 色泽  | 根蒂 | 敲声      | 纹理 | 脐部 | 触感 | 好瓜 |
| ------- | ----- | ---- | --------- | ---- | ---- | ---- | ---- |
| 1   | 青绿  | 蜷缩 | 浊响      | 清晰 | 凹陷 | 硬滑 | 是   |
| 2   | 乌黑  | 蜷缩 | 沉闷      | 清晰 | 凹陷 | 硬滑 | 是   |
| 3   | 乌黑  | 蜷缩 | 浊响      | 清晰 | 凹陷 | 硬滑 | 是   |
| 4   | 青绿  | 蜷缩 | 沉闷      | 清晰 | 凹陷 | 硬滑 | 是   |
| 5   | 浅白  | 蜷缩 | 浊响      | 清晰 | 凹陷 | 硬滑 | 是   |
| 6   | 青绿  | 稍蜷 | 浊响      | 清晰 | 稍凹 | 软粘 | 是   |
| 7   | 乌黑  | 稍蜷 | 浊响      | 稍糊 | 稍凹 | 软粘 | 是   |
| 8 | 乌黑  | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 | 是   |
| 9   | 乌黑  | 稍蜷 | 沉闷      | 稍糊 | 稍凹 | 硬滑 | 否   |
| 10  | 青绿  | 硬挺 | 清脆      | 清晰 | 平坦 | 软粘 | 否   |
| 11  | 浅白  | 硬挺 | 清脆      | 模糊 | 平坦 | 硬滑 | 否   |
| 12  | 浅白  | 蜷缩 | 浊响      | 模糊 | 平坦 | 软粘 | 否   |
| 13  | 青绿  | 稍蜷 | 浊响      | 稍糊 | 凹陷 | 硬滑 | 否   |
| 14  | 浅白  | 稍蜷 | 沉闷      | 稍糊 | 凹陷 | 硬滑 | 否   |
| 15  | 乌黑  | 稍蜷 | 浊响      | 清晰 | 稍凹 | 软粘 | 否   |
| 16  | 浅白 | 蜷缩 | 浊响      | 模糊 | 平坦 | 硬滑 | 否   |
| 17  | 青绿  | 蜷缩 | 沉闷     | 稍糊 | 稍凹 | 硬滑 | 否   |



说明：

- $|\mathcal{Y}|=2$，为是和否。

计算：

- 根节点信息熵：
  - $D$ 中的所有样例，其中正例占 $p_1 =\frac{8}{17}$，反例占的 $p_2 =\frac{9}{17}$。

$$
\begin{aligned}
\operatorname{Ent}(D)=&-\sum_{k=1}^{2} p_{k} \log _{2} p_{k}
\\=&-\left(\frac{8}{17} \log _{2} \frac{8}{17}+\frac{9}{17} \log _{2} \frac{9}{17}\right)
\\=&0.998
\end{aligned}
$$



- 计算出当前属性集合  {色泽，根蒂，敲声，纹理，脐部，触感}  中每个属性的信息增益
  - 以属性"色泽"为例：
    - 它有 3 个可能的取值:  {青绿，乌黑，浅自}，如果使用该属性对 $D$ 进行划分，则可得到 3个子集，分别记为:
      - $D^1$ (色泽=青绿)，
      - $D^2$ (色泽=乌黑)，
      - $D^3$ (色泽=浅白)。
    - 分析每个子集：
      - 子集 $D^1$ 包含编号为 { 1, 4, 6, 10, 13, 17 } 的 6 个样例，其中正例占 $p_1 =\frac{3}{6}$ ，反例占 $p_2 =\frac{3}{6}$;
      - 子集 $D^2$ 包含编号为 { 2, 3, 7, 8, 9, 15 } 的 6 个样例，其中正、反例分别占 $p_1 =\frac{4}{6}$， $p_1 =\frac{2}{6}$
      - 子集 $D^3$ 包含编号为 { 5, 11, 12, 14, 16 } 的 5 个样例，其中正、反例分别占 $p_1 =\frac{1}{5}$，$p_1 =\frac{4}{5}$
    - 则，用 "色泽" 划分之后所获得的 3 个分支结点的信息熵为：
      $$\operatorname{Ent}\left(D^{1}\right)=-\left(\frac{3}{6} \log _{2} \frac{3}{6}+\frac{3}{6} \log _{2} \frac{3}{6}\right)=1.000$$
      $$\operatorname{Ent}\left(D^{2}\right)=-\left(\frac{4}{6} \log _{2} \frac{4}{6}+\frac{2}{6} \log _{2} \frac{2}{6}\right)=0.918$$
      $$\operatorname{Ent}\left(D^{3}\right)=-\left(\frac{1}{5} \log _{2} \frac{1}{5}+\frac{4}{5} \log _{2} \frac{4}{5}\right)=0.722$$
    - 此时，以计算出属性 "色泽" 的信息增益为：
    $$
        \begin{aligned}
        \operatorname{Gain}(D，色泽) &=\operatorname{Ent}(D)-\sum_{v=1}^{3} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right) \\ &=0.998-\left(\frac{6}{17} \times 1.000+\frac{6}{17} \times 0.918+\frac{5}{17} \times 0.722\right) \\& =0.109
        \end{aligned}
        $$
  - 类似的，我们可计算出其他属性的信息增益:
    - Gain(D，根蒂) = 0.143
    - Gain(D，敲声) = 0.141
    - Gain(D，纹理) = 0.381
    - Gain(D，脐部) = 0.289
    - Gain(D，触感) = 0.006


- 选择划分属性。由于 "纹理" 的信息增益最大，因此，选它为划分属性。如下：

<p align="center">
    <img width="60%" height="70%" src="http://images.iterate.site/blog/image/180626/BeF6FHbm3G.png?imageslim">
</p>

- 进一步划分。以上图第一个分支结点 ( "纹理=清晰" ) 为例。
  - 该结点包含的样例集合 $D^1$ 中有编号为 {1 ,2, 3, 4, 5, 6, 8, 10, 15} 的 9 个样例，可用属性集合为{色泽，根蒂，敲声，脐部，触感}。
  - 我们依次算出各属性的信息增益：
    - Gain($D^1$ ，色泽) = 0.043
    - Gain($D^1$ ，根蒂) = 0.458
    - Gain($D^1$ ，敲声) = 0.331
    - Gain($D^1$ ，脐部) = 0.458
    - Gain($D^1$ ，触感) = 0.458
  - 可见，"根蒂"、 "脐部"、 "触感" 3 个属性均取得了最大的信息增益，这里我们可以任选其中之一作为划分属性。
- 递归进行上述操作，得到的决策树：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200606/XcY88BEKr37D.png?imageslim">
</p>



## 基于增益率来划分 C4.5


ID3 存在的问题：


- 如果，有一个属性，每个样本值在这个属性上的属性值都不同，那么会发生什么？
  - 如果按照这个属性来划分，那么每个节点只有一个样本，这个节点的信息熵就是 0
  - 也就是说，这种划分的信息增益就是 0.998-0=0.998 。
  - 此时，由于按照信息增益进行划分，那么肯定会选择这个属性进行划分，那么，划分后每个节点只有一个样本，划分停止。
  - 这样的划分泛化能力弱，无法对新样本进行有效预测。
  - 也即：信息增益准则 对可取值数目较多的属性是有所偏好的。

因此，出现 C4.5 决策树算法:

- 使用增益率 (gain ratio) 来选择 划分属性。


增益率：

$$
\operatorname{Gain\_ratio}(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}
$$

其中：

$$
IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$

说明：

- $IV(a)$ 是特征 $a$ 的熵。称为属性 $a$ 的 "固有值" (intrinsic value) 。属性 $a$ 的可能取值数目越多(即 $V$ 越大)，则 $IV(a)$ 的值通常会越大。


举例：


对于样本：

| 编号    | 色泽  | 根蒂 | 敲声      | 纹理 | 脐部 | 触感 | 好瓜 |
| ------- | ----- | ---- | --------- | ---- | ---- | ---- | ---- |
| 1   | 青绿  | 蜷缩 | 浊响      | 清晰 | 凹陷 | 硬滑 | 是   |
| 2   | 乌黑  | 蜷缩 | 沉闷      | 清晰 | 凹陷 | 硬滑 | 是   |
| 3   | 乌黑  | 蜷缩 | 浊响      | 清晰 | 凹陷 | 硬滑 | 是   |
| 4   | 青绿  | 蜷缩 | 沉闷      | 清晰 | 凹陷 | 硬滑 | 是   |
| 5   | 浅白  | 蜷缩 | 浊响      | 清晰 | 凹陷 | 硬滑 | 是   |
| 6   | 青绿  | 稍蜷 | 浊响      | 清晰 | 稍凹 | 软粘 | 是   |
| 7   | 乌黑  | 稍蜷 | 浊响      | 稍糊 | 稍凹 | 软粘 | 是   |
| 8 | 乌黑  | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 | 是   |
| 9   | 乌黑  | 稍蜷 | 沉闷      | 稍糊 | 稍凹 | 硬滑 | 否   |
| 10  | 青绿  | 硬挺 | 清脆      | 清晰 | 平坦 | 软粘 | 否   |
| 11  | 浅白  | 硬挺 | 清脆      | 模糊 | 平坦 | 硬滑 | 否   |
| 12  | 浅白  | 蜷缩 | 浊响      | 模糊 | 平坦 | 软粘 | 否   |
| 13  | 青绿  | 稍蜷 | 浊响      | 稍糊 | 凹陷 | 硬滑 | 否   |
| 14  | 浅白  | 稍蜷 | 沉闷      | 稍糊 | 凹陷 | 硬滑 | 否   |
| 15  | 乌黑  | 稍蜷 | 浊响      | 清晰 | 稍凹 | 软粘 | 否   |
| 16  | 浅白 | 蜷缩 | 浊响      | 模糊 | 平坦 | 硬滑 | 否   |
| 17  | 青绿  | 蜷缩 | 沉闷     | 稍糊 | 稍凹 | 硬滑 | 否   |

有：

- IV(触感) = 0.874 (V = 2)
- IV(色泽) = 1.580 (V = 3)
- IV(编号) = 4.088 (V = 17)


注意：

- 增益率对特征值较少的特征有一定偏好，因此 C4.5 选择特征的方法是，先从候选特征中选出信息增益高于平均水平的特征，再从这些特征中选择增益率最高的。




## 基于基尼指数来划分 CART


CART 决策树：

- 使用基尼指数 (Gini index) 来选择划分属性。

$D$ 的基尼值：

- 假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k(k =1,2,...,|y|)$，则 $D$ 的基尼值为


$$
\begin{aligned}
Gini(D) &=\sum_{k=1}^{|y|}\sum_{k\neq{k'}}{p_k}{p_{k'}}\\
&=\sum_{k=1}^{|y|}{p_k}{(1-p_k)} \\
&=1-\sum_{k=1}^{|y|}p_k^2
\end{aligned}
$$


说明：

- $\operatorname{Gini}(D)$ 反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。
- $\operatorname{Gini}(D)$ 越小，则数据集的纯度越高。

属性 $a$ 的基尼指数：

$$
(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)
$$

因此，在候选属性集合 $A$ 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即：

$$
a_{*}=\underset{a \in A}{\arg \min } \text { Gini\_index }(D, a)
$$





## 对比

- ID3
  - 采用信息增益作为评价标准，倾向于取值较多的特征。因为，信息增益反映的是给定条件以后不确定性减少的程度，特征取值越多就意味着确定性更高，也就是条件熵越小，信息增益越大。
  - ID3 只能处理离散型变量
  - 只能用于分类任务。
  - 对样本特征缺失值比较敏感
  - 可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复用
- C4.5
  - 是对 ID3 进行优化，通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，避免 ID3 出现过拟合的特性，提升决策树的泛化能力。
  - 可以处理连续型变量。
    - C4.5 处理连续型变量时，通过对数据排序之后找到类别不同的分割线作为切分点，根据切分点把连续属性转换为布尔型，从而将连续型变量转换多个取值区间的离散型变量。
  - 只能用于分类任务。
  - 可以对缺失值进行处理
  - 可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复用
- CART
  - 可以处理连续型变量。由于 CART 构建时每次都会对特征进行二值划分，因此可以很好地适用于连续性变量。（补充）
  - 可以应用于回归任务（回归树使用最小平方误差准则）。
  - 可以对缺失值进行处理
  - CART 每个结点只会产生两个分支，因此最后会形成一颗二叉树，且每个特征可以被重复使用

对于为了保持树的准确性和泛化能力来说：

- ID3 和 C4.5 通过剪枝来权衡树的准确性与泛化能力，
- 而 CART 直接利用全部数据发现所有可能的树结构进行对比。（没明白）


