
# 原型聚类

原型聚类

亦称基于原型的聚类  prototype-based clustering

原型聚类：

- 假设：聚类结构能通过一组原型刻画。
- 在现实聚类任务中极为常用。
- 通常情形下，算法先对原型进行初始化，然后对原型进行迭代更新求解。
  - 采用不同的原型表示、不同的求解方式，将产生不同的算法。



几种著名的原型聚类算法.

## k 均值算法

描述：

- 给定样本集 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$  
- “k均值” (k-means)算法针对聚类所得簇划分 $\mathcal{C}=\left\{C_{1}, C_{2}, \ldots, C_{k}\right\}$ 最小化平方误差

$$
E=\sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_{i}}\left\|\boldsymbol{x}-\boldsymbol{\mu}_{i}\right\|_{2}^{2}\tag{9.24}
$$

- 其中：
  - $\boldsymbol{\mu}_{i}=\frac{1}{\left|C_{i}\right|} \sum_{\boldsymbol{x} \in C_{i}} \boldsymbol{x}$ 是簇 $C_{i}$ 的均值向量。
- 直观来看，上式在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度， $E$ 值越小则簇内样本相似度越高.

求解：

- 最小化 $E=\sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_{i}}\left\|\boldsymbol{x}-\boldsymbol{\mu}_{i}\right\|_{2}^{2}$ 并不容易，找到它的最优解需考察样本集 $D$ 所有可能的簇划分，这是一个 NP 难问题。
- 因此，$k$ 均值算法采用了贪心策略，通过迭代优化来近似求解 $E=\sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_{i}}\left\|\boldsymbol{x}-\boldsymbol{\mu}_{i}\right\|_{2}^{2}$。


算法流程如下：



- 输入：
  - 样本集 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$
  - 聚类族数 $k$ 
- 过程:
  - 01: 从 $D$ 中随机选择 $k$ 个样本作为初始均值向量 $\left\{\boldsymbol{\mu}_{1}, \boldsymbol{\mu}_{2}, \ldots, \boldsymbol{\mu}_{k}\right\}$
  - 02: repeat
  - 03: $\quad$ 令 $C_{i}=\varnothing(1 \leqslant i \leqslant k)$
  - 04: $\quad$ for $j=1,2, \dots, m$ do
  - 05: $\quad$ $\quad$ 计算样本 $\boldsymbol{x}_{j}$ 与各均值向量 $\boldsymbol{\mu}_{i}(1 \leqslant i \leqslant k)$ 的距离: $d_{j i}=\left\|\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}\right\|_{2}$
  - 06: $\quad$ $\quad$ 根据距离最近的均值向量确定 $\boldsymbol{x}_{j}$ 的族标记: $\lambda_{j}=\arg \min _{i \in\{1,2, \ldots, k\}} d_{j i}$
  - 07: $\quad$ $\quad$ 将样本 $\boldsymbol{x}_{j}$ 划入相应的族: $C_{\lambda_{j}}=C_{\lambda_{j}} \bigcup\left\{\boldsymbol{x}_{j}\right\}$
  - 08: $\quad$ end for
  - 09: $\quad$ for $i=1,2, \dots, k$ do
  - 10: $\quad$ $\quad$ 计算新均值向量: $\boldsymbol{\mu}_{i}^{\prime}=\frac{1}{\left|C_{i}\right|} \sum_{\boldsymbol{x} \in C_{i}} \boldsymbol{x}$
  - 11: $\quad$ $\quad$ if $\mu_{i}^{\prime} \neq \mu_{i}$ then
  - 12: $\quad$ $\quad$ $\quad$ 将当前均值向量 $\boldsymbol{\mu}_{i}$ 更新为 $\boldsymbol{\mu}_{i}^{\prime}$
  - 13: $\quad$ $\quad$ else
  - 14: $\quad$ $\quad$ $\quad$保持当前均值向量不变
  - 15: $\quad$ $\quad$ end if
  - 16: $\quad$ end for
  - 17: until 当前均值向量均未更新
- 输出：
  - 族划分 $\mathcal{C}=\left\{C_{1}, C_{2}, \ldots, C_{k}\right\}$



说明：

- 其中第 1 行对均值向量进行初始化，
- 在第 4-8行与第 9-16行依次对当前簇划分及均值向量迭代更新，
- 若迭代更新后聚类结果保持不变，则在第 18 行将当前簇划分结果返回.


**举例：**

样本：


| 编号	| 密度	 | 含糖率	| 编号	| 密度	| 含糖率 |	编号 |	密度	| 含糖率 |
| -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 1	    | 0.697	 | 0.460	| 11	| 0.245	| 0.057	| 21 |	0.748 |	0.232 |
| 2	    | 0.774	 | 0.376	| 12	| 0.343	| 0.099	| 22 |	0.714 |	0.346 |
| 3     | 0.634	 | 0.264	| 13	| 0.639	| 0.161	| 23 |	0.483 |	0.312 |
| 4	    | 0.608	 | 0.318	| 14	| 0.657	| 0.198	| 24 |	0.478 |	0.437 |
| 5	    | 0.556	 | 0.215	| 15	| 0.360	| 0.370	| 25 |	0.525 |	0.369 |
| 6	    | 0.403	 | 0.237	| 16	| 0.593	| 0.042	| 26 |	0.751 |	0.489 |
| 7	    | 0.481	 | 0.149	| 17	| 0.719	| 0.103	| 27 |	0.532 |	0.472 |
| 8	    | 0.437	 | 0.211	| 18	| 0.359	| 0.188	| 28 |	0.473 |	0.376 |
| 9	    | 0.666	 | 0.091	| 19	| 0.339	| 0.241	| 29 |	0.725 |	0.445 |
| 10	| 0.243	 | 0.267	| 20	| 0.282	| 0.257	| 30 |	0.446 |	0.459 |


过程：

- 将编号为 $i$ 的样本称为 $\boldsymbol{x}_{i}$ ，这是一个包含“密度”与“含糖率” 两个属性值的二维向量。
- 假定聚类簇数 $k = 3$。
- 算法开始时随机选取三个样本 $x_6$,$x_{12}$,$x_{27}$ 作为初始均值向量，即：
  - $\mu_{1}=(0.403 ; 0.237)$
  - $\mu_{2}=(0.343 ; 0.099)$
  - $\mu_{3}=(0.532 ; 0.472)$
- 考察各个样本：
  - 考察样本 $\boldsymbol{x}_{1}=(0.697 ; 0.460)$：
    - 它与当前均值向量 $\boldsymbol{\mu}_{1}, \boldsymbol{\mu}_{2}, \boldsymbol{\mu}_{3}$ 的距离分别为 $0.369,0.506,0.166$
    - 因此 $\boldsymbol{x}_{1}$ 将被划入簇 $C_3$ 中。。
  - 类似的，对数据集中的所有样本考察一遍。
- 得当前簇划分为
  - $C_{1}=\left\{\boldsymbol{x}_{5}, \boldsymbol{x}_{6}, \boldsymbol{x}_{7}, \boldsymbol{x}_{8}, \boldsymbol{x}_{9}, \boldsymbol{x}_{10}, \boldsymbol{x}_{13}, \boldsymbol{x}_{14}, \boldsymbol{x}_{15}, \boldsymbol{x}_{17}, \boldsymbol{x}_{18}, \boldsymbol{x}_{19}, \boldsymbol{x}_{20}, \boldsymbol{x}_{23}\right\}$
  - $C_{2}=\left\{\boldsymbol{x}_{11}, \boldsymbol{x}_{12}, \boldsymbol{x}_{16}\right\}$
  - $C_{3}=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \boldsymbol{x}_{3}, \boldsymbol{x}_{4}, \boldsymbol{x}_{21}, \boldsymbol{x}_{22}, \boldsymbol{x}_{24}, \boldsymbol{x}_{25}, \boldsymbol{x}_{26}, \boldsymbol{x}_{27}, \boldsymbol{x}_{28}, \boldsymbol{x}_{29}, \boldsymbol{x}_{30}\right\}$
- 从 $C_1$、 $C_2$、 $C_3$ 分别求出新的均值向量
  - $\boldsymbol{\mu}_{1}^{\prime}=(0.473 ; 0.214)$
  - $\boldsymbol{\mu}_{2}^{\prime}=(0.394 ; 0.066)$
  - $\boldsymbol{\mu}_{3}^{\prime}=(0.623 ; 0.388)$
- 更新当前均值向量后，不断重复上述过程。
- 当第五轮迭代时，产生的结果与第四轮迭代相同，于是算法停止。
- 如图所示，得到最终的簇划分：

<p align="center">
    <img width="80%" height="70%" src="http://images.iterate.site/blog/image/180628/Ldbddlm2kL.png?imageslim">
</p>


**优缺点：**

优点：

- 对于大数据集，K 均值聚类算法相对是可伸缩和高效的，它的计算复杂度是 $O(NKt)$ 接近于线性。
  - 其中：
    - $N$ 是数据对象的数目
    - $K$ 是聚类的簇数
    - $t$ 是迭代的轮数。
- 尽管算法经常以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。
- 当簇近似为高斯分布时，它的效果较好。（是因为是 GMM 模型的一个特例，所以才对高斯分布情况支持好吗？）


缺点：

- 在簇的平均值可被定义的情况下才能使用，可能不适于某些应用。
- 结果通常不是全局最优而是局部最优解
- 需要人工预先确定初始 K 值，且该值和真实的数据分布未必吻合。
- 不适合于发现非凸形状的簇或者大小差别很大的簇
- 对躁声和孤立点数据敏感
- 不太适用于离散分类等。
- K 均值只能收敛到局部最优，效果受到初始值很大。
- 样本点只能被划分到单一的类中。



注：

- 因此，可以作为其他聚类方法的基础算法，如谱聚类


**调优：**


- 数据归一化和离群点处理
  - K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。
  - 同时，离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，因此使用 K 均值聚类算法之前通常需要对数据做预处理。
- 合理选择 K 值
  - K值的选择是 K 均值聚类最大的问题之一，这也是 K 均值聚类算法的主要缺点。
  - 实际上，我们希望能够找到一些可行的办法来弥补这一缺点，或者说找到 K 值的合理估计方法。但是，K值的选择一般基于经验和多次实验结果。
    - 手肘法
      - 我们可以尝试不同的 K 值，并将不同 K 值所对应的损失函数画成折线，横轴为 K 的取值，纵轴为误差平方和所定义的损失函数，如图所示：K均值算法中 K 值的选取：手肘法
      - 由图可见，K值越大，距离和越小；并且，当 K=3时，存在一个拐点，就像人的肘部一样；当 $K\in(1,3)$ 时，曲线急速下降；当 $K>3$ 时，曲线趋于平稳。
      - 手肘法认为拐点就是 K 的最佳值。
        <p align="center">
            <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190331/64iz3OJvaHtc.png?imageslim">
        </p>

    - Gap Statistic 方法
      - 手肘法是一个经验方法，缺点就是不够自动化，因此研究员们又提出了一些更先进的方法，其中包括比较有名的 Gap Statistic方法[5]。
      - Gap Statistic方法的优点是，不再需要肉眼判断，而只需要找到最大的 Gap statistic所对应的 K 即可，因此该方法也适用于批量化作业。
      - 在这里我们继续使用上面的损失函数，当分为 K 簇时，对应的损失函数记为 $D_k$ 。
      - Gap Statistic 定义为：
      - $Gap(K)=E(logD_k)−logD_k$
        - $E(logD_k)$ 是 $logD_k$ 的期望，一般通过蒙特卡洛模拟产生。
          - 我们在样本所在的区域内按照均匀分布随机地产生和原始样本数一样多的随机样本，并对这个随机样本做 K 均值，得到一个 $D_k$；重复多次就可以计算出 $E(logD_k)$ 的近似值。<span style="color:red;">什么意思？</span>
      - $Gap(K)$ 有什么物理含义呢？
        - 它可以视为随机样本的损失与实际样本的损失之差。试想实际样本对应的最佳簇数为 $K$，那么实际样本的损失应该相对较小，随机样本损失与实际样本损失之差也相应地达到最小值，从而 $Gap(K)$ 取得最大值所对应的 $K$ 值就是最佳的簇数。根据 $Gap(K)=E(logD_k)−logD_k$ 计算 $K=1,2,...,9$ 所对应的 Gap Statistic，如图所示。

        <p align="center">
            <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190331/b8DyJ6G9CYai.png?imageslim">
        </p>

       - 由图可见，当 K=3时， $Gap(K)$ 取值最大，所以最佳的簇数是 $K=3$。
- 采用核函数
  - 采用核函数是另一种可以尝试的改进方向。
  - 传统的欧式距离度量方式，使得 $K$ 均值算法本质上假设了各个数据簇的数据具有一样的先验概率，并呈现球形或者高维球形分布，这种分布在实际生活中并不常见。
  - 面对非凸的数据分布形状时，可能需要引入核函数来优化，这时算法又称为核 $K$ 均值算法，是核聚类方法的一种。
  - 核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。



**改进：**

- 对初始聚类中心的选择的改进。
  - K-means++
    - 对于初始值的选择：
      - 原始 K 均值算法最开始随机选取数据集中 K 个点作为聚类中心，
      - 而 K-means++ 按照如下的思想选取 K 个聚类中心。
        - 假设已经选取了 n 个初始聚类中心（$0<n<K$），则在选取第 n＋1 个聚类中心时，距离当前 n 个聚类中心越远的点会有更高的概率被选为第 n＋1 个聚类中心。
        - 在选取第一个聚类中心（n=1）时同样通过随机的方法。
        - 可以说这也符合我们的直觉，聚类中心当然是互相离得越远越好。
    - 当选择完初始点后，K-means++ 后续的执行和经典 K 均值算法相同，这也是对初始值选择进行改进的方法等共同点。
- 对于 K 值大小不确定的改进。
  - ISODATA 迭代自组织数据分析法
    - 在 K 均值算法中，聚类个数 K 的值需要预先人为地确定，并且在整个算法过程中无法更改。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出 K 的大小。
    - ISODATA算法就是针对这个问题进行了改进，它的思想也很直观：
      - 当属于某个类别的样本数过少时，把该类别去除
      - 当属于某个类别的样本数过多、分散程度较大时，把该类别分为两个子类别。
    - 即，ISODATA 算法在 K 均值算法的基础之上增加了两个操作：
      - 一是分裂操作，对应着增加聚类中心数
      - 二是合并操作，对应着减少聚类中心数
    - ISODATA 算法是一个比较常见的算法，其缺点是需要指定的参数比较多，不仅仅需要一个参考的聚类数量 $K_o$，还需要指定 3 个阈值。
      - 预期的聚类中心数目 $K_o$。在 ISODATA 运行过程中聚类中心数可以变化， $K_o$ 是一个用户指定的参考值，该算法的聚类中心数目变动范围也由其决定。具体地，最终输出的聚类中心数目常见范围是从 $K_o$ 的一半，到两倍 $K_o$。
      - 每个类所要求的最少样本数目 $N_{min}$。如果分裂后会导致某个子类别所包含样本数目小于该阈值，就不会对该类别进行分裂操作。
      - 最大方差 $Sigma$。用于控制某个类别中样本的分散程度。当样本的分散程度超过这个阈值时，且分裂后满足 $K_o$ 的范围，则进行分裂操作。
      - 两个聚类中心之间所允许最小距离 $D_{min}$。如果两个类靠得非常近（即这两个类别对应聚类中心之间的距离非常小），小于该阈值时，则对这两个类进行合并操作。
- 如果希望样本不划分到单一的类中
  - 可以使用模糊 $C$ 均值或者高斯混合模型。（什么是模糊 C 均值？为什么可以不划分到单一的类中？）



## 学习向量量化 LVQ

学习向量量化 Learning Vector Quantization LVQ


学习向量量化：

- 与 $k$ 均值算法类似，也是试图找到一组原型向量来刻画聚类结构。
- 但与一般聚类算法不同的是，LVQ 假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类.

介绍：

- 给定样本集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$ ，每个样本 $\boldsymbol{x}_{j}$ 是由 $n$ 个属性描述的特征向量 $\left(x_{j 1} ; x_{j 2} ; \ldots ; x_{j n}\right)$，$y_{j} \in \mathcal{Y}$ 是样本 $\boldsymbol{x}_{j}$ 的类别标记。
- LVQ 的目标是学得一组 $n$ 维原型向量 $\left\{\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \ldots, \boldsymbol{p}_{q}\right\}$ ，每个原型向量代表一个聚类簇，簇标记 $t_{i} \in \mathcal{Y}$ 。


学习向量量化算法过程：

- 输入: 
  - 样本集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$
  - 原型向量个数 $q,$ 各原型向量预设的类别标记 $\left\{t_{1}, t_{2}, \ldots, t_{q}\right\}$
  - 学习率 $\eta \in(0,1)$
- 过程:
  - 01: 初始化一组原型向量 $\left\{\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \ldots, \boldsymbol{p}_{q}\right\}$
  - 02: repeat
  - 03: $\quad$ 从样本集 $D$ 随机选取样本 $\left(\boldsymbol{x}_{j}, y_{j}\right)$
  - 04: $\quad$ 计算样本 $\boldsymbol{x}_{j}$ 与 $\boldsymbol{p}_{i}(1 \leqslant i \leqslant q)$ 的距离: $d_{j i}=\left\|\boldsymbol{x}_{j}-\boldsymbol{p}_{i}\right\|_{2}$
  - 05: $\quad$ 找出与 $\boldsymbol{x}_{j}$ 距离最近的原型向量 $p_{i^{*}}, i^{*}=\arg \min _{i \in\{1,2, \ldots, q\}} d_{j i}$
  - 06: $\quad$ if $y_{j}=t_{i^{*}}$ then
  - 07: $\quad$ $\quad$ $\boldsymbol{p}^{\prime}=\boldsymbol{p}_{i^{*}}+\eta \cdot\left(\boldsymbol{x}_{j}-\boldsymbol{p}_{i^{*}}\right)$
  - 08: $\quad$ else
  - 09: $\quad$ $\quad$ $\boldsymbol{p}^{\prime}=\boldsymbol{p}_{i^{*}}-\eta \cdot\left(\boldsymbol{x}_{j}-\boldsymbol{p}_{i^{*}}\right)$
  - 10: $\quad$ end if
  - 11: $\quad$ 将原型向量 $\boldsymbol{p}_{i^{*}}$ 更新为 $\boldsymbol{p}^{\prime}$
  - 12: until 满足停止条件
- 输出：
  - 原型向量 $\left\{\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \ldots, \boldsymbol{p}_{q}\right\}$


说明：

- 算法第 1 行先对原型向量进行初始化，例如对第 $q$ 个簇可从类别标记为 $t_q$ 的样本中随机选取一个作为原型向量。
- 算法第 2~12 行对原型向量进行迭代优化。在每一轮迭代中，算法随机选取一个有标记训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行相应的更新。
- 在第 12 行中，若算法的停止条件已满足(例如已达到最大迭代轮数，或原型向量更新很小甚至不再更新)，则将当前原型向量作为最终结果返回。
- LVQ的关键是第 6-10行，即如何更新原型向量，直观上看，对样本 $\boldsymbol{x}_{j}$：
  - 若最近的原型向量 $\boldsymbol{p}_{i^{*}}$ 与 $\boldsymbol{x}_{j}$ 的类别标记相同
    - 则令 $\boldsymbol{p}_{i^{*}}$ 向 $\boldsymbol{x}_{j}$ 的方向靠拢， 如第 7 行所示，此时新原型向量为:
    $$
    \boldsymbol{p}^{\prime}=\boldsymbol{p}_{i^{*}}+\eta \cdot\left(\boldsymbol{x}_{j}-\boldsymbol{p}_{i^{*}}\right)\tag{9.25}
    $$
    - $\boldsymbol{p}^{\prime}$ 与 $\boldsymbol{x}_{j}$ 之间的距离为
    $$
    \begin{aligned}\left\|\boldsymbol{p}^{\prime}-\boldsymbol{x}_{j}\right\|_{2} &=\left\|\boldsymbol{p}_{i^{*}}+\eta \cdot\left(\boldsymbol{x}_{j}-\boldsymbol{p}_{i^{*}}\right)-\boldsymbol{x}_{j}\right\|_{2} \\ &=(1-\eta) \cdot\left\|\boldsymbol{p}_{i^{*}}-\boldsymbol{x}_{j}\right\|_{2} \end{aligned}\tag{9.26}
    $$
    - 令学习率 $\eta\in(0,1)$ ，则原型向量 $\boldsymbol{p}_{i^{*}}$ 在更新为 $\boldsymbol{p}^{\prime}$ 之后将更接近 $\boldsymbol{x}_{j}$ 。
  - 若 $\boldsymbol{p}_{i^{*}}$ 与 $\boldsymbol{x}_{j}$ 的类别标记不同
    - 则更新后的圆形向量与 $\boldsymbol{x}_{j}$ 之间的距离将增大为 $(1+\eta) \cdot\left\|\boldsymbol{p}_{i^{*}}-\boldsymbol{x}_{j}\right\|_{2}$，从而远离 $\boldsymbol{x}_{j}$ 。

- 在学得一组原型向量 $\left\{\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \ldots, \boldsymbol{p}_{q}\right\}$ 后，即可实现对样本空间 $\mathcal{X}$ 的簇划分。
  - 对任意样本 $\boldsymbol{x}$ ，它将被划入与其距离最近的原型向量所代表的簇中；
  - 换言之，每个原型向量 $\boldsymbol{p}_{i}$ 定义了与之相关的一个区域 $R_{i}$ ，该区域中每个样本与 $\boldsymbol{p}_{i}$ 的距离不大于它与其他原型向量 $\boldsymbol{p}_{i^{\prime}}\left(i^{\prime} \neq i\right)$ 的距离，即

    $$
    R_{i}=\left\{\boldsymbol{x} \in \mathcal{X} |\left\|\boldsymbol{x}-\boldsymbol{p}_{i}\right\|_{2} \leqslant\left\|\boldsymbol{x}-\boldsymbol{p}_{i^{\prime}}\right\|_{2}, i^{\prime} \neq i\right\}\tag{9.27}
    $$
  - 由此形成了对样本空间 $\mathcal{X}$ 的簇划分 $\left\{R_{1}, R_{2}, \ldots, R_{q}\right\}$。
  - 该划分通常称为 “Voronoi 剖分” (Voronoi tessellation).

**举例：**


样本：


| 编号	| 密度	 | 含糖率	| 编号	| 密度	| 含糖率 |	编号 |	密度	| 含糖率 |
| -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 1	    | 0.697	 | 0.460	| 11	| 0.245	| 0.057	| 21 |	0.748 |	0.232 |
| 2	    | 0.774	 | 0.376	| 12	| 0.343	| 0.099	| 22 |	0.714 |	0.346 |
| 3     | 0.634	 | 0.264	| 13	| 0.639	| 0.161	| 23 |	0.483 |	0.312 |
| 4	    | 0.608	 | 0.318	| 14	| 0.657	| 0.198	| 24 |	0.478 |	0.437 |
| 5	    | 0.556	 | 0.215	| 15	| 0.360	| 0.370	| 25 |	0.525 |	0.369 |
| 6	    | 0.403	 | 0.237	| 16	| 0.593	| 0.042	| 26 |	0.751 |	0.489 |
| 7	    | 0.481	 | 0.149	| 17	| 0.719	| 0.103	| 27 |	0.532 |	0.472 |
| 8	    | 0.437	 | 0.211	| 18	| 0.359	| 0.188	| 28 |	0.473 |	0.376 |
| 9	    | 0.666	 | 0.091	| 19	| 0.339	| 0.241	| 29 |	0.725 |	0.445 |
| 10	| 0.243	 | 0.267	| 20	| 0.282	| 0.257	| 30 |	0.446 |	0.459 |

过程：

- 令 21 号样本的类别标记为 $c_2$ ，其他样本的类别标记为 $c_1$ 。
- 假定 q=5，即：
  - 学习目标是找到 5 个原型向量 $\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \boldsymbol{p}_{3}, \boldsymbol{p}_{4}, \boldsymbol{p}_{5}$
  - 并假定其对应的类别标记分别为 $c_1$,$c_2$,$c_2$,$c_1$,$c_1$。
- 算法开始时，根据样本的类别标记和簇的预设类别标记对原型向量进行随机初始化，
- 假定初始化为样本 $\boldsymbol{x}_{5}, \boldsymbol{x}_{12}, \boldsymbol{x}_{18}, \boldsymbol{x}_{23}, \boldsymbol{x}_{29}$。
- 在第一轮迭代中，
  - 假定随机选取的样本为 $\boldsymbol{x}_{1}$
  - 该样本与当前原型向量 $\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \boldsymbol{p}_{3}, \boldsymbol{p}_{4}, \boldsymbol{p}_{5}$ 的距离分别为 $0.283,0.506,0.434,0.260,0.032$。
  - 由于 $\boldsymbol{p}_{5}$ 与 $\boldsymbol{x}_{1}$ 距离最近且两者具有相同的类别标记 $c_2$ ，假定学习率 $\eta=0.1$
  - 则 LVQ 更新 $\boldsymbol{p}_{5}$ 得到新原型向量

    $$
    \begin{aligned} \boldsymbol{p}^{\prime} &=\boldsymbol{p}_{5}+\eta \cdot\left(\boldsymbol{x}_{1}-\boldsymbol{p}_{5}\right) \\ &=(0.725 ; 0.445)+0.1 \cdot((0.697 ; 0.460)-(0.725 ; 0.445)) \\ &=(0.722 ; 0.442) \end{aligned}
    $$
  - 将 $p_5$ 更新为 $p'$ 后，不断重复上述过程。
- 不同轮数之后的聚类结果如图所示：
  - $c1$, $c2$ 类样本点与原型向量分别用 $\cdot$，$。$ 与$+$表示，
  - 红色虚线显示出聚类形成的Voronoi剖分

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200613/bWUavjHLufKL.png?imageslim">
</p>



## 高斯混合聚类


高斯混合聚类 Mixture-of-Gaussian


- 与 k 均值、LVQ用原型向量来刻画聚类结构不同，高斯混合聚类采用概率模型来表达聚类原型。


回顾多元高斯分布的定义：

- 对 $n$ 维样本空间 $\mathcal{X}$ 中的随机 向量 $\boldsymbol{x}$ ，
- 若 $\boldsymbol{x}$ 服从高斯分布，则其概率密度函数为：

    $$
    p(\boldsymbol{x})=\frac{1}{(2 \pi)^{\frac{n}{2}}|\boldsymbol{\Sigma}|^{\frac{1}{2}}} e^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathbf{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}\tag{9.28}
    $$

- 其中：
  - $\boldsymbol{\mu}$ 是 $n$ 维均值向量， 
  - $\boldsymbol{\Sigma}$ 是 $n\times n$ 的协方差矩阵。
    - $\mathbf{\Sigma}$: 对称正定矩阵;
    - $|\mathbf{\Sigma}|: \mathbf{\Sigma}$ 的行列式;
    - $\mathbf{\Sigma}^{-1}: \mathbf{\Sigma}$ 的逆矩阵.
- 由上式可知：高斯分布完全由均值向量 $\boldsymbol{\mu}$ 和协方差矩阵 $\boldsymbol{\Sigma}$ 这两个参数确定。
- 为了明确显示高斯分布与相应参数的依赖关系，将概率密度函数记为 $p(\boldsymbol{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma})$。

定义高斯混合分布：

$$
p_{\mathcal{M}}(\boldsymbol{x})=\sum_{i=1}^{k} \alpha_{i} \cdot p\left(\boldsymbol{x} | \boldsymbol{\mu}_{i}, \mathbf{\Sigma}_{i}\right)\tag{9.29}
$$

说明：

- 该分布共由 $k$ 个混合成分组成，每个混合成分对应一个高斯分布。
- 其中 $\boldsymbol{\mu}_{i}$ 与 $\boldsymbol{\Sigma}_{i}$ 是第 $i$ 个高斯混合成分的参数
- $\alpha_i>0$ 为相应的“混合系数” (mixture coefficient)，$\sum_{i=1}^{k}\alpha_i=1$ 。







过程：

- 假设：样本的生成过程由高斯混合分布给出：
  - 首先，根据 $\alpha_{1}, \alpha_{2}, \dots, \alpha_{k}$ 定义的先验分布选择高斯混合成分，其中 $\alpha_i$ 为选择第 i 个混合成分的概率；
  - 然后，根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。
- 若训练集 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$ 由上述过程生成，令随机变量 $z_{j} \in\{1,2, \ldots, k \}$  表示生成样本 $\boldsymbol{x}_{j}$ 的高斯混合成分，其取值未知。
- 显然，$z_j$ 的先验概率 $P(z_j = i)$ 对应于 $\alpha_i(i=1,2,\cdots ,k)$ 。
- 根据贝叶斯定理， $z_j$ 的后验分布对应于

$$
\begin{aligned} p_{\mathcal{M}}\left(z_{j}=i | \boldsymbol{x}_{j}\right) &=\frac{P\left(z_{j}=i\right) \cdot p_{\mathcal{M}}\left(\boldsymbol{x}_{j} | z_{j}=i\right)}{p_{\mathcal{M}}\left(\boldsymbol{x}_{j}\right)} \\ &=\frac{\alpha_{i} \cdot p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{i}, \mathbf{\Sigma}_{i}\right)}{\sum_{l=1}^{k} \alpha_{l} \cdot p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{l}, \boldsymbol{\Sigma}_{l}\right)} \end{aligned}\tag{9.30}
$$

- 换言之，$p_{\mathcal{M}}\left(z_{j}=i | \boldsymbol{x}_{j}\right)$ 给出了样本 $\boldsymbol{x}_{j}$ 由第 $i$ 个高斯混合成分生成的后验概率。
  - 为方便叙述，将其简记为 $\gamma_{j i}(i=1,2, \ldots, k)$。
- 当高斯混合分布(9.29)已知时，高斯混合聚类将把样本集 $D$ 划分为 $k$ 个簇 $\mathcal{C}=\left\{C_{1}, C_{2}, \ldots, C_{k}\right\}$ ，每个样本 $\boldsymbol{x}_{j}$ 的簇标记 $\lambda_j$ 如下确定：

$$
\lambda_{j}=\underset{i \in\{1,2, \ldots, k\}}{\arg \max } \gamma_{j i}\tag{9.31}
$$


- 因此，从原型聚类的角度来看，高斯混合聚类是采用概率模型(高斯分布)对原型进行刻画，簇划分则由原型对应后验概率确定。
- 那么，对于式(9.29)，模型参数 $\left\{\left(\alpha_{i}, \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right) | 1 \leqslant i \leqslant k\right\}$ 如何求解呢？
- 显然，给定样本集 $D$ ，可采用极大似然估计，即最大化(对数)似然

$$
\begin{aligned} L L(D) &=\ln \left(\prod_{j=1}^{m} p_{\mathcal{M}}\left(\boldsymbol{x}_{j}\right)\right) \\ &=\sum_{j=1}^{m} \ln \left(\sum_{i=1}^{k} \alpha_{i} \cdot p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{i}, \mathbf{\Sigma}_{i}\right)\right) \end{aligned}\tag{9.32}
$$

- 常采用 EM 算法进行迭代优化求解。

下面我们做一个简单的推导：

- 若参数 $\left\{\left(\alpha_{i}, \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right) | 1 \leqslant i \leqslant k\right\}$ 能使式(9.32)最大化。
  - 根据公式(9.28)可知：

    $$
    p(\boldsymbol{x_{j}|\boldsymbol\mu_{i},\boldsymbol\Sigma_{i}})=\frac{1}{(2\pi)^\frac{n}{2}\left| \boldsymbol\Sigma_{i}\right |^\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol{x_{j}}-\boldsymbol\mu_{i})^T\boldsymbol\Sigma_{i}^{-1}(\boldsymbol{x_{j}-\mu_{i}})}
    $$

- 则由 $\frac{\partial L L(D)}{\partial \boldsymbol{\mu}_{i}}=0$ 有
  - 展开：

    $$\begin{aligned}
    \frac {\partial LL(D)}{\partial\boldsymbol\mu_{i}}&=\frac {\partial}{\partial \boldsymbol\mu_{i}}\sum_{j=1}^mln\Bigg(\sum_{i=1}^k \alpha_{i}\cdot p(\boldsymbol{x_{j}}|\boldsymbol\mu_{i},\boldsymbol\Sigma_{i})\Bigg) \\
    &=\sum_{j=1}^m\frac{\partial}{\partial\boldsymbol\mu_{i}}ln\Bigg(\sum_{i=1}^k \alpha_{i}\cdot p(\boldsymbol{x_{j}}|\boldsymbol\mu_{i},\boldsymbol\Sigma_{i})\Bigg) \\
    &=\sum_{j=1}^m\frac{\alpha_{i}\cdot \frac{\partial}{\partial\boldsymbol{\mu_{i}}}(p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}}))}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})} \\
    &=\sum_{j=1}^m\frac{\alpha_{i}\cdot \frac{1}{(2\pi)^\frac{n}{2}\left| \boldsymbol\Sigma_{i}\right |^\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol{x_{j}}-\boldsymbol\mu_{i})^T\boldsymbol\Sigma_{i}^{-1}(\boldsymbol{x_{j}-\mu_{i}})}}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}\frac{\partial}{\partial \boldsymbol\mu_{i}}\left(-\frac{1}{2}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right) \\
    &=\sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}(-\frac{1}{2})\left(\left(\boldsymbol\Sigma_{i}^{-1}+\left(\boldsymbol\Sigma_{i}^{-1}\right)^T\right)\cdot\left(\boldsymbol{x_{j}-\mu_{i}}\right)\cdot(-1)\right) \\
    &=\sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}(-\frac{1}{2})\left(-\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)-\left(\boldsymbol\Sigma_{i}^{-1}\right)^T\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right) \\
    &=\sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}(-\frac{1}{2})\left(-\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)-\left(\boldsymbol\Sigma_{i}^T\right)^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right) \\
    &=\sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}(-\frac{1}{2})\left(-\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)-\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right) \\
    &=\sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}(-\frac{1}{2})\left(-2\cdot\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right) \\
    &=\sum_{j=1}^m \frac{\alpha_{i}\cdot p\left(\boldsymbol{x_{j}}|\boldsymbol\mu _{i},\boldsymbol\Sigma_{i}\right)}{\sum_{l=1}^k \alpha_{l}\cdot p(\boldsymbol{x_{j}}|\boldsymbol\mu_{l},\boldsymbol\Sigma_{l})}\boldsymbol\Sigma_{i}^{-1}(\boldsymbol{x_{j}-\mu_{i}}) 
    \\&=0
    \end{aligned}$$

  - 即：

    $$\frac {\partial LL(D)}{\partial \boldsymbol\mu_{i}}=\sum_{j=1}^m \frac{\alpha_{i}\cdot p\left(\boldsymbol{x_{j}}|\boldsymbol\mu _{i},\boldsymbol\Sigma_{i}\right)}{\sum_{l=1}^k \alpha_{l}\cdot p(\boldsymbol{x_{j}}|\boldsymbol\mu_{l},\boldsymbol\Sigma_{l})}\boldsymbol\Sigma_{i}^{-1}(\boldsymbol{x_{j}-\mu_{i}})=0 $$

  - 此时，左右两边同时乘上 $\boldsymbol\Sigma_{i}$ 可得：

    $$
    \sum_{j=1}^{m} \frac{\alpha_{i} \cdot p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{i}, \mathbf{\Sigma}_{i}\right)}{\sum_{l=1}^{k} \alpha_{l} \cdot p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{l}, \mathbf{\Sigma}_{l}\right)}\left(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}\right)=0\tag{9.33}
    $$

  - 由式(9.30)以及 $\gamma_{j i}=p_{\mathcal{M}}\left(z_{j}=i | \boldsymbol{x}_{j}\right)$ ， 有

    $$
    \boldsymbol{\mu}_{i}=\frac{\sum_{j=1}^{m} \gamma_{j i} \boldsymbol{x}_{j}}{\sum_{j=1}^{m} \gamma_{j i}}\tag{9.34}
    $$

  - 即各混合成分的均值可通过样本加权平均来估计，样本权重是每个样本属于该成分的后验概率。
- 类似的，由 $\frac{\partial L L(D)}{\partial \boldsymbol{\Sigma}_{i}}=0$ 有：
  - 展开

    $$\begin{aligned}
    \frac {\partial LL(D)}{\partial\boldsymbol\Sigma_{i}}&=\frac {\partial}{\partial \boldsymbol\Sigma_{i}}\sum_{j=1}^mln\Bigg(\sum_{i=1}^k \alpha_{i}\cdot p(\boldsymbol{x_{j}}|\boldsymbol\mu_{i},\boldsymbol\Sigma_{i})\Bigg) \\
    &=\sum_{j=1}^m\frac{\partial}{\partial\boldsymbol\Sigma_{i}}ln\Bigg(\sum_{i=1}^k \alpha_{i}\cdot p(\boldsymbol{x_{j}}|\boldsymbol\mu_{i},\boldsymbol\Sigma_{i})\Bigg) \\
    &=\sum_{j=1}^m \frac{\alpha_{i}\cdot \frac{\partial}{\partial\boldsymbol\Sigma_{i}}p(\boldsymbol x_{j}|\boldsymbol \mu_{i},\boldsymbol\Sigma_{i})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j},|\boldsymbol \mu_{l},\boldsymbol\Sigma_{l})} \\
    &=\sum_{j=1}^m \frac{\alpha_{i}\cdot \frac{\partial}{\partial\boldsymbol\Sigma_{i}}\frac{1}{(2\pi)^\frac{n}{2}\left| \boldsymbol\Sigma_{i}\right |^\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol{x_{j}}-\boldsymbol\mu_{i})^T\boldsymbol\Sigma_{i}^{-1}(\boldsymbol{x_{j}-\mu_{i}})}}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j},|\boldsymbol \mu_{l},\boldsymbol\Sigma_{l})}\\
    &=\sum_{j=1}^m \frac{\alpha_{i}\cdot \frac{\partial}{\partial\boldsymbol\Sigma_{i}}e^{ln\left(\frac{1}{(2\pi)^\frac{n}{2}\left| \boldsymbol\Sigma_{i}\right |^\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol{x_{j}}-\boldsymbol\mu_{i})^T\boldsymbol\Sigma_{i}^{-1}(\boldsymbol{x_{j}-\mu_{i}})}\right)}}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j},|\boldsymbol \mu_{l},\boldsymbol\Sigma_{l})} \\
    &=\sum_{j=1}^m \frac{\alpha_{i}\cdot \frac{\partial}{\partial\boldsymbol\Sigma_{i}}e^{-\frac{n}{2}ln\left(2\pi\right)-\frac{1}{2}ln\left(|\boldsymbol\Sigma_{i}|\right)-\frac{1}{2}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)}}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j},|\boldsymbol \mu_{l},\boldsymbol\Sigma_{l})} \\
    &=\sum_{j=1}^m \frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol \mu_{i},\boldsymbol\Sigma_{i})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j},|\boldsymbol \mu_{l},\boldsymbol\Sigma_{l})}\frac{\partial}{\partial\boldsymbol\Sigma_{i}}\left(-\frac{n}{2}ln\left(2\pi\right)-\frac{1}{2}ln\left(|\boldsymbol\Sigma_{i}|\right)-\frac{1}{2}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right) \\
    &=\sum_{j=1}^m \frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol \mu_{i},\boldsymbol\Sigma_{i})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j},|\boldsymbol \mu_{l},\boldsymbol\Sigma_{l})}\left(-\frac{1}{2}\left(\boldsymbol\Sigma_{i}^{-1}\right)^T-\frac{1}{2}\frac{\partial}{\partial\boldsymbol\Sigma_{i}}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right)\\&=0
    \end{aligned}$$
  - 其中：
    - 为求得

    $$
    \frac{\partial}{\partial\boldsymbol\Sigma_{i}}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)
    $$

    - 首先查看 对 $\boldsymbol \Sigma_{i}$ 中单一元素的求导
      - 用 $r$ 代表矩阵 $\boldsymbol\Sigma_{i}$ 的行索引
      - $c$ 代表矩阵 $\boldsymbol\Sigma_{i}$ 的列索引

    $$\begin{aligned}
    \frac{\partial}{\partial\Sigma_{i_{rc}}}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)&=\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\frac{\partial\boldsymbol\Sigma_{i}^{-1}}{\partial\Sigma_{i_{rc}}}\left(\boldsymbol{x_{j}-\mu_{i}}\right) \\
    &=-\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\frac{\partial\boldsymbol\Sigma_{i}}{\partial\Sigma_{i_{rc}}}\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)
    \end{aligned}$$

    - 设 $B=\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)$，则

    $$\begin{aligned}
    B^T&=\left(\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right)^T \\
    &=\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\left(\boldsymbol\Sigma_{i}^{-1}\right)^T \\
    &=\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}
    \end{aligned}$$
    - 所以

    $$\begin{aligned}
    \frac{\partial}{\partial\Sigma_{i_{rc}}}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)=-B^T\frac{\partial\boldsymbol\Sigma_{i}}{\partial\Sigma_{i_{rc}}}B\end{aligned}$$
    - 其中：
      - $B$ 为 $n\times1$ 阶矩阵
      - $\frac{\partial\boldsymbol\Sigma_{i}}{\partial\Sigma_{i_{rc}}}$ 为 $n$ 阶方阵，且 $\frac{\partial\boldsymbol\Sigma_{i}}{\partial\Sigma_{i_{rc}}}$ 仅在 $\left(r,c\right)$ 位置处的元素值为 1，其它位置处的元素值均为 $0$
    - 所以：

    $$
    \begin{aligned}
    \frac{\partial}{\partial\Sigma_{i_{rc}}}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)=&-B^T\frac{\partial\boldsymbol\Sigma_{i}}{\partial\Sigma_{i_{rc}}}B
    \\=&-B_{r}\cdot B_{c}
    \\=&-\left(B\cdot B^T\right)_{rc}
    \\=&\left(-B\cdot B^T\right)_{rc}
    \end{aligned}
    $$

    - 即对 $\boldsymbol\Sigma_{i}$ 中特定位置的元素的求导结果对应于 $\left(-B\cdot B^T\right)$ 中相同位置的元素值。
    - 所以
    $$\begin{aligned}
    \frac{\partial}{\partial\boldsymbol\Sigma_{i}}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)&=-B\cdot B^T
    \\&=-\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\left(\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right)^T
    \\&=-\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}
    \end{aligned}$$
  - 将上式带入 $\frac {\partial LL(D)}{\partial \boldsymbol\Sigma_{i}}$，得：

    $$
    \begin{aligned}
    \frac {\partial LL(D)}{\partial \boldsymbol\Sigma_{i}}=&\sum_{j=1}^m \frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol \mu_{i},\boldsymbol\Sigma_{i})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j},|\boldsymbol \mu_{l},\boldsymbol\Sigma_{l})}\left(-\frac{1}{2}\left(\boldsymbol\Sigma_{i}^{-1}\right)^T-\frac{1}{2}\frac{\partial}{\partial\boldsymbol\Sigma_{i}}\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\right)
    \\=&\sum_{j=1}^m \frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol \mu_{i},\boldsymbol\Sigma_{i})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j},|\boldsymbol \mu_{l},\boldsymbol\Sigma_{l})}\left( -\frac{1}{2}\left(\boldsymbol\Sigma_{i}^{-1}-\boldsymbol\Sigma_{i}^{-1}\left(\boldsymbol{x_{j}-\mu_{i}}\right)\left(\boldsymbol{x_{j}-\mu_{i}}\right)^T\boldsymbol\Sigma_{i}^{-1}\right)\right)=0
    \end{aligned}
    $$

  - 整理可得

    $$
    \boldsymbol{\Sigma}_{i}=\frac{\sum_{j=1}^{m} \gamma_{j i}\left(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}\right)\left(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}\right)^{\mathrm{T}}}{\sum_{j=1}^{m} \gamma_{j i}}\tag{9.35}
    $$

- 对于混合系数 $\alpha_i$ ，
  - 除了要最大化 $LL(D)$ ，还需满足 $\alpha_{i} \geqslant 0, \sum_{i=1}^{k} \alpha_{i}=1$ 。
  - 因此，考虑 $LL(D)$ 的拉格朗日形式

    $$
    L L(D)+\lambda\left(\sum_{i=1}^{k} \alpha_{i}-1\right)\tag{9.36}
    $$

  - 其中 $\lambda$ 为拉格朗日乘子。
  - 由式(9.36)对 $\alpha_i$ 的导数为 $0$，有：

    $$
    \sum_{j=1}^{m} \frac{p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{i}, \mathbf{\Sigma}_{i}\right)}{\sum_{l=1}^{k} \alpha_{l} \cdot p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{l}, \mathbf{\Sigma}_{l}\right)}+\lambda=0\tag{9.37}
    $$
  - 推导：

    $$
    \begin{aligned}
    \sum_{j=1}^m\frac{p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}+\lambda=&0
    \\ \Rightarrow\sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}+\alpha_{i}\lambda=&0
    \\ \Rightarrow\sum_{i=1}^k\left(\sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}+\alpha_{i}\lambda\right)=&0
    \\ \Rightarrow\sum_{i=1}^k\sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}+\sum_{i=1}^k\alpha_{i}\lambda=&0
    \\\Rightarrow\lambda=-\sum_{i=1}^k\sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}=&-m
    \end{aligned}
    $$

  - 说明：
    - 第一行到第二行：同时乘以 $\alpha_i$。
    - 第二行到第三行：对所有混合成分进行求和：
    - 由最后一行得：$\lambda =-m$
  - 又
    $$
    \begin{aligned}
    \sum_{j=1}^m\frac{\alpha_{i}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{i},\Sigma_{i}})}{\sum_{l=1}^k\alpha_{l}\cdot p(\boldsymbol x_{j}|\boldsymbol{\mu_{l},\Sigma_{l}})}+\alpha_{i}\lambda=&0
    \\\Rightarrow\sum_{j=1}^m\gamma_{ji}+\alpha_{i}\lambda=&0
    \\\Rightarrow\alpha_{i}=&-\frac{\sum_{j=1}^m\gamma_{ji}}{\lambda}\\=&\frac{1}{m}\sum_{j=1}^m\gamma_{ji}
    \end{aligned}
    $$
  - 即：
    $$
    \alpha_{i}=\frac{1}{m} \sum_{j=1}^{m} \gamma_{j i}\tag{9.38}
    $$
  - 即每个高斯成分的混合系数由样本属于该成分的平均后验概率确定。

由上述推导即可获得高斯混合模型的 EM 算法：

- 在每步迭代中，先根据当前参数来计算每个样本属于每个高斯成分的后验概率 $\gamma_{ji}$ (E步)
- 再根据式（9.34）、（9.35）和（9.38）更新模型参数 $\left\{\left(\alpha_{i}, \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right) | 1 \leqslant i \leqslant k\right\}$ （M步）.

高斯混合聚类算法描述如下：

- 输入: 
  - 样本集 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$
  - 高斯混合成分个数 $k$
- 过程:
  - 01: 初始化高斯混合分布的模型参数 $\left\{\left(\alpha_{i}, \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right) | 1 \leqslant i \leqslant k\right\}$
  - 02: repeat
  - 03: $\quad$ for $j=1,2, \dots, m$ do
  - 04: $\quad$ $\quad$ 根据式(9.30)计算 $\boldsymbol{x}_{j}$ 由各混合成分生成的后验概率, 即 $\gamma_{j i}=p_{\mathcal{M}}\left(z_{j}=i | \boldsymbol{x}_{j}\right)(1 \leqslant i \leqslant k)$
  - 05: $\quad$ end for
  - 06: $\quad$ for $i=1,2, \dots, k$ do
  - 07: $\quad$ $\quad$ 计算新均值向量: $\boldsymbol{\mu}_{i}^{\prime}=\frac{\sum_{j=1}^{m} \gamma_{j i} \boldsymbol{x}_{j}}{\sum_{j=1}^{m} \gamma_{j i}}$
  - 08: $\quad$ $\quad$ 计算新协方差矩阵: $\boldsymbol{\Sigma}_{i}^{\prime}=\frac{\sum_{j=1}^{m} \gamma_{j i}\left(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}^{\prime}\right)\left(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}^{\prime}\right)^{\mathrm{T}}}{\sum_{j=1}^{m} \gamma_{j i}}$
  - 09: $\quad$ $\quad$ 计算新混合系数: $\alpha_{i}^{\prime}=\frac{\sum_{j=1}^{m} \gamma_{j i}}{m}$
  - 10: $\quad$ end for
  - 11: $\quad$ 将模型参数 $\left\{\left(\alpha_{i}, \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right) | 1 \leqslant i \leqslant k\right\}$ 更新为 $\left\{\left(\alpha_{i}^{\prime}, \boldsymbol{\mu}_{i}^{\prime}, \boldsymbol{\Sigma}_{i}^{\prime}\right) | 1 \leqslant i \leqslant k\right\}$
  - 12: until 满足停止条件
  - 13: $C_{i}=\varnothing(1 \leqslant i \leqslant k)$
  - 14: for $j=1,2, \ldots, m$ do
  - 15: $\quad$ 根据式(9.31)确定 $\boldsymbol{x}_{j}$ 的族标记 $\lambda_{j}$
  - 16: $\quad$ 将 $\boldsymbol{x}_{j}$ 划入相应的族: $C_{\lambda_{j}}=C_{\lambda_{j}} \bigcup\left\{\boldsymbol{x}_{j}\right\}$
  - 17: end for 
- 输出: 
  - 族划分 $\mathcal{C}=\left\{C_{1}, C_{2}, \ldots, C_{k}\right\}$


说明：

- 算法第 1 行对高斯混合分布的模型参数进行初始化。
- 然后，在第 2-12 行基于 EM 算法对模型参数进行迭代更新. 若 EM 算法的停止条件满足（例如已达到最大迭代轮数，或似然函数 $LL(D)$ 増长很少甚至不再增长)，则在第 14-17行根据高斯混合分布确定簇划分，在第 18 行返回最终结果.

**举例：**


样本：


| 编号	| 密度	 | 含糖率	| 编号	| 密度	| 含糖率 |	编号 |	密度	| 含糖率 |
| -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 1	    | 0.697	 | 0.460	| 11	| 0.245	| 0.057	| 21 |	0.748 |	0.232 |
| 2	    | 0.774	 | 0.376	| 12	| 0.343	| 0.099	| 22 |	0.714 |	0.346 |
| 3     | 0.634	 | 0.264	| 13	| 0.639	| 0.161	| 23 |	0.483 |	0.312 |
| 4	    | 0.608	 | 0.318	| 14	| 0.657	| 0.198	| 24 |	0.478 |	0.437 |
| 5	    | 0.556	 | 0.215	| 15	| 0.360	| 0.370	| 25 |	0.525 |	0.369 |
| 6	    | 0.403	 | 0.237	| 16	| 0.593	| 0.042	| 26 |	0.751 |	0.489 |
| 7	    | 0.481	 | 0.149	| 17	| 0.719	| 0.103	| 27 |	0.532 |	0.472 |
| 8	    | 0.437	 | 0.211	| 18	| 0.359	| 0.188	| 28 |	0.473 |	0.376 |
| 9	    | 0.666	 | 0.091	| 19	| 0.339	| 0.241	| 29 |	0.725 |	0.445 |
| 10	| 0.243	 | 0.267	| 20	| 0.282	| 0.257	| 30 |	0.446 |	0.459 |

过程：

- 令高斯混合成分的个数 $k = 3$。
- 算法开始时，假定将高斯混合分布的模型参数初始化为：
  - $\alpha_{1}=\alpha_{2}=\alpha_{3}=\frac{1}{3}$；
  - $\boldsymbol{\mu}_{1}=\boldsymbol{x}_{6}$，$\boldsymbol{\mu}_{2}=\boldsymbol{x}_{22}$，$\boldsymbol{\mu}_{3}=\boldsymbol{x}_{27}$；
  - $\mathbf{\Sigma}_{1}=\mathbf{\Sigma}_{2}=\mathbf{\Sigma}_{3}=\left(\begin{array}{cc}{0.1} & {0.0} \\ {0.0} & {0.1}\end{array}\right)$。
- 在第一轮迭代中，先计算样本由各混合成分生成的后验概率。
  - 以 $\boldsymbol{x}_{1}$ 为例， 
    - 由式(9.30)算出后验概率 $\gamma_{11}= 0.219$, $\gamma_{12} = 0.404$,$\gamma_{13}=0.377$。
  - 所有样本的后验概率算完后，得到如下新的模型参数：
    - $\alpha_{1}^{\prime}=0.361, \quad \alpha_{2}^{\prime}=0.323, \alpha_{3}^{\prime}=0.316$
    - $\boldsymbol{\mu}_{1}^{\prime}=(0.491 ; 0.251), \boldsymbol{\mu}_{2}^{\prime}=(0.571 ; 0.281), \boldsymbol{\mu}_{3}^{\prime}=(0.534 ; 0.295)$
    - $\Sigma_{1}^{\prime}=\left( \begin{array}{cc}{0.025} & {0.004} \\ {0.004} & {0.016}\end{array}\right), \Sigma_{2}^{\prime}=\left( \begin{array}{cc}{0.023} & {0.004} \\ {0.004} & {0.017}\end{array}\right), \Sigma_{3}^{\prime}=\left( \begin{array}{cc}{0.024} & {0.005} \\ {0.005} & {0.016}\end{array}\right)$
- 模型参数更新后，不断重复上述过程。
- 不同轮数之后的聚类结果如图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180629/I5LKJ9kB92.png?imageslim">
</p>

