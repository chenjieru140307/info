
# 可以补充进来的

- 还是有些需要掌握的。这个高斯混合模型之前好像没有怎么使用过。

# 高斯混合模型


高斯混合模型 GMM Gaussian Mixture Model


一些特性：

- 一种常见的聚类算法
- 与 K 均值算法类似，同样使用了 EM 算法进行迭代计算。

高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的，当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。<span style="color:red;">什么意思？每个簇的数据都是符合高斯分布的？就是中心比较多，然后周边比较少是吧？</span>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190401/uOQd6z6JvWwG.png?imageslim">
</p>

图 5.6是一个数据分布的样例，如果只用一个高斯分布来拟合图中的数据，图中所示的椭圆即为高斯分布的二倍标准差所对应的椭圆。

直观来说，图中的数据明显分为两簇，因此只用一个高斯分布来拟和是不太合理的，需要推广到用多个高斯分布的叠加来对数据进行拟合。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190401/tVD60SI3n9e9.png?imageslim">
</p>

图 5.7是用两个高斯分布的叠加来拟合得到的结果。这就引出了高斯混合模型，即用多个高斯分布函数的线形组合来对数据分布进行拟合。理论上，高斯混合模型可以拟合出任意类型的分布。<span style="color:red;">嗯，好的。</span>


高斯分布，高斯混合模型，EM算法


## 高斯混合模型的核心思想是什么？它是如何迭代计算的？

说起高斯分布，大家都不陌生，通常身高、分数等都大致符合高斯分布。因此，当我们研究各类数据时，假设同一类的数据符合高斯分布，也是很简单自然的假设；当数据事实上有多个类，或者我们希望将数据划分为一些簇时，可以假设不同簇中的样本各自服从不同的高斯分布，由此得到的聚类算法称为高斯混合模型。<span style="color:red;">嗯，是的，听起来还挺在理。</span>


高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来的。在该假设下，每个单独的分模型都是标准高斯模型，其均值 $\mu_i$ 和方差 $\sum_i$ 是待估计的参数。此外，每个分模型都还有一个参数 $\pi_i$，可以理解为权重或生成数据的概率。高斯混合模型的公式为：


$$
p(x)=\sum_{i=1}^{K} \pi_{i} N\left(x | \mu_{i}, \Sigma_{i}\right)\tag{5.14}
$$


高斯混合模型是一个生成式模型。可以这样理解数据的生成过程，假设一个最简单的情况，即只有两个一维标准高斯分布的分模型 $\mathrm{N}(0,1)$ 和 $\mathrm{N}(5,1)$，其权重分别为 $0.7$ 和 $0.3$。那么，在生成第一个数据点时，先按照权重的比例，随机选择一个分布，比如选择第一个高斯分布，接着从 $\mathrm{N}(0,1)$  中生成一个点，如 $−0.5$，便是第一个数据点。在生成第二个数据点时，随机选择到第二个高斯分布 $\mathrm{N}(5,1)$，生成了第二个点 $4.7$。如此循环执行，便生成出了所有的数据点。<span style="color:red;">嗯。</span>


然而，通常我们并不能直接得到高斯混合模型的参数，而是观察到了一系列数据点，给出一个类别的数量 K 后，希望求得最佳的 K 个高斯分模型。

因此，高斯混合模型的计算，便成了最佳的均值 $\mu$ ，方差 $\Sigma$、权重 $\pi$ 的寻找，这类问题通常通过最大似然估计来求解。

遗憾的是，此问题中直接使用最大似然估计，得到的是一个复杂的非凸函数，目标函数是和的对数，难以展开和对其求偏导。

在这种情况下，可以用上一节已经介绍过的 EM 算法框架来求解该优化问题。EM算法是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一个循环。<span style="color:red;">嗯，对于 EM 算法还是要完全掌握才行。</span>

具体到高斯混合模型的求解，EM算法的迭代过程如下。

首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。

1. E 步骤。根据当前的参数，计算每个点由某个分模型生成的概率。
2. M 步骤。使用 E 步骤估计出的概率，来改进每个分模型的均值，方差和权重。

也就是说，我们并不知道最佳的 K 个高斯分布的各自 3 个参数，也不知道每个数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组更佳的高斯分布。循环往复，直到参数的不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。<span style="color:red;">嗯，一直想知道为什么这样是合理的？</span>

高斯混合模型与 K 均值算法的相同点是：

- 它们都是可用于聚类的算法
- 都需要指定 K 值
- 都是使用 EM 算法来求解
- 都往往只能收敛于局部最优。

而它相比于 K 均值算法的优点是：

- 可以给出一个样本属于某类的概率是多少。<span style="color:red;">这个挺好的，的确，有的时候是想要这个参数的。</span>
- 不仅仅可以用于聚类，还可以用于概率密度的估计；<span style="color:red;">嗯，不错</span>
- 并且可以用于生成新的样本点。<span style="color:red;">这个也不错，不错。</span>


# 原文及引用

- 《百面机器学习》
