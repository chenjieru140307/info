
# 核岭回归

上一节的表示定理(Representer Theorem)说明，任何 $L_2$ 正则化的线性模型的最优解都可以表示成数据的线性组合，也就可以引入 Kernel 了，进而更加 Powerful。现在看之前的岭回归( $L_2$ 正则化的线性回归):

$$
\min_{\mathbf w} \quad {\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\left(y_n-\mathbf w^T\mathbf z_n\right)^2
$$

显然最优解 $\mathbf w_{*} = \sum \beta_n \mathbf z_n$ ，那么完全按照上一节 KLR 的形式，引入 Kernel Function，用 $\beta$ 替换 $\mathbf w$:

$$
\begin{aligned} & \min_\beta \quad {\lambda\over N}\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(\mathbf x_n,\mathbf x_m)+{1\over N}\sum_{n=1}^N\left(y_n-\sum_{n=1}^N\beta_mK(\mathbf x_n,\mathbf x_m)\right)^2 \\   \Leftrightarrow & \min_\beta \quad {\lambda \over N } \beta^TK\beta +{1 \over N} \left( \mathbf y^T \mathbf y  - 2\beta^TK^T\mathbf y +  \beta^TK^TK\beta\right)\end{aligned}
$$

用矩阵表示一来看起来简介，求微分简单，二来矩阵计算比循环要快很多。(PS: 每一个 $\sum$ 都可以表示为矩阵的乘积，如果一眼看不出来，就展开只能一点点的凑)。

同样得到一个这个无约束的优化问题了， 可以用梯度方式来直接接线性方程(KLR则需要梯度下降来求):

$$
{\partial E_{in} \over \partial \beta} = {2 \over N}\left( \lambda K^TI\beta+K^TK\beta - K^T\mathbf y \right)
$$
(引入 I 是为了后面与 $\lambda$ 计算方便)然后令微分为 0，得到下面的结果:
$$
\beta = (\lambda I +K)^{-1}\mathbf y
$$
前面在介绍 SVM 的时候提到过核函数矩阵 K 是半正定对称方阵，因此逆矩阵一定存在，对于 $N \times N$ 的矩阵求逆矩阵复杂大是 O(N^3)，而且由于核函数矩阵大部分都不为 0，最后得到的 $\beta$ 也是非稀疏矩阵，因此在数据量比较大的时候，计算量还是蛮大的。

到此，上述的模型就称为: Kernel Ridge Regression，完整如下为:
$$
g(\mathbf x) = \sum\limits_{n=1}^{N}\beta_nK(\mathbf x_n, \mathbf x)
$$
通过引入核，可以解决非线性的回归预测问题了。下面对比带核与否的 Ridge Regression的一些区别:

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190827/0glnU0QIldUG.png?imageslim">
</p>

两种模型的优缺点与适用场合也就很明显了，只要数据量没有远大于数据维度的话，就可以用 kernel ridge regression, 不过还是有一个原则: Linear First。




# 相关

- [机器学习技法笔记(8)-SVR(支持向量回归)](https://shomy.top/2017/03/09/support-vector-regression/)
