
# 逻辑回归

逻辑回归 Logistic Regression


## 介绍


缘由：

- 对于分类任务，怎么使用线性回归模型？
- 根据广义线性模型，我们只要找一个单调可微函数，将分类任务的真实标记 $y$ 与线性回归模型的预测值联系起来，就可以使用线性回归模型了。


优点：

- 不需要太多的计算资源。
- 比较容易理解，这样有很好的可解释性。
- 直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题。
- 它不是仅预测出"类别"，而是可得到近似概率预测，这对许多需利用概率辅助决策的任务很有用。
- 此外，对率函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。


缺点：

- 容易欠拟合。
- 分类精度可能不高。

应用：

- 用于概率预测。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。
- 用于分类。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。
- 寻找危险因素。寻找某一疾病的危险因素等。
- 仅能用于线性问题。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。
- 各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。




## 二分类任务

对于二分类任务：

- 输出标记 $y \in\{0,1\}$
- 线性回归模型产生的预测值 $z=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$ 是实值

那么，进行转化，可以使用：

- 单位阶跃函数 unit-step function
    $$
    y=\left\{\begin{array}{cl}{0,} & {z<0} \\ {0.5,} & {z=0} \\ {1,} & {z>0}\end{array}\right.
    $$
  - 广义线性模型要求，联系函数 $g$ 必须是一个连续且充分平滑的函数。单位阶跃函数它并不连续，因此，我们不能直接把它用作 $g$ 。可以使用一个近似的替代函数。
- Logistic 函数
    $$
    y=\frac{1}{1+e^{-z}}
    $$


图示：

<p align="center">
    <img width="60%" height="70%" src="http://images.iterate.site/blog/image/180625/CgCam8lLEc.png?imageslim">
</p>

将线性模型带入 logistic 函数：


$$
y=\frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)}}
$$


$$
\ln \frac{y}{1-y}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b
$$

说明：

- 如果将 $y$ 视为样本 $\boldsymbol{x}$ 作为正例的可能性，那么 $1-y$ 就是其反例可能性
- 那么两者的比 $\frac{y}{1-y}$ 称为几率(odds) 。
  - 几率反映了作为正例的相对可能性
- 对几率取对数得到 $\ln \frac{y}{1-y}$，为对数几率 log odds，亦称 logit。
  - 由此我们可以把这个式子 $ln\frac{y}{1-y}=w^Tx+b$ 解读成：我们是在用线性回归模型的预测结果去逼近真实标记的对数几率。


### Logistic 回归

Logistic 回归：

$$
y=\frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)}}
$$


求解：

- $w$ 和 $b$

**过程：**

对于：

$$
y=\frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)}}
$$

转化为：

$$
$$

将 $y$ 视为类后验概率估计 $p(y=1|x)$。

则：

$$
\begin{aligned}
p(y=1 | \boldsymbol{x})=&\frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)}}
\\=&\frac{e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}
\end{aligned}
$$


$$
\begin{aligned}
p(y=0 | \boldsymbol{x})=&1-p(y=1 | \boldsymbol{x})
\\=&\frac{1}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}
\end{aligned}
$$

$$
\begin{aligned}
\ln \frac{p(y=1 | \boldsymbol{x})}{p(y=0 | \boldsymbol{x})}=&\ln \frac{y}{1-y}
\\=&\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b
\end{aligned}
$$




此时，使用极大似然法来估计 $w$ 和 $b$：（为什么是这个？）

- 给定数据集 $\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{m}$ ，"对数似然" (log likelihood) 为：

$$
\ell(\boldsymbol{w}, b)=\sum_{i=1}^{m} \ln p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)
$$



最大化"对数似然" ，即令每个样本属于其真实标记的概率越大越好。

为便于讨论，令：

- $\boldsymbol{\beta}=(\boldsymbol{w} ; b)$，$\hat{\boldsymbol{x}}=(\boldsymbol{x} ; 1)$

则 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$ 可简写为 $\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}$

- 再令 $p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})=p(y=1 | \hat{\boldsymbol{x}} ; \boldsymbol{\beta})$ ，$p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})=p(y=0 | \hat{\boldsymbol{x}} ; \boldsymbol{\beta})=1-p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})$

则上式中的似然项可重写为 ：

$$
p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)=y_{i} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)+\left(1-y_{i}\right) p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)
$$

将上式带入对数似然函数，则：



$$
\begin{aligned}
\ell(\boldsymbol{\beta})=&\sum_{i=1}^{m}\ln\left(y_ip_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})+(1-y_i)p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)
\\=&\sum_{i=1}^{m}\ln\left(\cfrac{y_ie^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}+1-y_i}{1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}}\right) 
\\=&\sum_{i=1}^{m}\left(\ln(y_ie^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}+1-y_i)-\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})\right)
\\=& 
\begin{cases}
\sum_{i=1}^{m}(-\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})),  & y_i=0 \\
\sum_{i=1}^{m}(\boldsymbol{\beta}^T\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})), & y_i=1
\end{cases}
\\=&\sum_{i=1}^{m}\left(y_i\boldsymbol{\beta}^T\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})\right)
\end{aligned}
$$




说明：

- 第一行到第二行：$p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})=\cfrac{e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}}{1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}},p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})=\cfrac{1}{1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}}$
- 第三行到第四行： $y_i$=0 或 1
- 第四行到第五行：将 $y_i$=0 或 1 两种情况可以进行合并。


由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，也即在似然函数前添加负号，得到：


$$
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(-y_{i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}+\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}}\right)\right)
$$



此时，我们要求的就是这个对数似然函数最大的时候的 $\beta$ 值。


这个式子是关于 $\beta$ 的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法 (gradient descent method) 、牛顿法 (Newton method) 等都可求得其最优解，下面使用牛顿法：

令：

$$
\boldsymbol{\beta}^{*}=\underset{\boldsymbol{\beta}}{\arg \min } \ell(\boldsymbol{\beta})
$$

对于牛顿法，其第 $t+1$ 轮迭代解的更新公式为：（没明白）


$$
\boldsymbol{\beta}^{t+1}=\boldsymbol{\beta}^{t}-\left(\frac{\partial^{2} \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^{\mathrm{T}}}\right)^{-1} \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}
$$

其中：

关于 $\boldsymbol{\beta}$  的一阶导数为：

$$
\begin{aligned}
\frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}=&-\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i}\left(y_{i}-p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)
\\=& -\sum_{i=1}^{m}\hat{\boldsymbol x}_i(y_i-\hat{y}_i) 
\\=&\sum_{i=1}^{m}\hat{\boldsymbol x}_i(\hat{y}_i-y_i) 
\\=&{\boldsymbol X^T}(\hat{\boldsymbol y}-\boldsymbol{y})
\\=&{\boldsymbol X^T}(p_1(\boldsymbol X;\beta)-\boldsymbol{y}) 
\end{aligned}
$$

说明：


- 第一行到第二行：$p_1(\hat{\boldsymbol x}_i;\beta)=\hat{y}_i$

关于 $\boldsymbol{\beta}$  的二阶导数为

$$
\frac{\partial^{2} \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^{\mathrm{T}}}=\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i} \hat{\boldsymbol{x}}_{i}^{\mathrm{T}} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\left(1-p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)
$$


疑问：


- 为什么 logistic 回归用交叉熵而不用欧氏距离损失，是因为前者是凸函数，后者不是

注：

- 经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数。

补充：

- softmax




## 逻辑回归为什么使用对数损失函数


常见的逻辑回归使用的就是对数损失函数，有很多人认为逻辑回归的损失函数式平方损失， 其实不然。

逻辑回归假设样本服从伯努利分布，进而求得满足该分布的似然函数，接着取对数求极值等。

逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看， 就是对数损失函数。

**过程：**

逻辑回归模型：

$$
P(y=1|x;\theta)=\frac{1}{1+e^{-\theta^{T}x}}
$$

假设逻辑回归模型的概率分布是伯努利分布，其概率质量函数为：


$$
P(X=n)=
\begin{cases}
1-p, n=0\\
 p,n=1
\end{cases}
$$

则其似然函数为：

$$
L(\theta)=\prod_{i=1}^{m}
P(y=1|x_i)^{y_i}P(y=0|x_i)^{1-y_i}
$$

则对数似然函数为：

$$
\begin{aligned}
\ln L(\theta)=&\sum_{i=1}^{m}[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln{P(y=0|x_i)}]
\\=&\sum_{i=1}^m[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln(1-P(y=1|x_i))]
\end{aligned}
$$

对数函数在单个数据点上的定义为：

$$
\text{cost}(y,p(y|x))=-y\ln{p(y|x)-(1-y)\ln(1-p(y|x))}
$$

则全局样本损失函数为：

$$
\text{cost}(y,p(y|x)) = -\sum_{i=1}^m[y_i\ln p(y_i|x_i)+(1-y_i)\ln(1-p(y_i|x_i))]
$$

可见：

- 对数损失函数与极大似然估计的对数似然函数本质上是相同的。所以逻辑回归直接采用对数损失函数。



## 逻辑回归与朴素贝叶斯有什么区别

逻辑回归与朴素贝叶斯区别有以下几个方面：（没有很清楚）

- 逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。
- 朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，两种概率哲学间的区别。
- 朴素贝叶斯需要条件独立假设。
- 逻辑回归需要求特征参数间是线性的。
