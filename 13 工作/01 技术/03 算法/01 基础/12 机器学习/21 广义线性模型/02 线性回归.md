
# 线性回归


## 线性回归

线性回归：

- 们的目标是建立一个系统，将向量 $\boldsymbol{x} \in \mathbb{R}^{n}$ 作为输入，预测标量 $y \in \mathbb{R}$ 作为输出。
- 线性回归的输出是其输入的线性函数。
- 令 $\hat{y}$ 表示模型预测 $y$ 应该取的值。我们定义输出为：$\hat{y}=\boldsymbol{w}^{\top} \boldsymbol{x}$。其中 $\boldsymbol{w} \in \mathbb{R}^{n}$ 是参数向量。



线性模型的优点：

- 形式简单、易于建模
- 可以在线性模型的基础上通过引入层级结构或高维映射来得到一些强大的非线性模型。
- 线性模型中的参数 $w$，实际上可以看成是对应的特征的重要程度，因此，线性模型可以具有很好的可解释性。


## 计算 $w$

任务：

- 输出一个可以从 $\mathcal{x}$ 预测 $y$ 的函数， $\hat{y}=\boldsymbol{w}^{\top} \boldsymbol{x}$。


度量输出的性能：

- 可以使用均方误差：
  - $$\mathrm{MSE}_{\mathrm{test}}=\frac{1}{m} \sum_{i}\left(\hat{\boldsymbol{y}}^{(\mathrm{test})}-\boldsymbol{y}^{(\mathrm{test})}\right)_{i}^{2}$$

设计算法：

- 可以在训练集上最小化 $\mathrm{MSE}_{\text { train }}$


计算：



$$
\begin{aligned}
\nabla_{\boldsymbol{w}} \mathrm{MSE}_{\mathrm{train}}=&0
\\\Rightarrow \nabla_{w} \frac{1}{m} \| \hat{\boldsymbol{y}}-\boldsymbol{y}\|_{2}^{2}=&0
\\\Rightarrow \frac{1}{m} \nabla_{w}\left\|\boldsymbol{X} \boldsymbol{w}-\boldsymbol{y}\right\|_{2}^{2}=&0
\\\Rightarrow \nabla_{w}\left(\boldsymbol{X} \boldsymbol{w}-\boldsymbol{y}\right)^{\top}\left(\boldsymbol{X} \boldsymbol{w}-\boldsymbol{y}\right)=&0
\\\Rightarrow \nabla_{w}\left(\boldsymbol{w}^{\top} \boldsymbol{X} ^{ \top} \boldsymbol{X} \boldsymbol{w}-2 \boldsymbol{w}^{\top} \boldsymbol{X}^{ \top} \boldsymbol{y}+\boldsymbol{y}^{ \top} \boldsymbol{y}\right)=&0
\\\Rightarrow 2 \boldsymbol{X}^{ \top} \boldsymbol{X} \boldsymbol{w}-2 \boldsymbol{X}^{ \top} \boldsymbol{y}=&0
\\\Rightarrow \boldsymbol{w}=&\left(\boldsymbol{X}^{ \top} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{ \top} \boldsymbol{y}
\end{aligned}
$$

这样，可以直接求出 $\boldsymbol{w}$。


基于均方误差最小化来进行模型求解的方法被称为 "最小二乘法"(least squre method)。



## 添加偏置 $b$

同样使用最小二乘法来对 $w$ 和 $b$ 进行估计：

$$
f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b
$$


- 为便于讨论，我们把 $w$ 和 $b$ 吸收入向量形式 $\hat{w}=(w;b)$
- 相应的，把数据集 $D$ 表示为一个 $m \times (d + 1)$ 大小的矩阵 $X$ ，其中每行对应于一个示例，该行前 $d$ 个元素对应于示例的 $d$ 个属性值，最后一个元素恒置为 1 :
  - $$\begin{aligned}  \mathbf{X}=&\left(\begin{array}{ccccc}{x_{11}} & {x_{12}} & {\dots} & {x_{1 d}} & {1} \\ {x_{21}} & {x_{22}} & {\dots} & {x_{2 d}} & {1} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\vdots} \\ {x_{m 1}} & {x_{m 2}} & {\dots} & {x_{m d}} & {1}\end{array}\right)\\=&\left(\begin{array}{cc}{\boldsymbol{x}_{1}^{\mathrm{T}}} & {1} \\ {\boldsymbol{x}_{2}^{\mathrm{T}}} & {1} \\ {\vdots} & {\vdots} \\ {\boldsymbol{x}_{m}^{\mathrm{T}}} & {1}\end{array}\right)\end{aligned}$$
- 把标记也写成向量形式 $\boldsymbol{y}=\left(y_{1} ; y_{2} ; \ldots ; y_{m}\right)$，
- 则有：

$$
\hat{\boldsymbol{w}}^{*}=\underset{\boldsymbol{\hat { w }}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})
$$

求解：

由：

$$
\begin{aligned}
E_{\hat{\boldsymbol{w}}}=&(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})
\\=& \boldsymbol{y}^T\boldsymbol{y}-\boldsymbol{y}^T\mathbf{X}\hat{\boldsymbol w}-\hat{\boldsymbol w}^T\mathbf{X}^T\boldsymbol{y}+\hat{\boldsymbol w}^T\mathbf{X}^T\mathbf{X}\hat{\boldsymbol w}
\end{aligned}
$$

求导得：

$$
\begin{aligned}
\cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol w}}=& \cfrac{\partial \boldsymbol{y}^T\boldsymbol{y}}{\partial \hat{\boldsymbol w}}-\cfrac{\partial \boldsymbol{y}^T\mathbf{X}\hat{\boldsymbol w}}{\partial \hat{\boldsymbol w}}-\cfrac{\partial \hat{\boldsymbol w}^T\mathbf{X}^T\boldsymbol{y}}{\partial \hat{\boldsymbol w}}+\cfrac{\partial \hat{\boldsymbol w}^T\mathbf{X}^T\mathbf{X}\hat{\boldsymbol w}}{\partial \hat{\boldsymbol w}}
\\=& 0-\mathbf{X}^T\boldsymbol{y}-\mathbf{X}^T\boldsymbol{y}+(\mathbf{X}^T\mathbf{X}+\mathbf{X}^T\mathbf{X})\hat{\boldsymbol w}
\\=& 2\mathbf{X}^T(\mathbf{X}\hat{\boldsymbol w}-\boldsymbol{y})
\end{aligned}
$$


令上式为零可得 $\hat{\boldsymbol{w}}$ 最优解的闭式解。

由于涉及矩阵逆的计算，因此进行讨论：

- $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 为满秩矩阵时：
  - 令上式为零，得：$\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$
    - 其中：$\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1}$ 是矩阵 $\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)$ 的逆矩阵
  - 令 $\hat{\boldsymbol{x}}_{i}=\left(\boldsymbol{x}_{i}, 1\right)$ ，则最终学得的多元线性回归模型为 $f\left(\hat{\boldsymbol{x}}_{i}\right)=\hat{\boldsymbol{x}}_{i}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$
- $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 为不满秩矩阵时：
  - 此时可解出多个 $\hat{w}$，它们都能使均方误差最小化。此时，选择哪一个解作为输出 ，将由学习算法的归纳偏好决定，常见的做法是引入正则化 (regularization) 项。
  - 现实任务中 $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 往往不是满秩矩阵。例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致 $\mathbf{X}$ 的列数多于行数 ，$\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 显然不满秩。





## 正则化项 L1 范数 L2 范数

出现缘由：

- 为了解决两个问题：
  - 线性回归出现的过拟合
  - 在 $\boldsymbol{w}=\left(\boldsymbol{X}^{ \top} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{ \top} \boldsymbol{y}$ 时 $(\boldsymbol{X}^{ \top} \boldsymbol{X})$ 不可逆



添加正则化后的一般形式：


$$
\min _{f \in F}\left[\frac{1}{m} \sum_{i=1}^{m} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)\right]
$$

说明：

- $\lambda \geq 0$ 是用来平衡正则化项和经验风险的系数。


正则化项：

- 可以是模型参数向量的范数
  - 经常用的有 $L_{1}$ 范数，$L_{2}$ 范数
    - $L_{1}$ 范数：$\|x\|_{1}=\sum_{i=1}^{m} | x_{i} |$，
    - $L_{2}$ 范数:$\|x\|_{2}=\sqrt{\sum_{i=1}^{m} x_{i}^{2}}$


考虑线性回归模型：


- 给定数据集 $D=\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{m}$，其中，$x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i d}\right), y_{i} \in R$。
- 代价函数为：

$$
J(w)=\frac{1}{m}\left\|y-w^{T} X\right\|^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}
$$


添加范数：

- 添加 L2 范数（Ridge Regression，岭回归）

$$
J(w)=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\|w\|_{2}^{2}(\lambda>0)
$$

- 添加 L1 范数（LASSO，Least Absoulute Shrinkage and Selection Operator，最小绝对收缩选择算子）


$$
J(w)=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\|w\|_{1}(\lambda>0)
$$

- 正则项结合（Elastic Net）


$$
J(w)=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\left(\rho\|w\|_{1}+(1-\rho)\|w\|_{2}^{2}\right)
$$


说明：

- $L_{1}$ 范数正则化、$L_{2}$ 范数正则化都有助于降低过拟合风险
- $L_{2}$ 范数通过对参数向量各元素平方和求平方根，使得 $L_{2}$ 范数最小，从而使得参数的各个元素接近 0 ，但不等于 0。 
- 而 $L_{1}$ 范数正则化比 $L_2$ 范数更易获得“稀疏”解，即 $L_{1}$ 范数正则化求得的会有更多的零分量，所以 $L_{1}$ 范数可用于特征选择，而 $L_{2}$ 范数在参数规则化时经常用到（事实上，$L_{0}$ 范数得到的“稀疏”解最多，但 $L_{0}$ 范数 $\|x\|=\#\left(i | x_{i} \neq 0\right)$ 是 $x$ 中非零元素的个数，不连续，难以优化求解。因此常用 $L_{1}$ 范数来近似代替）。此时，由于大部分参数为 0，计算量上，Lasso 回归远小于岭回归。
- $\lambda$ 称为正则化参数，如果 $\lambda$ 选取过大，会把所有参数 $\theta$ 均最小化，造成欠拟合，如果 $\lambda$ 选取过小，会导致对过拟合问题解决不当，因此 $\lambda$ 的选取是一个技术活。

### 岭回归求解

岭回归代价函数为：

$$
J(w)=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\|w\|_{2}^{2}(\lambda>0)
$$

**如果使用梯度下降法进行迭代求解：**


$$
\frac{\partial J(w)}{\partial w_{j}}=\frac{1}{m} \sum_{i=1}^{m}\left(w^{T} x_{i}-y_{i}\right) x_{i j}+2 \lambda w_{j}
$$

则：

$$
\begin{aligned}
w_{j+1}=&w_{j}-\frac{\alpha}{m} \sum_{i=1}^{m}\left(w^{T} x_{i}-y_{i}\right) x_{i j}-2 \lambda w_{j} 
\\ =&(1-2 \lambda) w_{j}-\frac{\alpha}{m} \sum_{i=1}^{m}\left(w^{T} x_{i}-y_{i}\right) x_{i j}
\end{aligned}
$$

**如果直接求最优解：**

则最优解为：（过程补上）

$$
w^{*}=\left(X^{T} X+\lambda I\right)^{-1} X^{T} y
$$





### LASSO 回归求解


方法：

- 由于 $L_{1}$ 范数用的是绝对值，导致 LASSO 的优化目标不是连续可导的，也就是说，最小二乘法，梯度下降法，牛顿法，拟牛顿法都不能用。
- $L_{1}$ 正则化问题求解可采用近端梯度下降法（Proximal Gradient Descent，PGD）。


准备：


- 利普希茨连续条件 L-Lipschitz：
  - 对于函数 $f(x)$，若其任意定义域中的 $x_{1}, x_{2}$ 都存在 $L>0$，使得 $\left|f\left(x_{1}\right)-f\left(x_{2}\right)\right| \leq L\left|x_{1}-x_{2}\right|$，即对于 $f(x)$ 上每对点，连接它们的线的斜率的绝对值总是不大于这个实数 $L$。
- soft thresholding 软阈值函数
  - 对于优化问题：$\underset{x}{\arg \min }\left[\|x-z\|_{2}^{2}+\lambda\|x\|_{1}\right]$
  - 其解为：$\operatorname{prox}_{u}(z)=\operatorname{sign}(z) \max \{|z|-u, 0\}$


**过程：**

代价函数为：

$$
J(w)=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\|w\|_{1}(\lambda>0)
$$

将其写为：

$$
\min _{x}\left[f(x)+\lambda\|x\|_{1}\right]
$$

此时：

- 假设 $f(x)$ 可导，梯度 $\nabla f(x)$ 满足 L-Lipschitz条件（利普希茨连续条件）。即存在常数 $L>0$，使得：

$$
\frac{\left\|\nabla f\left(x^{\prime}\right)-\nabla f(x)\right\|_{2}^{2}}{\left\|x^{\prime}-x\right\|_{2}^{2}} \leq L, \forall\left(x, x^{\prime}\right)
$$


因此：

$$
\begin{aligned}
f(x)=&f\left(x_{k}\right)+\nabla f\left(x_{k}\right)\left(x-x_{k}\right)+\frac{f^{\prime \prime}\left(x_{k}+\xi\right)}{2}\left(x-x_{k}\right)^{2}
\\ \approx & f\left(x_{k}\right)+\nabla f\left(x_{k}\right)\left(x-x_{k}\right)+\frac{L}{2}\left(x-x_{k}\right)^{2}
\\ =&\frac{L}{2}\left[\left(x-x_{k}\right)^{2}+\frac{2}{L} \nabla f\left(x_{k}\right)\left(x-x_{k}\right)+\frac{1}{L^{2}}\left(\nabla f\left(x_{k}\right)\right)^{2}\right]-\frac{L}{2} \frac{1}{L^{2}}\left(\nabla f\left(x_{k}\right)\right)^{2}+f\left(x_{k}\right) 
\\ =&\frac{L}{2}\left[x-\left(x_{k}-\frac{1}{L} \nabla f\left(x_{k}\right)\right)\right]^{2}+\varphi\left(x_{k}\right) 
\\ =&\frac{L}{2}\left\|x-\left(x_{k}-\frac{1}{L} \nabla f\left(x_{k}\right)\right)\right\|_{2}^{2}+\varphi\left(x_{k}\right)
\end{aligned}
$$

说明：

- 第一行：在 $x_{k}$ 处将 $f(x)$ 进行二阶泰勒展开。
- 第一行到第二行：将泰勒将展开式中的二阶导用 L-Lipschitz 条件中的 L代替。
- 第三行到第四行：$-\frac{L}{2} \frac{1}{L^{2}}\left(\nabla f\left(x_{k}\right)\right)^{2}+f\left(x_{k}\right)$ 是 $x$ 无关的常数，用 $\varphi\left(x_{k}\right)$ 代替。

此时：

- 若通过梯度下降法对 $f(x)$（ $f(x)$ 连续可导）进行最小化，则每一步下降迭代实际上等价于最小化二次函数 $\widehat{f}(x)$
- 推广到 $\min _{x}\left[f(x)+\lambda\|x\|_{1}\right]$，可得到每一步迭代公式：

$$
x_{k+1}=\underset{x}{\arg \min }\left[\frac{L}{2}\left\|x-\left(x_{k}-\frac{1}{L} \nabla f\left(x_{k}\right)\right)\right\|_{2}^{2}+\lambda\|x\|_{1}\right]
$$

令 $z=x_{k}-\frac{1}{L} \nabla f\left(x_{k}\right)$，则，可以先求 $z$，再求解：

$$
\begin{aligned}
x_{k+1}=&\underset{x}{\arg \min }\left[\frac{L}{2}\|x-z\|_{2}^{2}+\lambda\|x\|_{1}\right]
\\ =&\underset{x}{\arg \min }\left(\frac{L}{2}\left\|x^{1}-z^{1}\right\|_{2}^{2}+\lambda\left\|x^{1}\right\|_{1}\right)+\underset{x}{\arg \min }\left(\frac{L}{2}\left\|x^{2}-z^{2}\right\|_{2}^{2}+\lambda\left\|x^{2}\right\|_{1}\right)+\cdots 
\\ &+\underset{x}{\arg \min }\left(\frac{L}{2}\left\|x^{d}-z^{d}\right\|_{2}^{2}+\lambda\left\|x^{d}\right\|_{1}\right)
\end{aligned}
$$

说明：

- 第一行到第二行：$x^{i}$ 为 $x$ 的第 $i$ 个分量，将第一行按分量展开。其中不存在 $x^{i} x^{j}(i \neq j)$ 这样的项，即 $x$ 的各分量之间互不影响，所以 $f(x)$ 有闭式解。（没明白）

此时：

- 优化问题变为求解 $d$ 个独立的函数：

$$f(x)=(x-z)^{2}+\lambda\|x\|_{1}$$


对于上述优化问题，使用 soft thresholding 软阈值函数，得到闭式解为：（没有很清楚）

$$
x_{k+1}^{i}=\left\{\begin{array}{ccc}{z^{i}-\frac{2 \lambda}{L}} & {,} & {z^{i}>\frac{2 \lambda}{L}} \\ {0} & {,} & {-\frac{2 \lambda}{L}<z^{i}<\frac{2 \lambda}{L}} \\ {z^{i}+\frac{2 \lambda}{L}} & {,} & {z^{i}<-\frac{2 \lambda}{L}}\end{array}\right\}
$$

说明：

- $x_{k+1}^{i}$ 与 $z^{i}$ 分别是 $x_{k+1}$ 与 $z$ 的第 $i$ 个分量。

（这里补充完整，把 $w$ 的也补充进来。）

因此，通过 PGD 能使 LASSO 和其他基于 $L_{1}$ 范数最小化的方法得以快速求解。





## 从线性回归到广义线性模型


对数线性回归：log-linear regression

对数线性回归：

- 对于线性模型：$y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$
- 如果，样本的目标 $y$ 实际上是在指数尺度上变化的，那么，我们是不是可以将输出标记的对数作为线性模型逼近的目标？
- 即：$\ln y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$

说明：

- 它实际上是在试图让 $e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}$ 逼近 $y$。
- 实际上，上面这个式子虽然在形式上仍是线性回归，但实质上已经是在求取输入空间到输出空间的非线性函数映射了


广义线性模型：generalized linear model GLM

- 单调可微函数 $g(\cdot )$， $g(\cdot )$ 连续且充分光滑。
令：$y=g^{-1}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)$
  - 其中 函数 $g(\cdot )$ 称为联系函数。

求解：

- 广义线性模型的参数估计常通过加权最小二乘法或极大似然法进行。（什么是加权最小二乘法？到底要怎么计算广义线性模型的参数估计？）

疑问：

- 联系函数我们在一开始的时候怎么知道呢？

