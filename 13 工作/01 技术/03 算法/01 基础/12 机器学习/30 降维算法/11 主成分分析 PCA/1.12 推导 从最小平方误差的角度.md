
# 可以补充进来的

- 自己重新推导下。公式自己重新写下。


# PCA 从最小平方误差的角度来推导


- 在高维空间中，我们实际上是要找到一个 $d$ 维超平面，使得数据点到这个超平面的距离平方和最小。
  - 以 $d=1$ 为例，超平面退化为直线，即把样本点投影到最佳直线，最小化的就是所有点到直线的距离平方之和，如图所示：

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190331/iXYqzDMIX0gc.png?imageslim">
    </p>

过程：

- 数据集中每个点 $x_k$ 到 $d$ 维超平面 $D$ 的距离为：

$$
\operatorname{distance}\left(\boldsymbol{x}_{k}, \boldsymbol{D}\right)=\left\|\boldsymbol{x}_{k}-\tilde{\boldsymbol{x}}_{k}\right\|_{2}\tag{4.6}
$$

- 其中
  - $\widetilde{x}_k$ 表示 $x_k$ 在超平面 D 上的投影向量。
- 如果该超平面由 $d$ 个标准正交基 $W=\{\omega_1,\omega_2,...,\omega_d\}$ 构成，根据线性代数理论 $\widetilde{x_k}$ 可以由这组基线性表示，

$$
\widetilde{\boldsymbol{x}}_{k}=\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\tag{4.7}
$$

- 其中
  - $\omega_i^Tx_k$ 表示 $x_k$ 在 $\omega_i$ 方向上投影的长度。
- 因此，实际上 $\widetilde{x_k}$ 就是 $x_k$ 在 $W$ 这组标准正交基下的坐标。而 PCA 要优化的目标为：
$$
\left\{\begin{matrix}
\underset{\omega_{1}, \ldots, \omega_{d}}{\arg \min } \sum_{k=1}^{n}\left\|\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right\|_{2}^{2}\\
s.t.\quad \underset{\forall i,j}{\omega_{i}^{\mathrm{T}} \omega_{j}}=\delta_{i j}=\left\{\begin{array}{l}{1, i=j} \\ {0, i \neq j}\end{array}\right.
\end{matrix}\right. \tag{4.8}
$$



由向量内积的性质，我们知道 $x_k^T\widetilde{x}_k=\widetilde{x}_k^Tx_k$，于是将式（4.8）中的每一个距离展开：

$$
\begin{aligned}\left\|\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right\|_{2}^{2} &=\left(\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right)^{\mathrm{T}}\left(\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}_{k}}\right) \\ &=\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}-\boldsymbol{x}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}_{k}}-\widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}+\widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}}_{k} \\ &=\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}-2 \boldsymbol{x}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}_{k}}+\widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}}_{k} \end{aligned}\tag{4.9}
$$


其中第一项 $x_k^Tx_k$ 与选取的 $W$ 无关，是个常数。将式（4.7）代入式（4.9）的第二项和第三项可得到，

$$
\begin{aligned}\boldsymbol{x}_{k}^{\mathbf{T}} \widetilde{\boldsymbol { x }}_{k} &=\boldsymbol{x}_{k}^{\mathrm{T}} \sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i} \\& =\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i} \\&=\sum_{i=1}^{d} \boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i}\end{aligned}\tag{4.10}
$$

$$
\begin{aligned}\widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}_{k}}&=\left(\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\right)^{\mathrm{T}}\left(\sum_{j=1}^{d}\left(\boldsymbol{\omega}_{j}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{j}\right) \\& =\sum_{i=1}^{d} \sum_{j=1}^{d}\left(\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\right)^{\mathrm{T}}\left(\left(\boldsymbol{\omega}_{j}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{j}\right)\end{aligned}\tag{4.11}
$$



注意到，其中 $\omega_i^Tx_k$ 和 $\omega_j^Tx_k$ 表示投影长度，都是数字。且当 $i\neq j$ 时，$\omega_i^T\omega_j=0$ ，因此式（4.11）的交叉项中只剩下 d 项：

$$
\begin{aligned} \widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}}_{k} &=\sum_{i=1}^{d}\left(\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\right)^{\mathrm{T}}\left(\left(\omega_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\right)\\&=\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right)\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \\ &=\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right)\left(\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i}\right)\\&=\sum_{i=1}^{d} \boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i} \end{aligned}\tag{4.12}
$$



注意到， $\sum_{i=1}^{d} \omega_{i}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i}$ 实际上就是矩阵 $\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}$ 的迹（对角线元素之和），于是可以将式（4.9）继续化简：

$$
\begin{aligned}\left\|\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right\|_{2}^{2}&=-\sum_{i=1}^{d} \boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i}+\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k} \\ &=-\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}\right)+\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}\end{aligned}\tag{4.13}
$$


因此式（4.8）可以写成：

$$
\begin{aligned}\arg \min _{W} \sum_{k=1}^{n}\left\|\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right\|_{2}^{2}&=\sum_{k=1}^{n}\left(-\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}\right)+\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \\ &=-\sum_{k=1}^{n} \operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}\right)+C\end{aligned}\tag{4.14}
$$



根据矩阵乘法的性质 $\sum_{k} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}}=\boldsymbol{X} \boldsymbol{X}^{\mathbf{T}}$，因此优化问题可以转化为 $\arg \max _{W} \sum_{k=1}^{n} \operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}\right)$，这等价于求解带约束的优化问题：


$$
\left\{\begin{array}{c}{\arg \max _{W} \operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{X} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{W}\right)} \\ {\text { s.t. } \boldsymbol{W}^{\mathrm{T}} \boldsymbol{W}=I}\end{array}\right.\tag{4.15}
$$

如果我们对 W 中的 d 个基 $\boldsymbol{\omega}_{1}, \boldsymbol{\omega}_{2}, \ldots, \boldsymbol{\omega}_{d}$ 依次求解，就会发现和最大方差理论的方法完全等价。比如当 d=1时，我们实际求解的问题是：

$$
\left\{\begin{array}{c}{\arg \max _{\omega} \boldsymbol{\omega}^{\mathrm{T}} \boldsymbol{X} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{\omega}} \\ {\text {s.t.} \quad \boldsymbol{\omega}^{\mathrm{T}} \boldsymbol{\omega}=1}\end{array}\right.\tag{4.16}
$$

最佳直线 $\omega$ 与最大方差法求解的最佳投影方向一致，即协方差矩阵的最大特征值所对应的特征向量，差别仅是协方差矩阵 $\sum$ 的一个倍数，以及常数 $\sum_{k=1}^{n}x_k^Tx_k$ 偏差，但这并不影响我们对最大值的优化。


## 总结

至此，我们从最小平方误差的角度解释了 PCA 的原理、目标函数和求解方法。不难发现，这与最大方差角度殊途同归，从不同的目标函数出发，得到了相同的求解方法。


# 相关

- 《百面机器学习》

