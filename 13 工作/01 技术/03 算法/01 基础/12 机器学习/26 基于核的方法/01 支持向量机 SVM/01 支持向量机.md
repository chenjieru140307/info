
# 支持向量机


支持向量机 Support Vector Machine，SVM


介绍：

- 是众多监督学习方法中十分出色的一种。
- 与逻辑回归比较：
  - 类似于逻辑回归，这个模型也是基于线性函数 $\boldsymbol{w}^{\top} \boldsymbol{x}+b$ 的。
  - 不同于逻辑回归的是，支持向量机不输出概率，只输出类别。当 $\boldsymbol{w}^{\top} \boldsymbol{x}+b$ 为正时，支持向量机预测属于正类。类似地，当 $\boldsymbol{w}^{\top} \boldsymbol{x}+b$ 为负时，支持向量机预测属于负类。



缘由：

- 想用一个超平面来划分样本集
  - 给定的训练样本集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$，$y_{i} \in\{-1,+1\}$
  - 分类学习最基本的想法就是基于训练集 $D$ 在样本空间中找到一个划分超平面，将不同类别的样本分开。
- 但是，实际上能够将训练样本分开的划分超平面可能有很多，如下图所示，我们选择哪一个最好呢？
  - 直观上看，我们应该去找位于两类训练样本的所谓 “正中间” 的划分超平面，因为这样的超平面看起来对训练样本局部扰动的 “容忍” 性最好。也就是说，这样的分类结果应该是最鲁棒的，泛化能力最强的。
  - OK，到这里，我们就想问了，什么是所谓的 “正中间” ？怎么衡量？怎么求出这个 “正中间”？


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200612/Nc1qVgGalo2t.png?imageslim">
</p>






## 划分超平面的表示

先对于这个划分超平面进行描述：

- $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b=0$
- 其中:
  - $\boldsymbol{w}=\left(w_{1} ; w_{2} ; \ldots ; w_{d}\right)$ 为法向量，决定了超平面的方向。
  - $b$ 为位移项，决定了超平面与原点之间的距离。

此时：

- 由于一个划分超平面可以被法向量 $w$ 和位移 $b$ 所确定，我们可以把这个超平面记为 $(w,b)$ 。
- 样本空间中任意点 $\boldsymbol{x}$ 到超平面 $(\boldsymbol{w}, b)$ 的距离表示为：

$$
r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{\|\boldsymbol{w}\|}\tag{6.2}
$$



## 支持向量与间隔

假设超平面 $\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}+b^{\prime}=0$ 能将训练样本正确分类，此时，对于 $\left(\boldsymbol{x}_{i}, y_{i}\right) \in D$，有：

$$
\begin{aligned}
&\left\{\begin{array}{ll}{\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+b^{\prime}>0,} & {y_{i}=+1} \\ {\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+b^{\prime}<0,} & {y_{i}=-1}\end{array}\right.
\\\Rightarrow &\left\{\begin{array}{ll}{\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+b^{\prime} \geq+\zeta,} & {y_{i}=+1} \\ {\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+b^{\prime} \leq-\zeta,} & {y_{i}=-1}\end{array}\right.
\\\Rightarrow &\left\{\begin{array}{ll}{\left(\frac{1}{\zeta} \boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+\frac{b^{\prime}}{\zeta} \geq+1,} & {y_{i}=+1} \\ {\left(\frac{1}{\zeta} \boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+\frac{b^{\prime}}{\zeta} \leq-1,} & {y_{i}=-1}\end{array}\right.
\\\Rightarrow &\left\{\begin{array}{ll}{\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \geq+1,} & {y_{i}=+1} \\ {\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \leq-1,} & {y_{i}=-1}\end{array}\right.
\end{aligned}
$$

说明：

- 第一行到第二行：根据几何间隔，进行修正，其中 $\zeta$ 为某个大于零的常数
- 第二行到第三行：两边同除以 $\zeta$，再次修正。
- 第三行到第四行：令：$\boldsymbol{w}=\frac{1}{\zeta} \boldsymbol{w}^{\prime}, b=\frac{b^{\prime}}{\zeta}$。

此时：

- 支持向量：
  - 对于 $\left\{\begin{array}{ll}{\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \geq+1,} & {y_{i}=+1} \\ {\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \leq-1,} & {y_{i}=-1}\end{array}\right.$，距离超平面最近的这几个训练样本点使得等号成立。
  - 由于每个样本点对应一个特征向量，因此，这几个使得取等号的样本点就称为支持向量 (support vector)。
- 间隔 margin
  - 我们把两个异类支持向量到超平面的距离的和 $\gamma=\frac{2}{\|\boldsymbol{w}\|}$ 定义为间隔(margin)。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180627/h9jKJD4J15.png?imageslim">
</p>

说明：

- 这三个被画了圆圈的点即为支持向量。
- 间隔为：$\gamma=\frac{2}{\|\boldsymbol{w}\|}$




## 最大间隔的超平面


具有最大间隔的划分超平面：maximum margin：

- 我们想找到一个具有最大间隔 (maximum margin) 的划分超平面。
- 也就是：

    $$
    \begin{array}{l}{\max _{\boldsymbol{w}, b} \frac{2}{\|\boldsymbol{w}\|}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.5}
    $$
  - 即：在满足 $y_i(w^Tx_i+b)\geqslant 1$ 的条件下，使得 $\gamma$ 最大的 $w$ 和 $b$ 。
- 由于仅需要最大化 $\|\boldsymbol{w}\|^{-1}$，这等价于最小化 $\|\boldsymbol{w}\|^{2}$，于是，上式可写为：

    $$
    \begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.6}
    $$
- 这个，就是支持向量机 (Support Vector Machine，简称 SVM ) 的基本型。
- 注意：上面这个式子看起来这个最小仅与 $w$ 有关, 但是事实上 $b$ 通过约束隐式地影响着 $w$ 的取值，进而对间隔产生影响。


## 求解 使用拉格朗日乘子法求解对偶问题


SVM 基本型：

$$
\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} 
\\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.6}
$$


求解：


- 注意到，上面这个式子本身是一个凸二次规划 (convex quadratic programming) 问题，是能够直接用现成的优化计算包求解的。
- 但是，我们可以有更高效的办法。可以使用拉格朗日乘子法来求解它的对偶问题(dual problem)。
- 构建拉格朗日函数：
  - 即对上面的式子的每个约束条件添加拉格朗日乘子 $\alpha_{i} \geqslant 0$ 。
  - $\boldsymbol{\alpha}=\left(\alpha_{1} ; \alpha_{2} ; \ldots ; \alpha_{m}\right)$。

$$
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)\tag{6.8}
$$

- 展开：

$$\begin{aligned}
L(\boldsymbol{w},b,\boldsymbol{\alpha}) &= \frac{1}{2}||\boldsymbol{w}||^2+\sum_{i=1}^m\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)) \\
& =  \frac{1}{2}||\boldsymbol{w}||^2+\sum_{i=1}^m(\alpha_i-\alpha_iy_i \boldsymbol{w}^T\boldsymbol{x}_i-\alpha_iy_ib)\\
& =\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i=1}^m\alpha_i -\sum_{i=1}^m\alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}_i-\sum_{i=1}^m\alpha_iy_ib
\end{aligned}​$$

- 令 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})$ 对 $\boldsymbol{w}$ 和 $b$ 求偏导为 0：

$$
\begin{aligned}
\frac {\partial L}{\partial \boldsymbol{w}}=&\frac{1}{2}\times2\times\boldsymbol{w} + 0 - \sum_{i=1}^{m}\alpha_iy_i \boldsymbol{x}_i-0= 0 
\\\Rightarrow \boldsymbol{w}=&\sum_{i=1}^{m}\alpha_iy_i \boldsymbol{x}_i
\end{aligned}\tag{6.9}
$$

$$
\begin{aligned}
\frac {\partial L}{\partial b}=&0+0-0-\sum_{i=1}^{m}\alpha_iy_i=0  
\\\Rightarrow 0=& \sum_{i=1}^{m}\alpha_iy_i
\end{aligned}\tag{6.10}
$$

- 将 $\boldsymbol{w}$ 代入 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})$：
  - 第三行到第四行：由 $\sum\limits_{i=1}^{m}\alpha_iy_i=0$，所以第三行最后一项可化为 0。



    $$\begin{aligned}
    \min_{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha})  &=\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i=1}^m\alpha_i -\sum_{i=1}^m\alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}_i-\sum_{i=1}^m\alpha_iy_ib \\
    &=\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i-\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_
    i -b\sum _{i=1}^m\alpha_iy_i \\
    & = -\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i -b\sum _{i=1}^m\alpha_iy_i
    \\&= -\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i \\
    &=-\frac {1}{2}(\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i)^T(\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i)+\sum _{i=1}^m\alpha_i \\
    &=-\frac {1}{2}\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i \\
    &=\sum _{i=1}^m\alpha_i-\frac {1}{2}\sum_{i=1 }^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
    \end{aligned}$$

- 此时：


$$\max_{\boldsymbol{\alpha}}\min_{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha}) =\max_{\boldsymbol{\alpha}} \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i = 1}^m\sum_{j=1}^m\alpha_i \alpha_j y_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j $$

- 然后，结合 $\sum\limits_{i=1}^{m}\alpha_iy_i=0$ 的约束，就得到了最开始的问题的对偶问题：
  - 这个对偶问题的求解，放在 SMO 里面了。


$$
\begin{aligned}
\max_{\boldsymbol{\alpha}} & \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i = 1}^m\sum_{j=1}^m\alpha_i \alpha_j y_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j \\
s.t. & \sum_{i=1}^m \alpha_i y_i =0 \\
& \alpha_i \geq 0 \quad i=1,2,\dots ,m
\end{aligned}\tag{6.11}
$$


- 解出 $\boldsymbol{\alpha}$ 之后，我们就可以求出 $\boldsymbol{w}$ 和 $b$ ，然后就可以得到模型：（怎么求？）

$$
\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b \end{aligned}\tag{6.12}
$$

关于 $\alpha_{i}$：

- 对偶问题求解出来的 $\alpha_{i}$ 就是(6.8)拉格朗日函数的乘子，它恰好对应着训练样本 $\left(\boldsymbol{x}_{i}, y_{i}\right)$
- 我们注意到SVM 基本型中有不等式约束 $y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1$，因此上述过程需满足 KKT(Karush-Kuhn-Tucker) 条件，即要求：

$$
\left\{\begin{array}{l}{\alpha_{i} \geqslant 0 ;} \\ {y_{i} f\left(x_{i}\right)-1 \geqslant 0} \\ {\alpha_{i}\left(y_{i} f\left(x_{i}\right)-1\right)=0 }\end{array}\right.\tag{6.13}
$$

- 于是，对任意训练样本 $\left(\boldsymbol{x}_{i}, y_{i}\right)$ ，总有 $\alpha_i=0$ 或 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1$。
  - 若 $\alpha_i=0$ ，则该样本将不会在式 (6.12) 的求和中出现，也就不会对 $f(\boldsymbol{x})$ 有任何影响；
  - 若 $\alpha_i>0$ , 则必有 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1$ ，所对应的样本点位于最大间隔边界上，是一个支持向量。
  - 这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关。