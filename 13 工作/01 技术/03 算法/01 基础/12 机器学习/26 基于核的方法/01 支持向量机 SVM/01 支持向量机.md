
# 支持向量机


支持向量机 Support Vector Machine，SVM


介绍：

- 是众多监督学习方法中十分出色的一种。
- 与逻辑回归比较：
  - 类似于逻辑回归，这个模型也是基于线性函数 $\boldsymbol{w}^{\top} \boldsymbol{x}+b$ 的。
  - 不同于逻辑回归的是，支持向量机不输出概率，只输出类别。当 $\boldsymbol{w}^{\top} \boldsymbol{x}+b$ 为正时，支持向量机预测属于正类。类似地，当 $\boldsymbol{w}^{\top} \boldsymbol{x}+b$ 为负时，支持向量机预测属于负类。



优点：

- SVM 是一种有坚实理论基础的新颖的适用小样本学习方法。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。
- SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。
- 少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本，而且注定了该方法不但算法简单，而且具有较好的“鲁棒性”。这种鲁棒性主要体现在：
	- 增、删非支持向量样本对模型没有影响；
	- 支持向量样本集具有一定的鲁棒性；
	- 有些成功的应用中，SVM 方法对核的选取不敏感；（为什么对核的选取不敏感？）
- SVM 学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。<span style="color:red;">哪些有效的求解凸优化问题的方法？</span>而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。<span style="color:red;">是的。</span>
- SVM 通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。
- SVM 在小样本训练集上能够得到比其它算法好很多的结果。SVM 优化目标是结构化风险最小，而不是经验风险最小，避免了过拟合问题，通过 margin 的概念，得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求，有优秀的泛化能力。

缺点：

- SVM算法对大规模训练样本难以实施
  - SVM的空间消耗主要是存储训练样本和核矩阵，由于 SVM 是借助二次规划来求解支持向量，而求解二次规划将涉及 $m$ 阶矩阵的计算（$m$ 为样本的个数），当 $m$ 数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。
  - 如果数据量很大，SVM的训练时间就会比较长，如垃圾邮件的分类检测，没有使用 SVM 分类器，而是使用简单的朴素贝叶斯分类器，或者是使用逻辑回归模型分类。<span style="color:red;">什么是朴素贝叶斯分类器？</span>
- 用 SVM 解决多分类问题存在困难
  - 经典的支持向量机算法只给出了二类分类的算法，而在实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和 SVM 决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服 SVM 固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗糙集理论结合，形成一种优势互补的多类问题的组合分类器。<span style="color:red;">这些思想还是要掌握的，比如 SVM 决策树、粗糙集理论等。</span>
- 对缺失数据敏感，对参数和核函数的选择敏感
  - 支持向量机性能的优劣主要取决于核函数的选取，所以对于一个实际问题而言，如何根据实际的数据模型选择合适的核函数从而构造 SVM 算法。目前比较成熟的核函数及其参数的选择都是人为的，根据经验来选取的，带有一定的随意性。在不同的问题领域，核函数应当具有不同的形式和参数，所以在选取时候应该将领域知识引入进来，但是目前还没有好的方法来解决核函数的选取问题。<span style="color:red;">现在有什么好的方法可以进行核函数的选择吗？</span>



缘由：

- 想用一个超平面来划分样本集
  - 给定的训练样本集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$，$y_{i} \in\{-1,+1\}$
  - 分类学习最基本的想法就是基于训练集 $D$ 在样本空间中找到一个划分超平面，将不同类别的样本分开。
- 但是，实际上能够将训练样本分开的划分超平面可能有很多，如下图所示，我们选择哪一个最好呢？
  - 直观上看，我们应该去找位于两类训练样本的所谓 “正中间” 的划分超平面，因为这样的超平面看起来对训练样本局部扰动的 “容忍” 性最好。也就是说，这样的分类结果应该是最鲁棒的，泛化能力最强的。
  - OK，到这里，我们就想问了，什么是所谓的 “正中间” ？怎么衡量？怎么求出这个 “正中间”？


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200612/Nc1qVgGalo2t.png?imageslim">
</p>






## 划分超平面的表示

先对于这个划分超平面进行描述：

- $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b=0$
- 其中:
  - $\boldsymbol{w}=\left(w_{1} ; w_{2} ; \ldots ; w_{d}\right)$ 为法向量，决定了超平面的方向。
  - $b$ 为位移项，决定了超平面与原点之间的距离。

此时：

- 由于一个划分超平面可以被法向量 $w$ 和位移 $b$ 所确定，我们可以把这个超平面记为 $(w,b)$ 。
- 样本空间中任意点 $\boldsymbol{x}$ 到超平面 $(\boldsymbol{w}, b)$ 的距离表示为：

$$
r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{\|\boldsymbol{w}\|}\tag{6.2}
$$



## 支持向量与间隔

假设超平面 $\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}+b^{\prime}=0$ 能将训练样本正确分类，此时，对于 $\left(\boldsymbol{x}_{i}, y_{i}\right) \in D$，有：

$$
\begin{aligned}
&\left\{\begin{array}{ll}{\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+b^{\prime}>0,} & {y_{i}=+1} \\ {\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+b^{\prime}<0,} & {y_{i}=-1}\end{array}\right.
\\\Rightarrow &\left\{\begin{array}{ll}{\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+b^{\prime} \geq+\zeta,} & {y_{i}=+1} \\ {\left(\boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+b^{\prime} \leq-\zeta,} & {y_{i}=-1}\end{array}\right.
\\\Rightarrow &\left\{\begin{array}{ll}{\left(\frac{1}{\zeta} \boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+\frac{b^{\prime}}{\zeta} \geq+1,} & {y_{i}=+1} \\ {\left(\frac{1}{\zeta} \boldsymbol{w}^{\prime}\right)^{\top} \boldsymbol{x}_{i}+\frac{b^{\prime}}{\zeta} \leq-1,} & {y_{i}=-1}\end{array}\right.
\\\Rightarrow &\left\{\begin{array}{ll}{\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \geq+1,} & {y_{i}=+1} \\ {\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \leq-1,} & {y_{i}=-1}\end{array}\right.
\end{aligned}
$$

说明：

- 第一行到第二行：根据几何间隔，进行修正，其中 $\zeta$ 为某个大于零的常数
- 第二行到第三行：两边同除以 $\zeta$，再次修正。
- 第三行到第四行：令：$\boldsymbol{w}=\frac{1}{\zeta} \boldsymbol{w}^{\prime}, b=\frac{b^{\prime}}{\zeta}$。

此时：

- 支持向量：
  - 对于 $\left\{\begin{array}{ll}{\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \geq+1,} & {y_{i}=+1} \\ {\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \leq-1,} & {y_{i}=-1}\end{array}\right.$，距离超平面最近的这几个训练样本点使得等号成立。
  - 由于每个样本点对应一个特征向量，因此，这几个使得取等号的样本点就称为支持向量 (support vector)。
- 间隔 margin
  - 我们把两个异类支持向量到超平面的距离的和 $\gamma=\frac{2}{\|\boldsymbol{w}\|}$ 定义为间隔(margin)。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180627/h9jKJD4J15.png?imageslim">
</p>

说明：

- 这三个被画了圆圈的点即为支持向量。
- 间隔为：$\gamma=\frac{2}{\|\boldsymbol{w}\|}$




## 最大间隔的超平面


具有最大间隔的划分超平面：maximum margin：

- 我们想找到一个具有最大间隔 (maximum margin) 的划分超平面。
- 也就是：

    $$
    \begin{array}{l}{\max _{\boldsymbol{w}, b} \frac{2}{\|\boldsymbol{w}\|}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.5}
    $$
  - 即：在满足 $y_i(w^Tx_i+b)\geqslant 1$ 的条件下，使得 $\gamma$ 最大的 $w$ 和 $b$ 。
- 由于仅需要最大化 $\|\boldsymbol{w}\|^{-1}$，这等价于最小化 $\|\boldsymbol{w}\|^{2}$，于是，上式可写为：

    $$
    \begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.6}
    $$
- 这个，就是支持向量机 (Support Vector Machine，简称 SVM ) 的基本型。
- 注意：上面这个式子看起来这个最小仅与 $w$ 有关, 但是事实上 $b$ 通过约束隐式地影响着 $w$ 的取值，进而对间隔产生影响。


## 求解 使用拉格朗日乘子法求解对偶问题


SVM 基本型：

$$
\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} 
\\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.6}
$$


求解：


- 注意到，上面这个式子本身是一个凸二次规划 (convex quadratic programming) 问题，是能够直接用现成的优化计算包求解的。
- 但是，我们可以有更高效的办法。可以使用拉格朗日乘子法来求解它的对偶问题(dual problem)。
- 构建拉格朗日函数：
  - 即对上面的式子的每个约束条件添加拉格朗日乘子 $\alpha_{i} \geqslant 0$ 。
  - $\boldsymbol{\alpha}=\left(\alpha_{1} ; \alpha_{2} ; \ldots ; \alpha_{m}\right)$。

$$
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)\tag{6.8}
$$

- 展开：

$$\begin{aligned}
L(\boldsymbol{w},b,\boldsymbol{\alpha}) &= \frac{1}{2}||\boldsymbol{w}||^2+\sum_{i=1}^m\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)) \\
& =  \frac{1}{2}||\boldsymbol{w}||^2+\sum_{i=1}^m(\alpha_i-\alpha_iy_i \boldsymbol{w}^T\boldsymbol{x}_i-\alpha_iy_ib)\\
& =\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i=1}^m\alpha_i -\sum_{i=1}^m\alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}_i-\sum_{i=1}^m\alpha_iy_ib
\end{aligned}​$$

- 令 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})$ 对 $\boldsymbol{w}$ 和 $b$ 求偏导为 0：

$$
\begin{aligned}
\frac {\partial L}{\partial \boldsymbol{w}}=&\frac{1}{2}\times2\times\boldsymbol{w} + 0 - \sum_{i=1}^{m}\alpha_iy_i \boldsymbol{x}_i-0= 0 
\\\Rightarrow \boldsymbol{w}=&\sum_{i=1}^{m}\alpha_iy_i \boldsymbol{x}_i
\end{aligned}\tag{6.9}
$$

$$
\begin{aligned}
\frac {\partial L}{\partial b}=&0+0-0-\sum_{i=1}^{m}\alpha_iy_i=0  
\\\Rightarrow 0=& \sum_{i=1}^{m}\alpha_iy_i
\end{aligned}\tag{6.10}
$$

- 将 $\boldsymbol{w}$ 代入 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})$：
  - 第三行到第四行：由 $\sum\limits_{i=1}^{m}\alpha_iy_i=0$，所以第三行最后一项可化为 0。



    $$\begin{aligned}
    \min_{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha})  &=\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i=1}^m\alpha_i -\sum_{i=1}^m\alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}_i-\sum_{i=1}^m\alpha_iy_ib \\
    &=\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i-\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_
    i -b\sum _{i=1}^m\alpha_iy_i \\
    & = -\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i -b\sum _{i=1}^m\alpha_iy_i
    \\&= -\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i \\
    &=-\frac {1}{2}(\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i)^T(\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i)+\sum _{i=1}^m\alpha_i \\
    &=-\frac {1}{2}\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i \\
    &=\sum _{i=1}^m\alpha_i-\frac {1}{2}\sum_{i=1 }^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
    \end{aligned}$$

- 此时：


$$\max_{\boldsymbol{\alpha}}\min_{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha}) =\max_{\boldsymbol{\alpha}} \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i = 1}^m\sum_{j=1}^m\alpha_i \alpha_j y_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j$$

- 然后，结合 $\sum\limits_{i=1}^{m}\alpha_iy_i=0$ 的约束，就得到了最开始的问题的对偶问题：
  - 这个对偶问题的求解，放在 SMO 里面了。


$$
\begin{aligned}
\max_{\boldsymbol{\alpha}} & \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i = 1}^m\sum_{j=1}^m\alpha_i \alpha_j y_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j \\
s.t. & \sum_{i=1}^m \alpha_i y_i =0 \\
& \alpha_i \geq 0 \quad i=1,2,\dots ,m
\end{aligned}\tag{6.11}
$$


- 解出 $\boldsymbol{\alpha}$ 之后，我们就可以求出 $\boldsymbol{w}$ 和 $b$ ，然后就可以得到模型：（怎么求？）

$$
\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b \end{aligned}\tag{6.12}
$$

关于 $\alpha_{i}$：

- 对偶问题求解出来的 $\alpha_{i}$ 就是(6.8)拉格朗日函数的乘子，它恰好对应着训练样本 $\left(\boldsymbol{x}_{i}, y_{i}\right)$
- 我们注意到SVM 基本型中有不等式约束 $y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1$，因此上述过程需满足 KKT(Karush-Kuhn-Tucker) 条件，即要求：

$$
\left\{\begin{array}{l}{\alpha_{i} \geqslant 0 ;} \\ {y_{i} f\left(x_{i}\right)-1 \geqslant 0} \\ {\alpha_{i}\left(y_{i} f\left(x_{i}\right)-1\right)=0 }\end{array}\right.\tag{6.13}
$$

- 于是，对任意训练样本 $\left(\boldsymbol{x}_{i}, y_{i}\right)$ ，总有 $\alpha_i=0$ 或 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1$。
  - 若 $\alpha_i=0$ ，则该样本将不会在式 (6.12) 的求和中出现，也就不会对 $f(\boldsymbol{x})$ 有任何影响；
  - 若 $\alpha_i>0$ , 则必有 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1$ ，所对应的样本点位于最大间隔边界上，是一个支持向量。
  - 这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关。




## 理解 SVM 中的对偶问题

在硬边界支持向量机中，问题的求解可以转化为凸二次规划问题。

假设优化目标为：

$$
\begin{aligned}
&\min_{\boldsymbol w, b}\frac{1}{2}||\boldsymbol w||^2\\
&s.t. y_i(\boldsymbol w^T\boldsymbol x_i+b)\geqslant 1, i=1,2,\cdots,m.\\
\end{aligned}  \tag{1}
$$


- 将上面问题转化为：

$$
\min_{\boldsymbol w, b} \max_{\alpha_i \geqslant 0}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}  \tag{2}
$$

- 说明：
  - 上式等价于原问题，因为：
    - 若满足(1)中不等式约束，则(2)式求 $\max$ 时,$\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))$ 必须取 $0$，与(1)等价；
    - 若不满足(1)中不等式约束，(2)中求 $\max$ 会得到无穷大。

- 交换 $\min$ 和 $\max$ 获得其对偶问题:
  - 交换之后的对偶问题和原问题并不相等，上式的解小于等于原问题的解。

$$
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}
$$

- 那么，此时，如何找到问题(1) 的最优值的一个最好的下界？

$$
\frac{1}{2}||\boldsymbol w||^2 < v\\
1 - y_i(\boldsymbol w^T\boldsymbol x_i+b) \leqslant 0\tag{3}
$$

- 若方程组(3)无解， 则 $v$ 是问题(1)的一个下界。
- 若(3)有解， 则

$$
\forall \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} < v
$$

- 由逆否命题得：若

$$
\exists \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} \geqslant v
$$

- 则(3)无解。

- 那么 $v$ 是问题(1)的一个下界。
- 要求得一个好的下界，取最大值即可：

$$
\max_{\alpha_i \geqslant 0}  \min_{\boldsymbol w, b} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}
$$

- 令

$$
L(\boldsymbol w, b,\boldsymbol a) =   \frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))
$$

- $p^*$ 为原问题的最小值，对应的 $w,b$ 分别为 $w^*,b^*$，则对于任意的 $a>0$:

$$
p^* = \frac{1}{2}||\boldsymbol w^*||^2 \geqslant  L(\boldsymbol w^*, b,\boldsymbol a) \geqslant \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)
$$

- 则 $\min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)$ 是问题（1）的一个下界。

- 此时，取最大值即可求得好的下界，即

$$
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)
$$


## 逻辑回归与 SVM 比较


相同点：

- LR和 SVM 都是**分类**算法。
- LR和 SVM 都是**监督学习**算法。
- LR和 SVM 都是**判别模型**。
- 如果不考虑核函数，LR和 SVM 都是**线性分类**算法，也就是说他们的分类决策面都是线性的。说明：LR也是可以用核函数的。但 LR 通常不采用核函数的方法。（**计算量太大**）

不同点：

- LR 采用 log 损失，SVM 采用合页(hinge) 损失。
  - 逻辑回归的损失函数：
    $$
    J(\theta)=-\frac{1}{m}\sum^m_{i=1}\left[y^{i}logh_{\theta}(x^{i})+ (1-y^{i})log(1-h_{\theta}(x^{i}))\right]
    $$
  - 支持向量机的目标函数:
    $$
    L(w,n,a)=\frac{1}{2}||w||^2-\sum^n_{i=1}\alpha_i \left( y_i(w^Tx_i+b)-1\right)
    $$
  - ​逻辑回归方法基于概率理论，假设样本为 $1$ 的概率可以用 sigmoid 函数来表示，然后通过极大似然估计的方法估计出参数的值。
  - ​支持向量机基于几何 边界最大化 原理，认为存在最大几何边界的分类面为最优分类面。

- LR 对异常值敏感，SVM 对异常值不敏感
  - ​支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。
  - LR 模型找到的那个超平面，是尽量让所有点都远离他，而 SVM 寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。
  - 支持向量机改变非支持向量样本并不会引起决策面的变化。逻辑回归中改变任何样本都会引起决策面的变化。
- 计算复杂度不同。对于海量数据，SVM 的效率较低，LR 效率比较高
  - 当样本较少，特征维数较低时，SVM 和 LR 的运行时间均比较短，SVM 较短一些。准确率的话，LR 明显比 SVM 要高。
  - 当样本稍微增加些时，SVM 运行时间开始增长，但是准确率赶超了 LR。SVM 时间虽长，但在可接受范围内。
  - 当数据量增长到 20000 时，特征维数增长到 200 时，SVM 的运行时间剧烈增加，远远超过了 LR 的运行时间。但是准确率却和 LR 相差无几。(这其中主要原因是大量非支持向量参与计算，造成 SVM 的二次规划问题)<span style="color:red;">这里面的运行时间指的是训练时间吧？</span><span style="color:blue;">嗯，是的，因为预测的时候这些都是提供的一个分类面。</span><span style="color:red;">造成 SVM 的二次规划问题是什么意思？</span>
- 对非线性问题的处理方式不同
  - LR 主要靠特征构造，必须组合交叉特征，特征离散化。SVM 也可以这样，还可以通过核函数 kernel（因为只有支持向量参与核计算，计算复杂度不高）。由于可以利用核函数，SVM 则可以通过对偶求解高效处理。LR 则在特征空间维度很高时，表现较差。
- SVM的损失函数就自带正则
  - 损失函数中的 $\frac{1}{2}||w||^2$ 项，这就是为什么 SVM 是结构风险最小化算法的原因！！！而 LR 必须另外在损失函数上添加正则项！！！<span style="color:red;">为什么呢？这个怎么能算作正则项呢？</span>
- SVM自带 结构风险最小化，LR则是 经验风险最小化。
- SVM 会用核函数而 LR 一般不用核函数。