

### 2.18.9 SVM主要特点

特点：

- SVM方法的理论基础是非线性映射，SVM利用内积核函数代替向高维空间的非线性映射。
- SVM的目标是对特征空间划分得到最优超平面，SVM方法核心是最大化分类边界。<span style="color:red;">最大化分类边界是什么意思？之前好像有分类边界的概念，再补充下。</span>
- 支持向量是 SVM 的训练结果，在 SVM 分类决策中起决定作用的是支持向量。
- SVM 是一种有坚实理论基础的新颖的适用小样本学习方法。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。<span style="color:red;">不涉及概率测度以及大数定律，嗯。</span>
- SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。<span style="color:red;">嗯。</span>
- 少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本，而且注定了该方法不但算法简单，而且具有较好的“鲁棒性”。这种鲁棒性主要体现在：
	- 增、删非支持向量样本对模型没有影响；
	- 支持向量样本集具有一定的鲁棒性；<span style="color:red;">什么叫支持向量的样本集具有一定的鲁棒性？</span>
	- 有些成功的应用中，SVM 方法对核的选取不敏感；<span style="color:red;">为什么对核的选取不敏感？</span>
- SVM 学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。<span style="color:red;">哪些有效的求解凸优化问题的方法？</span>而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。<span style="color:red;">是的。</span>
- SVM 通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。<span style="color:red;">是的，这个要设定多少呢？</span>
- SVM 在小样本训练集上能够得到比其它算法好很多的结果。SVM 优化目标是结构化风险最小，而不是经验风险最小，避免了过拟合问题，通过 margin 的概念，得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求，有优秀的泛化能力。
- 它是一个凸优化问题，因此局部最优解一定是全局最优解的优点。
