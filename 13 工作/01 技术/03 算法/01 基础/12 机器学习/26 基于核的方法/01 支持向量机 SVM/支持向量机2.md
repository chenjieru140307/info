

## 支持向量机

支持向量机(support vector machine,SVM)是监督学习中最有影响力的方法之一。




支持向量机的一个重要创新是核技巧(kernel trick)。核技巧观察到许多机器学习算法都可以写成样本间点积的形式。<span style="color:red;">样本间点积？</span>例如，支持向量机中的线性函数可以重写为

$$
\boldsymbol{w}^{\top} \boldsymbol{x}+b=b+\sum_{i=1}^{m} \alpha_{i} \boldsymbol{x}^{\top} \boldsymbol{x}^{(i)}\tag{5.82}
$$


其中，$\boldsymbol{x}^{(i)}$ 是训练样本，$\boldsymbol{\alpha}$ 是系数向量。学习算法重写为这种形式允许我们将 $\boldsymbol{x}$ 替换为特征函数 $\phi(\boldsymbol{x})$ 的输出，点积替换为被称为核函数(kernel function)的函数 $k\left(\boldsymbol{x}, \boldsymbol{x}^{(i)}\right)=\phi(\boldsymbol{x}) \cdot \phi\left(\boldsymbol{x}^{(i)}\right)$。<span style="color:red;">没有很明白。</span>运算符 $\cdot$ 表示类似于 $\phi(\boldsymbol{x})^{\top} \phi\left(\boldsymbol{x}^{(i)}\right)$ 的点积。对于某些特征空间，我们可能不会书面地使用向量内积。在某些无限维空间中，我们需要使用其他类型的内积，如基于积分而非加和的内积。这种类型内积的完整介绍超出了本书的范围。

<span style="color:red;">再看下，再理解下。</span>

使用核估计替换点积之后，我们可以使用如下函数进行预测：

$$
f(\boldsymbol{x})=b+\sum_{i} \alpha_{i} k\left(\boldsymbol{x}, \boldsymbol{x}^{(i)}\right)\tag{5.83}
$$


这个函数关于 $\boldsymbol{x}$ 是非线性的，关于 $\phi(\boldsymbol{x})$ 是线性的。$\boldsymbol{\alpha}$ 和 $f(\boldsymbol{x})$ 之间的关系也是线性的。核函数完全等价于用 $\phi(\boldsymbol{x})$ 预处理所有的输入，然后在新的转换空间学习线性模型。

核技巧十分强大有两个原因：其一，它使我们能够使用保证有效收敛的凸优化技术来学习非线性模型(关于 $\boldsymbol{x}$ 的函数)。这是可能的，因为我们可以认为 $\phi$ 是固定的，仅优化 $\alpha$，即优化算法可以将决策函数视为不同空间中的线性函数。其二，核函数 $k$ 的实现方法通常比直接构建 $\phi(\boldsymbol{x})$ 再算点积高效很多。

在某些情况下，$\phi(\boldsymbol{x})$ 甚至可以是无限维的，对于普通的显式方法而言，这将是无限的计算代价。在很多情况下，即使 $\phi(\boldsymbol{x})$ 是难算的，$k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ 却会是一个关于 $\boldsymbol{x}$ 非线性的、易算的函数。举个无限维空间易算的核的例子，我们构建一个作用于非负整数 $x$ 上的特征映射 $\phi(x)$。假设这个映射返回一个由开头 $x$ 个 $1$，随后是无限个 $0$ 的向量。我们可以写一个核函数 $k\left(x, x^{(i)}\right)=\min \left(x, x^{(i)}\right)$ ，完全等价于对应的无限维点积。

最常用的核函数是高斯核(Gaussian kernel)，

$$
k(\boldsymbol{u}, \boldsymbol{v})=\mathcal{N}\left(\boldsymbol{u}-\boldsymbol{v} ; \mathbf{0}, \sigma^{2} I\right)\tag{5.84}
$$

其中 $\mathcal{N}(x ; \boldsymbol{\mu}, \mathbf{\Sigma})$ 是标准正态密度。这个核也被称为径向基函数(radial basis function，RBF)核，因为其值沿 $\boldsymbol{v}$ 中从 $\boldsymbol{u}$ 向外辐射的方向减小。高斯核对应于无限维空间中的点积，但是该空间的推导没有整数上最小核的示例那么直观。

我们可以认为高斯核在执行一种模板匹配(template matching)。训练标签 $y$ 相关的训练样本 $\boldsymbol{x}$ 变成了类别 $y$ 的模板。当测试点 $\boldsymbol{x}^{\prime}$ 到 $\boldsymbol{x}$ 的欧几里得距离很小，对应的高斯核响应很大时，表明 $\boldsymbol{x}^{\prime}$ 和模板 $\boldsymbol{x}$ 非常相似。该模型进而会赋予相对应的训练标签 $y$ 较大的权重。总的来说，预测将会组合很多这种通过训练样本相似度加权的训练标签。

支持向量机不是唯一可以使用核技巧来增强的算法。许多其他的线性模型也可以通过这种方式来增强。使用核技巧的算法类别被称为核机器(kernel machine)或核方法(kernel method)(Williams and Rasmussen,1996;Sch-lkopf et al.,1999)。

核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。因为第 $i$ 个样本贡献 $\alpha_{i} k\left(\boldsymbol{x}, \boldsymbol{x}^{(i)}\right)$ 到决策函数。支持向量机能够通过学习主要包含零的向量 $\boldsymbol{\alpha}$，以缓和这个缺点。那么判断新样本的类别仅需要计算非零 $\alpha_{i}$ 对应的训练样本的核函数。这些训练样本被称为支持向量(support vector)。

当数据集很大时，核机器的计算量也会很大。我们将会在第 5.9 节回顾这个想法。带通用核的核机器致力于泛化得更好。我们将在第 5.11 节解释原因。现代深度学习的设计旨在克服核机器的这些限制。当前深度学习的复兴始于 Hinton et al.(2006b)表明神经网络能够在 MNIST 基准数据上胜过 RBF 核的支持向量机。<span style="color:red;">嗯，始于这个。</span>

