
# 核函数

​
引入核函数目的：

- 把原坐标系里线性不可分的数据用核函数 Kernel 投影到另一个空间，尽量使得数据在新的空间里线性可分。

核函数方法的特点：

- 核函数的引入避免了“维数灾难”，大大减小了计算量。而输入空间的维数 $n$ 对核函数矩阵无影响。因此，核函数方法可以有效处理高维输入。
- 无需知道非线性变换函数 $\phi$ 的形式和参数。（为什么不需要知道？是自动定的吗？）
- 核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。
- 核函数方法可以和不同的算法相结合，形成多种不同的基于核函数技术的方法，且这两部分的设计可以单独进行，并可以为不同的应用选择不同的核函数和算法。（怎么和不同的算法结合的？）


缘由：


- 之前，我们假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类。
- 然而在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面。


解决：

- 对这样的间题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。
- 如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分.（为什么一定存在呢？而且，我们怎么知道这个一定存在的高维空间是几维呢？这种映射为什么一定）


举例：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200612/ASuVqNbdOC7L.png?imageslim">
</p>

- 对于异或问题，不是线性可分的。
- 可以将原始的二维空间映射到一个合适的三维空间，就能找到一个合适的划分超平面

高维映射过程：

- 令 $\phi(\boldsymbol{x})$ 表示将 $\boldsymbol{x}$ 映射后的特征向量，于是，在特征空间中划分超平面所对应的模型可表示为：

$$
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b\tag{6.19}
$$

- 其中 $\boldsymbol{w}$ 和 $b$ 是模型参数。类似之前的 SVM，有：

$$
\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \phi\left(\boldsymbol{x}_{i}\right)+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.20}
$$

- 其 Lagrange 对偶问题为：

$$
\begin{array}{l}{\max _{\alpha} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)} \\ {\text { s.t. } \sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\qquad \alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}\tag{6.21}
$$

- 求解上式涉及到计算 $\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)$ ，这是样本 $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{j}$ 映射到特征空间之后的内积。
- 由于特征空间维数可能很高，甚至可能是无穷维，因此直接计算 $\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)$ 通常是困难的。
- 为了避开这个障碍，可以设想这样一个函数：

$$
\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\langle\phi\left(\boldsymbol{x}_{i}\right), \phi\left(\boldsymbol{x}_{j}\right)\right\rangle=\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)\tag{6.22}
$$

- 即将  $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{j}$ 在特征空间的内积等价于它们在原始样本空间中通过函数 $\kappa(\cdot, \cdot)$ 计算的结果。
- 带入对偶函数，得：

$$
\begin{array}{ll}{\max _{\alpha}} & {\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)} \\ {\text { s.t. }} & {\sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {} & {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}\tag{6.23}
$$

- 求解后，得：

$$
\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)+b \end{aligned}\tag{6.24}
$$


- 注：这里的函数  $\kappa(\cdot, \cdot)$  就是“核函数”(kernel function)。
- 上式显示出模型最优解可通过训练样本的核函数展开，这一展式亦称 “支持向量展式” (support vector expansion)。

核函数的选择：

- 显然，若已知合适映射 $\phi(\cdot)$ 的具体形式，则可写出核函数 $\kappa(\cdot,\cdot)$ 。
- 但在现实任务中我们通常不知道 $\phi(\cdot)$ 是什么形式，那么，合适的核函数是否一定存在呢? 什么样的函数能做核函数呢？
- 我们有下面的定理：
  - 令 $\mathcal{X}$ 为输入空间， $\kappa(\cdot,\cdot)$ 是定义在 $\mathcal{X}\times \mathcal{X}$ 上的对称函数，则 $\kappa$ 是核函数当且仅当对于任意数据 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$ ，“核矩阵” (kernel matrix) $\mathbf{K}$ 总是半正定的：

    $$
    \mathbf{K}=\left[\begin{array}{ccccc}{\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{1}\right)} & {\dots} & {\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{j}\right)} & {\dots} & {\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{m}\right)} \\ {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)} & {\dots} & {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{m}\right)} \\ {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{j}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{m}\right)}\end{array}\right]
    $$

- 上面定理表明：
  - 只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。
    - 事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射 $\phi$。换言之，任何一个核函数都隐式地定义了一个称为 “再生核希尔伯特空间” (Reproducing Kernel Hilbert Space，简称 RKHS) 的特征空间。
    - 这方面有一些基本的经验，例如对文本数据通常使用线性核，情况不明时可尝试高斯核。

- 通过前面的讨论可知，我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要。
- 需注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间。
- 于是，“核函数选择”成为支持向量机的最大变数。若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。



几种常用的核函数：


| 名称 | 表达式 | 参数 |
| -- | -- | -- |
| 线性核 | $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}$ | |
| 多项式核 | $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}\right)^{d}$ | $d \geqslant 1$ 为多项式的次数 |
| 高斯核 | $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\exp \left(-\frac{\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|^{2}}{2 \sigma^{2}}\right)$ | $\sigma>0$ 为高斯核的带宽（width）|
| 拉普拉斯核 | $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\exp \left(-\frac{\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|}{\sigma}\right)$ | $\sigma>0$ |
| Sigmoid 核 | $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\tanh \left(\beta \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}+\theta\right)$ | tanh 为双曲正切函数， $\beta>0, \theta<0$|


可以通过核函数的组合得到新的核函数：

- 若 $\kappa_1$ 和 $\kappa_2$ 为核函数，则对于任意正数 $\gamma_1$ 、 $\gamma_2$，其线性组合也是核函数。 

$$
\gamma_{1} \kappa_{1}+\gamma_{2} \kappa_{2}\tag{6.25}
$$


- 若 $\kappa_1$ 和 $\kappa_2$ 为核函数，则核函数的直积也是核函数。

$$
\kappa_{1} \otimes \kappa_{2}(\boldsymbol{x}, \boldsymbol{z})=\kappa_{1}(\boldsymbol{x}, \boldsymbol{z}) \kappa_{2}(\boldsymbol{x}, \boldsymbol{z})\tag{6.26}
$$


- 若 $\kappa_1$ 为核函数，则对于任意函数 $g(x)$ 也是核函数。

$$
\kappa(\boldsymbol{x}, \boldsymbol{z})=g(\boldsymbol{x}) \kappa_{1}(\boldsymbol{x}, \boldsymbol{z}) g(\boldsymbol{z})\tag{6.27}
$$












