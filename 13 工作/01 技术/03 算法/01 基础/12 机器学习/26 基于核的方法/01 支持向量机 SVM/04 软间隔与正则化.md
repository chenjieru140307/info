
# 软间隔与正则化

缘由：

- 之前，我们假设存在超平面或者通过核函数转化得到的高维超平面，将样本完全划分。
- 但是，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分；
- 退一步说, 即便恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的。

解决：

- 缓解该问题的一个办法是允许支持向量机在一些样本上出错。
- 为此，要引入 “软间隔” (soft margin) 的概念


软间隔示意图，红色圈出了一些不满足约束的样本：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200612/jJv3pXvJ1QSw.png?imageslim">
</p>

过程：

- 之前介绍的 SVM 要求所有样本均满足约束 $\left\{\begin{array}{ll}{\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \geq+1,} & {y_{i}=+1} \\ {\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b \leq-1,} & {y_{i}=-1}\end{array}\right.$， 即所有样本都必须划分正确，这称为“硬间隔” (hard margin)。
- 而软间隔则是允许某些样本不满足约束：

$$
y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1\tag{6.28}
$$

- 当然，在最大化间隔的同时，不满足约束的样本应尽可能少。于是，优化目标可写为：

$$
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{0 / 1}\left(y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)-1\right)\tag{6.29}
$$

- 其中：
  - $C>0$ 是一个常数.
    - 当 $C$ 为无穷大时，上式迫使所有样本均满足約束 $y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1$，此时，上式等价于 $\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}$
    - 当 $C$ 取有限值时，上式允许一些样本不满足約束。
  - $\ell_{0/1}$ 是 “0/1损失函数” ：$\ell_{0 / 1}(z)=\left\{\begin{array}{ll}{1,} & {\text { if } z<0} \\ {0,} & {\text { otherwise }}\end{array}\right.$
- 然而，$\ell_{0/1}$ 非凸、非连续，数学性质不太好，使得上式不易直接求解。
- 于是，人们通常用其他一些函数来代替 $\ell_{0/1}$ ，称为 “替代损失”(surrogate loss)。
- 替代损失函数一般具有较好的数学性质，如它们通常是凸的连续函数且是 $\ell_{0/1}$  的上界。有如下三种常用的替代损失函数：
  - hinge 损失：
    $$
    \ell_{\text {hinge}}(z)=\max (0,1-z)\tag{6.31}
    $$
  - 指数损失 exponential loss：
    $$
    \ell_{\exp }(z)=\exp (-z)\tag{6.32}
    $$
  - 对率损失 logistic loss：
    $$
    \ell_{\log }(z)=\log (1+\exp (-z))\tag{6.32}
    $$

- 此时，若采用 hinge 损失，则式(6.29)变成：

$$
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \max \left(0,1-y_{i}\left(\boldsymbol{w}^{\mathbf{T}} \boldsymbol{x}_{i}+b\right)\right)\tag{6.34}
$$

- 此时，引入“松弛变量”(slack variables) $\xi_i\geqslant 0$ ，将上式重写为：（怎么写的？）


$$
\begin{array}{ll}{\min _{\boldsymbol{w}, b, \xi_{i}}}& {\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}\tag{6.35}}\\{\text { s.t. }} & {y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1-\xi_{i}} \\ {} & {\xi_{i} \geqslant 0, i=1,2, \ldots, m}\end{array}
$$

- 这就是常用的 “软间隔支持向量机”.


求解：

- 显然，式(6.35)中每个样本都有一个对应的松弛变量，用以表征该样本不满足约束 $y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1$ 的程度。但是，与式 $\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}$ 相似，这仍是一个二次规划问题。
- 于是，类似式 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)$，通过拉格朗日乘子法可得到式(6.35)的拉格朗日函数

$$
\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{\alpha}, \boldsymbol{\xi}, \boldsymbol{\mu})=& \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i} \\ &+\sum_{i=1}^{m} \alpha_{i}\left(1-\xi_{i}-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i} \end{aligned}\tag{6.36}
$$

- 其中：$\alpha_i\geqslant 0$ ，$\mu_i\geqslant 0$ 是拉格朗日乘子。

- 令 $L(\boldsymbol{w}, b, \boldsymbol{\alpha}, \boldsymbol{\xi}, \boldsymbol{\mu})$ 对 $\boldsymbol{w}$，$b$，$\xi_{i}$ 的偏导为零可得：

$$
\boldsymbol{w}=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\tag{6.37}
$$
$$
0=\sum_{i=1}^{m} \alpha_{i} y_{i}\tag{6.38}
$$


$$
\begin{aligned}
\frac{\partial L}{\partial \xi_i}=&0+C \times 1 - \alpha_i \times 1-\mu_i
\times 1 =0 
\\\Rightarrow C=&\alpha_i +\mu_i
\end{aligned}\tag{6.39}
$$

- 将上面三个式子带入 (6.36) ：


$$\begin{aligned}
 \min_{\boldsymbol{w},b,\boldsymbol{\xi}}L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu}) &= \frac{1}{2}||\boldsymbol{w}||^2+C\sum_{i=1}^m \xi_i+\sum_{i=1}^m \alpha_i(1-\xi_i-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))-\sum_{i=1}^m\mu_i \xi_i  \\
&=\frac{1}{2}||\boldsymbol{w}||^2+\sum_{i=1}^m\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))+C\sum_{i=1}^m \xi_i-\sum_{i=1}^m \alpha_i \xi_i-\sum_{i=1}^m\mu_i \xi_i \\
& = -\frac {1}{2}\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i +\sum_{i=1}^m C\xi_i-\sum_{i=1}^m \alpha_i \xi_i-\sum_{i=1}^m\mu_i \xi_i \\
&  = -\frac {1}{2}\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i +\sum_{i=1}^m (C-\alpha_i-\mu_i)\xi_i \\
&=\sum _{i=1}^m\alpha_i-\frac {1}{2}\sum_{i=1 }^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
\end{aligned}$$

- 则：

$$\begin{aligned}
\max_{\boldsymbol{\alpha},\boldsymbol{\mu}} \min_{\boldsymbol{w},b,\boldsymbol{\xi}}L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})&=\max_{\boldsymbol{\alpha},\boldsymbol{\mu}}\sum _{i=1}^m\alpha_i-\frac {1}{2}\sum_{i=1 }^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j \\
&=\max_{\boldsymbol{\alpha}}\sum _{i=1}^m\alpha_i-\frac {1}{2}\sum_{i=1 }^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
\end{aligned}$$

- 又

$$\begin{aligned}
\alpha_i &\geq 0 \\
\mu_i &\geq 0 \\
C &= \alpha_i+\mu_i
\end{aligned}$$

- 消去 $\mu_i$ 可得等价约束条件为：

$$0 \leq\alpha_i \leq C \quad i=1,2,\dots ,m$$


- 此时，得到式 (6.35) 的对偶问题

$$
\begin{aligned}
\max_{\boldsymbol{\alpha}}&\sum _{i=1}^m\alpha_i-\frac {1}{2}\sum_{i=1 }^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j \\
 s.t. &\sum_{i=1}^m \alpha_i y_i=0 \\
 &  0 \leq\alpha_i \leq C \quad i=1,2,\dots ,m
 \end{aligned}\tag{6.40}
 $$

- 将上式与硬间隔下的对偶问题 $\begin{aligned}\max_{\boldsymbol{\alpha}} & \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i = 1}^m\sum_{j=1}^m\alpha_i \alpha_j y_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j \\s.t. & \sum_{i=1}^m \alpha_i y_i =0 \\& \alpha_i \geq 0 \quad i=1,2,\dots ,m\end{aligned}$ 对比，
- 可看出，两者唯一的差别就在于对偶变量的约束不同：前者是 $0\leqslant \alpha_i\leqslant C$ ，后者是 $0\leqslant \alpha_i$ 。
- 于是，同样可采用 SMO 算法求解式(6.40)；
- 同样的，在引入核函数后能得到与式 $\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)+b \end{aligned}$同样的支持向量展式。

- 类似 $\left\{\begin{array}{l}{\alpha_{i} \geqslant 0 ;} \\ {y_{i} f\left(x_{i}\right)-1 \geqslant 0} \\ {\alpha_{i}\left(y_{i} f\left(x_{i}\right)-1\right)=0 }\end{array}\right.$，对软间隔支持向量机，KKT 条件要求

$$
\left\{\begin{array}{l}{\alpha_{i} \geqslant 0, \quad \mu_{i} \geqslant 0} \\ {y_{i} f\left(\boldsymbol{x}_{i}\right)-1+\xi_{i} \geqslant 0} \\ {\alpha_{i}\left(y_{i} f\left(\boldsymbol{x}_{i}\right)-1+\xi_{i}\right)=0} \\ {\xi_{i} \geqslant 0, \mu_{i} \xi_{i}=0}\end{array}\right.\tag{6.41}
$$

- 于是，对任意训练样本 $\left(\boldsymbol{x}_{i}, y_{i}\right)$ ，总有 $\alpha_i=0$ 或 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1-\xi_{i}$ ，
  - 若 $\alpha_i=0$ ，则该样本不会对 $f(\boldsymbol{x})$ 有任何影响；
  - 若 $\alpha_i>0$ ，则必有 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1-\xi_{i}$，即该样本是支持向量：
    - 由式(6.39)可知，
      - 若 $\alpha_i<C$ ，则 $\mu_i>0$ ，进而有 $\xi_i=0$ ，即该样本恰在最大间隔边界上；
      - 若 $\alpha_i=C$ ，则有 $\mu_i = 0$ ，
        - 此时若 $\xi_i\leq 1$ 则该样本落在最大间隔内部，
        - 若 $\xi_i>1$ 则该样本被错误分类。
  - 由此可看出，软间隔支持向量机的最终模型仅与支持向量有关，即通过采用 hinge 损失函数仍保持了稀疏性.

那么，能否对式(6.29)使用其他的替代损失函数呢？

- 可以发现，如果使用对率损失函数 $\ell_{\log }$ 来替代式(6.29)中的 $0/1$ 损失函数
  - 则几乎就得到了对率回归模型 (3.27) （对率回归模型是什么？）
  - 实际上，支持向量机与对率回归的优化目标相近，通常情形下它们的性能也相当。
    - 对率回归的优势主要在于其输出具有自然的概率意义，即在给出预测标记的同时也给出了概率，而支持向量机的输出不具有概率意义，欲得到概率输出需进行特殊处理；
    - 此外，对率回归能直接用于多分类任务，支持向量机为此则需进行推广，
  - 另一方面，hinge损失有一块“平坦”的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的单调递减函数，不能导出类似支持向量的概念，因此对率回归的解依赖于更多的训练样本，其预测开销更大。
- 我们还可以把式(6.29)中的 $0/1$ 损失函数换成别的替代损失函数以得到其他学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性：
  - 优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项 $\sum_{i=1}^{m} \ell\left(f\left(\boldsymbol{x}_{i}\right), y_{i}\right)$ 用来表述训练集上的误差，可写为更一般的形式
    $$
    \min _{f} \Omega(f)+C \sum_{i=1}^{m} \ell\left(f\left(\boldsymbol{x}_{i}\right), y_{i}\right)\tag{6.42}
    $$
  - 其中：
    - $\Omega(f)$ 称为“结构风险” (structural risk)，用于描述模型 $f$ 的某些性质；
    - 第二项 $\sum_{i=1}^{m} \ell\left(f\left(\boldsymbol{x}_{i}\right), y_{i}\right)$ 称为“经验风险”(empirical risk)，用于描述模型与训练数据的契合程度；
    - $C$ 用于对二者进行折中。
  - 从经验风险最小化的角度来看，$\Omega(f)$ 表述了我们希望获得具有何种性质的模型(例如希望获得复杂度较小的模型) 这为引入领域知识和用户意图提供了途径；
  - 另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。
    - 从这个角度来说，式(6.42)称为“正则化”(regularization)问题， 
    - $\Omega(f)$ 称为正则化项，
    - $C$ 则称为正则化常数. 
    - $\mathbf{L}_{p}$ 范数(norm)是常用的正则化项，
      - 其中 $\mathrm{L}_{2}$ 范数 $\|\boldsymbol{w}\|_{2}$ 倾向于 $\boldsymbol{w}$ 的分量取值尽量均衡，即非零分量个数尽量稠密，
      - 而 $\mathrm{L}_{0}$ 范数 $\|\boldsymbol{w}\|_{0}$ 和 $\mathrm{L}_{1}$ 范数 $\|\boldsymbol{w}\|_{1}$ 则倾向于 $\boldsymbol{w}$ 的分量尽量稀疏，即非零分量个数尽量少。

