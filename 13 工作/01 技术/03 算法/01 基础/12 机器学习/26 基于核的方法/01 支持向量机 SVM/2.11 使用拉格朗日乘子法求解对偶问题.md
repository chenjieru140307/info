---
title: 2.11 使用拉格朗日乘子法求解对偶问题
toc: true
date: 2018-07-30 22:01:21
---
# 可以补充进来的

# 对偶问题

之前我们已经得到了这个支持向量机 SVM 的基本型了：

$$
\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.6}
$$

OK，那么怎么求解这个式子中的 $w$ 和 $b$ 呢？

$$
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b
$$

## 使用拉格朗日乘子法求解这个基本型的对偶问题

我们注意到，上面这个式子本身是一个凸二次规划 (convex quadratic programming) 问题，是能够直接用现成的优化计算包求解的。<span style="color:red;">到底怎么求解？</span>

但是，我们可以有更高效的办法。

我们可以使用拉格朗日乘子法来求解它的对偶问题(dual problem)。

具体来说，对上面的式子的每个约束条件添加拉格朗日乘子 $\alpha_{i} \geqslant 0$ 。

拉格朗日函数如下：

$$
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)\tag{6.8}
$$


> $$
> L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right)\right)
> $$
> [推导]：
> 待求目标:
> $$\begin{aligned}
> \min_{\boldsymbol{x}}\quad f(\boldsymbol{x})\\
> s.t.\quad h(\boldsymbol{x})&=0\\
> g(\boldsymbol{x}) &\leq 0
> \end{aligned}$$
>
> 等式约束和不等式约束：$h(\boldsymbol{x})=0, g(\boldsymbol{x}) \leq 0$ 分别是由一个等式方程和一个不等式方程组成的方程组。
>
> 拉格朗日乘子：$\boldsymbol{\lambda}=\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)$  $\qquad\boldsymbol{\mu}=\left(\mu_{1}, \mu_{2}, \ldots, \mu_{n}\right)$
>
> 拉格朗日函数：$L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})=f(\boldsymbol{x})+\boldsymbol{\lambda} h(\boldsymbol{x})+\boldsymbol{\mu} g(\boldsymbol{x})$



其中，$\boldsymbol{\alpha}=\left(\alpha_{1} ; \alpha_{2} ; \ldots ; \alpha_{m}\right)$。

我们令 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})$ 对 $\boldsymbol{w}$ 和 $b$ 求偏导为 0，可得：

$$
\boldsymbol{w}=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\tag{6.9}
$$

$$
0=\sum_{i=1}^{m} \alpha_{i} y_{i}\tag{6.10}
$$


> $$\begin{aligned}
> w &= \sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i \\
> 0 &=\sum_{i=1}^m\alpha_iy_i
> \end{aligned}​$$
> [推导]：式（6.8）可作如下展开：
> $$\begin{aligned}
> L(\boldsymbol{w},b,\boldsymbol{\alpha}) &= \frac{1}{2}||\boldsymbol{w}||^2+\sum_{i=1}^m\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)) \\
> & =  \frac{1}{2}||\boldsymbol{w}||^2+\sum_{i=1}^m(\alpha_i-\alpha_iy_i \boldsymbol{w}^T\boldsymbol{x}_i-\alpha_iy_ib)\\
> & =\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i=1}^m\alpha_i -\sum_{i=1}^m\alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}_i-\sum_{i=1}^m\alpha_iy_ib
> \end{aligned}​$$
> 对 $\boldsymbol{w}$ 和 $b$ 分别求偏导数​并令其等于 0：
>
> $$\frac {\partial L}{\partial \boldsymbol{w}}=\frac{1}{2}\times2\times\boldsymbol{w} + 0 - \sum_{i=1}^{m}\alpha_iy_i \boldsymbol{x}_i-0= 0 \Longrightarrow \boldsymbol{w}=\sum_{i=1}^{m}\alpha_iy_i \boldsymbol{x}_i$$
>
> $$\frac {\partial L}{\partial b}=0+0-0-\sum_{i=1}^{m}\alpha_iy_i=0  \Longrightarrow  \sum_{i=1}^{m}\alpha_iy_i=0$$


我们将式子(6.9) 代入(6.8)，可将 $L(\boldsymbol{w}, b, \boldsymbol{\alpha})$ 中的 $\boldsymbol{w}$ 和 $b$ 消去，<span style="color:red;">代入计算的过程还是要自己算一下的。</span>然后，我们再考虑 (6.10) 的约束，就得到了最开始的问题的对偶问题：<span style="color:red;">这里没有特别理解，再补充下。</span>


$$
\begin{array}{l}{\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}}\\{\text { s.t. } \sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\qquad \alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}\tag{6.11}
$$

> $$\begin{aligned}
> \max_{\boldsymbol{\alpha}} & \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i = 1}^m\sum_{j=1}^m\alpha_i \alpha_j y_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j \\
> s.t. & \sum_{i=1}^m \alpha_i y_i =0 \\
> & \alpha_i \geq 0 \quad i=1,2,\dots ,m
> \end{aligned}$$
> [推导]：将式 (6.9)代入 (6.8) ，即可将 $L(\boldsymbol{w},b,\boldsymbol{\alpha})$ 中的 $\boldsymbol{w}$ 和 $b$ 消去，再考虑式 (6.10) 的约束，就得到式 (6.6) 的对偶问题：
> $$\begin{aligned}
> \min_{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha})  &=\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i=1}^m\alpha_i -\sum_{i=1}^m\alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}_i-\sum_{i=1}^m\alpha_iy_ib \\
> &=\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i-\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_
> i -b\sum _{i=1}^m\alpha_iy_i \\
> & = -\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i -b\sum _{i=1}^m\alpha_iy_i
> \end{aligned}$$
> 又 $\sum\limits_{i=1}^{m}\alpha_iy_i=0$，所以上式最后一项可化为 0，于是得：
> $$\begin{aligned}
> \min_{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha}) &= -\frac {1}{2}\boldsymbol{w}^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i \\
> &=-\frac {1}{2}(\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i)^T(\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i)+\sum _{i=1}^m\alpha_i \\
> &=-\frac {1}{2}\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}_i^T\sum _{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum _{i=1}^m\alpha_i \\
> &=\sum _{i=1}^m\alpha_i-\frac {1}{2}\sum_{i=1 }^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
> \end{aligned}$$
> 所以
> $$\max_{\boldsymbol{\alpha}}\min_{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha}) =\max_{\boldsymbol{\alpha}} \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i = 1}^m\sum_{j=1}^m\alpha_i \alpha_j y_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j $$



<span style="color:red;">第一次看到这种对偶问题的转化，感觉真心厉害。发明对偶问题的人是怎么想到的。</span>

解出 $\boldsymbol{\alpha})$ 之后，我们就可以求出 $\boldsymbol{w}$ 和 $b$ ，然后就可以得到模型：

$$
\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b \end{aligned}\tag{6.12}
$$


对偶问题求解出来的 $\alpha_{i}$ 就是(6.8)拉格朗日函数的乘子，它恰好对应着训练样本 $\left(\boldsymbol{x}_{i}, y_{i}\right)$ ，<span style="color:red;">什么叫恰好对应着训练样本  $(x_i,y_i)$ ？</span>我们注意到 (6.6) 中有不等式约束，因此上述过程需满足 KKT(Karush-Kuhn-Tucker) 条件，即要求：

$$
\left\{\begin{array}{l}{\alpha_{i} \geqslant 0 ;} \\ {y_{i} f\left(x_{i}\right)-1 \geqslant 0} \\ {\alpha_{i}\left(y_{i} f\left(x_{i}\right)-1\right)=0 }\end{array}\right.\tag{6.13}
$$

<span style="color:red;">对于 KKT 条件还是不清楚，为什么要满足这个条件？为什么有不等式约束就要满足这个条件？</span>

于是，对任意训练样本 $\left(\boldsymbol{x}_{i}, y_{i}\right)$ ，总有 $\alpha_i=0$ 或 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1$。<span style="color:red;">没明白，这个于是是为什么？</span>若 $\alpha_i=0$ ，则该样本将不会在式 (6.12) 的求和中出现，也就不会对 $f(\boldsymbol{x})$ 有任何影响；若 $\alpha_i>0$ , 则必有 $y_{i} f\left(\boldsymbol{x}_{i}\right)=1$ ，所对应的样本点位于最大间隔边界上，是一个支持向量。

这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关。<span style="color:red;">虽然我知道训练完后，模型只与支持向量有关，但是这一段还是没看懂。</span>



# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
