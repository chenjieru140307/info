
## LR和 SVM 的联系：

- 都是监督的分类算法
- 都是线性分类方法 (不考虑核函数时）
- 都是判别模型 



## LR和 SVM 的不同

1、损失函数的不同

- LR是 cross entropy

$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]
$$

- SVM的损失函数是最大化间隔距离

$$
\mathcal{L}(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{n} \alpha_{i}\left(y_{i}\left(w^{T} x_{i}+b\right)-1\right)
$$

​逻辑回归方法基于概率理论，假设样本为 1 的概率可以用 sigmoid 函数来表示，然后通过极大似然估计的方法估计出参数的值

支持向量机​基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面

2、SVM不能产生概率，LR可以产生概率

LR本身就是基于概率的，所以它产生的结果代表了分成某一类的概率，而 SVM 则因为优化的目标不含有概率因素，所以其不能直接产生概率。

3、SVM自带结构风险最小化，LR则是经验风险最小化

在假设空间、损失函数和训练集确定的情况下，经验风险最小化即最小化损失函数

结构最小化是为了防止过拟合，在经验风险的基础上加上表示模型复杂度的正则项

4、SVM会用核函数而 LR 一般不用核函数

SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。 而 LR 则每个点都需要两两计算核函数，计算量太过庞大

5、LR和 SVM 在实际应用的区别

根据经验来看，对于小规模数据集，SVM的效果要好于 LR，但是大数据中，SVM的计算复杂度受到限制，而 LR 因为训练简单，可以在线训练，所以经常会被大量采用

6、SVM的处理方法是只考虑 support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。

