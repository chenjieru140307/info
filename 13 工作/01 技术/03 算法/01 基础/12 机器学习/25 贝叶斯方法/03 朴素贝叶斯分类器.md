

# 朴素贝叶斯分类器

朴素贝叶斯 Naive Bayesian NB

## 朴素贝叶斯

缘由：

- 计算后验概率时，涉及计算 $P(\boldsymbol x|c)$。
  - 假设样本 $\boldsymbol{x}$ 包含 $d$ 个属性，即 $\boldsymbol{x}=\{ x_1,x_2,...,x_d\}$。
  - 于是有：$
P(\boldsymbol{x}|c_i)=P(x_1,x_2,\cdots,x_d|c_i)$
  - 这个联合概率难以从有限的训练样本中直接估计得到。
- 于是，朴素贝叶斯（Naive Bayesian，简称 NB）采用了“属性条件独立性假设”：对已知类别，假设所有属性相互独立。
  - 于是有：$P(x_1,x_2,\cdots,x_d|c_i)=\prod_{j=1}^d P(x_j|c_i)$
  - 此时，满足条件的 $h$ 为：
  - $h_{nb}(\boldsymbol{x})=\mathop{\arg \max}_{c_i\in Y} P(c_i)\prod_{j=1}^dP(x_j|c_i)$
    - nb 即 Naive Bayesian</span>



显然，朴素贝叶斯分类器的训练过程就是基于训练集 $D$ 来估计类先验概率 $P(c)$ ，并为每个属性估计条件概率 $P(x_i|c)$。

对于 $P(c)$：

- 令 $D_c$ 表示训练集 $D$ 中第 $c$ 类样本组成的集合，若有充足的独立同分布样本，则可容易地估计出类先验概率：

$$
P(c)=\frac{\left|D_{c}\right|}{|D|}\tag{7.16}
$$

对于 $P(x_i|c)$：

- 对离散属性
  - 令 $D_{c, x_{i}}$ 表示 $D_c$ 在第 i 个属性上取值为 $x_i$ 的样本组成的集合，则条件概率 $P(x_i|c)$ 可估计为

  $$
  P\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|}{\left|D_{c}\right|}\tag{7.17}
  $$
- 对连续属性
  - 可考虑概率密度函数，假定 $p\left(x_{i} | c\right) \sim \mathcal{N}\left(\mu_{c, i}, \sigma_{c, i}^{2}\right)$ ，其中 $\mu_{c,i}$ 和 $\sigma_{c, i}^{2}$ 分别是第 $c$ 类样本在第 $i$ 个属性上取值的均值和方差，则有

$$
p\left(x_{i} | c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp \left(-\frac{\left(x_{i}-\mu_{c, i}\right)^{2}}{2 \sigma_{c, i}^{2}}\right)\tag{7.18}
$$

注意：

- 对离散属性而言，$P_{(\boldsymbol x_{i}|c)}\in[0,1]$ 为条件概率。对连续属性而言，$p_{(\boldsymbol x_{i}| c)}$ 为条件概率密度而非概率，其值并不在局限于区间 $[0,1]$ 之内。






疑问：

- 朴素贝叶斯的 属性条件独立性假设怎么达成？需要在使用前用什么方法进行确认是否独立吗？
- 是不是用 PCA 或 LDA  去除线性相关属性之后就可以使用朴素贝叶斯来进行分类了？还说非线性相关的属性也要去掉？在真实工作中要怎么去掉这种相关的属性？怎么知道属性是独立的？

优点：

- 在数据较少的情况下仍然有效，
- 可以处理多类别问题。

缺点：

- 很多情况下，所有属性两两之间独立，这几乎是不可能的。
  - 举例：Y = 这个人是否是举重运动员。X1 = 性别，X2 = 这个人能否举起 100 公斤的箱子。此时，变量 X1 和 X2 显然不是独立的。
- 由于属性两两独立情况比较少，所以，朴素贝叶斯的预测精度往往不是很高。



应用：

- 中文分词
  - 分词后，得分的假设是基于两词之间是独立的，后词的出现与前词无关
- 统计机器翻译
  - 统计机器翻译因为其简单，无需手动添加规则，迅速成为了机器翻译的事实标准。
- 贝叶斯图像识别
  - 首先是视觉系统提取图形的边角特征，然后使用这些特征自底向上地激活高层的抽象概念，然后使用一个自顶向下的验证来比较到底哪个概念最佳地解释了观察到的图像。


使用方式：

- 在现实任务中朴素贝叶斯分类器有多种使用方式。
  - 例如，
    - 若任务对预测速 度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事先计算好存储起来，这样在进行预测时只需 “查表” 即可进行判别；
    - 若任务数据更替频繁，则可采用 “懒惰学习”(lazy learning)方式，先不进行任何训练，待收到预测请求时再根据当前数据集进行概率估值；
    - 若数据不断増加，则可在现有估值基础上，仅对新増样本的属性值所涉及的概率估值进行计数修正即可实现增量学习。


**举例：**

样本：

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度  | 含糖率 | 好瓜 |
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:-----:|:------:|:----:|
|  1   | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 | 0.460  |  是  |
|  2   | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.774 | 0.376  |  是  |
|  3   | 乌黑 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.634 | 0.264  |  是  |
|  4   | 青绿 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.608 | 0.318  |  是  |
|  5   | 浅白 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.556 | 0.215  |  是  |
|  6   | 青绿 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 0.403 | 0.237  |  是  |
|  7   | 乌黑 | 稍蜷 | 浊响 | 稍糊 | 稍凹 | 软粘 | 0.481 | 0.149  |  是  |
|  8   | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 | 0.437 | 0.211  |  是  |
|  9   | 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.666 | 0.091  |  否  |
|  10  | 青绿 | 硬挺 | 清脆 | 清晰 | 平坦 | 软粘 | 0.243 | 0.267  |  否  |
|  11  | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 0.245 | 0.057  |  否  |
|  12  | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 软粘 | 0.343 | 0.099  |  否  |
|  13  | 青绿 | 稍蜷 | 浊响 | 稍糊 | 凹陷 | 硬滑 | 0.639 | 0.161  |  否  |
|  14  | 浅白 | 稍蜷 | 沉闷 | 稍糊 | 凹陷 | 硬滑 | 0.657 | 0.198  |  否  |
|  15  | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 0.360 | 0.370  |  否  |
|  16  | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 硬滑 | 0.593 | 0.042  |  否  |
|  17  | 青绿 | 蜷缩 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.719 | 0.103  |  否  |

测试集：

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度  | 含糖率 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: | :----: | :--: |
| 测 1  | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 | 0.460  |  ？  |


过程：

- 计算类先验概率 $P(c_j)$：
  - $P_{好瓜=是}=\frac{8}{17}=0.471$
  - $PP_{好瓜=否}=\frac{9}{17}=0.529$
- 为每个属性估计条件概率。
  - 对于标签属性：
    - $P_{色泽=青绿|好瓜=是}=\frac{3}{8}=0.375$
    - $P_{色泽=青绿|好瓜=否}=\frac{3}{9}\approx0.333$
    - $P_{根蒂=蜷缩|好瓜=是}=\frac{5}{8}=0.625$
    - $P_{根蒂=蜷缩|好瓜=否}=\frac{3}{9}=0.333$
    - $P_{敲声=浊响|好瓜=是}=\frac{6}{8}=0.750$
    - $P_{敲声=浊响|好瓜=否}=\frac{4}{9}\approx 0.444$
    - $P_{纹理=清晰|好瓜=是}=\frac{7}{8}= 0.875$
    - $P_{纹理=清晰|好瓜=否}=\frac{2}{9}\approx 0.222$
    - $P_{脐部=凹陷|好瓜=是}=\frac{6}{8}= 0.750$
    - $P_{脐部=凹陷|好瓜=否}=\frac{2}{9} \approx 0.222$
    - $P_{触感=硬滑|好瓜=是}=\frac{6}{8}= 0.750$
    - $P_{触感=硬滑|好瓜=否}=\frac{6}{9} \approx 0.667$
  - 对于连续属性。（我们假定它们服从正态分布）
    - $\begin{aligned}\rho_{密度=0.697|好瓜=是}&=\frac{1}{\sqrt{2 \pi}\times0.129}exp\left( -\frac{(0.697-0.574)^2}{2\times0.129^2}\right) \\&\approx 1.959\end{aligned}$
    - $\begin{aligned}\rho_{密度=0.697|好瓜=否}&=\frac{1}{\sqrt{2 \pi}\times0.195}exp\left( -\frac{(0.697-0.496)^2}{2\times0.195^2}\right) \\&\approx 1.203\end{aligned}$
    - $\begin{aligned}\rho_{含糖=0.460|好瓜=是}&=\frac{1}{\sqrt{2 \pi}\times0.101}exp\left( -\frac{(0.460-0.279)^2}{2\times0.101^2}\right) \\&\approx 0.788\end{aligned}$
    - $\begin{aligned}\rho_{含糖=0.460|好瓜=否}&=\frac{1}{\sqrt{2 \pi}\times0.108}exp\left( -\frac{(0.460-0.154)^2}{2\times0.108^2}\right) \approx 0.066\end{aligned}$
- 于是有
  - $\begin{aligned}P_{测1好瓜是}=&P_{好瓜=是}\times P_{青绿|是} \times P_{蜷缩|是} \\&\times P_{浊响|是} \times P_{清晰|是} \times P_{凹陷|是} \times P_{硬滑|是} \\&\times p_{密度：0.697|是} \times p_{含糖：0.460|是} \\\approx& 0.063\end{aligned}$
  - $\begin{aligned}P_{测1好瓜否}=&P_{好瓜=否}\times P_{青绿|否} \times P_{蜷缩|否} \\&\times P_{浊响|否} \times P_{清晰|否} \times P_{凹陷|否}\times P_{硬滑|否} \\&\times p_{密度：0.697|否} \times p_{含糖：0.460|否} \\\approx& 6.80\times 10^{-5}\end{aligned}$
- 由于 $0.063>6.80\times 10^{-5}$，因此，朴素贝叶斯分类器将测试样本“测 1”判别为“好瓜”。


疑问：

- 上面的 $\rho_{密度：0.697|是}$ 里面的额 0.129 和 0.547 是哪里来的？怎么算的？还有 $\rho_{密度：0.697|否}$ 里面的 0.195 为什么不是 0.129？ 如果是从正太分布来的，那么是怎么基于这几个样本得到一个正态分布的？补充。

## 处理某个类别先验概率为 0 的情况



当，某个属性值在训练集中没有与某个类同时出现过，会出现问题。


比如：

- 上例中，对于 “敲声=清脆” 的测试例，有

$$
P_{\text {敲声=清脆|好瓜=是 }}=\frac{0}{8}=0
$$

- 由于的连乘式计算出的概率值为零，因此，无论该样本的其他属性是什么，哪怕在其他属性上明显像好瓜，分类的结果都将是 “好瓜=否”，这显然不太合理。

拉普拉斯修正：

- 为了避免其他属性携带的信息被训练集中未出现的属性值 “抹去”， 在估计概率值时通常要进行“平滑”(smoothing)，常用 “拉普拉斯修正” (Laplacian correction)。


具体来说：

- 令 $N$ 表示训练集 D 中可能的类别 数，$N_i$ 表示第 i 个属性可能的取值数，则

$$
P(c)=\frac{\left|D_{c}\right|}{|D|}\tag{7.16}
$$

$$
P\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|}{\left|D_{c}\right|}\tag{7.17}
$$

分别修正为：

$$
\hat{P}(c)=\frac{\left|D_{c}\right|+1}{|D|+N}\tag{7.19}
$$
$$
\hat{P}\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|+1}{\left|D_{c}\right|+N_{i}}\tag{7.20}
$$


注意：

- 有时仅对先验概率为 $0$ 的特征采用贝叶斯估计，一般情况下会对所有参与训练的特征都采用贝叶斯估计。（是这样吗？是会对所有参与训练的特征都采用贝叶斯估计吗？）



**举例：**

样本：

|A1|A2|A3|Y|
|:---:|:---:|:---:|:---:|
|1|1|0|1|
|0|1|1|1|
|1|0|1|0|
|0|1|0|0|
|0|0|1|0|

预测：

|A1|A2|A3|Y|
|:---:|:---:|:---:|:---:|
|1|0|0|?|

此时：

- 计算类先验概率 $P(c_j)$：
  - $P_{Y=1}=2/5$
  - $P_{Y=0}=3/5$
- 为每个属性估计条件概率。
  - A1：
    - $P_{A1=1|Y=1}=1/2$
    - $P_{A1=0|Y=1}=1/2$
    - $P_{A1=1|Y=0}=1/3$
    - $P_{A1=0|Y=0}=2/3$
  - A2：
    - $P_{A2=1|Y=1}=2/2$
      - 进行拉普拉斯平滑，即 $\lambda =1$。由于此时，$O_j=2$，得：
      - $P_{A2=1|Y=1}=\frac{2+1}{2+2*1}=3/4$
    - $P_{A2=0|Y=1}=0/2$
      - 进行拉普拉斯平滑，即 $\lambda =1$。由于此时，$O_j=2$，得：
      - $P_{A2=0|Y=1}=\frac{0+1}{2+2*1}=1/4$
    - $P_{A2=1|Y=0}=1/3$
    - $P_{A2=0|Y=0}=2/3$
  - A3：
    - $P_{A3=1|Y=1}=1/2$
    - $P_{A3=0|Y=1}=1/2$
    - $P_{A3=1|Y=0}=2/3$
    - $P_{A3=0|Y=0}=1/3$
- 此时：
  - $P_{测=1}=\frac{2}{5}\times\frac{1}{2}\times\frac{1}{4}\times\frac{1}{2}=1/40$
  - $P_{测=0}=\frac{3}{5}\times\frac{1}{3}\times\frac{2}{3}\times\frac{1}{3}=2/45$
- 因此，测试样本为类别为 0。

注意：

- 拉普拉斯修正实质上假设了属性值与类别均匀分布，这是在朴素贝叶斯学习过程中额外引入的关于数据的先验。