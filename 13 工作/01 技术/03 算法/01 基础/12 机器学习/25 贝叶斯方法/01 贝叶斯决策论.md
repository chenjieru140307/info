

# 贝叶斯决策论

贝叶斯决策论 Bayesian decision theory


贝叶斯决策论：

- 是概率框架下实施决策的基本方法。
  - 对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。

下面我们以多分类任务为例来解释其基本原理。

贝叶斯判定准则：

- 为最小化总体风险，只需要在每个样本上选择哪个能使条件风险 $R(c | \boldsymbol{x})$ 最小的类别标记

由来：

- 假设有 $N$ 种可能的类别标记，即 $\mathcal{Y}=\left\{c_{1}, c_{2}, \ldots, c_{N}\right\}$ 
- 使用 $\lambda_{ij}$ 来表示，如果将一个真实的被标记为 $c_j$ 的样本 误分类为 $c_i$，所产生的损失。
- 基于后验概率 $P\left(c_{i} | \boldsymbol{x}\right)$ 可获得将样本 $\boldsymbol{x}$ 分类为 $c_i$ 所产生的期望损失(expected loss)，即在样本 $\boldsymbol{x}$ 上的“条件风险” (conditional risk)
  - 最小化误差，也就是最大化 $P(c|\boldsymbol{x})$，但由于 $P(c|\boldsymbol{x})$ 属于后验概率，所以无法直接计算。<span style="color:red;">这个地方由疑问，到底 R 是什么？是误差吗？那么 $P\left(c_{j} | \boldsymbol{x}\right)$ 是什么？是正确推断出类别的概率吗？那么 $\lambda_{i j}$ 是正还是负数？为什么有的地方使用 $h(\boldsymbol{x})=\mathop{\arg \max}_{c_i\in Y} P(c_i)\prod_{j=1}^dP(x_j|c_i)$ 呢？是 max 还是 min？</span>

$$
R\left(c_{i} | \boldsymbol{x}\right)=\sum_{j=1}^{N} \lambda_{i j} P\left(c_{j} | \boldsymbol{x}\right)
$$

- 因此，我们想寻找的是这样一个判断准则 $h : \mathcal{X} \mapsto \mathcal{Y}$ ，来使得上面的总体风险最小：

$$
R(h)=\mathbb{E}_{\boldsymbol{x}}[R(h(\boldsymbol{x}) | \boldsymbol{x})]
$$

- 这时候，其实，对每个样本 $\boldsymbol{x}$ ，若 $h$ 能最小化风险 $R(h(\boldsymbol{x}) | \boldsymbol{x})$ ，则总体风险 $R(h)$ 也将被最小化。
- 这时，就产生了 贝叶斯判定准则（Bayes decision rule）：
  - 为最小化总体风险，只需要在每个样本上选择哪个能使条件风险 $R(c | \boldsymbol{x})$ 最小的类别标记，即：

$$
h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \min } R(c | \boldsymbol{x})
$$

$$
h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c | \boldsymbol{x})
$$

说明：

- $h^*$ 称为贝叶斯最优分类器(Bayes optimal classifier)
- 与之对应的总体风险 $R(h^*)$ 称为贝叶斯风险(Bayes risk)
- $1-R(h^*)$ 反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限

举例：

- 如果我们的目标是最小化分类错误率
- 这时，误判损失 $\lambda_{ij}$ 可写为：

$$
\lambda_{i j}=\left\{\begin{array}{ll}{0,} & {\text { if } i=j} \\ {1,} & {\text { otherwise }}\end{array}\right.
$$

- 将 $R\left(c_{i} | \boldsymbol{x}\right)=\sum_{j=1}^{N} \lambda_{i j} P\left(c_{j} | \boldsymbol{x}\right)$ 带入上式，得到条件风险：

$$
\begin{aligned}
R(c_i|\boldsymbol x)=&1*P(c_1|\boldsymbol x)+1*P(c_2|\boldsymbol x)+...
\\&+0*P(c_i|\boldsymbol x)+...+1*P(c_N|\boldsymbol x)
\\=&\sum_{j=1}^{N}P(c_j|\boldsymbol x)-P(c_i|\boldsymbol x)
\\=&1−P(c_i|\boldsymbol x)
\end{aligned}
$$

- 此时，最小化分类错误率的贝叶斯最优分类器为：
  - 即对每个样本 $\boldsymbol{x}$，选择能使后验概率 $P(c | \boldsymbol{x})$ 最大的类别标记.


$$
h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c | \boldsymbol{x})
$$


注：

- 可见，要想使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率 $P(c | \boldsymbol{x})$
- 注意，这只是从概率框架的角度来理解机器学习 事实上很多机器学习技术 无须准确估计出后验概率、就能准确进行分类。（没有很清楚）
