
# EM 算法


在前面的讨论中，我们一直假设训练样本所有属性变量的值都已被观测到, 即训练样本是 “完整” 的。但在现实应用中往往会遇到 “不完整” 的训练样本，例如由于西瓜的根蒂已脱落，无法看出是 “蜷缩” 还是 “硬挺”，则训练样本的 “根蒂” 属性变量值未知。在这种存在 “未观测” 变量的情形下，是否仍能对模型参数进行估计呢？

未观测变量的学名是 “隐变量”(latent variable)。令 $\mathbf{X}$ 表示已观测变量集，$\mathbf{Z}$ 表示隐变量集，$\Theta$ 表示模型参数。若欲对 $\Theta$ 做极大似然估计，则应最大化 对数似然

$$
L L(\Theta | \mathbf{X}, \mathbf{Z})=\ln P(\mathbf{X}, \mathbf{Z} | \Theta)\tag{7.34}
$$

然而由于 $\mathbf{Z}$ 是隐变量，上式无法直接求解。此时我们可通过对 $\mathbf{Z}$ 计算期望，来最大化已观测数据的对数 “边际似然” (marginal likelihood)：

$$
L L(\Theta | \mathbf{X})=\ln P(\mathbf{X} | \Theta)=\ln \sum_{\mathbf{z}} P(\mathbf{X}, \mathbf{Z} | \Theta)\tag{7.35}
$$

EM (Expectation-Maximization)算法是常用的估计参数隐变量的利器，它是一种迭代式的方法，其基本想法是：若参数 $\Theta$ 已知，则可根据训练数据推断出最优隐变量 $\mathbf{Z}$ 的值( $E$ 步)；反之，若 $\mathbf{Z}$ 的值已知，则可方便地对参数 $\Theta$ 做极大似然估计( $M$ 步)。

于是，以初始值 $\Theta^{0}$ 为起点，对式(7.35)，可迭代执行以下步骤直至收敛：

- 基于 $\Theta^t$ 推断隐变量 $\mathbf{Z}$ 的期望，记为 $\mathbf{Z}^{t}$
- 基于已观测变量 $\mathbf{X}$ 和 $\mathbf{Z}^{t}$ 对参数 $\Theta$ 做极大似然估计，记为 $\Theta^{t+1}$

这就是 EM 算法的原型.

进一步，若我们不是取 $\mathbf{Z}$ 的期望，而是基于 $\Theta^t$ 计算隐变量 $\mathbf{Z}$ 的概率分布 $P(Z|X,\Theta^t)$ ，则 EM 算法的两个步骤是：

- $\mathbf{E}$ 步(Expectation):以当前参数 $\Theta^t$ 推断隐变量分布 $P\left(\mathbf{Z} | \mathbf{X}, \Theta^{t}\right)$ ，并计算对数似然 $L L(\Theta | \mathbf{X}, \mathbf{Z})$ 关于 $\mathbf{Z}$ 的期望:

$$
Q\left(\Theta | \Theta^{t}\right)=\mathbb{E}_{\mathbf{Z} | \mathbf{X}, \Theta^{t}} L L(\Theta | \mathbf{X}, \mathbf{Z})\tag{7.36}
$$

- $\mathbf{M}$ 步(Maximization):寻找参数最大化期望似然，即

$$
\Theta^{t+1}=\underset{\Theta}{\arg \max } Q\left(\Theta | \Theta^{t}\right)\tag{7.37}
$$


简要来说，EM 算法使用两个步骤交替计算：第一步是期望(E)步，利用当 前估计的参数值来计算对数似然的期望值；第二步是最大化(M)步，寻找能使 E步产生的似然期望最大化的参数值。然后，新得到的参数值重新被用于 E 步，……直至收敛到局部最优解.

事实上，隐变量估计问题也可通过梯度下降等优化算法求解，但由于求和的项数将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而 EM 算法则可看作一种非梯度优化方法。





# 相关

- 《机器学习》周志华
