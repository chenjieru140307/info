

# 基于有限样本估计后验概率

如何基于有限的样本集，尽可能准确地估计出后验概率 $P(c | \boldsymbol{x})$ ：也即：

- 给定 $\boldsymbol{x}$ 可通过直接建模  $P(c | \boldsymbol{x})$ 来 预测 $c$
  - 这样得到的是判别式模型 (discriminative models);
    - 决策树、BP神经网络、支持向量机等，都可归入判别式模型的范畴。
- 也可先对联合概率分布  $P(\boldsymbol{x}, c)$ 建模，然后再由此获得 $P(c | \boldsymbol{x})$
  - 这样得到的是生成式模型(generative models)。


对于生成式模型:

- 对生成式模型来说，必然考虑：

$$
P(c | \boldsymbol{x})=\frac{P(\boldsymbol{x}, c)}{P(\boldsymbol{x})}
$$

- 基于贝叶斯定理，$P(c | \boldsymbol{x})$ 可写为：

$$
P(c | \boldsymbol{x})=\frac{P(c) P(\boldsymbol{x} | c)}{P(\boldsymbol{x})}
$$

- 此时：
  - $P(\boldsymbol x)$ $P(\boldsymbol{x})$ 是 用于归一化的证据(evidence)因子。对给定样本 $x$，证据因子 $P(\boldsymbol{x})$ 与类标记无关，因此 可以省略。
  - $P(c)$ 为先验概率。
  - $P(\boldsymbol x|c)$ 称为似然。
- 也即：估计 $P(c | \boldsymbol{x})$ 的问题，转化为，基于训练数据 $D$ 来估计先验 $P(c)$ 和似然  $P(c | \boldsymbol{x})$ 。
  - $P(c)$。根据大数定律，当样本量到了一定程度且服从独立同分布，$c$ 的出现的频率就是 $c$ 的概率。（如果不是独立同分布怎么办？怎么确定是不是？）
  - $P(\boldsymbol x|c)$。
    - 因为样本 $\boldsymbol{x}$ 包含 $n$ 个属性，这 $n$ 个属性的联合概率难以从有限的训练样本中直接估计得到。于是，朴素贝叶斯（Naive Bayesian）采用了 属性条件独立性假设：对已知类别，假设所有属性相互独立。
      - 基于该假设，可将 $P(\boldsymbol x|c)$ 转化为：
      - $P(\boldsymbol x|c)=P(x_1,x_2,\cdots,x_d|c)=\prod_{i=1}^{n} P\left(x_{i} | c \right)$。
    - 当样本属性独依赖时，也就是除了 $c$ 多加一个依赖条件，式子变成了：
      - $\prod_{i=1}^{n} P\left(x_{i} | c, p_{i}\right)$
        - $p_i$ 是 $x_i$ 所依赖的属性。（怎么知道是依赖哪个属性呢？）
    - 当样本属性相关性未知时，我们采用贝叶斯网的算法，对相关性进行评估，以找出一个最佳的分类模型。（贝叶斯网）
    - 当遇到不完整的训练样本时，可通过使用 EM 算法对模型参数进行评估来解决。（EM 算法怎么对应不完整样本？）

