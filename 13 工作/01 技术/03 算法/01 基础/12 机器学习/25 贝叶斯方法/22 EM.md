
# EM算法


- EM 算法和聚类很相似，都是解决无标记样本的分类问题的。是这样吗？


# EM 算法主要内容


期望最大化算法 Expection Maximium

- 通过实例直观求解高斯混合模型 GMM
  - 适合快速掌握 GMM，及编程实现
- 通过极大似然估计详细推导 EM 算法
  - 适合理论层面的深入理解
  - 用坐标上升理解 EM 的过程
- 推导 GMM 的参数φ、μ、σ
  - 复习多元高斯模型
  - 复习拉格朗日乘子法





## 极大似然估计


极大似然估计：

$$\mu=\frac{1}{n}\sum_{i} x_i$$

$$\sigma^2=\frac{1}{n}\sum_{i} (x_i-\mu)^2$$


- 即可以通过极大似然估计对正态分布的参数进行估计：
  - 用样本的均值作为这个高斯分布的均值，样本的伪方差作为高斯分布的方差。





# 高斯混合模型


问题：随机变量无法直接(完全)观察到


- 随机挑选10000位志愿者，测量他们 的身 高：若样本中存在男性和女性，身高分别服 从N( $\left.\mu_{1}, \sigma_{1}\right)$ 和N $\left(\mu_{2}, \sigma_{2}\right)$ 的分布，试估 $\mu_{1}, \sigma_{1}, \mu_{2}, \sigma_{2}$
- 给定一幅图像，将图像的前景背景分开
- 无监督分类： 聚类/EM

也就是说：现在我们手头上只拿到了样本的高度，没有拿到样本的性别。这种情况就是不完全观测的。


GMM 要研究的问题：

- 随机变量 $X$ 是有 $K$ 个高斯分布混合而成，取各个高斯分布的概率为 $\pi_{1} \pi_{2} \ldots \pi_{{K}}$
- 第 ${i}$ 个高斯分布的均值为 $\mu_{{i}},$ 方差为 $\Sigma_{{i}}$。
- 若观侧到随机变量X的一换列样本 ${x}_{1}, {x}_{2}, \ldots, {x}_{{n}}$，试估计参数 $\pi, \mu, \Sigma$。


这个就是高斯混合模型要研究的内容，手段就是 EM 算法。

做法：

- 建立一个目标函数：对数似然函数

$$l_{\pi, \mu, \Sigma}(x)=\sum_{i=1}^{N} \log \left(\sum_{k=1}^{K} \pi_{k} N\left(x_{i} | \mu_{k}, \Sigma_{k}\right)\right)$$


- 说明：
  - $N\left(x_{i} | \mu_{k}, \Sigma_{k}\right)$ 就是那个正态分布。这个是第 k 个高斯分布。
  - $\left(\sum_{k=1}^{K} \pi_{k} N\left(x_{i} | \mu_{k}, \Sigma_{k}\right)\right)$ 就是一个混合的高斯分布。
  - $\sum_{i=1}^{N} \log \left(\sum_{k=1}^{K} \pi_{k} N\left(x_{i} | \mu_{k}, \Sigma_{k}\right)\right)$ 。对这样的混合高斯分布求极大似然估计，就可以先乘起来，然后取对数，就相当于先取对数再求和。
  - $l_{\pi, \mu, \Sigma}(x)$ 由于这是一个对数似然，所以是小写的 $l$
- 注意：
  - 这个地方强调一下：似然函数只有在指数族分布的情况下才能得到全局最优，而 GMM 并不是，他是有多个极值点的。
- 怎么求这个目标函数为最大值时候的参数呢？
  - 由于在对数函数里面又有加和，我们没法直接用求导解方程的办法直接求得极大值。为了解决这个问题，我们分成两步。
- 第一步：估算数据来自哪个组份
  - 对于每个样本 ${x}_{{i}}, \quad$ 它由第 $k$ 个组份生成的概率为
    $$
    \gamma(i, k)=\frac{\pi_{k} N\left(x_{i} | \mu_{k}, \Sigma_{k}\right)}{\sum_{j=1}^{K} \pi_{j} N\left(x_{i} | \mu_{j}, \Sigma_{j}\right)}
    $$
  - 说明：
    - $N(x_i|\mu_k,\Sigma _k)$ 的意思是：对于第 $k$ 个高斯分布来说，如果我们已经知道了 $\mu$ ，$\Sigma$，那么我带入 $x$ ，总是能够求出这个高斯分布在 $x$ 处的概率密度。
    - $\pi$指的是：
      - 比如，我有 2000 个学生，其中 1200 个男生，800个女生，男生和女生的身高都满足各自的高斯分布。那么这个男生的占比 $\pi$ 男。
      - 注意：这个占比与一个学生是男生的概率是不同的。比如：我随便拿出一个学生，这个学生是男生的概率$\pi$就是 60%。但是，如果我拿出一个学生，而且知道这个学生的身高是 2.2m，那么也许就不能认为他是男生的概率是 60%了，因为男生和女生的身高都满足各自的高斯分布，有可能这个身高是男生的概率是 90%。
      - 所以，这个$\pi$只是我字面上的男生的数量占比，而并不能作为是男生的概率。当然了，因为我事先并不知道学生中男生有多少，女生有多少，所以这个$\pi$还是一个先验概率，在现在是不知道的。
      - 那么 $\pi_kN(x_i|\mu_k,\Sigma _k)$ 乘起来是什么意思呢？就是随便拿出一个学生，男生的占比是$\pi$，而且他在男生里面这个身高出现的概率是$N(x_i|\mu_k,\Sigma _k)$，所以，这个$\pi_kN(x_i|\mu_k,\Sigma _k)$ 就是这个学生是男生的概率。
    - $\Sigma_{j=1}^{K}\pi_jN(x_i\mid \mu_j,\Sigma_j)$这个只是为了做归一化。因为对所有的$\pi$加起来是 1，对所有的$\pi_kN(x_i\mid \mu_k,\Sigma_k)$加起来就不一定是 1 了，所以做一个归一化。
    - 从上面的公式可以知道，假定我知道$\pi$，$\mu$，$\Sigma$，那么我是可以算出$\gamma(i,k)$的，但是现在我都不知道，那么我可以吧所有的 $k$ 套的参数都猜出来。这样就能吧$\gamma(i,k)$给算出来。
  - 上式中的 $\mu$ 和 $\Sigma$ 也是待估计的值，因此采用达代法：在计算 $\gamma(i,k)$ 时假定 $\mu$ 和 $\Sigma$ 已知：
    - 需要先验给定 $\mu$ 和 $\Sigma$。
    - $\gamma({i}, {k})$ 亦可看成组份 $k$ 在生成数据 ${x}_{{i}}$ 时所做的贡献。
- 第二步：估计每个组份的参数
  - 对于所有的样本点，对于组份 $k$ 而言，可看做生成了 $\left\{\gamma(i, k) x_{i} | i=1,2, \cdots N\right\}$ 这些点。组份 ${k}$ 是一个标淮的高斯分布，利用：
    $$\left\{\begin{array}{l}
    \mu=\frac{1}{n} \sum_{i} x_{i} \\
    \sigma^{2}=\frac{1}{n} \sum_{i}\left(x_{i}-\mu\right)^{2}
    \end{array}\right.$$
  - 得到：
    $$\left\{\begin{array}{l}
    N_{k}=\sum_{i=1}^{N} \gamma(i, k) \\
    \mu_{k}=\frac{1}{N_{k}} \sum_{i=1}^{N} \gamma(i, k) x_{i} \\
    \Sigma_{k}=\frac{1}{N_{k}} \sum_{i=1}^{N} \gamma(i, k)\left(x_{i}-\mu_{k}\right)\left(x_{i}-\mu_{k}\right)^{T} \\
    \pi_{k}=\frac{N_{k}}{N}=\frac{1}{N} \sum_{i=1}^{N} \gamma(i, k)
    \end{array}\right.$$

- 所以这里我就得到了 k 组的三个值，那么这时候，我把这些值代入到：$\gamma(i,k)=\frac{\pi_kN(x_k\mid \mu_k,\Sigma_k)}{\sum_{j=1}^{K}\pi_jN(x_i\mid \mu_k,\Sigma_k)}$ 里面，就可以对之前由随便猜的值算出来的 $\gamma(I,k) 进行更新。


说明：

- 初值很极端的情况下，会收敛吗？
  - 注意：如果给的是一个很差的初值，那么就只能得到一个很差的结论，也就是说 EM 算法只是局部最优，不是全局最优。顺便提一下。K-means模型也是初值敏感的。



# EM 算法

EM 算法怎么做的：

- 假定有训练集 $\left\{x^{(1)}, \ldots, x^{(m)}\right\}$，包含 $m$ 个独立样本。
- 希望从中找到该组数据的模型 ${p}({x}, {z})$ 的参数。
  - 想得出样本的概率是 $p(x)$，但是有隐变量 $z$。所以我们希望得到带隐变量的概率分布。
  - 比如：对于上面举的身高的例子而言，x是身高，z是性别

过程：

- 同样的，通过极大似然估计建立目标函数
- 对数似然函数为：

$$\begin{aligned}l(\theta)&=\sum_{i=1}^{m}log\,p(x;\theta)\\&=\sum_{i=1}^{m}log\sum_{z}p(x,z;\theta)\end{aligned}$$

- 说明：
  - $\sum_{z}^{ } p(x,z;\theta)$ 这个就把 $z$ 强制暴露了出来，用 x 和 z的联合分布对 z 求积分。现在我们就开始想办法得到这个函数的极大值。
- 求这个函数的极大值
  - 对于上面这个式子而言，z是隐随机变量，直接找到参数的估计是很困难的。那么怎么办呢？
  - 如果 $l(\theta)$ 严格大于某个函数，并且我们方便求出那个函数的极大值，那么就很好了，我们就可以用那个函数的极大值代替 $l(\theta)$ 这个函数的极大值。
  - 我们首先假设 $z$ 的分布是 $Q_i$。那么$Q_i\geq 0$。那么：

    $$\begin{aligned}\sum_{i} log\,p(x^{(i)};\theta)&=\sum_{i}log\sum_{z^{(i)} }p(x^{(i)},z^{(i)};\theta)\\&=\sum_{i}log\sum_{z^{(i)} } Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\\&\geq \sum_{i}\sum_{z^{(i)} } Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\end{aligned}$$

  - 说明：
    - 第二行到第三行：
      - 这个$log\sum_{z^{(i)} } Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$其实可以看成是一个 log 函数，里面是 $Ex$，那么 $x$ 是什么呢？就是$\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$。
      - 也就是说这个式子可以看成是$f(Ex)$，那么由 Jensen 公式：对于任意一个凸函数而言：$f(Ex)\leq Ef(x)$。我们自然可以想到它对应的 $Ef(x)$，写出来是：$\sum_{z^{(i)} } Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$，正是上面的最后一个式子。
      - 这个时候要注意，Jensen 不等式的前提条件是 $f(x)$ 是一个凸函数，而这个地方的 $log$ 却是一个凹函数。所以上面的最后一个就不是小于等于，而是大于等于。
  - 这个时候我们就已经得到了一个比它小的函数。而我们之前想做的是什么呢？是相求 Lagrange 函数的极大值，那么现在由一个函数小于等于这个 Lagrange 函数。所以我们不仅仅想求这个函数的极大值，我们还想让这个函数尽可能的大，最好是与原来的 Lagrange 函数相等。这样我求得的这个函数的极大值就与原函数基本差不多了。
    - 也就是说：我们需要使这个下界函数尽可能的大
  - 下界函数最大能多大呢？
    - 大于等于符号取等于的时候最大。那么这个地方取等于是什么情况？

    <p align="center">
        <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/6e3AbJjIB9.png?imageslim">
    </p>

    - 如图，$f(Ex)\leq Ef(x)$ 在什么情况下取等于？
      - 或者是线段在无穷远处。
      - 或者是线段退化成一个点。
    - 当线段蜕化成一个点的时候意味着什么呢？
      - 意味着 $x$，也就是 $\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$ 一直不变，是一个常数。
      - 即两个函数成正比：$Q_i(z^{(i)})\propto p(x^{(i)},z^{(i)};\theta)$
      - 而，对于$Q_i$这个隐变量的概率分布来说：$\sum_{z} Q_i(z^{(i)})=1$
      - 那么也就是说：我只要对$p(x^{(i)},z^{(i)};\theta)$进行归一化，那么它就等于$Q_i(z^{(i)})$：

        $$\begin{aligned} Q_i(z^{(i)})&=\frac{p(x^{(i)},z^{(i)};\theta)}{\sum_{z}p(x^{(i)},z;\theta)}\\&=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}\\&=p(z^{(i)}\mid x^{(i)};\theta)\end{aligned}$$
      - 最后的结果是 $x$ 给定的时候 $z$ 的条件概率。所以，当我的 $Q_i(z^{(i)})$ 为这么一个概率的时候，我的式子就可以为等号。
      - 所以， $Q_i(z^{(i)})$ 就是样本给定的时候隐变量的条件分布。
  - 而上面说的这个用给定样本的条件分布对样本值求期望，这个 就是 E 的做法，也就是 EM 算法的第一步。
- OK，剩下的就是把刚才求得的 $Q_i(z^{(i)})$ 带入到$\sum_{i}\sum_{z^{(i)} } Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$来使它最大，并求出最大时候对应的$\theta$。
  - OK，这一步就是 EM 算法的 M。也就是第二步。


EM 算法流程：


- Repeat until convergence：
  - (E-step) For each $i,$ set
    $$
    Q_{i}\left(z^{(i)}\right):=p\left(z^{(i)} | x^{(i)} ; \theta\right)
    $$
  - (M-step) Set
    $$
    \theta:=\arg \max _{\theta} \sum_{i} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}
    $$
- end


即：

- 先写出给定样本的时候隐变量的条件分布，然后将 $Q$ 带入这个式子中求出他的极大值。不管用任何一种求极值的方法。
- 当你求出了 $\theta$ 的时候，你把 $\theta$ 带回到第一步就能求出这个 $Q$ ，新的 $Q$ 得到了，再带入第二步。不断循环。
- 最终会收敛，这个最终的 就可以算出来。



可以把这个一个过程看作使坐标上升:

Remark. If we define
$$
J(Q, \theta)=\sum_{i} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}
$$
the we know $\ell(\theta) \geq J(Q, \theta)$ from our previous derivation. The EM can also be viewed a coordinate ascent on $J,$ in which the E-step maximizes it with respect to $Q,$ and the M-step maximizes it with respect to $\theta$


## 从理论公式推导 GMM

- 随机变量 $X$ 是有 $K$ 个高斯分布混合而成，取各个高斯分布的概率为 $\pi_{1} \pi_{2} \ldots \pi_{{K}}$
- 第 ${i}$ 个高斯分布的均值为 $\mu_{{i}},$ 方差为 $\Sigma_{{i}}$。
- 若观侧到随机变量X的一换列样本 ${x}_{1}, {x}_{2}, \ldots, {x}_{{n}}$，试估计参数 $\pi, \mu, \Sigma$。


E-step：

$$w_{j}^{(i)}=Q_{i}\left(z^{(i)}=j\right)=P\left(z^{(i)}=j | x^{(i)} ; \phi, \mu, \Sigma\right)$$

M-step

将多项分布和高斯分布的参数带入：

$$\begin{array}{l}
\sum_{i=1}^{m} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \phi, \mu, \Sigma\right)}{Q_{i}\left(z^{(i)}\right)} \\
\quad=\sum_{i=1}^{m} \sum_{j=1}^{k} Q_{i}\left(z^{(i)}=j\right) \log \frac{p\left(x^{(i)} | z^{(i)}=j ; \mu, \Sigma\right) p\left(z^{(i)}=j ; \phi\right)}{Q_{i}\left(z^{(i)}=j\right)} \\
\quad=\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \log \frac{\frac{1}{(2 \pi)^{n / 2}\left|\Sigma_{j}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x^{(i)}-\mu_{j}\right)^{T} \Sigma_{j}^{-1}\left(x^{(i)}-\mu_{j}\right)\right) \cdot \phi_{j}}{w_{j}^{(i)}}
\end{array}$$

对均值求偏导：


$$\begin{array}{l}
\nabla_{\mu_{l}} \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \log \frac{\frac{1}{(2 \pi)^{n / 2}\left|\Sigma_{j}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x^{(i)}-\mu_{j}\right)^{T} \Sigma_{j}^{-1}\left(x^{(i)}-\mu_{j}\right)\right) \cdot \phi_{j}}{w_{j}^{(i)}} 
\\\quad=-\nabla_{\mu_{l}} \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \frac{1}{2}\left(x^{(i)}-\mu_{j}\right)^{T} \Sigma_{j}^{-1}\left(x^{(i)}-\mu_{j}\right) 
\\\quad=\frac{1}{2} \sum_{i=1}^{m} w_{l}^{(i)} \nabla_{\mu_{l}} 2 \mu_{l}^{T} \Sigma_{l}^{-1} x^{(i)}-\mu_{l}^{T} \Sigma_{l}^{-1} \mu_{l} 
\\\quad=\sum_{i=1}^{m} w_{l}^{(i)}\left(\Sigma_{l}^{-1} x^{(i)}-\Sigma_{l}^{-1} \mu_{l}\right)
\end{array}$$


令上式等于 0，解的均值：


$$\mu_{l}:=\frac{\sum_{i=1}^{m} w_{l}^{(i)} x^{(i)}}{\sum_{i=1}^{m} w_{l}^{(i)}}$$

高斯分布的方差：求偏导，等于 0

$$\Sigma_{j}=\frac{\sum_{i=1}^{m} w_{j}^{(i)}\left(x^{(i)}-\mu_{j}\right)\left(x^{(i)}-\mu_{j}\right)^{T}}{\sum_{i=1}^{m} w_{j}^{(i)}}$$

## 多项分布的参数

考察 M-step 的目标函数，对于 $\phi$， 删除常数项：

$$\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \log \frac{\frac{1}{(2 \pi)^{n / 2}\left|\Sigma_{j}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x^{(i)}-\mu_{j}\right)^{T} \Sigma_{j}^{-1}\left(x^{(i)}-\mu_{j}\right)\right) \cdot \phi_{j}}{w_{j}^{(i)}}$$

得到：

$$\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \log \phi_{j}$$

拉格朗日乘子法：由于 多项分布的概率和为1， 建立拉格朗日方程

$$
\mathcal{L}(\phi)=\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \log \phi_{j}+\beta\left(\sum_{j=1}^{k} \phi_{j}-1\right)
$$

注： 这样求解 的 $\phi i$ 一定非负，所以，不用考虑 $\phi i \geqslant 0$ 这个条件


求偏导，等于 0：


$$\begin{array}{c}
\frac{\partial}{\partial \phi_{j}} \mathcal{L}(\phi)=\sum_{i=1}^{m} \frac{w_{j}^{(i)}}{\phi_{j}}+\beta \\
-\beta=\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)}=\sum_{i=1}^{m} 1=m \\
\phi_{j}:=\frac{1}{m} \sum_{i=1}^{m} w_{j}^{(i)}
\end{array}$$

总结

$$\left\{\begin{array}{l}
N_{k}=\sum_{i=1}^{N} \gamma(i, k) \\
\mu_{k}=\frac{1}{N_{k}} \sum_{i=1}^{N} \gamma(i, k) x_{i} \\
\Sigma_{k}=\frac{1}{N_{k}} \sum_{i=1}^{N} \gamma(i, k)\left(x_{i}-\mu_{k}\right)\left(x_{i}-\mu_{k}\right)^{T} \\
\pi_{k}=\frac{N_{k}}{N}=\frac{1}{N} \sum_{i=1}^{N} \gamma(i, k)
\end{array}\right.$$







# pLSA 模型



缘由：

- 朴素贝叶斯可以胜任许多文本分类问题。不过，无法解决语料中一词多义和多词一义的问题——它更像是词法分析，而非语义分析。如果使用词向量作为文档的特征，一词多义和多词一义会造成计算文档间相似度的不准确性。
- 可以通过增加“主题”的方式，一定程度的解决上述问题：
  - 一个词可能被映射到多个主题中：一词多义
  - 多个词可能被映射到某个主题的概率很高：多词一义

pLSA

- 基于概率统计的 pLSA 模型(probabilistic Latent Semantic Analysis，概率隐语义分析)，增加了主题模型，形成简单的贝叶斯网络，可以使用 EM 算法学习模型参数。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/BD5ghJKB8K.png?imageslim">
</p>

说明：

- $D$ 代表文档，$Z$ 代表主题(隐含类别)，$W$ 代表单词
  - $P(d_i)$ 表示文档 $d_i$ 的出现概率，
  - $P(z_k|d_i)$ 表示文档 $d_i$ 中主题 $z_k$ 的出现概率。
  - $P(w_j|z_k)$ 表示给定主题 $z_k$ 出现单词 $w_j$ 的概率。
- 每个主题在所有词项上服从多项分布，每个文档在所有主题上服从多项分布。
- 整个文档的生成过程是这样的：
  - 以 $P(d_i)$ 的概率选中文档 $d_i$
  - 以 $P(z_k|d_i)$ 的概率选中主题 $z_k$
  - 以 $P(w_j|z_k)$ 的概率产生一个单词 $w_j$。

此时：

- 观察数据为 $(d_i,w_j)$ 对， 主题 $z_k$ 是隐含变量
- $(d_i,w_j)$ 的联合分布为：
    $$
    \begin{array}{l}
    P\left(d_{i}, w_{j}\right)=P\left(w_{j} | d_{i}\right) P\left(d_{i}\right) \\
    P\left(w_{j} | d_{i}\right)=\sum_{k=1}^{K} P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right)
    \end{array}
    $$
- 而 $P\left(w_{j} | z_{k}\right)$, $P\left(z_{k} | d_{i}\right)$ 对应两组多项分布，而计算每个文档的主题分布，就是该模型的任务目 标。

使用极大似然估计来求 $w_{j}$ 在 $d_{i}$ 中出现的次数 $n\left(d_{i}, w_{j}\right)$：


$$L=\prod_{i=1}^{N} \prod_{j=1}^{M} P\left(d_{i}, w_{j}\right)=\prod_{i} \prod_{j} P\left(d_{i}, w_{j}\right)^{n\left(d_{i}, w_{j}\right)}$$

$$\begin{array}{l}
l=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log P\left(d_{i}, w_{j}\right) \\
=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log P\left(w_{j} | d_{i}\right) P\left(d_{i}\right) \\
=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log \left(\sum_{k=1}^{K} P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right)\right) P\left(d_{i}\right) \\
=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log \left(\sum_{k=1}^{K} P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right) P\left(d_{i}\right)\right)
\end{array}$$

说明:

- 第一行到第二行：$P\left(d_{i}, w_{j}\right)=P\left(w_{j} | d_{i}\right) P\left(d_{i}\right)$
- 第二行到第三行：$P\left(w_{j} | d_{i}\right)=\sum_{k=1}^{K} P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right)$


目标函数分析：

- 观察数据为 $\left(d_{i}, w_{j}\right)$ 对，主题 $z_{k}$ 是隐含变量。
- 目标函数：$l=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log \left(\sum_{k=1}^{K} P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right) P\left(d_{i}\right)\right)$
- 未知变量/自变量 $P\left(w_{j} | z_{k}\right), P\left(z_{k} | d_{i}\right)$
- 使用逐次逼近的办法：
  - 假定 ${P}\left({z}_{{k}} | {d}_{{i}}\right), {P}\left({w}_{{j}} | {z}_{{k}}\right)$ 已知，求隐含变量 z_k 的后验概率。
  - 在 $\left(d_{i}, w_{j}, z_{k}\right)$ 已知的前提下，求关于参数 ${P}\left({z}_{{k}} | {d}_{{i}}\right), {P}\left({w}_{{j}} | {z}_{{k}}\right)$ 的似然函数期望的极大值，得到最优解 ${P}\left({z}_{{k}} | {d}_{{i}}\right)$，$P\left(w_{j} | z_{k}\right)$ 带入上一步，从而循环迭代。
  - 即：EM算法。



求隐含变量主题$z_k$的后验概率：


假定 $P\left({z}_{{k}} | {d}_{{i}}\right), {P}\left({w}_{{j}} | {z}_{{k}}\right)$ 已知，求隐含变量 ${{z}}_{{k}}$ 的后验概率：

$$
P\left(z_{k} | d_{i}, w_{j}\right)=\frac{P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right)}{\sum_{l=1}^{K} P\left(w_{j} | z_{l}\right) P\left(z_{l} | d_{i}\right)}
$$

- 在 $\left({d}_{{i}}, {w}_{{j}}, {z}_{{k}}\right)$ 已知的前提下，求关于参数数 ${P}\left({z}_{{k}} | {d}_{{i}}\right), {P}\left({w}_{{j}} | {z}_{{k}}\right)$ 的似然函数期望的极大值，得到最优解 ${P}\left({z}_{{k}} | {d}_{{i}}\right), {P}\left({w}_{{j}} | {z}_{{k}}\right)$，带入上一步，从而循环迭代。



关于参数 $P(z_k,d_{i}$，$P(w_{j} | z_{k})$ 的似然函数期望：




$$\begin{array}{l}
l=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log P\left(d_{i}, w_{j}\right) \\
=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log \left(P\left(w_{j} | d_{i}\right) P\left(d_{i}\right)\right) \\
=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right)\left(\log P\left(w_{j} | d_{i}\right)+\log P\left(d_{i}\right)\right) \\
=\left(\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log P\left(w_{j} | d_{i}\right)\right)+\left(\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log P\left(d_{i}\right)\right)
\end{array}$$

令：

$$l_{\text {new}}=\left(\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \log P\left(w_{j} | d_{i}\right)\right)$$

则：

$$
\begin{aligned}
E\left(l_{\text {new}}\right)=&\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \sum_{k=1}^{K} P\left(z_{k} | d_{l}, w_{j}\right) \log P\left(w_{j}, z_{k} | d_{i}\right)
\\=&\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \sum_{k=1}^{K} P\left(z_{k} | d_{i}, w_{j}\right) \log P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right)
\end{aligned}
$$

完成目标函数的建立：

- 关于参数 ${P}\left({z}_{{k}} | {d}_{{i}}\right), {P}\left({w}_{{j}} | {z}_{{k}}\right)$ 的函数 $E$，并且，带有概率和 为 1 的约束条件：

    $$\begin{array}{c}
    E=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \sum_{k=1}^{K} P\left(z_{k} | d_{i}, w_{j}\right) \log P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right) \\
    \quad \text { s.t }\left\{\begin{array}{l}
    \sum_{j=1}^{M} P\left(w_{j} | z_{k}\right)=1 \\
    \sum_{k=1}^{K} P\left(z_{k} | d_{i}\right)=1
    \end{array}\right.
    \end{array}$$

- 显然，这是只有等式约束的求极值问题，使用 Lagrange 乘子法解决。


目标函数的求解：

- Lagrange 函数为：

$$\begin{aligned}
\operatorname{Lag} &=\sum_{i} \sum_{j} n\left(d_{i}, w_{j}\right) \sum_{k=1}^{K} P\left(z_{k} | d_{i}, w_{j}\right) \log P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right) \\
&+\sum_{k=1}^{K} \tau_{k}\left(1-\sum_{j=1}^{M} P\left(w_{j} | z_{k}\right)\right)+\sum_{i=1}^{N} \rho_{i}\left(1-\sum_{k=1}^{K} P\left(z_{k} | d_{i}\right)\right)
\end{aligned}$$

- 求驻点，令等于0：

$$\frac{\partial L a g}{\partial P\left(w_{j} | z_{k}\right)} =\frac{\sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)}{P\left(w_{j} | z_{k}\right)}-\tau_{k}=0$$

$$\frac{\partial L a g}{\partial P\left(z_{k} | d_{i}\right)} =\frac{\sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)}{P\left(z_{k} | d_{i}\right)}-\rho_{i}=0$$


分析第一个等式：

- 由：

    $$\frac{\partial L a g}{\partial P\left(w_{j} | z_{k}\right)}=\frac{\sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)}{P\left(w_{j} | z_{k}\right)}-\tau_{k}=0$$

- 推导得：

    $$\begin{array}{l}
    \Rightarrow \sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)=\tau_{k} P\left(w_{j} | z_{k}\right) 
    \\\Rightarrow \sum_{m=1}^{M} \sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)=\sum_{m=1}^{M} \tau_{k} P\left(w_{j} | z_{k}\right) 
    \\\Rightarrow \sum_{m=1}^{M} \sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)=\tau_{k} \sum_{m=1}^{M} P\left(w_{j} | z_{k}\right)
    \\\Rightarrow \sum_{m=1}^{M} \sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)=\tau_{k}
    \end{array}$$

- 将 $\tau_{k}$ 带回上面推导过程中的第一行，得到：

$$\sum_{l} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)=\sum_{m=1}^{M} \sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right) P\left(w_{j} | z_{k}\right)$$

- 转换后：

$$P\left(w_{j} | z_{k}\right)=\frac{\sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)}{\sum_{m=1}^{M} \sum_i n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)}$$


分析第二个等式：

- 求极值时的解 M-Step：

$$\left\{\begin{array}{l}
P\left(w_{j} | z_{k}\right)=\frac{\sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)}{\sum_{m=1}^{M} \sum_{i} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)} \\
P\left(z_{k} | d_{i}\right)=\frac{\sum_{j} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)}{\sum_{k=1}^{K} \sum_{j} n\left(d_{i}, w_{j}\right) P\left(z_{k} | d_{i}, w_{j}\right)}
\end{array}\right.$$

- E-Step:

$$P\left(z_{k} | d_{i}, w_{j}\right)=\frac{P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right)}{\sum_{l=1}^{K} P\left(w_{j} | z_{l}\right) P\left(z_{l} | d_{i}\right)}$$



pLSA的总结：

- pLSA应用于信息检索、过滤、自然语言处理等领域，pLSA 考虑到词分布和主题分布，使用 EM 算法来学习参数。
- 虽然推导略显复杂，但最终公式简洁清晰，很符合直观理解，需用心琢磨；此外，推导过程使用了 EM 算法，也是学习 EM 算法的重要素材。



pLSA进一步思考：

- 相对于“简单”的链状贝叶斯网络，可否给出“词”“主题”“文档”更细致的网络拓扑，形成更具一般性的模型？
- pLSA不需要先验信息即可完成自学习——这是它的优势。如果在特定的要求下，需要有先验知识的影响呢？
- 答：LDA模型：
  - 三层结构的贝叶斯模型
  - 需要超参数

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200611/fetkXfbCBXGh.png?imageslim">
</p>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200611/4Tq1rY508WEL.png?imageslim">
</p>




