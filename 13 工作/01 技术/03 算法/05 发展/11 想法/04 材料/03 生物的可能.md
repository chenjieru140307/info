# 生物的可能

## 神经

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190505/GhOpc2hfw79Q.png?imageslim">
</p>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190505/lWXcL3fqYzVW.png?imageslim">
</p>


联结主义的主要见解之一是，当动物的许多神经元一起工作时会变得聪明。单独神经元或小集合的神经元不是特别有用。连接主义的这种见解的理论基础是什么？

神经元学说：1903年，西班牙科学家圣地亚哥•拉蒙•卡哈尔(Santiago Ramony Cajal)改良了高尔基染色法，首次观察到了神经细胞里更细微的结构，并于 1904 年发表 著作确立了神经元学说。其中提出的神经活动的基本原则，成了后来启发人工神经网络理论的仿生基础。<span style="color:red;">想知道现在的神经元学说发展到什么程度了？以及到底与现在的深度神经网络有哪些差别？</span>

当时在人工智能领域除了以罗森布拉特为首的山寨大脑神经元结构的连接主义 (connectionism)外，还有另一个流派，<span style="color:red;">感觉上两个派都有道理，一个是相当于让他自发产生，一个是严格的控制它产生。嗯，不一定对，想详细的了解下这两个派别。</span>


- 神经网络。
- 连接主义的问题：连接主义学习的最大局限是其 “试错性”；简单地说，其学习过程涉及大量参数，而参数的设置缺乏理论指导，主要靠手工 “调参”，夸张一点 说，参数调节上失之毫厘，学习结果可能谬以千里。
- 深度学习



神经科学已经给了我们依靠单一深度学习算法解决许多不同任务的理由。神经学家们发现，如果将雪貂的大脑重新连接，使视觉信号传送到听觉区域，它们可以学会用大脑的听觉处理区域去“看”(Von Melchner et al.,2000)。这暗示着大多数哺乳动物的大脑使用单一的算法就可以解决其大脑可以解决的大部分不同任务。在这个假设之前，机器学习研究是比较分散的，研究人员在不同的社群研究自然语言处理、计算机视觉、运动规划和语音识别。如今，这些应用社群仍然是独立的，但是对于深度学习研究团体来说，同时研究许多甚至所有这些应用领域是很常见的。<span style="color:red;">嗯，是的。</span>




连接，网络，优化

RBM，马尔科夫随机场

强化学习




l  目前主导机器视觉的深层卷积网络的关键核心直接受到大脑的启发。其中包括腹侧流中的分层视觉处理，它表明深度的重要性;视网膜的发现是整个视觉皮层的组织原理，导致卷积的出现;发现简单和复杂的细胞激发了最大池化等操作。




l  关于稀疏编码的研究工作是为了理解初级视觉皮层中定向边缘检测器，导致稀疏编码成为现代AI系统中的基本构建块。


## 神经动力学

l Dropout等现代正则化技术的灵感来自于神经动力学的内在随机性。



## 仿生学



## 认知心理学



## 奖励 多巴胺

l   时序差分学习等算法现在是强化学习领域的基础，它受到经典条件反射的动物实验的启发。

l   反过来，强化学习对基底神经节功能的解释具有显着影响，其中多巴胺能为基底神经节提供了非常重要的奖励预测误差信号，该信号也驱动许多强化学习算法。



## 遗忘

可能是一个关键关键。

对人类：

- 遗忘不仅仅意味着记忆的遗失，也意味着这也是一个帮助大脑吸收新知识并有效做出决策的积极过程。
- 当神经元之间的连接随着时间的推移变得减弱甚至是消失时，遗忘就会发生，而随着新神经元的出现，它们会重新连接海马体的回路并覆盖现有记忆。

好处：

1. 它通过减少过时信息对人们产生的影响来提高决策灵活性。
2. 它能够预防人们过度沉浸于某些过去的特定事件，提高适应能力。


举例：

- 教一个会说英语的孩子学习西班牙语，那么他会将学习英语过程中的相关经验应用到学习西班牙语的过程中，这有可能是名词、动词时态和句子结构等，同时忽略掉那些不相关的部分，比如口音、晦涩词语和语调等。孩子们可以基于战略性遗忘策略不断地学习与重建。
- 灾难性遗忘。神经网络训练以解决英语问题，如果要解决西班牙语，那么就需要清空之前的权重。


三种算法解决：


- 长短期记忆网络 LSTM。帮助神经网络实现1）遗忘/记忆，2）保存，3）聚焦。
- 弹性权重固化（EWC，Elastic Weight Consolidation） DeepMind 2017
  - 弹性权重固化是谷歌旗下DeepMind的研究人员于2017年3月创建的一种算法，该算法模拟了一种叫做“突触整合”的神经科学过程。在突触整合过程中，人们的大脑会进行任务评估，计算众多用于执行任务的神经元的重要性，并确认一些神经元所扮演的重要角色使其对正确执行任务发挥更加重要的作用。这些关键性神经元被标记上重要程度并使其在后续任务中被覆盖的几率减小。在神经网络中也有多个如神经元的连接用于执行任务。弹性权重固化将某些连接编码标注上关键性，从而保护其不被过度写入遗忘/记忆程序。
- 瓶颈理论（Bottleneck Theory）
  - 2017 “其思想是，一个网络会排除掉那些无关重要的嘈杂的输入数据，这就如同通过瓶颈向其中压缩信息，只保留与一般概念最相关的特性。”
  - 正如提斯比所解释的那样，神经网络在学习过程中需要经历两个阶段，拟合与压缩。在拟合过程中，网络对其训练数据进行标记。而在更为复杂的压缩过程中，其“将基于标记的数据信息只追踪那些具备巨大特征的信息。”而这一过程对于之后的概化过程极为重要。在此过程中，压缩是一种策略上的遗忘方式，人工智能研究人员可以在未来用其构建更强大的神经网络和框架。



## 注意力


l 人类注意力系统激发了注意力机制和神经网络的结合，这些神经网络可以被训练以动态地注意力或忽略其状态和输入的不同方面以进行未来的计算决策。



## 环境

我从来不反对大数据，因为人就是大数据训练出来的一个结果。我们从单细胞动物进化到今天这么复杂的一个身体、神经系统，那是因为进化过程中的大数据。不管我们叫不叫它大数据，也就是环境、地球的自然环境给予我们的刺激和约束，使我们不断地去调整系统，这是一个大数据。 从广义的大数据意义上我是同意的。今天收集这么多数据变成静态的、标好的图像，这只不过是一种比较僵化的大数据，从大的思路上讲还是必要的，因为没有这样足够数据和环境的刺激，我们很难做出更强的智能。



## 生物学上可信的信用分配（plausible credit assignment）**

信用分配问题可能是神经科学和人工智能领域最大的开放性问题之一。很明显，假设你正在打网球而且你没有击中球。你的100万亿个突触中有哪一个应该受到指责？大脑如何在你的运动系统中专门找到并纠正突触组，尤其是在错误发生后几百毫秒内通过视觉系统传递错误时？

在AI中，这种信用分配问题在许多情况下通过多层计算的反向传播来解决。然而，目前尚不清楚大脑如何解决这个问题。

真实的情况是，大脑使用本地学习规则解决它：即每个突触仅使用物理上可用的信息来调整其强度，例如，由突触连接的两个神经元的电活动来奖励和惩罚的任何神经调节输入。解释这些本地突触规则是什么以及它们如何工作可能会对AI产生巨大影响，这可以一定程度上减少反向传播的通信开销。但更一般地说，解决困扰神经科学和人工智能的常见未解决问题应该通过将突触生理学家，计算神经科学家和AI从业者聚集在一起来集体解决生物学上可信的信用分配问题来推动进步。

## 融合突触复杂性**

生物和人工神经模型之间的主要区别在于我们模拟连接神经元的突触的方式。在人工网络中，突触由单个标量值建模，反映乘法增益因子，转换神经元的输入如何影响神经元的输出。相反，每个生物突触都隐藏在极其复杂的分子信号通路中。例如，我们对最近事件记忆的海马突触各自包含数百种不同类型分子的化学反应网络，同时它具有整个复杂时间处理能力的动力系统。

在看到这种复杂性后，理论家或工程师可能会试图简单地将其视为生物学上的混乱，而这种混乱就是一种进化的偶然事件。然而，理论研究表明，这种突触复杂性可能确实对学习和记忆至关重要。事实上，在突触具有有限动态范围的记忆网络模型中，这样的突触本身就要求是具有复杂时间滤波特性的动态系统，以实现合理的网络存储容量。此外，最近在AI中正在利用更智能的突触作为解决灾难性遗忘问题的一种方法，其中训练学习两个任务的网络只能学习第二个任务，因为学习第二个任务会改变突触权重以这种方式消除从学习第一项任务中获得的知识。

一般地说，我们的人工智能系统很可能通过忽略生物突触的动态复杂性而取得重大的性能提升。正如我们为我们的网络添加空间深度以实现复杂的层次表示一样，我们可能还需要为突触添加动态深度以实现复杂的时间学习功能。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200219/Ygw8V0RnIys7.png?imageslim">
</p>

单个突触内的复杂分子状态可以帮助学习和记忆。


## **从系统级模块化大脑架构中获取灵感**

通常，当前的商业AI系统涉及具有相对均匀的分层或循环架构的训练网络，其从随机权重开始。但是，对于更复杂的任务来说，这可能是一个难以解决的问题。事实上，生物进化的道路却截然不同。所有脊椎动物的最后共同祖先生活在5亿年前。从那以后，它的基本大脑一直在发展，导致大约1亿年前出现哺乳动物大脑，以及几百万年前的人类大脑。这种不间断的进化链导致了一个错综复杂的大脑结构，具有高度保守的计算元素和巨大的系统级模块化。事实上，我们目前缺乏工程设计原则，来解释像大脑一样复杂的传感，通信，控制和记忆网络可以在5亿年的时间内不断扩大规模和复杂性，同时永远不会失去在动态环境中自适应运行的能力。因此，AI从大脑的系统级结构中获取灵感可能非常有趣。

一个关键的系统属性是功能和结构的模块化。大脑不像我们目前的AI架构是同质的，而是有不同的模块，如海马（保留情节记忆和导航），基底神经节（潜在的强化学习和动作选择）和小脑（自动化的运动控制和通过监督学习获得更高层次的认知）。此外，人脑中的记忆系统（习惯记忆，运动技能，短期记忆，长期记忆，情景记忆，语义记忆）也是功能模块化的。此外，在运动系统中，嵌套反馈环架构占主导地位，通过简单的快速循环在20毫秒内实现自动运动校正，稍慢的智能循环通过运动皮层在50毫秒内实现更复杂的运动校正，最后经过整个大脑的视觉反馈实现对运动错误的有意识的校正。最后，所有哺乳动物大脑的一个主要特征是由大量相似的6层皮质柱组成的新皮层，所有这些都被认为是在单个规范计算模块上实现的变异。

总体而言，现代哺乳动物大脑具有显著的模块性，通过1亿年的进化保存下来，表明这种系统级模块化可能有利于在AI系统中实施。目前从白板上训练神经网络的方法是不可能走向更普遍的人类智能的途径。实际上，系统级模块化的组合带来的不同类型的纠错嵌套循环和动态复杂的突触可能都是解决生物学上可信的信用分配的关键因素。

5亿年的脊椎动物大脑进化创造了一个高度异构和模块化的计算系统。



## **无监督学习，迁移学习和工程设计**

AI系统与人类学习之间的另一个主要差异在于AI系统所需的大量标记数据才可以达到人类级别的性能。例如，最近的语音识别系统在11940小时的语音训练后才能对齐转录。如果我们每天大声地听到另一个人类阅读文本两个小时，那么我们需要16年才能获取到这个数据集。AlphaGozero练习了490万场才击败人类围棋大师。如果一个人每天玩围棋30年，那么他每天必须玩450场比赛才能达到AlphaGozero的练习量。此外，最近关于视觉问答的数据集包含了0.25M图像，0.76M问题和10M答案。如果我们每天收到关于图像的100个问题的答案，我们需要274年的时间来吸收这种规模的数据集。很明显人类接受的标记训练数据量要少得多，但他们可以识别语音，玩围棋并很好地回答有关图像的问题。

其中，人工智能和生物智能之间差距的几个关键在于人类从未标记数据中学习的能力（无监督学习），以及在解决先前任务时获得的强大先验知识，并将这些知识转移到新任务中（迁移学习）。最后，人类社会建立了教育系统，精心挑选一些学习任务进行教学，以促进知识获取。为了在人工系统中有效地实例化这些概念，我们需要更深入地理解和数学形式化人类和其他动物如何进行无监督学习及知识如何在任务之间转移，这需要计算机科学家、心理学家和教育工作者的参与。因为这对于在标记数据稀缺的领域中训练AI是至关重要。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200219/uSgfTFXPO4Cb.png?imageslim">
</p>

斯坦福大学进行了26个不同的视觉任务之间迁移学习的研究。

## **建立理解，规划和主动因果学习的世界模型**

当前AI在商业环境中的成功很多是通过监督方法实现的，其中AI系统被动地接收输入，被告知正确的输出，并且它调整其参数以匹配每个输入-输出组合。相比之下，婴儿就像活跃的科学家一样探索他们的环境。例如：利用魔术，婴儿会看到两个“魔法”物体：物体A，它似乎穿过墙壁，而物体B，它在掉落时不会掉落。给婴儿A，B，婴儿将尝试将物体A穿过墙壁，然后放下物体B以查看它是否会掉落。这项非凡的实验表明，婴儿就像科学家一样，积极地探索他们的世界。

因此，与当前大多数的商业AI系统不同，婴儿具有学习和利用世界模型的卓越能力。我们需要在神经科学和人工智能方面进一步研究从经验中学习世界模型，使用这些世界模型进行规划（即，根据当前行动想象不同的未来），并使用这些未来的计划来做出决策。这种基于模型的规划和决策可能是当前无模型强化学习系统的有力支持，该系统简单地将世界状态映射到值或预期的未来奖励。人工智能中的这项工作可以与神经科学的工作携手并进，揭示动物的神经活动如何与想象的和未来相关。像好奇心这样的基本驱动可以形式化为强化学习系统，以此来促进学习和探索。更一般地，深入理解多个系统和促进动物和人类学习的内在生物驱动可能对加速人工系统的学习非常有益。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200219/yvKHmr4vrdJv.jpeg?imageslim">
</p>

科学家发现他的感官体验统计数据有变化

## **在后摩尔定律时代实现节能计算**

生物系统和AI系统之间的另一个数量级差异在于它们的能量消耗。人脑仅消耗20瓦的功率，而超级计算机则以兆瓦的功率运行。造成这种差异的一个关键原因可能是过度依赖数字计算本身，虽然数字革命推动了现代信息技术的兴起，但现在我们对实现人工智能的追求被认为是次优遗留技术。原因是数字计算需要在计算的中间阶段以极高的可靠性翻转每一位。然而，热力学定律则为每个快速可靠的位翻转确定了相当大的能量成本。

相比之下，生物的细胞内的分子以及脑内神经元的计算看起来令人惊讶地嘈杂和不精确。然而，生物计算的每个中间步骤都足够可靠，以使最终答案足够好。此外，大脑智能地向上或向下调节能量成本根据所需的通信速度。例如，考虑大脑中通过目标神经元的单位的成本。它开始于囊泡的随机释放，其以1毫米/秒的速度扩散到源神经元和目标神经元之间的空间，仅燃烧2.3毫微微焦耳（fj）。速度刚刚好，因为神经元连接之间的空间只有20纳米。该化学信号被转换为无源电信号，其以1米/秒的速度流过神经元细胞体，燃烧23fj横穿约10微米。最后，它到达轴突终端并转换为长轴，沿着轴突每秒行进100米，燃烧6000 fJ行进1厘米。因此，在从化学信号传递到被动电信号时，大脑动态地将通信速度上调1000倍，以跨越增加1000倍的距离，从而导致能量消耗增加10倍。

因此，只有在需要更高速度且仅需要更高可靠性时，大脑才会消耗更多能量。相比之下，数字计算机在刚性同步时钟上运行，并且在每个时钟周期，许多晶体管必须可靠地翻转状态。总之，生物计算的明显混乱不一定是不可避免的混乱，而是可能反映出高能效设计的理想原则。为了在我们的AI硬件中实现这样的效率，遵循生物计算的这些原则可能是必要的。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200219/O3opmh91SnOd.jpeg?imageslim">
</p>

Neurogrid：由斯坦福大脑开发的一种生物启发的神经形态计算机。




## **用于AI的神经科学和神经科学的AI：一种良性的科学螺旋**

最近神经科学和AI之间相互作用促进了深度和递归神经网络模型的发展。在许多情况下，当训练深度或递归网络来解决任务时，其内部表现看起来与训练为解决相同任务的动物中测量的内部神经活动模式非常相似。因此，我们通常会在不同的任务中获得不同大脑区域操作的高度复杂但令人惊讶的真实模型，从而提出了一个基本问题：我们如何理解这些模型正在做什么以及它们如何工作？更确切地说，学习网络连接和神经动态如何产生高性能？AI目前在理解它的神经模型正在做什么时面临同样的问题，虽然一些工程师认为没有必要了解神经网络是如何工作的。然而，对于当前网络的成功和失败如何因其连通性和动态性而产生的更深入的科学理解将导致网络的优化。然而，科学与技术之间的相互作用历史上几乎没有更深入的科学认识，也不会导致更好的技术。但是，在AI的某些应用中，特别是在医学诊断或法律中，可解释的AI是必不可少的。例如，如果医生和法官无法理解为什么这些系统做出了他们做出的决定，他们就不会在他们的案件中使用人工智能系统的建议。

因此，神经科学需要共享理解网络性能和决策如何作为网络连接和动态的新兴属性。因此，理论神经科学，应用物理学和数学的思想和理论的发展可以帮助分析AI系统。此外，AI系统的行为可能会改变神经科学中实验设计的本质，将实验工作集中在AI中难以理解的网络功能方面。总体而言，神经科学，人工智能和许多其他理论学科之间的紧密联系可以获得很多灵感，这可能会为生物和人工系统中的智能的出现带来统一的规律。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200219/CV2gkqeyUjWd.png?imageslim">
</p>

[任务驱动的视觉系统卷积循环模型](https://yq.aliyun.com/go/articleRenderRedirect?url=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F7775-task-driven-convolutional-recurrent-models-of-the-visual-system)可以同时执行机器视觉任务并解释猴子视觉系统的动态

## **寻求管理生物和人工智能的普遍规律**

在人工智能系统设计中，一种经常被引用的无视生物学的争论常涉及到飞机与鸟类的比较。然而，仔细观察这个想法会发现更多的细微差别。飞行的一般问题涉及解决两个基本问题：（1）为了前进而产生推力，（2）升力的大小使我们不会脱离天空。鸟类和飞机用不同方法解决了推力问题：鸟儿拍翅膀和飞机使用喷气发动机。但是，它们以完全相同的方式解决升力问题，通过使用弯曲的翼形，在低于和低于上方的气压下产生更高的气压。因此，滑翔的鸟类和飞机的运作非常相似。

实际上，我们知道空气动力学的一般物理定律：不同形状通过空气时，都可以用计算的方法来预测产生的力，如升力和推力。而且，任何解决飞行问题的方法，无论是生物还是人工，都必须遵守空气动力学定律。

更一般地说，在我们对物理世界的研究中，我们习惯于存在管理其行为的原则或规律。例如，正如空气动力学控制飞行物体的运动一样，广义相对论控制着空间和时间的曲率，量子力学控制着纳米世界的演化。我们认为，可能存在普世原则或法律来管理智能行为如何从大型互连神经元网络的合作活动中产生。这些法律可以连接和统一神经科学、心理学、认知科学和人工智能的相关学科，他们的阐述也需要帮助分析和计算领域，如物理，数学和统计学。事实上，这篇文章的作者使用了动力系统理论、统计力学、黎曼几何、随机矩阵理论和自由概率理论等技术，获得了对生物和人工网络运作的概念性见解。然而，为了阐明管理非线性分布式网络中出现智能的一般规律和设计原则，还需要进一步的工作，包括开发新概念，分析方法和工程能力。最终，就像鸟类，飞机和空气动力学的故事一样，创造智能机器的问题可能存在多种解决方案，其中一些组件在生物解决方案和人工解决方案之间共享，而其他组件则可能不同。通过寻求一般的智力法则，发现适用于生物和人工系统的新兴智能的潜在法则，以及建立受神经科学和心理学启发的新型AI，需要许多研究人员共同努力：计算机科学家追求更好的AI系统，神经科学家，心理学家和认知科学家探索大脑和思想的属性，数学家，物理学家，统计学家和其他理论家寻求形式化我们的综合知识并发现一般的法律和原则。

