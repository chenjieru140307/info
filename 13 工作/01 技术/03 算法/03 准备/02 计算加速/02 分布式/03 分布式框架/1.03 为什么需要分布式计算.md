

# 为什么需要分布式计算？

在这个数据爆炸的时代，产生的数据量不断地在攀升，从 GB,TB,PB,ZB。挖掘其中数据的价值也是企业在不断地追求的终极目标。但是要想对海量的数据进行挖掘，首先要考虑的就是海量数据的存储问题，比如 Tb 量级的数据。

谈到数据的存储，则不得不说的是磁盘的数据读写速度问题。早在上个世纪 90 年代初期，普通硬盘的可以存储的容量大概是 1G 左右，硬盘的读取速度大概为 4.4MB/s。读取一张硬盘大概需要 5 分钟时间，但是如今硬盘的容量都在 1TB 左右了，相比扩展了近千倍。但是硬盘的读取速度大概是 100MB/s。读完一个硬盘所需要的时间大概是 2.5个小时。所以如果是基于 TB 级别的数据进行分析的话，光硬盘读取完数据都要好几天了，更谈不上计算分析了。那么该如何处理大数据的存储，计算分析呢？

一个很简单的减少数据读写时间的方法就是同时从多个硬盘上读写数据，比如，如果我们有 100 个硬盘，每个硬盘存储 1%的数据 ，并行读取，那么不到两分钟就可以完成之前需要 2.5小时的数据读写任务了。这就是大数据中的分布式存储的模型。当然实现分布式存储还需要解决很多问题，比如硬件故障的问题，使用多台主机进行分布式存储时，若主机故障，会出现数据丢失的问题，所以有了副本机制：系统中保存数据的副本。一旦有系统发生故障，就可以使用另外的副本进行替换（著名的 RAID 冗余磁盘阵列就是按这个原理实现的）。其次比如一个很大的文件如何进行拆分存储，读取拆分以后的文件如何进行校验都是要考虑的问题。比如我们使用 Hadoop 中的 HDFS 也面临这个问题，只是框架给我们实现了这些问题的解决办法，开发中开发者不用考虑这些问题，底层框架已经实现了封装。

同样假如有一个 10TB 的文件，我们要统计其中某个关键字的出现次数，传统的做法是遍历整个文件，然后统计出关键字的出现次数，这样效率会特别特别低。基于分布式存储以后，数据被分布式存储在不同的服务器上，那么我们就可以使用分布式计算框架（比如 MapReduce,Spark等）来进行并行计算（或者说是分布式计算），即：每个服务器上分别统计自己存储的数据中关键字出现的次数，最后进行一次汇总，那么假如数据分布在 100 台服务器上，即同时 100 台服务器同时进行关键字统计工作，效率一下子可以提高几十倍。
