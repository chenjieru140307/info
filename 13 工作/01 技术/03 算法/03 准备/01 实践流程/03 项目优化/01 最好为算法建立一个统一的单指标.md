


## 最好为算法优化建立一个单一的数字评估指标


分类准确率是 **单一数字评估指标（single-number evaluation metric）** 的示例：你在开发集（或测试集）上运行分类器，然后得到样本正确分类的比例(fraction)单个数字。根据这个指标，如果分类器 A 获得 97%的准确率，而分类器 B 获得 90%的准确率，那么我们认为分类器 A 更好。

相比之下，查准率（Precision）和查全率（Recall）就不是一个单一数字的评估指标：它给出了两个数字来评估分类器。拥有多个数字的评估指标使得比较算法更加困难。

假设你的算法表现如下： 

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180812/ahgfj6B6gJ.png?imageslim">
</p>

如上所示，两个分类器都没有显而易见地比另一个更好，所以它不能立即引导你选择其中一个。

在开发期间，你的团队会尝试大量关于算法架构、模型参数、特征选择等方面的想法。使用 **单一数字的评估指标（single-number evaluation metric）**（如精度）使得你可以根据其在该指标上的表现快速对所有模型进行排序，从而快速决定哪一个是能工作得最好的。<span style="color:red;">嗯。</span>

如果你真的即关心查准率（Precision）又关心查全率（Recall），我推荐使用一种标准方法将它们组合成一个单一的数字。例如，可以取 Precision 和 Recall 的平均值，最终得到单个数字。或者，你可以计算“F1度量（F1 score）”，这是一种基于其平均值改善的方法，比简单地取平均值效果要好。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180812/0FHEG9CGc5.png?imageslim">
</p>

当你面在大量的分类器中进行选择时，使用单一数字的评估指标可以加快你做出决策的能力。所有这些都给出了明确的表现排名，从而给出一个清晰的前进方向。

假如你分别得在四个主要市场跟踪猫分类器的准确率。那么可以通过对这四个数据进行平均或加权平均，最终得到一个单一数字度量。

取平均值或加权平均值是将多个指标合并为一个的最常见的方法之一。







# 相关

- [machine-learning-yearning](https://github.com/xiaqunfeng/machine-learning-yearning/)
- 《Machine Learning Yearning》 by Andrew NG
