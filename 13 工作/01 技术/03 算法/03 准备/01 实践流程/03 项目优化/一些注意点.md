
# 一些注意点



1. 在用大数据集训练之前先用小数据集试一下，排除一些明显的错误。
2. 不要忘记切换网络的训练/评估模式，因为在训练时和评价时可能会用不同的 Batch Normalization 和 Dropout 方法模式。
3. 不要忘记在.backward()之前.zero_grad()（在pytorch中），忘记了写.zero_grad()会各种nan，导致结果非常差。可以大家可以在实在找不到原因时，打印梯度出来看看，是不是有某些层参数为0，几乎没有学习，有些为nan了，去逐步找原因。
4. 传给损失函数的值，本来期望是logits 的值，而不是过了softmax之后的值。这种情况不会报错，但是效果很差。很难发现错误，第一感觉，我去，idea不work，所以大家效果不好的时候，不要放弃啊，仔细看看有没有常见的bug！！
5. 使用BatchNorm时，您没有对线性/ 二维卷积层使用bias = False，或者反过来忘记将其包含在输出层中。 这个倒不会让你失败，但它们是虚假的参数。如果卷积层后面跟着BatchNormalization的话，卷积层就不需要偏置参数b，只需要w参数。


6. **以为view()和permute()是一样的事情（不正确地使用view）**

举例说明：比如要将一个(2, 12)的tensor改为(4, 2, 3)的tensor。这样就不能直接用view而需要多次用permute()来交换axis(转置2D的matrix)来达到目的。



```py
tc.manual_seed(1)
x = tc.randn(2, 12)
print(x)
y = x.permute(1, 0)
print(y)
z = y.view(4, 3, 2)
print(z)
a = z.permute(0, 2, 1)
print(a)
```



# 相关

- [Andrej Karpathy提到的神经网络六大坑](https://zhuanlan.zhihu.com/p/38937612)
