
# 网络不工作时需要确认的点

神经网络不工作的37种可能原因应对法



原文地址

网络的训练已经持续了12个小时，看上去一切都挺正常：梯度在波动，损失值在减小。但是真正用起来的时候却令人大跌眼镜：输出都是0，或者输出一些噪声，总而言之产生的结果完全不能用。“我做错了什么？”——我问我的计算机，换来的只是沉默。



如果你的模型产生的都是垃圾（比如只是输出平均值，或者准确率很差），那你该从哪儿入手检查呢？



网络没有得到训练的原因有很多，经历过多次调试之后，我发现我做的检查项目是类似的。因此，我把这些经验汇编出来，给出如下这份便捷指南。这份列表提供了很多不错的点子，希望对正在阅读的你也能有所帮助



## 如何阅读本指南？



很多事情都可能出错，但是其中有一些出错的可能性更大。通常我的应急预案包含如下几项：

1. 先使用一种简单的，对你要解决的问题久经考验的模型（例如对图像问题使用VGG）。如果可能的话，使用标准的损失函数
2. 关掉所有的额外功能，例如正则化和数据增强
3. 如果是在微调模型，仔细比对一下两边的预处理过程。新模型输入数据的预处理和训练原始模型时做的预处理应该一样
4. 验证输入数据是对的
5. 先使用一个非常小的数据集（2-20个样本）。让模型过拟合这个数据集，然后逐渐加入更多数据
6. 慢慢把之前关掉的额外功能都开开：数据增强、正则化、自定义损失函数等等。试着使用更复杂的模型

如果上述步骤都不奏效，沿着下面这个大表开始一项项检查吧



## 一、数据集问题



## （1）检查你的输入数据

检查你送入网络的数据是否有意义。比如我曾经不止一次将图像的宽和高弄混。有时候我还会不小心给进全是0的数据。或者我还把同样一批数据一遍又一遍作为输入。所以把输入输出的若干批数据打印出来看一看，确认它们都是对的

## （2）尝试随机输入

试着向网络送入随机数，而不是真实数据，看看是不是产生同样的错误。如果的确产生了同样的错误，那这基本上证实了你的网络中的某一部分是把数据变成了垃圾。试着逐层或者逐操作符调试一下，看看哪儿出问题了

## （3）检查读数据部分的代码

可能你的数据没问题，但是把输入送进网络的这部分代码有些问题。打印出网络第一层的、没有经过其它操作的输入，然后检查一下

## （4）确定输入与输出之间的关系没问题

随机选择一些样本，看看它们的标签对不对。确定在将输入样本打乱时没有打乱输出标签

## （5）输入与输出之间的关系是否太过随机？

也有可能是输入和输出之间的关系里确定性的部分太小，随机性的部分太大（可能有人会争辩说，股票价格就是这样的），也就是说，输入与输出之间不够相关。不过没有一种普遍的方法能对此作出判定，因为这取决于数据本身的性质

## （6）数据集里的噪声是不是太多？

我曾经遇到过一次这样的情况，那次是我想从一个食品网站上爬取一个图片数据集，但是错误的标签太多了，网络根本学不到东西。人肉检查一些输入看看标签是不是有问题。至于错误标签占比超过多少能称为“噪声数据太多”，这个问题一直都比较有争议。这篇论文使用了MNIST数据集，不过有50%的数据都对应了错误的标签，而模型最后也能得到超过50%的准确率

## （7）打乱数据集

如果数据集没有被打乱过，而且内部存在某些特别的顺序（比如数据是按标签排列的），那么这会对学习造成很差的影响。因此要把数据集打乱以避免这样的现象——注意，打乱时要把输入和标签一起打乱

## （8）减小类别的不平衡性

标记为类别A的图像数量是否千倍于标记为类别B的图像数量？那可能需要重新设计损失函数，或者使用其它不平衡数据集处理办法

## （9）训练样本够吗？

如果你是从头训练一个网络（不是在已有模型基础上微调），那么可能需要很多数据。对图像分类问题，讨论指出对每个类别需要至少1000张图像

## （10）确定每批数据不止包含一种标签

如果数据集是排序过的（例如前10000个样本的标签都相同），那么就会出现这样的情况。把数据集打乱即可

## （11）降低每批数据的数量（batch size）

这篇论文指出如果一批数据包含的数据量太大，会降低模型的泛化能力

## 附一条：使用标准数据集（例如MNIST、CIFAR10）

感谢@hengcherkeng网友的建议

“当测试新的网络结构或者编写新代码时，先使用标准数据集，不要使用自己的数据。因为对这些数据集已经有很多可参考的评估结果，而且这些数据集背后的问题已经被证明是‘可以被解决的’。对这些数据集，不会有标签噪声的问题，不会有训练集/测试集分布不同的问题，也不会有数据不可分的问题，等等”

## 二、数据归一化/数据增强

## （12）标准化特征

你把输入标准化，使其均值为0方差为1了吗

## （13）是否做了太多数据增强

数据增强能起到正则化的效果。如果做了太多数据增强并且又使用了其它正则化方法（例如权重的L2范数、dropout等等）可能会导致网络欠拟合

## （14）检查预训练模型的预处理

如果使用了预训练模型，确保你在训练时使用了训练原始模型时一样的归一化和预处理方法。例如，图像像素点是应该在[0, 1]内，还是应该在[-1, 1]内，还是应该在[0, 255]内？

## （15）检查训练集/验证集/测试集的预处理过程

CS231n给出了一个经常被人踩的坑

“所有预处理的统计过程（例如对数据求均值的过程）只能在训练数据上做，然后再用到验证集/测试集上。比如，在整个数据集上计算均值然后把每张图片减掉均值，再把处理后的数据集分成训练/验证/测试集是不对的”

另外，要看看是不是对每个样本/每批数据使用了同样的预处理方法

## 三、实现问题

## （16）试着解决原始问题的一个简化版本

这有助于你找出问题出在哪儿。例如，如果原始问题的目标输出是物体类别和坐标，先试着只预测类别

## （17）计算随机情况下模型应该取得的正确损失值

这条仍然来自于CS231n的讲义：“使用小参数初始化模型，不加正则化。例如，假如我们有10个类，那么最开始模型随机乱猜的话，对每条样本，有10%的可能猜到正确类别，而Softmax损失函数值是猜到正确类别概率的负对数，因此最开始模型的损失值应该是-ln(0.1) = 2.302”

在之后，再试着加入正则化，这样在开始模型的损失值会变大

## （18）检查损失函数

如果是自己实现一个损失函数，检查有没有bug，编写单元测试。我自己写的损失函数可能会有哪儿不太对，因此会暗搓搓地拖垮网络的性能

## （19）验证损失函数的输入

如果使用了某个框架提供的损失函数，要保证你传进去的是它想要的值。比如，我在用PyTorch的时候总会弄混NLLLoss和CrossEntropyLoss，这两个函数的唯一区别是前者需要的参数是被softmax过的，而后者不用

## （20）调整损失权重

如果你的损失函数是多个更小损失函数的组合体，要确保每个子损失函数值的相对大小没问题。这需要你测试各损失函数权重的不同组合

## （21）监测其它指标

有时候损失函数值不是衡量网络是否正确训练的最好指标。如果可以，试着使用一些其它指标，比如准确率

## （22）测试自定义的网络层

你是否自己实现了网络中的若干层？仔细检查一下，以确保它们是按你想的那样工作的

## （23）检查“被冻结的”层或变量

有些层/变量本来是应该被学习的，检查看看你是否不小心把它们的梯度更新停掉了

## （24）增大网络大小

可能你的网络表示能力不足，难以拟合目标函数。试着增多层数或增多全连接层的隐藏单元数

## （25）检查隐藏维度错误

如果你的输入类似于 (k, H, W) = (64, 64, 64) ，那么很容易漏过与维度错误有关的问题。对输入维度使用一些比较怪的数字（比如每个维度设置一个不同的质数），然后检查输入是如何在网络中传播的

## （26）检查梯度

如果你自己手动实现了梯度下降，可以试着做一些梯度检查来保证反向传播的正确性。可参考 1 23

## 四、训练问题

## （27）先解决一个小问题

在数据的一个很小的子集上让模型过拟合，以保证模型的有效性。比如，只在一个或两个样本上训练模型，看看网络能否把它们区分开来，然后对各个类别加入更多样本

## （28）检查权重的初始化

如果没底，使用Xavier初始化法或He初始化法。此外，不好的初始化可能会把模型带入到一个差的局部极小值点，因此可以试试不同的初始化结果，看看是否管用

## （29）修改超参数

可能你用的超参数比较差，如果可以的话，试试网格搜索

## （30）降低正则程度

太多正则化会导致网络严重欠拟合。考虑将诸如dropout、batch norm、权重/偏置L2正则化等手段都放松一些。fast.ai有一门很棒的课程叫“软开人员的深度学习实践”，在这里Jeremy Howard建议我们先要避免欠拟合，也就是说要先足够过拟合训练数据，然后再考虑解决过拟合问题

## （31）保持耐心

可能你的网络需要更多训练时间才能开始给出有意义的预测。如果损失函数值一直在稳定下降，就让它再训练一会儿

## （32）从训练模式切换到测试模式

在某些网络实现中，有些层会使用batch norm或者dropout，它们在训练和测试过程中的表现是不一样的。将网络切换到合适的模式可以帮助它给出正确预测

## （33）将训练过程可视化

- 监测每层的激活值、权重和权重更新量，确保这些值的量级都匹配。比如，参数（权重和偏置）更新量的量级应该是1e-3左右
- 考虑使用Tensorboard和Crayon这些可视化库。或者可以使用它们的乞丐版——打印出权重/偏置/激活函数值
- 注意那些激活函数值平均情况下还远大于0的层，考虑使用batch norm和ELU
- Deeplearning4j指出权重和偏置的直方图应该满足如下性质：
  “对于权重，在一段时间以后，它们的直方图形状应该近似于高斯分布（正态分布）。对于偏置，它们应该从0开始，最后也呈高斯分布（LSTM例外）。注意那些最后收敛到正/负无穷的参数，注意那些变得特别大的偏置。当你要解决分类问题而且输入数据的类别非常不平衡时，在输出层有时会出现这样的现象”
- 检查每层参数的更新量，它们也应该满足高斯分布

## （34）使用不同的优化器

一般情况下，优化器不应该是导致网络不训练的罪魁祸首，除非选择的超参数尤其糟糕。但是，选择合适的优化器有助于以最短时间基本完成训练过程。如果你是照着论文实现算法，论文里应该写出用了哪个优化器。如果没有，建议使用Adam或带动量的SGD



可以参考Sebastian Ruder写的这篇优秀的博客，从中你可以了解到基于梯度下降的优化器的更多细节

## （35）梯度爆炸/梯度消失

- 检查参数的更新值，太大意味着出现了梯度爆炸，可以做一些梯度裁剪来避免
- 检查激活值。deeplearning4j指出，“激活值的标准差理想状况下应该在0.5到2.0之间。否则说明出现了梯度爆炸或梯度消失”

## （36）调整学习率

太小的学习率会导致模型收敛速度很慢，太大的学习率可以在初始阶段迅速降低损失值，但是难以获得一个好的解。试着成10倍增大/缩小当前学习率

## （37）克服NaN带来的问题

训练RNN时更严重的问题是遇到了NaN（Non-a-Number）（我听说是这样）。可以试着使用如下方法应对

- 降低学习率，尤其是如果你在前100个迭代就遇到了NaN
- 把0做除数，或者对0/负数求对数也会导致NaN
- 参考Russell Stewart的这篇文章
- 逐层计算，看哪一层出了NaN


# 相关

- [神经网络不工作的37种应对法](https://zhuanlan.zhihu.com/p/42314204?utm_source=ZHShareTargetIDMore&utm_medium=social&utm_oi=56829493116928)
