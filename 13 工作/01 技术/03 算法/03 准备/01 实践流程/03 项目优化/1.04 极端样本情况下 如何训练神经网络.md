

### 14.3.4 极端批样本数量下，如何训练网络？

极端批样本情况一般是指 batch size为 1 或者 batch size在 6000 以上的情况。这两种情况，在使用不合理的情况下都会导致模型最终性能无法达到最优甚至是崩溃的情况。

在目标检测、分割或者 3D 图像等输入图像尺寸较大的场景，通常 batch size 会非常小。而在 14.2.4中，我们已经讲到这种情况会导致梯度的不稳定以及 batchnorm 统计的不准确。针对梯度不稳定的问题，通常不会太致命，若训练中发现梯度不稳定导致性能的严重降低时可采用累计梯度的策略，即每次计算完不反向更新，而是累计多次的误差后进行一次更新，这是一种在内存有限情况下实现有效梯度更新的一个策略。batch size过小通常对 batchnorm 的影响是最大的，若网络模型中存在 batchnorm，batch size若只为 1 或者 2 时会对训练结果产生非常大的影响。这时通常有两种策略，一、若模型使用了预训练网络，可冻结预训练网络中 batchnorm 的模型参数，有效降低 batch size引起的统计量变化的影响。二、在网络不是过深或者过于复杂时可直接移除 batchnorm 或者使用 groupnorm 代替 batchnorm，前者不多阐释，后者是有 FAIR 提出的一种用于减少 batch 对 batchnorm 影响，其主要策略是先将特征在通道上进行分组，然后在组内进行归一化。即归一化操作上完全与 batch size无关。这种 groupnorm 的策略被证实在极小批量网络训练上能达到较优秀的性能。当然这里也引入里 group 这个超参数，一般情况下建议不宜取 group 为 1 或者各通道单独为组的 group 数量，可结合实际网络稍加调试。

为了降低训练时间的成本，多机多卡的分布式系统通常会使用超大的 batch size进行网络训练。同样的在 14.2.4中，我们提到了超大 batch size会带来梯度方向过于一致而导致的精度大幅度降低的问题。这时通常可采用层自适应速率缩放（LARS）算法。从理论认知上将，batch size增大会减少反向传播的梯度更新次数，但为了达到相同的模型效果，需要增大学习率。但学习率一旦增大，又会引起模型的不收敛。为了解决这一矛盾，LARS算法就在各层上自适应的计算一个本地学习率用于更新本层的参数，这样能有效的提升训练的稳定性。目前利用 LARS 算法，腾讯公司使用 65536 的超大 batch size能将 ResNet50 在 ImageNet 在 4 分钟完成训练，而谷歌使用 32768 的 batch size使用 TPU 能将该时间缩短至 2 分钟。





# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
