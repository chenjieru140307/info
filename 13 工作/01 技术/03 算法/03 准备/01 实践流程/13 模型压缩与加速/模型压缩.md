# 模型压缩

减少推断所需开销的一个关键策略是模型压缩(model compression)(Buciluet al.,2006)。模型压缩的基本思想是用一个更小的模型代替原始耗时的模型，从而使得用来存储与评估所需的内存与运行时间更少。<span style="color:red;">一直想知道，模型压缩是怎么实现的？压缩掉的是什么？是网络的结构变了？还是网络在内存中的存储方式变了？还是计算的时候内存中计算的方式变了？</span>

当原始模型的规模很大，且我们需要防止过拟合时，模型压缩就可以起到作用。在许多情况下，拥有最小泛化误差的模型往往是多个独立训练而成的模型的集成。评估所有 $n$ 个集成成员的成本很高。<span style="color:red;">对呀，这种当最优的模型是几个模型的集合的时候，这个时候要怎么部署？要怎么压缩？或者难道能找到一个等价的模型等于这几个模型的集合？</span>有时候，当单个模型很大(例如，如果它使用 Dropout 正则化)时，其泛化能力也会很好。

这些巨大的模型能够学习到某个函数 $f(x)$，但选用的参数数量超过了任务所需的参数数量。只是因为训练样本数是有限的，所以模型的规模才变得必要。只要我们拟合了这个函数 $f(x)$，我们就可以通过将 $f$ 作用于随机采样点 $x$ 来生成有无穷多训练样本的训练集。然后，我们使用这些样本训练一个新的更小的模型，使其能够在这些点上拟合 $f(x)$。<span style="color:red;">哇塞！这样是有些牛逼，但是训练出来的模型是靠近 $f(x)$ 的吧？距离我们想要的真实的分布规律又差了点吧？还是说基本没有差别？而且，与 $f(x)$ 的差别要达到多少是允许的？而且，这个小模型的规模要怎么确定？怎么知道多大的规模可以使得拟合到 $f(x)$ 的指定误差范围内？想知道。</span>为了更加充分地利用这个新的小模型的容量，最好从类似于真实测试数据(之后将提供给模型)的分布中采样 $x$。这个过程可以通过损坏训练样本或者从原始训练数据训练的生成模型中采样完成。<span style="color:red;">哇塞，不错不错，这本花书真的厉害！嗯，生成模型知道，损坏训练样本是什么意思？</span>

此外，我们还可以仅在原始训练数据上训练一个更小的模型，但只是为了复制模型的其他特征，比如在不正确的类上的后验分布(Hinton et al.,2014,2015)。<span style="color:red;">什么意思？复制模型的什么特征？不确定的类的后验分布式什么意思？</span>
