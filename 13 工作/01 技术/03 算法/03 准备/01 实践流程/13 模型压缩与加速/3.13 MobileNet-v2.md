---
title: 3.13 MobileNet-v2
toc: true
date: 2019-08-31
---

### 17.9.3 MobileNet-v2
MobileNet-V2是 2018 年 1 月公开在 arXiv 上论文《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation》，是对 MobileNet-V1的改进，同样是一个轻量化卷积神经网络。
#### 3.1 设计思想
* 采用 Inverted residuals
    * 为了保证网络可以提取更多的特征，在 residual block中第一个 1\*1 Conv和 3*3 DW Conv之前进行通道扩充
* Linear bottlenecks
    * 为了避免 Relu 对特征的破坏，在 residual block的 Eltwise sum之前的那个 1\*1 Conv 不再采用 Relu
* stride=2的 conv 不使用 shot-cot，stride=1的 conv 使用 shot-cut

#### 3.2 网络架构
*  Inverted residuals
ResNet中 Residuals block先经过 1\*1的 Conv layer，把 feature map的通道数降下来，再经过 3\*3 Conv layer，最后经过一个 1\*1 的 Conv layer，将 feature map 通道数再“扩张”回去。即采用先压缩，后扩张的方式。而 inverted residuals采用先扩张，后压缩的方式。
MobileNet采用 DW conv提取特征，由于 DW conv本身提取的特征数就少，再经过传统 residuals block进行“压缩”，此时提取的特征数会更少，因此 inverted residuals对其进行“扩张”，保证网络可以提取更多的特征。
<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/XtuzsFor4Ru9.png?imageslim">
</p>

*  Linear bottlenecks
ReLu激活函数会破坏特征。ReLu对于负的输入，输出全为 0，而本来 DW conv特征通道已经被“压缩”，再经过 ReLu 的话，又会损失一部分特征。采用 Linear，目的是防止 Relu 破坏特征。
<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/Gw01E9yz0fp3.png?imageslim">
</p>

* shortcut
stride=2的 conv 不使用 shot-cot，stride=1的 conv 使用 shot-cut
<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/RkOIjl0gxGIM.png?imageslim">
</p>

* 网络架构
<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/JCqIDdYzuSj3.png?imageslim">
</p>








# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
