

# 低秩分解

基于低秩分解的深度神经网络压缩与加速的核心思想是利用矩阵或张量分解技术估计并分解深度模型中的原始卷积核。卷积计算是整个卷积神经网络中计算复杂度最高的计算操作，通过分解 4D 卷积核张量，可以有效地减少模型内部的冗余性．此外对于 2D 的全连接层矩阵参数，同样可以利用低秩分解技术进行处理．但由于卷积层与全连接层的分解方式不同，本文分别从卷积层和全连接层２个不同角度回顾与分析低秩分解技术在深度神经网络中的应用。

Denil等人从理论上利用低秩分解的技术并分析了深度神经网络存在大量的冗余信息，开创了基于低秩分解的深度网络模型压缩与加速的新思路．如图７所示，展示了主流的张量分解后卷积计算．

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/KwDySN8yBWj3.jpg?imageslim">
</p>








# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
