

# 模型压缩必要性及可行性

模型压缩是指利用数据集对已经训练好的深度模型进行精简，进而得到一个轻量且准确率相当的网络，压缩后的网络具有更小的结构和更少的参数，可以有效降低计算和存储开销，便于部署再受限的硬件环境中。

模型压缩算法能够有效降低参数冗余，从而减少存储占用、通信带宽和计算复杂度，有助于深度学习的应用部署。


## 为什么需要模型压缩和加速？

1. 随着 AI 技术的飞速发展，越来越多的公司希望在自己的移动端产品中注入 AI 能力。
2. 对于在线学习和增量学习等实时应用而言，如何减少含有大量层级及结点的大型神经网络所需要的内存和计算量显得极为重要。
3. 模型的参数在一定程度上能够表达其复杂性，相关研究表明，并不是所有的参数都在模型中发挥作用，部分参数作用有限、表达冗余，甚至会降低模型的性能。
4. 复杂的模型固然具有更好的性能，但是高额的存储空间、计算资源消耗是使其难以有效的应用在各硬件平台上的重要原因。
5. 智能设备的流行提供了内存、CPU、能耗和宽带等资源，使得深度学习模型部署在智能移动设备上变得可行。
6. 高效的深度学习方法可以有效的帮助嵌入式设备、分布式系统完成复杂工作，在移动端部署深度学习有很重要的意义。

## 模型压缩的必要性及可行性

必要性：

- 首先是资源受限
- 其次参数过多会使得计算量过大。在许多网络结构中，如 VGG-16网络，参数数量 1 亿 3 千多万，占用 500MB 空间，需要进行 309 亿次浮点运算才能完成一次图像识别任务。

可行性：

模型的参数在一定程度上能够表达其复杂性，相关研究表明，并不是所有的参数都在模型中发挥作用，部分参数作用有限、表达冗余，甚至会降低模型的性能。论文 'Predicting parameters in deep learning' 提出，很多的深度神经网络仅仅使用很少一部分（5%）权值就足以预测剩余的权值。该论文还提出这些剩下的权值甚至可以直接不用被学习。也就是说，仅仅训练一小部分原来的权值参数就有可能达到和原来网络相近甚至超过原来网络的性能（可以看作一种正则化）。

最终目的：

- 最大程度的减小模型复杂度
- 减少模型存储需要的空间
- 也致力于加速模型的训练和推测
