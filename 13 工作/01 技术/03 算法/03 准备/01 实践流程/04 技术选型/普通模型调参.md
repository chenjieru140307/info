
# 普通模型调参


在训练时，我们主要希望通过调整参数来得到一个性能不错的模型。一个模型往往有很多参数，但其中比较重要的一般不会太多。比如对 **sklearn** 的 `RandomForestClassifier` 来说，比较重要的就是随机森林中树的数量 `n_estimators` 以及在训练每棵树时最多选择的特征数量 `max_features`。所以**我们需要对自己使用的模型有足够的了解，知道每个参数对性能的影响是怎样的**。

通常我们会通过一个叫做 Grid Search 的过程来确定一组最佳的参数。其实这个过程说白了就是根据给定的参数候选对所有的组合进行暴力搜索。


```py
param_grid = {'n_estimators': [300, 500], 'max_features': [10, 12, 14]}

model = grid_search.GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=1, cv=10, verbose=20, scoring=RMSE)

model.fit(X_train, y_train)
```



顺带一提，Random Forest 一般在 `max_features` 设为 Feature 数量的平方根附近得到最佳结果。

这里要重点讲一下 Xgboost 的调参。通常认为对它性能影响较大的参数有：

- `eta`：每次迭代完成后更新权重时的步长。越小训练越慢。
- `num_round`：总共迭代的次数。
- `subsample`：训练每棵树时用来训练的数据占全部的比例。用于防止 Overfitting。
- `colsample_bytree`：训练每棵树时用来训练的特征的比例，类似 `RandomForestClassifier` 的 `max_features`。
- `max_depth`：每棵树的最大深度限制。与 Random Forest 不同，**Gradient Boosting 如果不对深度加以限制，最终是会 Overfit 的**。
- `early_stopping_rounds`：用于控制在 Out Of Sample 的验证集上连续多少个迭代的分数都没有提高后就提前终止训练。用于防止 Overfitting。

一般的调参步骤是：

1. 将训练数据的一部分划出来作为验证集。
2. 先将 `eta` 设得比较高（比如 0.1），`num_round` 设为 300 ~ 500。
3. 用 Grid Search 对其他参数进行搜索
4. 逐步将 `eta` 降低，找到最佳值。
5. 以验证集为 watchlist，用找到的最佳参数组合重新在训练集上训练。注意观察算法的输出，看每次迭代后在验证集上分数的变化情况，从而得到最佳的 `early_stopping_rounds`。


```py
X_dtrain, X_deval, y_dtrain, y_deval = cross_validation.train_test_split(X_train, y_train, random_state=1026, test_size=0.3)

dtrain = xgb.DMatrix(X_dtrain, y_dtrain)
deval = xgb.DMatrix(X_deval, y_deval)
watchlist = [(deval, 'eval')]

params = {
   'booster': 'gbtree',
   'objective': 'reg:linear',
   'subsample': 0.8,
   'colsample_bytree': 0.85,
   'eta': 0.05,
   'max_depth': 7,
   'seed': 2016,
   'silent': 0,
   'eval_metric': 'rmse'
}

clf = xgb.train(params, dtrain, 500, watchlist, early_stopping_rounds=50)
pred = clf.predict(xgb.DMatrix(df_test))
```


最后要提一点，所有具有随机性的 Model 一般都会有一个 `seed` 或是 `random_state` 参数用于控制随机种子。得到一个好的 Model 后，在记录参数时务必也记录下这个值，从而能够在之后重现 Model。



# 相关

- []
