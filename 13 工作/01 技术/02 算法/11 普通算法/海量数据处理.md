
# 海量数据处理


## TOP N问题

1. 如何在海量数据中找出重复最多一个。

- 通过 hash 映射为小文件

- 通过 hash_map统计各个小文件重读最多的并记录次数

- 对每个小文件重复最多的进行建立大根堆

2. 上亿有重数据，统计最多前 N 个。

- 内存存不下

    - 通过 hash 映射为小文件

    - 通过 hash_map统计各个小文件重读最多的并记录次数

    - 对每个小文件重复最多的进行建立大根堆并重复 N 次取走堆顶并重建堆操作

- 内存存得下

    - 直接内存通过 hash_map统计并建大根堆

    - 重复 N 次取走堆顶并重建堆操作

3. 海量日志数据，提取出某日访问百度次数最多的那个 IP（同 1）。

- 将 IP % 1000映射到 1000 个小文件中

    - 相同 IP 会被映射到同一个文件

    - 不会出现累加和更大情况

- 分 1000 次在内存处理小文件，得到频率最大 IP（使用 map 统计）

- 对这 1000 个 IP 建立大根堆

4. 1000w查询串统计最热门 10 个（同 2）。

- 同上


5. 1G的文件，里面 1 行 1 个不超过 16 字节的词。内存限制 1M，返回频数最高前 100（同 2）。

- 将单词 % 5000存入 5000 小文件

    - 平均各文件约 200K

    - 对超过 1M 的文件继续分割直到小于 200K

- 使用 map 统计各个词出现的频率

- 对 5000 词使用堆排序或归并排序

## 分布式 TOP N问题

6. 分布在 100 台电脑的海量数据，统计前十。

- 各数据只出现在一台机器中

    - 先在独立机器得到前十

        - 若可以放入内存直接堆排序

        - 若不可全放入内存：哈希分块 -> map统计 -> 归总堆排

    - 再将 100 台计算机的 TOP10 组合起来堆排序

- 同一元素可同时出现在不同机器中

    - 遍历所有数据，重新 hash 取模，使同一个元素只出现在单独的一台电脑中，然后采用上面方法先统计每台电脑 TOP10 再汇总起来

## 快速外排序问题

7. 有 10 个 1G 文件，每行都是一个可重复用户 query，按 query 频度排序。

- 顺序读取十个文件并采取哈希，将 query 写入 10 个文件中

- 通过 hash_map(query, count)统计每个 query 出现次数，至少 2G 内存

- 通过得到的 hash_map中 query 和 query_count，对 query_count排序并将重新输出到文件中，得到已排序好的文件

- 对十个文件进行归并排序（外排序）

## 公共数据问题

8. A,B两个文件各存放 50 亿 url，每个为 64Byte，限制内存 4G 找出公共 url。

- 对 A 和 B 两个大文件，先通过 url % 1000将数据映射到 1000 个文件中，单个文件大小约 320M（我们只需要检查对应小文件 A1 V B1......，不对应小文件不会有相同 url）

- 通过 hash_set统计，把 A1 的 url 存储到 hash_set中，再遍历对应的 B1 小文件，检查是否在 hash_set中，若存在则写入外存。重复循环处理对应的 1000 个对。

9. 1000w有重字符串，对字符串去重。

- 先 hash 分为多个文件

- 逐个文件检查并插入 set 中

- 多个 set 取交集

## 内存内 TOP N问题

10. 100w个数字找出最大 100 个。

- 堆排序法

    - 建大根堆，取走堆顶并重建堆，重复 100 次

- 快排法

    - 使用快速排序划分，若某次枢纽元在后 10000 时（具体情况具体分析），对后 10000 数据排序后取前 100

## 位图法

11. 在 2.5亿数字中找出不重复的整数。

- 使用 2-Bit位图法，00表示不存在，01表示出现一次，10表示出现多次，11无意义。这样只需要 1G 内存。

- 或者 hash 划分小文件，小文件使用 hash_set检查各个元素，得到的。

12. 如何在 40 亿数字中快速判断是否有某个数？

- 位图法标记某个数字是否存在，check标记数组。



# 相关

- [Skill-Tree](https://github.com/linw7/Skill-Tree)
