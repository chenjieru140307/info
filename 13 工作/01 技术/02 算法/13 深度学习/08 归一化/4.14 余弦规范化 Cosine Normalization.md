
# 余弦规范化 Cosine Normalization



Normalization 还能怎么做？

我们再来看看神经元的经典变换 $f_{\mathbf{w}}(\mathbf{x})=\mathbf{w} \cdot \mathbf{x}$.



对输入数据 $\mathbf{x}$ 的变换已经做过了，横着来是 LN，纵着来是 BN。

对模型参数 $\mathbf{w}$ 的变换也已经做过了，就是 WN。

好像没啥可做的了。



然而天才的研究员们盯上了中间的那个点，对，就是 $\cdot$。



他们说，我们要对数据进行规范化的原因，是数据经过神经网络的计算之后可能会变得很大，导致数据分布的方差爆炸，而这一问题的根源就是我们的计算方式——点积，权重向量 $\mathbf{w}$ 和 特征数据向量 $\mathbf{x}$ 的点积。向量点积是无界（unbounded）的啊！



那怎么办呢？我们知道向量点积是衡量两个向量相似度的方法之一。哪还有没有其他的相似度衡量方法呢？有啊，很多啊！夹角余弦就是其中之一啊！而且关键的是，夹角余弦是有确定界的啊，[-1, 1] 的取值范围，多么的美好！仿佛看到了新的世界！



于是，Cosine Normalization 就出世了。他们不处理权重向量 $\mathbf{w}$ ，也不处理特征数据向量 $\mathbf{x}$，就改了一下线性变换的函数：

$$
f_{\mathbf{w}}(\mathbf{x})=\cos \theta=\frac{\mathbf{w} \cdot \mathbf{x}}{\|\mathbf{w}\| \cdot\|\mathbf{x}\|}
$$

其中 $\theta$ 是 $\mathbf{w}$ 和 $\mathbf{x}$ 的夹角。然后就没有然后了，所有的数据就都是 $[-1, 1]$ 区间范围之内的了！



不过，回过头来看，CN 与 WN 还是很相似的。我们看到上式中，分子还是 $\mathbf{w}$ 和 $\mathbf{x}$ 的内积，而分母则可以看做用 $\mathbf{w}$ 和 $\mathbf{x}$ 二者的模之积进行规范化。对比一下 WN 的公式：

$$
f_{\mathrm{w}}(W N(\mathbf{x}))=f_{\mathrm{v}}\left(g \cdot \frac{\mathbf{x}}{\|\mathbf{v}\|}\right)
$$


一定程度上可以理解为，WN 用 权重的模 $\|\mathbf{v}\|$ 对输入向量进行 scale，而 CN 在此基础上用输入向量的模 $\|\mathbf{x}\|$ 对输入向量进行了进一步的 scale.



CN 通过用余弦计算代替内积计算实现了规范化，但成也萧何败萧何。原始的内积计算，其几何意义是 输入向量在权重向量上的投影，既包含 二者的夹角信息，也包含 两个向量的 scale 信息。去掉 scale 信息，可能导致表达能力的下降，因此也引起了一些争议和讨论。具体效果如何，可能需要在特定的场景下深入实验。







# 相关

- [详解深度学习中的 Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
