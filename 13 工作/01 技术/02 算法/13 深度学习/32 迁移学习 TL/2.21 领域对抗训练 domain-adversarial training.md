---
title: 2.21 领域对抗训练 domain-adversarial training
toc: true
date: 2019-09-01
---
# 可以补充进来的

# 领域对抗训练


领域对抗训练 Domain-adversarial training

源数据为标记数据，目标数据非标记。


## 举个例子

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190901/Bbxrh00h0AUN.png?imageslim">
</p>

上面的源数据对应的学习任务和目标数据对应的学习任务是比较相似的，都是做数字识别，但是两者的输入数据差别很大（目标数据加入了背景），那如何让源数据上学出来的模型也能在目标数据上发挥良好呢？

如果我们直接用 MNIST 的数据学一个 model，去做 MNIST-M的识别，效果是会非常差的。

而我们知道，一个 NN 前几层做的事情相当于特征抽取（feature extraction），后几层做的事相当于分类（Classification），所以我们将前面几层的输出结果拿出来看一下会是什么样子。

如下图，MNIST的数据可以看做蓝色的点，MNIST-M的点可以看做红色的点，所以它们分别对应的特征根本不一样，所以做迁移时当然效果很差。

![mark](http://images.iterate.site/blog/image/20190901/2HqIa6sO8FIx.png?imageslim)

于是思考：能不能让 NN 的前几层将不同域的各自特性消除掉，即把不同的域特性消除掉。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190901/EeR19cjzWadb.png?imageslim">
</p>


## 能不能把不同的域特性消除掉

怎么训练这样一个 NN 呢？

训练一个域分类器，让它无法准确地对特征提取器的结果进行分类。（“骗过”分类器，类似 GAN 的思想）。

但是，在 GAN 里面，生成器是要生成一张图片来骗过判别器，这个是很困难的，而这里如果只是要骗过域分类器就太简单了，直接让域分类器对任意输入都输出 0 就行了。

但是这样学出来的特征提取器很可能是完全无效的。所以，我们应该给特征提取器增加学习难度。

所以，我们又要求特征提取器的输出能够满足标签预测器（label predictor）的高精度判别需求。<span style="color:red;">不错不错。</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190901/rHYUcObeabKK.png?imageslim">
</p>

那么如何让特征提取器的训练满足我们上述两个要求呢？

也不难：

- 针对标签预测器的反向传播的误差，我们按照误差的方向进行正常的参数调整，
- 针对域分类器反向传回的误差，我们则按照误差的反方向进行误差调整（即域分类器要求调高某个参数值以提高准确度，而我们就故意调低对应参数值，以“欺负”它）。所以在域分类器的误差上加个负号就可以。<span style="color:red;">嗯，赞。</span>

即：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190901/WYIrLfGeQU6h.png?imageslim">
</p>


但是还需要注意的是，我们要让域分类器“奋力挣扎”，否则如果特征提取器随便生成的输出都能够骗过域生成器，那可能是域生成器本身的判别能力太差，那这样训练出来的特征提取器可能也其实是很弱的。



下图是对应论文的一个实验结果：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190901/CAOdokYH67sB.png?imageslim">
</p>


<span style="color:red;">感觉这个太有价值了！简直是少量数据时候的学习的福音！</span>



# 相关

- [迁移学习（Transfer Learning）](https://blog.csdn.net/qq_32690999/article/details/78849565)
