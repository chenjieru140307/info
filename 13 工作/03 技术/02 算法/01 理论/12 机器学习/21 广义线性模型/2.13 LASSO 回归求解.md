


## LASSO回归求解

由于 $L_{1}$ 范数用的是绝对值，导致 LASSO 的优化目标不是连续可导的，也就是说，最小二乘法，梯度下降法，牛顿法，拟牛顿法都不能用。

$L_{1}$ 正则化问题求解可采用近端梯度下降法（Proximal Gradient Descent，PGD）。

（1）优化目标

优化目标为：

$$
\min _{x}\left[f(x)+\lambda\|x\|_{1}\right.
$$

若 $f(x)$ 可导，梯度 $\nabla f(x)$ 满足 L-Lipschitz条件（利普希茨连续条件），即存在常数 $L>0$，使得：

$$
\frac{\left\|\nabla f\left(x^{\prime}\right)-\nabla f(x)\right\|_{2}^{2}}{\left\|x^{\prime}-x\right\|_{2}^{2}} \leq L, \forall\left(x, x^{\prime}\right)\tag{11}
$$

> L-Lipschitz（利普希茨连续条件）定义：
>
> 对于函数 $f(x)$，若其任意定义域中的 $x_{1}, x_{2}$ 都存在 $L>0$，使得 $\left|f\left(x_{1}\right)-f\left(x_{2}\right)\right| \leq L\left|x_{1}-x_{2}\right|$，即对于 $f(x)$ 上每对点，连接它们的线的斜率的绝对值总是不大于这个实数 $L$。

（2）泰勒展开

在 $x_{k}$ 处将 $f(x)$ 进行二阶泰勒展开：

$$
f(x)=f\left(x_{k}\right)+\nabla f\left(x_{k}\right)\left(x-x_{k}\right)+\frac{f^{\prime \prime}\left(x_{k}+\xi\right)}{2}\left(x-x_{k}\right)^{2}\tag{12}
$$

由（11）式，泰勒将展开式的二阶导用代替，得到：

$$
f(x) \approx f\left(x_{k}\right)+\nabla f\left(x_{k}\right)\left(x-x_{k}\right)+\frac{L}{2}\left(x-x_{k}\right)^{2}\tag{13}
$$

（3）简化泰勒展开式

将（13）式化简：

$$
\begin{array}{l}{f\left(x_{k}\right)+\nabla f\left(x_{k}\right)\left(x-x_{k}\right)+\frac{L}{2}\left(x-x_{k}\right)^{2}} \\ {=\frac{L}{2}\left[\left(x-x_{k}\right)^{2}+\frac{2}{L} \nabla f\left(x_{k}\right)\left(x-x_{k}\right)+\frac{1}{L^{2}}\left(\nabla f\left(x_{k}\right)\right)^{2}\right]-\frac{L}{2} \frac{1}{L^{2}}\left(\nabla f\left(x_{k}\right)\right)^{2}+f\left(x_{k}\right)} \\ {=\frac{L}{2}\left[x-\left(x_{k}-\frac{1}{L} \nabla f\left(x_{k}\right)\right)\right]^{2}+\varphi\left(x_{k}\right)} \\ {=\frac{L}{2}\left\|x-\left(x_{k}-\frac{1}{L} \nabla f\left(x_{k}\right)\right)\right\|_{2}^{2}+\varphi\left(x_{k}\right)}\end{array}\tag{14}
$$

其中，$\varphi\left(x_{k}\right)=f\left(x_{k}\right)-\frac{1}{2 L}\left(\nabla f\left(x_{k}\right)\right)^{2}$ 是 $x$ 无关的常数。

（4）简化优化问题

这里若通过梯度下降法对 $f(x)$（ $f(x)$ 连续可导）进行最小化，则每一步下降迭代实际上等价于最小化二次函数 $\widehat{f}(x)$，推广到优化目标（10），可得到每一步迭代公式：

$$
x_{k+1}=\underset{x}{\arg \min }\left[\frac{L}{2}\left\|x-\left(x_{k}-\frac{1}{L} \nabla f\left(x_{k}\right)\right)\right\|_{2}^{2}+\lambda\|x\|_{1}\right]\tag{15}
$$

令 $z=x_{k}-\frac{1}{L} \nabla f\left(x_{k}\right)$，

则可以先求 $z$，再求解优化问题：

$$
x_{k+1}=\underset{x}{\arg \min }\left[\frac{L}{2}\|x-z\|_{2}^{2}+\lambda\|x\|_{1}\right]\tag{16}
$$

（5）求解

令 $x^{i}$ 为 $x$ 的第 $i$ 个分量，将（16）式按分量展开，其中不存在 $x^{i} x^{j}(i \neq j)$ 这样的项，即 $x$ 的各分量之间互不影响，所以（12）式有闭式解。

> 为什么（16）式不存在 $x^{i} x^{j}(i \neq j)$ 这样的项？
>
> 因为展开（16）式得到，
>
> $$
> \begin{array}{l}{\arg \min _{x}\left[\frac{L}{2}\|x-z\|_{2}^{2}+\lambda\|x\|_{1}\right]} \\ {=\underset{x}{\arg \min }\left(\frac{L}{2}\left\|x^{1}-z^{1}\right\|_{2}^{2}+\lambda\left\|x^{1}\right\|_{1}\right)+\underset{x}{\arg \min }\left(\frac{L}{2}\left\|x^{2}-z^{2}\right\|_{2}^{2}+\lambda\left\|x^{2}\right\|_{1}\right)+\cdots} \\ {+\underset{x}{\arg \min }\left(\frac{L}{2}\left\|x^{d}-z^{d}\right\|_{2}^{2}+\lambda\left\|x^{d}\right\|_{1}\right)}\end{array}
> $$

从而优化问题变为求解 $d$ 个独立的函数：$f(x)=(x-z)^{2}+\lambda\|x\|_{1}$。

对于上述优化问题需要用到 soft thresholding软阈值函数（证明见参考文献 2），即对于优化问题：
$$
\underset{x}{\arg \min }\left[\|x-z\|_{2}^{2}+\lambda\|x\|_{1}\right]\tag{17}
$$

其解为：

$$
\operatorname{prox}_{u}(z)=\operatorname{sign}(z) \max \{|z|-u, 0\}\tag{18}
$$

而我们的优化问题为（16）式，则得到闭式解为：

$$
x_{k+1}^{i}=\left\{\begin{array}{ccc}{z^{i}-\frac{2 \lambda}{L}} & {,} & {z^{i}>\frac{2 \lambda}{L}} \\ {0} & {,} & {-\frac{2 \lambda}{L}<z^{i}<\frac{2 \lambda}{L}} \\ {z^{i}+\frac{2 \lambda}{L}} & {,} & {z^{i}<-\frac{2 \lambda}{L}}\end{array}\right\}\tag{19}
$$

其中，$x_{k+1}^{i}$ 与 $z^{i}$ 分别是 $x_{k+1}$ 与 $z$ 的第 $i$ 个分量。因此，通过 PGD 能使 LASSO 和其他基于 $L_{1}$ 范数最小化的方法得以快速求解。








# 相关

- [【机器学习】一文读懂正则化与 LASSO 回归，Ridge回归](https://blog.csdn.net/pxhdky/article/details/82960659)
