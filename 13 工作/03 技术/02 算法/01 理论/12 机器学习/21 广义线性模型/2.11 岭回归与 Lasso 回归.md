---
title: 2.11 岭回归与 Lasso 回归
toc: true
date: 2019-08-27
---
# 岭回归与 Lasso 回归

岭回归与 Lasso 回归的出现是为了解决：

- 线性回归出现的过拟合
- 在通过正规方程方法求解 $\theta$ 的过程中出现的 $x$ 转置乘以 $x$ 不可逆

这两类问题的。


## 正则化形式

正则化一般具有如下形式的优化目标：

$$
\min _{f \in F}\left[\frac{1}{m} \sum_{i=1}^{m} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)\right]
$$


其中，$\lambda \geq 0$ 是用来平衡正则化项和经验风险的系数。

正则化项可以是模型参数向量的范数，经常用的有 $L_{1}$ 范数，$L_{2}$ 范数（ $L_{1}$ 范数：$\|x\|_{1}=\sum_{i=1}^{m} | x_{i}$，$L_{2}$ 范数:$\|x\|_{2}=\sqrt{\sum_{i=1}^{m} x_{i}^{2}}$) 。

我们考虑最简单的线性回归模型。

给定数据集 $D=\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{m}$，其中，$x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i d}\right), y_{i} \in R$。

代价函数为：

$$
J(w)=\frac{1}{m}\left\|y-w^{T} X\right\|^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}
$$

（1）范数正则化（Ridge Regression，岭回归）

代价函数为：

$$
J(w)=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\|w\|_{2}^{2}(\lambda>0)
$$

（2）范数正则化（LASSO，Least Absoulute Shrinkage and Selection Operator，最小绝对收缩选择算子）

代价函数为：

$$
J(w)=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\|w\|_{1}(\lambda>0)
$$

（3）正则项正则项结合（Elastic Net）

代价函数为：

$$
J(w)=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\left(\rho\|w\|_{1}+(1-\rho)\|w\|_{2}^{2}\right)
$$

其中，$L_{1}$ 范数正则化、$L_{2}$ 范数正则化都有助于降低过拟合风险，$L_{2}$ 范数通过对参数向量各元素平方和求平方根，使得 $L_{2}$ 范数最小，从而使得参数的各个元素接近 0 ，但不等于 0。 

而 $L_{1}$ 范数正则化比范数更易获得“稀疏”解，即 $L_{1}$ 范数正则化求得的会有更少的非零分量，所以 $L_{1}$ 范数可用于特征选择，而 $L_{2}$ 范数在参数规则化时经常用到（事实上，$L_{0}$ 范数得到的“稀疏”解最多，但 $L_{0}$ 范数 $\|x\|=\#\left(i | x_{i} \neq 0\right)$ 是 $x$ 中非零元素的个数，不连续，难以优化求解。因此常用 $L_{1}$ 范数来近似代替）。



$\lambda$ 称为正则化参数，如果 $\lambda$ 选取过大，会把所有参数 $\theta$ 均最小化，造成欠拟合，如果 $\lambda$ 选取过小，会导致对过拟合问题解决不当，因此 $\lambda$ 的选取是一个技术活。


岭回归与 Lasso 回归最大的区别在于：

- 岭回归引入的是 L2 范数惩罚项，
- Lasso回归引入的是 L1 范数惩罚项，

Lasso回归能够使得损失函数中的许多 $\theta$ 均变成 $0$，这点要优于岭回归，因为岭回归是要所有的 $\theta$ 均存在的，这样计算量 Lasso 回归将远远小于岭回归。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190827/GtX7y4VMg6s8.png?imageslim">
</p>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190827/uybLI8Qbfpao.png?imageslim">
</p>

可以看到，Lasso回归最终会趋于一条直线，原因就在于好多  $\theta$ 值已经均为 $0$，而岭回归却有一定平滑度，因为所有的 $\theta$ 值均存在。




# 相关

- [【机器学习】一文读懂正则化与 LASSO 回归，Ridge回归](https://blog.csdn.net/pxhdky/article/details/82960659)
