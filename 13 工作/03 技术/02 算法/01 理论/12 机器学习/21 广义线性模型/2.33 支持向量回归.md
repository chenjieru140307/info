---
title: 2.33 支持向量回归
toc: true
date: 2019-08-27
---
# 支持向量回归

上面的核岭回归由于核函数矩阵不稀疏，计算量比较大。而之前 SVM 则只考虑少部分的 Support Vectors，得到的 $\alpha$ 是 sparse 的，这一节就从 Soft-Margin的角度来重新设计 Linear Regression的误差衡量。

定义 Tube Regression的误差为:
$$
err(y,s)=\max(0, |s-y|-\epsilon), \quad s=\mathbf w^Tz_n +b, \ \epsilon > 0
$$
也就是说，同 Soft-Margin SVM分类问题容许偏差的存在，如果预测值与实际值的差距小于 $\epsilon$ 的时候，就认为 $err=0$，tube可以理解为预测误差在允许范围内，如下图对比 tube error与 square error：


<center>

![mark](http://images.iterate.site/blog/image/20190827/CQf3WpgS3DyK.png?imageslim)


</center>
仿照 SVM，加上 $L_2$ 正则项的 Tube Regression的目标函数定义为:
$$
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w+C\sum_{n=1}^N\max\left(0, \left\lvert \mathbf w^T\mathbf z_n+b-y_n\right\rvert-\epsilon\right)\right)
$$
不过由于含有 max 项，无法微分因此求解很困难，我们可以参考在 SVM 的方法将问题转为二次规划问题来解决。下面引入新的松弛变量 $\xi_n \geq 0$ 表示第 $n$ 个数据的误差，这样目标函数转化为:
$$
\begin{aligned} \min\limits_{b,\mathbf w} \quad  &\left({1\over 2}\mathbf w^T\mathbf w+C\sum_{n=1}^N\xi_n)\right) \\ s.t. \quad & \left\lvert \mathbf w^T\mathbf z_n+b-y_n\right\rvert \leq\epsilon+\xi_n \\ &for \ all \ n, \xi_n \geq 0 \end{aligned}
$$
这样的话，对于误差不大于 $\epsilon$ 的数据，$\xi_n=0$，反之则 $\xi_n > 0$，不过到此为止，还是不够的，因为约束条件里面有绝对值，这个还是无法用二次规划求解的。下面去掉绝对值，根绝取值的正负分两种情况，把 $\xi_n$ 拆成两个个 $\xi_n^{\wedge}$, $\xi_n^{\vee}$，分别真实值大于模型得到的值，以及真实值小于模型得到的值的情形，得到的最终目标函数:
$$
\begin{aligned} \min\limits_{b,\mathbf w} \quad  &\left({1\over 2}\mathbf w^T\mathbf w+C\sum_{n=1}^N(\xi_n^{\vee}+\xi_n^{\wedge})\right) \\ s.t. \quad &  \mathbf y_n-\mathbf w^T\mathbf z_n-b \leq\epsilon+\xi_n^{\wedge} \ \ \ \quad (1) \\ & \mathbf y_n-\mathbf w^T\mathbf z_n-b \geq -\epsilon-\xi_n^{\vee}  \quad (2)\\&\xi_n^{\wedge}, \xi_n^{\vee} \geq 0 \end{aligned}
$$
当真实值大于模型值的时候，需要满足约束条件(1)，当真实值小于模型值的时候，需要满足约束条件(2)，这样就是一个标准的二次规划的问题了。

到此为止，支持向量回归的原始问题(SVR Primal)就得到了，考察一下这个 QP 的规模: 一共有 $\tilde{d}+1+2N$ 个变量(\mathbf w是 Z-Space的权重，维度为 $\tilde{d}+1)$，$2N+2N$ 个约束条件，跟 SVM 的原始问题存在一样的缺陷，原始数据映射到 Z-Space之后，维度太高，计算量过大。因此就引出了 SVR Dual(支持向量回归的对偶问题)。

SVR Dual

对偶问题首先需要加入拉格朗日乘子，因为每个数据有四个约束条件，因此引入四个 $\alpha_n^{\wedge}, \alpha_n^{\vee},\beta_n^{\wedge}, \beta_n^{\vee}\geq0$:
$$
\begin{aligned}\mathcal L(b, \mathbf w, \alpha^{\vee}, \alpha^{\wedge}) = &{1\over 2}\mathbf w^T \mathbf w+C\sum(\xi_n^{\vee}+\xi_n^{\wedge}) \\ +&\sum\alpha_n^{\wedge}(\mathbf y_n - \mathbf w^T\mathbf z_n -b-\epsilon-\xi_n^{\wedge}) \\+&\sum\alpha_n^{\vee}(-\epsilon-\xi_n^{\wedge}-\mathbf y_n + \mathbf w^T\mathbf z_n +b) \\+ & \sum\beta_n^{\wedge}(-\xi_n^{\wedge}) + \sum\beta_n^{\vee}(-\xi_n^{\vee})\end{aligned}
$$
根据强对偶理论，以及在对偶 SVM 介绍的内容，Primal SVR 与下面的对偶问题等价:
$$
\max \limits_{\alpha_n^{\vee}, \alpha_{n}^{\wedge}>0} \left( \min\limits_{b, \mathbf w, \epsilon} \mathcal L(b, \mathbf w, \xi^{\vee}, \xi^{\wedge})\right)
$$
下面推导 Dual SVR(课程中略):

首先求各个变量微分优化 $\min$:
$$
\begin{aligned} &{ \partial \mathcal L \over \partial \xi_n^{\wedge}} = C-\alpha_n^{\wedge}-\beta_n^{\wedge} = 0 \quad \quad \quad \quad(1) \\ & {\partial \mathcal L \over \partial \xi_n^{\vee}} = C-\alpha_n^{\vee}-\beta_n^{\vee} = 0 \quad \quad \quad \quad (2)\\ &{\partial  \mathcal L \over \partial b} = \sum(\alpha_n^{\vee}-\alpha_n^{\wedge}) = 0 \quad \quad \quad\quad (3)\\ & {\partial \mathcal L \over \partial \mathbf w } = \mathbf w - \sum(\alpha_n^{\wedge}- \alpha_n^{\vee})\mathbf z_n = 0 \quad(4)\end{aligned}
$$
一共得到了 4 个(组)等式，这几个等式最优解也会满足，其中 $1 \leq\alpha_n \leq C$，将 $\beta^{\vee}$, $\beta^{\wedge}$，以及(3)式代入到 $\mathcal L$ 中，消去 $\beta$,$b$,$\xi_n$，经过化简得到:
$$
\begin{aligned}\min\limits_{\mathbf w}  \mathcal{L}(b, \mathbf w, \xi_{\vee}, \xi^{\wedge}) &\Leftrightarrow \min \quad {1\over 2}\mathbf w^T\mathbf w + \sum(\alpha_n^{\vee}-\alpha_n^{\wedge})\mathbf w^T\mathbf z_n +\sum(\alpha_n^{\wedge}(y_n-\epsilon) + \alpha_n^{\vee}(-\epsilon-y_n) ) \\ &\Leftrightarrow \min \quad  {1\over 2}\mathbf w^T\mathbf w -\mathbf w^T\mathbf w +\sum(\alpha_n^{\wedge}(y_n-\epsilon) + \alpha_n^{\vee}(-\epsilon-y_n) ) \\ &\Leftrightarrow -{1 \over 2}\sum\limits_{n=1}^N\sum\limits_{m=1}^N(\alpha_n^{\wedge}- \alpha_n^{\vee}) (\alpha_m^{\wedge}- \alpha_m^{\vee})K(\mathbf x_n, \mathbf x_m)\\ & \quad \quad  +\sum(\alpha_n^{\wedge}(y_n-\epsilon) + \alpha_n^{\vee}(-\epsilon-y_n) ) \end{aligned}
$$
上面的过程我们已经引入了核函数代替 $\mathbf {z_nz_m}$，至此最小化过程完成，开始考虑最外面 $\max$ 部分，取相反数，结合约束来求最小值，最后的问题如下:

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190827/0bscfQ5cXwvg.png?imageslim">
</p>

上式就是一个标准的二次规划问题，一共有 $2N$ 个变量，按照格式写出 QP Solver的 Q,p,A,c，即可得到最优解 $\alpha_n^{\vee}, \alpha_n^{\wedge}$。

最后返回到模型，在对偶问题中，我们还有一个很重要的互补条件(complementary slackness)，如下:
$$
\begin{aligned}\alpha_n^\wedge\left(\epsilon+\xi_n^\wedge-y_n+\mathbf w^T\mathbf z_n+b\right)&=0\\ \alpha_n^\vee\left(\epsilon+\xi_n^\vee+y_n-\mathbf w^T\mathbf z_n-b\right)&=0 \\ \beta_n^{\vee}\xi_n^{\vee}=(C-\alpha_n^{\vee})\xi_{n}^{\vee} &=0 \\ \beta_n^{\wedge}\xi_n^{\wedge}=(C-\alpha_n^{\wedge})\xi_{n}^{\wedge} &=0 \end{aligned}
$$
现在考虑一些点: $|\mathbf w^T\mathbf z_n +b - y_n| < \epsilon$，也就是那些在 tube 内部的，预测值与真实值的误差在允许范围内的点，对于它们，以下条件成立:
$$
\begin{aligned} & \xi_n^{\vee} = \xi_n^{\wedge} = 0 \\ \Rightarrow &\epsilon -y_n+\mathbf w^T\mathbf z +b\neq0  \\ & \epsilon +y_n-\mathbf w^T\mathbf z -b\neq0 \\ \Rightarrow & \alpha_n^{\vee} =0 \\ & \alpha_n^{\wedge}=0 \end{aligned}
$$
根据前面条件 $\mathbf w=\sum\limits_{n=1}^N(\alpha_n^{\wedge}-\alpha_n^{\vee})\mathbf z_n$，我们开始的目标之一便是想得到一个比较稀疏的系数, 现在对于在误差范围内的点 $\mathbf z_n$ 的系数已经为 0。只有在 tube 外面的数据点的不为 0，这些点也就是 Supports Vectors。

对于 b 的解法，同 soft-margin SVM，从 Support Vectors中考虑，只需要找到某个 $0<\alpha_s<C$ 的数据，这样由互补约束便可知 $\xi_s=0$，从而得到 b:
$$
b = \epsilon + y_s -\mathbf w^T\mathbf z_s = \epsilon + y_s -\sum(\alpha_n^{\wedge}-\alpha_n^{\vee})K(\mathbf x_s, x_n)
$$
最后得到 Support Vector Regression(SVR)模型: $g(\mathbf x)=\sum(\alpha_n^{\wedge}-\alpha_n^{\vee})K(\mathbf x_n, \mathbf x) + b$



# 相关

- [机器学习技法笔记(8)-SVR(支持向量回归)](https://shomy.top/2017/03/09/support-vector-regression/)
