---
title: 1.05 PCA 与 LDA 比较
toc: true
date: 2019-03-31
---
# 可以补充进来的

- Eigenface 补充下。
- PCA 和 LDA 的核映射补充下。

# PCA 与 LDA


同样作为线性降维方法，PCA是有监督的降维算法，而 LDA 是无监督的降维算法。



- LDA：分类性能最好的方向
- PCA：样本点投影具有最大方差的方向


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/d4CH9ikjH6.png?imageslim">
</p>

虽然在原理或应用方面二者有一定的区别，但是从这两种方法的数学本质出发，我们不难发现二者有很多共通的特性。



| 异同点 | LDA                                                                                                                 | PCA                                |
|:------:|:------------------------------------------------------------------------------------------------------------------- |:---------------------------------- |
| 相同点 | 1. 两者均可以对数据进行降维；<br />2. 两者在降维时均使用了矩阵特征分解的思想；<br />3. 两者都假设数据符合高斯分布；<span style="color:red;">都假设数据符合高斯分布了吗？LDA 也是吗？</span> |                                    |
| 不同点 | 有监督的降维方法；                                                                                                  | 无监督的降维方法<span style="color:red;">PCA 是无监督的降维吗？</span>；                 |
|        | 降维最多降到 k-1维；                                                                                                 | 降维多少没有限制；<span style="color:red;">没有上限吗？</span>                 |
|        | 可以用于降维，还可以用于分类；                                                                                    | 只用于降维；                       |
|        | 选择分类性能最好的投影方向；                                                                                        | 选择样本点投影具有最大方差的方向；<span style="color:red;">再看下 PCA ，再理解下这里。</span><span style="color:blue;">嗯，是的，下面有说，是投影到样本集方差最大的那个方向</span> |
|        | 更明确，更能反映样本间差异；                                                                                        | 目的较为模糊；                     |




## 从数学推导的角度，两种降维算法在目标函数上有何区别与联系？

首先将 LDA 扩展到多类高维的情况，以和问题 1 中 PCA 的求解对应。

假设有 $N$ 个类别，并需要最终将特征降维至 $d$ 维。因此，我们要找到一个 $d$ 维投影超平面使得投影后的样本点满足 LDA 的目标——最大化类间距离和最小化类内距离。<span style="color:red;">是的。</span>


回顾两个散度矩阵，类内散度矩阵 $S_{w}=\sum_{x \in C_{i}}\left(x-\mu_{i}\right)\left(x-\mu_{i}\right)^{\mathrm{T}}$ 在类别增加至 N 时仍满足定义，而之前两类问题的类间散度矩阵 $S_{b}=\left(\mu_{1}-\mu_{2}\right)\left(\mu_{1}-\mu_{2}\right)^{T}$ 在类别增加后就无法按照原始定义。<span style="color:red;">是呀。</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190331/02AccSmufaWG.png?imageslim">
</p>

> 图 4.6是三类样本的分布情况，其中 $\mu_1,\mu_2,\mu_3$ 分别表示棕绿黄三类样本的中心，$\mu$ 表示这三个中心的均值（也即全部样本的中心），$S_{wi}$ 表示第 $i$ 类的类内散度。我们可以定义一个新的矩阵 $S_t$，来表示全局整体的散度，称为全局散度矩阵：<span style="color:red;">嗯，好像是合理的。</span>

$$
S_{t}=\sum_{i=1}^{n}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{\mathrm{T}}
$$

如果把全局散度定义为类内散度与类间散度之和，即 $S_t=S_b+S_w$，那么类间散度矩阵可表示为：<span style="color:red;">嗯。</span>


$$
\begin{aligned} \boldsymbol{S}_{b} &=\boldsymbol{S}_{t}-\boldsymbol{S}_{\boldsymbol{w}} \\ &=\sum_{i=1}^{n}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}\right)\left(x_{i}-\boldsymbol{\mu}\right)^{\mathrm{T}}-\sum_{\boldsymbol{x} \in \mathcal{C}_{i}}\left(x-\boldsymbol{\mu}_{i}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)^{\mathrm{T}} \\&=\sum_{j=1}^{N}\left(\sum_{x \in C_{j}}(x-\mu)(x-\mu)^{\mathrm{T}}-\sum_{x \in C_{j}}\left(x-\mu_{j}\right)\left(x-\mu_{j}\right)^{\mathrm{T}}\right)\end{aligned}\tag{4.29}
$$

其中 $m_j$ 是第 $j$ 个类别中的样本个数，$N$ 是总的类别个数。从式（4.29）可以看出，类间散度表示的就是每个类别中心到全局中心的一种加权距离。

我们最大化类间散度实际上优化的是每个类别的中心经过投影后离全局中心的投影足够远。<span style="color:red;">有些厉害。</span>

根据 LDA 的原理，可以将最大化的目标定义为：

$$
J(\boldsymbol{W})=\frac{\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{S}_{b} \boldsymbol{W}\right)}{\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{S}_{w} \boldsymbol{W}\right)}
$$


其中 $W$ 是需要求解的投影超平面，$W^TW=I$，根据问题 2 和问题 3 中的部分结论，我们可以推导出最大化 $J(W)$ 对应了以下广义特征值求解的问题：

$$
S_{b} \omega=\lambda S_{w} \omega
$$


求解最佳投影平面 $W=\{\omega_1,\omega_2,...,\omega_d\}$ 即求解 $S_w^{-1}S_b$ 矩阵特征值前 $d$ 大对应的特征向量组成的矩阵，这就将原始的特征空间投影到了新的 $d$ 维空间中。至此我们得到了与 PCA 步骤类似，但具有多个类别标签高维数据的 LDA 求解方法。

1. 计算数据集中每个类别样本的均值向量 $μ_j$，及总体均值向量 $μ$。
2. 计算类内散度矩阵 $S_w$，全局散度矩阵 $S_t$，并得到类间散度矩阵 $S_b=S_t-S_w$ 。
3. 对矩阵 $S_w^{-1}S_b$ 进行特征值分解，将特征值从大到小排列。
4. 取特征值前 d 大的对应的特征向量 $\omega_1,\omega_2,...,\omega_d$，通过以下映射将 n 维样本映射到 d 维．

$$
\boldsymbol{x}_{i}^{\prime}=\left[ \begin{array}{c}{\boldsymbol{\omega}_{1}^{\mathrm{T}} \boldsymbol{x}_{i}} \\ {\omega_{2}^{\mathrm{T}} \boldsymbol{x}_{i}} \\ {\vdots} \\ {\boldsymbol{\omega}_{d}^{\mathrm{T}} \boldsymbol{x}_{i}}\end{array}\right]\tag{4.32}
$$




从 PCA 和 LDA 两种降维方法的求解过程来看，它们确实有着很大的相似性，但对应的原理却有所区别。


首先从目标出发：

- PCA选择的是投影后数据方差最大的方向。由于它是无监督的，因此 PCA 假设方差越大，信息量越多，用主成分来表示原始数据可以去除冗余的维度，达到降维。
- 而 LDA 选择的是投影后类内方差小、类间方差大的方向。其用到了类别标签信息，为了找到数据中具有判别性的维度，使得原始数据在这些方向上投影后，不同类别尽可能区分开。


## 从应用的角度分析其原理的异同？


### 在语音识别中

- 当我们想从一段音频中提取出人的语音信号，这时可以使用 PCA 先进行降维，过滤掉一些固定频率（方差较小）的背景噪声。
- 但如果我们的需求是从这段音频中区分出声音属于哪个人，那么我们应该使用 LDA 对数据进行降维，使每个人的语音信号具有区分性。

### 在人脸识别领域中

在人脸识别领域中，PCA 和 LDA 都会被频繁使用：

基于 PCA 的人脸识别方法也称为特征脸（Eigenface）方法，该方法将人脸图像按行展开形成一个高维向量，对多个人脸特征的协方差矩阵做特征值分解，其中较大特征值对应的特征向量具有与人脸相似的形状，故称为特征脸。Eigenface for Recognition 一文中将人脸用 7 个特征脸表示（见图 4.7），于是可以把原始 65536 维的图像特征瞬间降到 7 维，人脸识别在降维后的空间上进行。<span style="color:red;">这样的方法效果怎么样？为什么感觉这个可以用在我现在的一个定位项目上？嗯，想了解下，可以用于定位上面吗？嗯，最好找个例子看下。补充下。</span>


然而由于其利用 PCA 进行降维，一般情况下保留的是最佳描述特征（主成分），而非分类特征。

如果我们想要达到更好的人脸识别效果，应该用 LDA 方法对数据集进行降维，使得不同人脸在投影后的特征具有一定区分性。<span style="color:red;">嗯。</span>

<span style="color:red;">感觉说的非常在理！</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190331/5u720hSA0SHS.png?imageslim">
</p>


从应用的角度，我们可以掌握一个基本的原则——对无监督的任务使用 PCA 进行降维，对有监督的则应用 LDA。<span style="color:red;">嗯。</span>

## 总结与扩展

至此，我们从数学原理、优化目标以及应用场景的角度对比了 PCA 和 LDA 这两种经典的线性降维方法，对于非线性数据，可以通过核映射等方法对二者分别进行扩展以得到更好的降维效果。关于特征脸这一降维应用，有兴趣的读者可以拜读最经典的 Eigenface 论文，更好地理解降维算法的实际应用。<span style="color:red;">嗯，这个 Eigenface 的论文还是需要总结进来的。而且，想知道 PCA 和 LDA 这两种的核映射要怎么处理，包括理论和实际的使用方式。以及使用效果。</span>


# 相关

- 《百面机器学习》
- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions) 原文
