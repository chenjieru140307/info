

# 可以补充进来的


- <span style="color:red;">没有总结，暂时放在这里，需要总结。</span>
- 之前在视频中看到的时候，lda 是与 pca 等降维一起讲的，没想到这本书里会放在线性模型里面。<span style="color:red;">嗯，文章最后也提了这也是一种降维方法。</span>


# 线性判别分析 LDA 介绍

## 前置知识

- 拉格朗日乘子法
- 奇异值分解


线性判别分析 (Linear Discriminant Analysis，简称 LDA) 是一种经典的线性学习方法， 在二分类问题上因为最早由 [Fisher ， 1936] 提出 ， 亦称 "Fisher 判别分析"。严格说来 LDA 与 Fisher 判别分析稍有不同，前者假设了各类样本的协方差矩阵相同且满秩。<span style="color:red;">什么是协方差矩阵相同且满秩？</span>


给定数据集 $D=\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{m}, y_{i} \in\{0,1\}$ ，令 $X_{i}$ 、$\boldsymbol{\mu}_{i}$ 、$\boldsymbol{\Sigma}_{i}$ 分别表示第 $i \in\{0,1\}$ 类示例的集合、均值向量、协方差矩阵。

如果若将数据投影到直线 $\boldsymbol{w}$ 上， 则两类样本的中心在直线上的投影分别为 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{0}$ 和 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{1}$ ；若将所有样本点都投影到直线上，则两类样本的协方差分别为 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{0} \boldsymbol{w}$ 和 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{1} \boldsymbol{w}$ 。由于直线是 一维空间，因此 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{0}$ 、$\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{1}$ 、$\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{0} \boldsymbol{w}$ 和 $\boldsymbol{w}^{\mathbf{T}} \boldsymbol{\Sigma}_{1} \boldsymbol{w}$ 均为实数。

欲使同类样例的投影点尽可能接近，我们可以让同类样例投影点的协方差尽可 能小，即 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{0} \boldsymbol{w}+\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{1} \boldsymbol{w}$ 尽可能小；而欲使异类样例的投影点尽可能远离, 可以让类中心之间的距离尽可能大，即 $\left\|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{0}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{1}\right\|_{2}^{2}$ 尽可能大。同时考虑二者，则可得到欲最大化的目标：


$$
\begin{aligned} J &=\frac{\left\|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{0}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{1}\right\|_{2}^{2}}{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{0} \boldsymbol{w}+\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{1} \boldsymbol{w}} \\ &=\frac{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\Sigma}_{0}+\boldsymbol{\Sigma}_{1}\right) \boldsymbol{w}} \end{aligned}\tag{3.32}
$$


> $$J=\cfrac{\boldsymbol w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^T\boldsymbol w}{\boldsymbol w^T(\Sigma_0+\Sigma_1)\boldsymbol w}$$
>
> [推导]：
> $$\begin{aligned}
> 	J &= \cfrac{\big|\big|\boldsymbol w^T\mu_0-\boldsymbol w^T\mu_1\big|\big|_2^2}{\boldsymbol w^T(\Sigma_0+\Sigma_1)\boldsymbol w} \\
> 	&= \cfrac{\big|\big|(\boldsymbol w^T\mu_0-\boldsymbol w^T\mu_1)^T\big|\big|_2^2}{\boldsymbol w^T(\Sigma_0+\Sigma_1)\boldsymbol w} \\
> 	&= \cfrac{\big|\big|(\mu_0-\mu_1)^T\boldsymbol w\big|\big|_2^2}{\boldsymbol w^T(\Sigma_0+\Sigma_1)\boldsymbol w} \\
> 	&= \cfrac{[(\mu_0-\mu_1)^T\boldsymbol w]^T(\mu_0-\mu_1)^T\boldsymbol w}{\boldsymbol w^T(\Sigma_0+\Sigma_1)\boldsymbol w} \\
> 	&= \cfrac{\boldsymbol w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^T\boldsymbol w}{\boldsymbol w^T(\Sigma_0+\Sigma_1)\boldsymbol w}
> \end{aligned}$$

我们定义 “类内散度矩阵” (within-class scatter matrix)：


$$
\begin{aligned} \mathbf{S}_{w} &=\boldsymbol{\Sigma}_{0}+\boldsymbol{\Sigma}_{1} \\ &=\sum_{\boldsymbol{x} \in X_{0}}\left(\boldsymbol{x}-\boldsymbol{\mu}_{0}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{0}\right)^{\mathrm{T}}+\sum_{\boldsymbol{x} \in X_{1}}\left(\boldsymbol{x}-\boldsymbol{\mu}_{1}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \end{aligned}\tag{3.33}
$$


以及 “类间散度矩阵”  (between-class scatter matrix)


$$
\mathbf{S}_{b}=\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}}\tag{3.34}
$$


则上面的最大化的目标 $J$ 可以重写为：

$$
J=\frac{\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{b} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{w} \boldsymbol{w}}\tag{3.35}
$$


这就是 LDA 欲最大化的目标，即 $\mathbf{S}_{b}$ 与 $\mathbf{S}_{w}$ 的 “广义瑞利商” (generalized Rayleigh quotient)。

如何确定 $\boldsymbol{w}$ 呢？注意到这个广义瑞利商的分子和分母都是关于 $\boldsymbol{w}$ 的二次项，因此它的解与 $\boldsymbol{w}$ 的长度无关，只与其方向有关。（若 $\boldsymbol{w}$ 是一个解，则对于任意常数 $\alpha$, $\alpha\boldsymbol{w}$ 也是解）

不失一般性，令 $\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{w} \boldsymbol{w}=1$ ，则上面的式子等价于：

$$
\begin{array}{cl}{\min _{\boldsymbol{w}}} & {-\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{b} \boldsymbol{w}} \\ {\text { s.t. }} & {\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{\boldsymbol{w}} \boldsymbol{w}=1}\end{array}\tag{3.36}
$$

由拉格朗日乘子法，上式等价于：

$$
\mathbf{S}_{b} \boldsymbol{w}=\lambda \mathbf{S}_{w} \boldsymbol{w}\tag{3.37}
$$


> $$\boldsymbol S_b\boldsymbol w=\lambda\boldsymbol S_w\boldsymbol w$$
>
> [推导]：由 3.36可列拉格朗日函数：
> $$l(\boldsymbol w)=-\boldsymbol w^T\boldsymbol S_b\boldsymbol w+\lambda(\boldsymbol w^T\boldsymbol S_w\boldsymbol w-1)$$
> 对 $\boldsymbol w$ 求偏导可得：
> $$\begin{aligned}
> \cfrac{\partial l(\boldsymbol w)}{\partial \boldsymbol w} &= -\cfrac{\partial(\boldsymbol w^T\boldsymbol S_b\boldsymbol w)}{\partial \boldsymbol w}+\lambda \cfrac{(\boldsymbol w^T\boldsymbol S_w\boldsymbol w-1)}{\partial \boldsymbol w} \\
> 	&= -(\boldsymbol S_b+\boldsymbol S_b^T)\boldsymbol w+\lambda(\boldsymbol S_w+\boldsymbol S_w^T)\boldsymbol w
> \end{aligned}$$
> 又 $\boldsymbol S_b=\boldsymbol S_b^T,\boldsymbol S_w=\boldsymbol S_w^T$，则：
> $$\cfrac{\partial l(\boldsymbol w)}{\partial \boldsymbol w} = -2\boldsymbol S_b\boldsymbol w+2\lambda\boldsymbol S_w\boldsymbol w$$
> 令导函数等于 0 即可得式 3.37。

其中 $\lambda$ 是拉格朗日乘子，我们注意到 $\mathbf{S}_{b} \boldsymbol{w}$ 的方向恒为 $\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}$ ，我们不妨令


$$
\mathbf{S}_{b} \boldsymbol{w}=\lambda\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)\tag{3.38}
$$


代入上面的式子得：

$$
\boldsymbol{w}=\mathbf{S}_{w}^{-1}\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)\tag{3.39}
$$



考虑到数值解的稳定性，在实践中通常是对 $\mathbf{S}_{w}$ 进行奇异值分解，即 $\mathbf{S}_{w}=\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\mathrm{T}}$ ，这里 $\boldsymbol{\Sigma}$ 是一个实对角矩阵，其对角线上的元素是 $\mathbf{S}_{w}$ 的奇异值，然后再由 $\mathbf{S}_{w}^{-1}=\mathbf{V} \boldsymbol{\Sigma}^{-1} \mathbf{U}^{\mathrm{T}}$ 得到 $\mathbf{S}_{w}^{-1}$ 。

值得一提的是，LDA 可从贝叶斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA 可达到最优分类。<span style="color:red;">这么厉害吗，想知道具体证明过程。</span>

可以将 LDA 推广到多分类任务中。假定存在 N 个类，且第 i 类示例数为 $m_i$ 。我们先定义“全局散度矩阵”：


$$
\begin{aligned} \mathbf{S}_{t} &=\mathbf{S}_{b}+\mathbf{S}_{w} \\ &=\sum_{i=1}^{m}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}\right)^{\mathrm{T}} \end{aligned}\tag{3.40}
$$


其中 $\boldsymbol{\mu}$ 是所有示例的均值向量。将类内散度矩阵 $\mathbf{S}_{w}$ 重定义为每个类别的散度矩阵之和，即


$$
\mathbf{S}_{w}=\sum_{i=1}^{N} \mathbf{S}_{w_{i}}\tag{3.41}
$$


其中


$$
\mathbf{S}_{w_{i}}=\sum_{\boldsymbol{x} \in X_{i}}\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)^{\mathrm{T}}\tag{3.42}
$$


得到：



$$
\begin{aligned} \mathbf{S}_{b} &=\mathbf{S}_{t}-\mathbf{S}_{w} \\ &=\sum_{i=1}^{N} m_{i}\left(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}\right)\left(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}\right)^{\mathrm{T}} \end{aligned}\tag{3.43}
$$


> $$\begin{aligned}
> \boldsymbol S_b &= \boldsymbol S_t - \boldsymbol S_w \\
> &= \sum_{i=1}^N m_i(\boldsymbol\mu_i-\boldsymbol\mu)(\boldsymbol\mu_i-\boldsymbol\mu)^T
> \end{aligned}$$
> [推导]：由式 3.40、3.41、3.42可得：
> $$\begin{aligned}
> \boldsymbol S_b &= \boldsymbol S_t - \boldsymbol S_w \\
> &= \sum_{i=1}^m(\boldsymbol x_i-\boldsymbol\mu)(\boldsymbol x_i-\boldsymbol\mu)^T-\sum_{i=1}^N\sum_{\boldsymbol x\in X_i}(\boldsymbol x-\boldsymbol\mu_i)(\boldsymbol x-\boldsymbol\mu_i)^T \\
> &= \sum_{i=1}^N\left(\sum_{\boldsymbol x\in X_i}\left((\boldsymbol x-\boldsymbol\mu)(\boldsymbol x-\boldsymbol\mu)^T-(\boldsymbol x-\boldsymbol\mu_i)(\boldsymbol x-\boldsymbol\mu_i)^T\right)\right) \\
> &= \sum_{i=1}^N\left(\sum_{\boldsymbol x\in X_i}\left((\boldsymbol x-\boldsymbol\mu)(\boldsymbol x^T-\boldsymbol\mu^T)-(\boldsymbol x-\boldsymbol\mu_i)(\boldsymbol x^T-\boldsymbol\mu_i^T)\right)\right) \\
> &= \sum_{i=1}^N\left(\sum_{\boldsymbol x\in X_i}\left(\boldsymbol x\boldsymbol x^T - \boldsymbol x\boldsymbol\mu^T-\boldsymbol\mu\boldsymbol x^T+\boldsymbol\mu\boldsymbol\mu^T-\boldsymbol x\boldsymbol x^T+\boldsymbol x\boldsymbol\mu_i^T+\boldsymbol\mu_i\boldsymbol x^T-\boldsymbol\mu_i\boldsymbol\mu_i^T\right)\right) \\
> &= \sum_{i=1}^N\left(\sum_{\boldsymbol x\in X_i}\left(- \boldsymbol x\boldsymbol\mu^T-\boldsymbol\mu\boldsymbol x^T+\boldsymbol\mu\boldsymbol\mu^T+\boldsymbol x\boldsymbol\mu_i^T+\boldsymbol\mu_i\boldsymbol x^T-\boldsymbol\mu_i\boldsymbol\mu_i^T\right)\right) \\
> &= \sum_{i=1}^N\left(-\sum_{\boldsymbol x\in X_i}\boldsymbol x\boldsymbol\mu^T-\sum_{\boldsymbol x\in X_i}\boldsymbol\mu\boldsymbol x^T+\sum_{\boldsymbol x\in X_i}\boldsymbol\mu\boldsymbol\mu^T+\sum_{\boldsymbol x\in X_i}\boldsymbol x\boldsymbol\mu_i^T+\sum_{\boldsymbol x\in X_i}\boldsymbol\mu_i\boldsymbol x^T-\sum_{\boldsymbol x\in X_i}\boldsymbol\mu_i\boldsymbol\mu_i^T\right) \\
> &= \sum_{i=1}^N\left(-m_i\boldsymbol\mu_i\boldsymbol\mu^T-m_i\boldsymbol\mu\boldsymbol\mu_i^T+m_i\boldsymbol\mu\boldsymbol\mu^T+m_i\boldsymbol\mu_i\boldsymbol\mu_i^T+m_i\boldsymbol\mu_i\boldsymbol\mu_i^T-m_i\boldsymbol\mu_i\boldsymbol\mu_i^T\right) \\
> &= \sum_{i=1}^N\left(-m_i\boldsymbol\mu_i\boldsymbol\mu^T-m_i\boldsymbol\mu\boldsymbol\mu_i^T+m_i\boldsymbol\mu\boldsymbol\mu^T+m_i\boldsymbol\mu_i\boldsymbol\mu_i^T\right) \\
> &= \sum_{i=1}^Nm_i\left(-\boldsymbol\mu_i\boldsymbol\mu^T-\boldsymbol\mu\boldsymbol\mu_i^T+\boldsymbol\mu\boldsymbol\mu^T+\boldsymbol\mu_i\boldsymbol\mu_i^T\right) \\
> &= \sum_{i=1}^N m_i(\boldsymbol\mu_i-\boldsymbol\mu)(\boldsymbol\mu_i-\boldsymbol\mu)^T
\end{aligned}$$



显然，多分类 LDA 可以有多种实现方法：使用 $\mathbf{S}_{b}$ 、$\mathbf{S}_{w}$ 、$\mathbf{S}_{t}$ 三者中的任何两个即可。常见的一种实现是采用优化目标：


$$
\max _{\mathbf{W}} \frac{\operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S}_{b} \mathbf{W}\right)}{\operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S}_{w} \mathbf{W}\right)}\tag{3.44}
$$

> $$\max\limits_{\mathbf{W}}\cfrac{
> tr(\mathbf{W}^T\boldsymbol S_b \mathbf{W})}{tr(\mathbf{W}^T\boldsymbol S_w \mathbf{W})}$$
> [解析]：此式是式 3.35的推广形式，证明如下：
> 设 $\mathbf{W}=[\boldsymbol w_1,\boldsymbol w_2,...,\boldsymbol w_i,...,\boldsymbol w_{N-1}]$，其中 $\boldsymbol w_i$ 为 $d$ 行 1 列的列向量，则：
> $$\left\{
> \begin{aligned}
> tr(\mathbf{W}^T\boldsymbol S_b \mathbf{W})&=\sum_{i=1}^{N-1}\boldsymbol w_i^T\boldsymbol S_b \boldsymbol w_i \\
> tr(\mathbf{W}^T\boldsymbol S_w \mathbf{W})&=\sum_{i=1}^{N-1}\boldsymbol w_i^T\boldsymbol S_w \boldsymbol w_i
> \end{aligned}
> \right.$$
> 所以式 3.44可变形为：
> $$\max\limits_{\mathbf{W}}\cfrac{
> \sum_{i=1}^{N-1}\boldsymbol w_i^T\boldsymbol S_b \boldsymbol w_i}{\sum_{i=1}^{N-1}\boldsymbol w_i^T\boldsymbol S_w \boldsymbol w_i}$$
> 对比式 3.35 易知上式即为式 3.35 的推广形式。


其中 $\mathbf{W} \in \mathbb{R}^{d \times(N-1)}$ ，$tr(\cdot )$ 表示矩阵的迹(trace)，上式可以通过如下广义特征值问题求解：

$$
\mathbf{S}_{b} \mathbf{W}=\lambda \mathbf{S}_{w} \mathbf{W}\tag{3.45}
$$


$\mathbf{W}$ 的闭式解则是 $\mathbf{S}_{w}^{-1} \mathbf{S}_{b}$ 的 $N-1$ 个最大广义特征值所对应的特征向量组成的矩阵.

若将 $\mathbf{W}$ 视为一个投影矩阵，则多分类 LDA 将样本投影到 $N-1$ 维空间， $N-1$ 通常远小于数据原有的属性数。于是，可通过这个投影来减小样本点的维数，且投影过程中使用了类别信息，因此 LDA 也常被视为一种经典的监督降维技术。




# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
in-book)
