---
title: 1.02 PCA 核心思想
toc: true
date: 2019-09-03
---

# PCA 核心思想

​PCA 可解决训练数据中存在数据特征过多或特征累赘的问题。

核心思想是将 $m$ 维特征映射到 $n$ 维（$n < m$），这 $n$ 维形成主元，是重构出来最能代表原始数据的正交特征。

## 怎样找到这个正交特征呢？

假设数据集是 $m$ 个 $n$ 维，$(\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \cdots, \boldsymbol x^{(m)})$。如果 $n=2$，需要降维到 $n'=1$，现在想找到某一维度方向代表这两个维度的数据。下图有 $u_1, u_2$ 两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/4x9JJbBcxXPG.png?imageslim">
</p>


从图可看出，$u_1$ 比 $u_2$ 好，为什么呢？有以下两个主要评价指标：

1. 样本点到这个直线的距离足够近。
2. 样本点在这个直线上的投影能尽可能的分开。<span style="color:red;">是的。</span>

OK，那么如果我们需要降维的目标维数是其他任意维，则我们关注的是：


- 最近重构性：样本点到这个超平面的距离都足够近；
- 最大可分性：样本点在这个超平面上的投影能尽可能分开。

有趣的是，基于最近重构性和最大可分性，能分别得到主成分分析的两种等价推导。




# 相关

- 《百面机器学习》
