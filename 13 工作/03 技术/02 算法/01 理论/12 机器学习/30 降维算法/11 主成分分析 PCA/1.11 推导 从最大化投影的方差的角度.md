---
title: 1.11 推导 从最大化投影的方差的角度
toc: true
date: 2019-03-30
---
# 可以补充进来的



# PCA 从最大化投影的方差的角度来推导

我们先从最简单的二维数据来看看 PCA 究竟是如何工作的，如图所示：

二维空间中 **经过中心化** 的一组数据：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190330/X2AXLsY5dzz4.png?imageslim">
</p>


该组数据的主成分：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190330/9ulkvvKCltnC.png?imageslim">
</p>

第一幅图是二维空间中经过中心化的一组数据，我们很容易看出主成分所在的轴（以下称为主轴）的大致方向，即第二幅图中黄线所处的轴。

因为在黄线所处的轴上，数据分布得更为分散，这也意味着数据在这个方向上方差更大。<span style="color:red;">嗯，是的，意味着数据在这个方向上方差更大</span>

在信号处理领域，我们认为信号具有较大方差，噪声具有较小方差，信号与噪声之比称为信噪比。信噪比越大意味着数据的质量越好，反之，信噪比越小意味着数据的质量越差。

由此我们不难引出 PCA 的目标，即最大化投影方差，也就是让数据在主轴上投影的方差最大。<span style="color:red;">嗯。是的。</span>


## 用式子进行表达和求解

对于给定的一组数据点 $\{v_1,v_2,...,v_n\}$ ，其中所有向量均为列向量，中心化后的表示为 $\{x_1,x_2,...,x_n\}=\{v_1-\mu,v_2-\mu,...,v_n-\mu\}$ ，其中 $\mu=\frac{1}{n}\sum_{i=1}^{n}v_i$ 。

我们知道，向量内积在几何上表示为第一个向量投影到第二个向量上的长度，因此向量 $x_i$ 在 $\omega$（单位方向向量）上的投影坐标可以表示为 $(x_i,\omega)=x_i^T\omega$ 。所以目标是找到一个投影方向 $\omega$，使得 $x_1,x_2,...,x_n$ 在 $\omega$ 上的投影方差尽可能大。

易知，投影之后均值为 0（因为 $\mu'=\frac{1}{n}\sum_{i=1}^{n}x_i^T\omega=(\frac{1}{n}\sum_{i=1}^{n}x_i^T)\omega=0$ ，这也是我们进行中心化的意义），因此投影后的方差可以表示为：


$$
\begin{aligned}
D(\boldsymbol{x})&=\frac{1}{n} \sum_{i=1}^{n}\left(\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{\omega}\right)^{2}\\&=\frac{1}{n} \sum_{i=1}^{n}\left(\boldsymbol{x}_{i}^{\mathrm{T}} \omega\right)^{\mathrm{T}}\left(\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{\omega}\right)\\&=\frac{1}{n} \sum_{i=1}^{n} \omega^{\mathrm{T}} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{\omega}\\&=\omega^{\mathrm{T}}\left(\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\mathrm{T}}\right) \omega\end{aligned}\tag{4.1}
$$


仔细一看，$(\frac{1}{n}\sum_{i=1}^{n}\omega^Tx_ix_i^T\omega)$ 其实就是样本协方差矩阵，我们将其写作 $\sum$。另外，由于 $\omega$ 是单位方向向量，即有 $\omega^T\omega=1$ 。因此我们要求解一个最大化问题，可表示为：

$$
\left\{\begin{array}{l}{\max \left\{\omega^{\mathrm{T}} \Sigma \omega\right\}} \\ {\text {s.t.} \omega^{\mathrm{T}} \omega=1}\end{array}\right.\tag{4.2}
$$

引入拉格朗日乘子，并对 $ω$ 求导令其等于 $0$，便可以推出 $Σ ω=λ ω$，此时：


$$
D(\boldsymbol{x})=\boldsymbol{\omega}^{\mathrm{T}} \Sigma \boldsymbol{\omega}=\lambda \boldsymbol{\omega}^{\mathrm{T}} \boldsymbol{\omega}=\lambda\tag{4.3}
$$


这里补充下西瓜书的内容：


> $$
> \boldsymbol X\boldsymbol X^T\boldsymbol w_i=\lambda _i\boldsymbol w_i
> $$
> [推导]：已知
> $$\begin{aligned}
> &\min\limits_{\boldsymbol W}-tr(\boldsymbol W^T\boldsymbol X\boldsymbol X^T\boldsymbol W)\\
> &s.t. \boldsymbol W^T\boldsymbol W=\boldsymbol I.
> \end{aligned}$$
> 运用拉格朗日乘子法可得，
> $$\begin{aligned}
> J(\boldsymbol W)&=-tr(\boldsymbol W^T\boldsymbol X\boldsymbol X^T\boldsymbol W+\boldsymbol\lambda'(\boldsymbol W^T\boldsymbol W-\boldsymbol I))\\
> \cfrac{\partial J(\boldsymbol W)}{\partial \boldsymbol W} &=-(2\boldsymbol X\boldsymbol X^T\boldsymbol W+2\boldsymbol\lambda'\boldsymbol W)
> \end{aligned}$$
> 令 $\cfrac{\partial J(\boldsymbol W)}{\partial \boldsymbol W}=\boldsymbol 0$，故
> $$\begin{aligned}
> \boldsymbol X\boldsymbol X^T\boldsymbol W&=-\boldsymbol\lambda'\boldsymbol W\\
> \boldsymbol X\boldsymbol X^T\boldsymbol W&=\boldsymbol\lambda\boldsymbol W\\
> \end{aligned}$$
> 其中，$\boldsymbol W=\{\boldsymbol w_1,\boldsymbol w_2,\cdot\cdot\cdot,\boldsymbol w_d\}$ 和 $\boldsymbol \lambda=\boldsymbol{diag}(\lambda_1,\lambda_2,\cdot\cdot\cdot,\lambda_d)$。


于是，只需对协方差矩阵 $\mathbf{X X}^{\mathrm{T}}$ 进行特征值分解，将求得的特征值排序: $\lambda_{1} \geqslant \lambda_{2} \geqslant \ldots \geqslant \lambda_{d}$ ，再取前 $d'$ 个特征值对应的特征向量构成 $\mathbf{W}=\left(\boldsymbol{w}_{1}\right.,\boldsymbol{w}_{2}, \dots, \boldsymbol{w}_{d^{\prime}} )$ 。这就是主成分分析的解。



熟悉线性代数的读者马上就会发现，原来，$x$ 投影后的方差就是协方差矩阵的特征值。<span style="color:red;">厉害呀！看来数学都是一环套一环的，每一个结论都有可能对另一个结论进行支撑。</span>

我们要找到最大的方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应的特征向量。次佳投影方向位于最佳投影方向的正交空间中，是第二大特征值对应的特征向量，以此类推。<span style="color:red;">厉害。</span>

## 对 PCA 的求解方法过程进行总结

至此，我们得到下面的求 PCA 的求解方法：



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180629/1J55LA54b7.png?imageslim">
</p>

对于特征值前 d 大对应的特征向量 $ω_1,ω_2,...,ω_d$，通过以下映射将 $n$ 维样本映射到 $d$ 维．


$$
\boldsymbol{x}_{i}^{\prime}=\left[ \begin{array}{c}{\boldsymbol{\omega}_{1}^{\mathrm{T}} \boldsymbol{x}_{i}} \\ {\boldsymbol{\omega}_{2}^{\mathrm{T}} \boldsymbol{x}_{i}} \\ {\vdots} \\ {\boldsymbol{\omega}_{d}^{\mathrm{T}} \boldsymbol{x}_{i}}\end{array}\right]\tag{4.4}
$$


新的 $x_i^′$ 的第 d 维就是 xi 在第 d 个主成分ωd方向上的投影，通过选取最大的 d 个特征值对应的特征向量，我们将方差较小的特征（噪声）抛弃，使得每个 n 维列向量 xi 被映射为 d 维列向量 $x_i^′$ ，定义降维后的信息占比为．


$$
\eta=\sqrt{\frac{\sum_{i=1}^{d} \lambda_{i}^{2}}{\sum_{i=1}^{n} \lambda_{i}^{2}}}\tag{4.5}
$$



降维后低维空间的维数 $d'$ 通常是由用户事先指定，或通过在 $d'$ 值不同的 低维空间中对 $k$ 近邻分类器(或其他开销较小的学习器)进行交叉验证来选取较好的 $d'$ 值。

对 PCA，还可从重构的角度设置一个重构阈值，例如 $t=95%$，然后选取使下式成立的最小值：

$$
\frac{\sum_{i=1}^{d^{\prime}} \lambda_{i}}{\sum_{i=1}^{d} \lambda_{i}} \geqslant t\tag{10.18}
$$


PCA 仅需保留 $\mathbf{W}$ 与样本的均值向量即可通过简单的向量减法和矩阵-向量乘法将新样本投影至低维空间中。显然，低维空间与原始高维空间必有不同, 因为对应于最小的 $d-d'$ 个特征值的特征向量被舍弃了，这是降维导致的结果。但舍弃这部分信息往往是必要的：

- 一方面，舍弃这部分信息之后能使样本的采样密度增大，这正是降维的重要动机；
- 另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到去噪的效果。





## 总结与扩展

至此，我们从最大化投影方差的角度解释了 PCA 的原理、目标函数和求解方法。

其实，PCA还可以用其他思路进行分析，比如从最小回归误差的角度得到新的目标函数。但最终我们会发现其对应的原理和求解方法与本文中的是等价的。

另外，由于 PCA 是一种线性降维方法，虽然经典，但具有一定的局限性。

有下面两种扩展方式：

- 我们可以通过核映射对 PCA 进行扩展得到核主成分分析（KPCA），
- 也可以通过流形映射的降维方法，比如等距映射、局部线性嵌入、拉普拉斯特征映射等，对一些 PCA 效果不好的复杂数据集进行非线性降维操作。

<span style="color:red;">上面这两种扩展方式还是要整理下的，以及在什么时候使用？</span>










# 相关

- 《百面机器学习》
- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
