
# 马尔可夫蒙特卡洛采样法

前面小节中提到，在高维空间中，拒绝采样和重要性重采样经常难以寻找合适的参考分布，<span style="color:red;">是呀。</span>采样效率低下（样本的接受概率小或重要性权重低），此时可以考虑马尔可夫蒙特卡洛（Markov Chain Monte Carlo，MCMC）采样法。

MCMC 采样法是机器学习中非常重要的一类采样算法，起源于物理学领域，到 20 世纪 80 年代后期才在统计学领域产生重要影响。它可以用于很多比较复杂的分布的采样，并且在高维空间中也能使用。<span style="color:red;">以前有看到过这个 MCMC ，但是一直没有怎么看。</span>

蒙特卡洛法，马尔可夫链，吉布斯采样，Metropolis-Hastings采样

## 简述 MCMC 采样法的主要思想。

从名字看，MCMC 采样法主要包括两个 MC，即蒙特卡洛法（Monte Carlo）和马尔可夫链（Markov Chain）：

- 蒙特卡洛法是指基于采样的数值型近似求解方法。<span style="color:red;">不明白。</span>
- 而马尔可夫链则用于进行采样

MCMC 采样法基本思想是：针对待采样的目标分布，构造一个马尔可夫链，使得该马尔可夫链的平稳分布就是目标分布；然后，从任何一个初始状态出发，沿着马尔可夫链进行状态转移，最终得到的状态转移序列会收敛到目标分布，由此可以得到目标分布的一系列样本。<span style="color:red;">还是不是很明白</span>

在实际操作中，核心点是如何构造合适的马尔可夫链，即确定马尔可夫链的状态转移概率，这涉及一些马尔可夫链的相关知识点，如时齐性、细致平衡条件、可遍历性、平稳分布等，感兴趣的读者可以参阅相关资料，这里不再细述。<span style="color:red;">这些都补充下，不然怎么使用呢？</span>

## 简单介绍几种常见的 MCMC 采样法。

MCMC 采样法的核心点是构造合适的马尔可夫链，不同的马尔可夫链对应着不同的 MCMC 采样法，常见的有 Metropolis-Hastings采样法和吉布斯采样法，如图 8.6所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190413/QfqxIOA4Jkxl.png?imageslim">
</p>


### Metropolis-Hastings 采样法

对于目标分布 $p(x)$，首先选择一个容易采样的参考条件分布 $q\left(x^{\prime} | x\right)$ ，并令：<span style="color:red;">为什么？</span>

$$
A\left(x, x^{*}\right)=\min \left\{1, \frac{p\left(x^{*}\right) q\left(x | x^{*}\right)}{p(x) q\left(x^{*} | x\right)}\right\}\tag{8.19}
$$

然后根据如下过程进行采样：

1. 随机选一个初始样本 $x^{(0)}$ 。
2. For t = 1，2，3，…：
    - 根据 $q\left(x^{*} | x^{(t-1)}\right)$ 参考条件分布抽取一个样本 $x^{\star}$；
    - 根据均匀分布 $U(0,1)$ 产生随机数 $u$；
    - 若 $\mathcal{u}<A\left(x^{(t-1)}, x^{*}\right)$ ，则令 $x^{(t)}=x^{*}$，否则令 $x^{(t)}=x^{(t-1)}$。

可以证明，上述过程得到的样本序列 $\left\{\ldots, x^{(t-1)}, x^{(t)}, \ldots\right\}$ 最终会收敛到目标分布 $p(x)$ 。

图 8.6（a）是 Metropolis-Hastings 算法采样过程的一个示意图，其中红线表示被拒绝的移动（维持旧样本），绿线表示被接受的移动（采纳新样本）。

### 吉布斯采样法

吉布斯采样法是 Metropolis-Hastings 算法的一个特例，其核心思想是每次只对样本的一个维度进行采样和更新。对于目标分布 $p(x)$ ，其中 $x=\left(x_{1}, x_{2}, \ldots, x_{d}\right)$ 是多维向量，按如下过程进行采样：


1. 随机选择初始状态 $x^{(0)}=\left(x_{1}^{(0)}, x_{2}^{(0)}, \ldots, x_{d}^{(0)}\right)$。
2. For t = 1，2，3，…：
    - 对于前一步产生的样本 $x^{(t-1)}=\left(x_{1}^{(t-1)}, x_{2}^{(t-1)}, \ldots, x_{d}^{(t-1)}\right)$ ，依次采样和更新每个维度的值，即依次抽取分量 $x_{1}^{(t)} \sim p\left(x_{1} | x_{2}^{(t-1)} ,x_{3}^{(t-1)}, \ldots, x_{d}^{(t-1)} \right)$，$x_{2}^{(t)} \sim p\left(x_{2} | x_{1}^{(t)}, x_{3}^{(t-1)}, \ldots, x_{d}^{(t-1)}\right)$，... ，$x_{d}^{(t)} \sim p\left(x_{d} | x_{1}^{(t)}, x_{2}^{(t)}, \ldots, x_{d-1}^{(t)}\right)$ 形成新的样本 $x^{(t)}=\left(x_{1}^{(t)}, x_{2}^{(t)}, \ldots, x_{d}^{(t)}\right)$。

同样可以证明，上述过程得到的样本序列 $\left\{\ldots, x^{(t-1)}, x^{(t)}, \ldots\right\}$ 会收敛到目标分布 $p(x)$。另外，步骤（2）中对样本每个维度的抽样和更新操作，不是必须按下标顺序进行的，可以是随机顺序。


在拒绝采样中，如果在某一步中采样被拒绝，则该步不会产生新样本，需要重新进行采样。与此不同，MCMC 采样法每一步都会产生一个样本，只是有时候这个样本与之前的样本一样而已。另外，MCMC 采样法是在不断迭代过程中逐渐收敛到平稳分布的，因此实际应用中一般会对得到的样本序列进行 “burn-in” 处理，即截除掉序列中最开始的一部分样本，只保留后面的样本。<span style="color:red;">嗯，截掉多少呢？</span>


## MCMC采样法如何得到相互独立的样本？

与一般的蒙特卡洛算法不同，MCMC 采样法得到的样本序列中相邻的样本不是独立的，因为后一个样本是由前一个样本根据特定的转移概率得到的，或者有一定概率就是前一个样本。<span style="color:red;">嗯。</span>

如果仅仅是采样，并不需要样本之间相互独立。如果确实需要产生独立同分布的样本，可以同时运行多条马尔可夫链，这样不同链上的样本是独立的；或者在同一条马尔可夫链上每隔若干个样本才选取一个，这样选取出来的样本也是近似独立的。<span style="color:red;">嗯，若干个是多少？</span>

## 总结与扩展

MCMC 采样法应用十分广泛，比如可以思考：

- 如何用 MCMC 采样法来求一个分布的众数？
- MCMC采样法在最大似然估计或贝叶斯推理中是如何使用的？

<span style="color:red;">嗯，这两个问题要总结进来。</span>


## 逸闻趣事 用 MCMC 采样法破解密码

斯坦福大学统计学教授 Persi Diaconis是一位传奇人物。他在 14 岁时就成了一名魔术师。为了看懂数学家 William Feller的概率论著作，他 24 岁就进入大学读书。由于 Diaconis 曾向《科学美国人》投稿介绍他的洗牌方法，使得在《科学美国人》上常年开设数学游戏专栏的著名数学科普作家 Martin Gardner给他写了推荐信去哈佛大学。当时哈佛大学的统计学家 Frederick Mosteller正在研究魔术，于是 Diaconis 成了 Mosteller 的学生（对他这段传奇经历有兴趣的读者可以看一看统计学史话《女士品茶》）。下面要讲的这个故事，是 Diaconis 在他的文章“The Markov Chain Monte Carlo Revolution”中给出的破译犯人密码的例子。

一天，一位研究犯罪心理学的医生来到斯坦福大学拜访 Diaconis。他带来了一个囚犯所写的密码信息，希望 Diaconis 能帮他找出密码中的信息。这个密码里的每个符号应该对应着某个字母（见图 8.7），但是如何把这些字母准确地找出来呢？Diaconis 和他的学生 Marc Coram 采用了 MCMC 采样法解决了这个问题。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190413/wYwhJwkBsPl3.png?imageslim">
</p>

这其实是一个非常典型的恺撒密码。手工用频率分析法，尝试不同的组合并观察结果是否有意义也可以解决这个问题。<span style="color:red;">嗯，是的。</span>

但是，除了部分高频字母，大部分字母的出现频率是差不多的，而且与文本内容有关，这样需要尝试非常多的组合，而且需要人为地判断结果是否有意义。<span style="color:red;">是呀。</span>

因此，单纯地依靠字母频率分析是不够的，应该考虑更一般的特征，比如字母之间的共同出现的频率。**更进一步地，可以考虑字母之间的转移概率**，例如，当前一个字母为辅音时，后一个字母出现元音的概率更大；或者，连续几个辅音出现之后再出现辅音的概率将非常低。<span style="color:red;">有些厉害了。</span>

这样就可以请出 MCMC 方法了，以大量英文语料为基础，统计从字母 x 到字母 y 的转移概率：无论是加密前还是加密后的文本，特定位置之间的转移概率是一致的，大致趋近于正常英文语料的转移概率。<span style="color:red;">是的，厉害了。</span>

Diaconis 和他的学生 Coram 按照这个思路对密文进行解密：

1. 首先，用《战争与和平》作为标准文本，统计一个字母到另一个字母的一步转移概率；
2. 然后，根据 Metropolis-Hastings 算法
    - 在假设所有对应关系出现的可能性相等的前提下（也就是无信息先验），随机给出了密码字符和字母的对应关系；
    - 再利用前边得到的转移概率，计算这种对应关系出现的概率 $p1$；
    - 然后，随机抽取两个密码字符，互换它们的对应字母，计算此时对应关系的概率 $p2$；
    - 最后，如果 $p2>p1$，接受新的对应关系；否则，抛一枚以 $p2/p1$ 的概率出现正面的硬币，如果出现正面，则接受新的对应关系，否则依然保持旧有的对应关系。

<span style="color:red;">哇塞，有些厉害！</span>

这就是 Metropolis-Hastings 算法的运用，当算法收敛时，就会得到真实的对应关系。事实上，当算法运行了 2000 多步的时候，就得到了一个混合了英语和西班牙语的文本段落，如图 8.8所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190413/nUO99HEyuWNG.png?imageslim">
</p>



# 相关

- 《百面机器学习》
