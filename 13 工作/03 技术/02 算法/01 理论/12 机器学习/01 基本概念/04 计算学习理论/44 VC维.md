---
title: 44 VC维
toc: true
date: 2018-08-21 17:48:33
---
# 可以补充进来的

# VC 维

现实学习任务所面临的通常是无限假设空间，例如实数域中的所有区间、$\mathbb{R}^{d}$ 空间中的所有线性超平面。

欲对此种情形的可学习性进行研究，需度量假设空间的复杂度。最常见的办法是考虑假设空间的 “VC维” (Vapnik-Chervonenkis dimension)。

介绍 VC 维之前，我们先引入几个概念：

- 増长函数(growth function)
- 对分(dichotomy)
- 打散(shattering).

给定假设空间 $\mathcal{H}$ 和示例集 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$ ,$\mathcal{H}$ 中每个假设 $h$ 都能对 $D$ 中示例赋予标记，标记结果可表示为

$$
\left.h\right|_{D}=\left\{\left(h\left(\boldsymbol{x}_{1}\right), h\left(\boldsymbol{x}_{2}\right), \ldots, h\left(\boldsymbol{x}_{m}\right)\right)\right\}
$$


随着 $m$ 的增大，$\mathcal{H}$ 中所有假设对 $D$ 中的示例所能赋予标记的可能结果数也会增大.

> 例如，对二分类问题，若 $D$ 中只有 2 个示例，则赋予标记的可能结果只有 4 种；若有 3 个示例，则可能结果有 8 种.


## 增长函数

对所有 $m \in \mathbb{N}$ ，假设空间 $\mathcal{H}$ 的増长函数 $\Pi_{\mathcal{H}}(m)$ 为

$$
\Pi_{\mathcal{H}}(m)=\max _{\left\{\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{m}\right\} \subseteq \mathcal{X}}\left|\left\{\left(h\left(\boldsymbol{x}_{1}\right), \ldots, h\left(\boldsymbol{x}_{m}\right)\right) | h \in \mathcal{H}\right\}\right|\tag{12.21}
$$

増长函数 $\Pi_{\mathcal{H}}(m)$ 表示假设空间 $\mathcal{H}$ 对 $m$ 个示例所能赋予标记的最大可能结果数。显然，$\mathcal{H}$ 对示例所能赋予标记的可能结果数越大,$\mathcal{H}$ 的表示能力越强, 对学习任务的适应能力也越强。因此/增长函数描述了假设空间 $\mathcal{H}$ 的表示能力， 由此反映出假设空间的复杂度。我们可利用増长函数来估计经验误差与泛化误 差之间的关系：


# 对分与打散

对假设空间 $\mathcal{H}$，$m \in \mathbb{N}$，$0<\epsilon<1$ 和任意 $h \in \mathcal{H}$ 有

$$
P(|E(h)-\widehat{E}(h)|>\epsilon) \leqslant 4 \Pi_{\mathcal{H}}(2 m) \exp \left(-\frac{m \epsilon^{2}}{8}\right)\tag{12.22}
$$

假设空间  $\mathcal{H}$ 中不同的假设对于 $D$ 中示例赋予标记的结果可能相同，也可能不同；尽管  $\mathcal{H}$ 可能包含无穷多个假设，但其对 $D$ 中示例赋予标记的可能结果 数是有限的：对 $m$ 个示例，最多有 $2^{m}$ 个可能结果。对二分类问题来说， $\mathcal{H}$ 中的假设对 $D$ 中示例赋予标记的每种可能结果称为对 $D$ 的一种“对分”。若假设空间 $\mathcal{H}$ 能实现示例集 $D$ 上的所有对分，即 $\Pi_{\mathcal{H}}(m)=2^{m}$，则称示例集 $D$ 能被假设空间  $\mathcal{H}$ “打散”.

> 每个假设会把 $D$ 中示例分为两类，因此称为对分.

## 定义 VC 维

现在我们可以正式定义 VC 维了：

假设空间 $\mathcal{H}$ 的 VC 维是能被 $\mathcal{H}$ 打散的最大示例集的大小，即

$$
\mathrm{VC}(\mathcal{H})=\max \left\{m : \Pi_{\mathcal{H}}(m)=2^{m}\right\}\tag{12.23}
$$

$\mathrm{VC}(\mathcal{H})=d$ 表明存在大小为 $d$ 的示例集能被假设空间 $\mathcal{H}$ 打散。注意：这并 不意味着所有大小为 $d$ 的示例集都能被假设空间 $\mathcal{H}$ 打散。细心的读者可能已发 现，VC维的定义与数据分布 $\mathcal{D}$ 无关！因此，在数据分布未知时仍能计算出假设空间 $\mathcal{H}$ 的 VC 维.

通常这样来计算 $\mathcal{H}$ 的 VC 维:若存在大小为 $d$ 的示例集能被 $\mathcal{H}$ 打散，但不 存在任何大小为 $d + 1$ 的示例集能被 $\mathcal{H}$ 打散，则 $\mathcal{H}$ 的 VC 维是 $d$ 。下面给出两个计算 VC 维的例子：

**例 12.1 实数域中的区间 $[a, b]$**:令 $\mathcal{H}$ 表示实数域中所有闭区间构成的集合 $\left\{h_{[a, b]} : a, b \in \mathbb{R}, a \leqslant b\right\}$,$\mathcal{X}=\mathbb{R}$ 。对 $x \in \mathcal{X}$ 若 $x \in[a, b]$，则 $h_{[a, b]}(x)=+1$， 否则 $h_{[a, b]}(x)=-1$ 。令 $x_{1}=0.5$ , $x_{2}=1.5$，则假设空间 $\mathcal{H}$ 中存在假设 $\left\{h_{[0,1]}, h_{[0,2]}, h_{[1,2]}, h_{[2,3]}\right\}$ 将 $\left\{x_{1}, x_{2}\right\}$ 打散，所以假设空间 $\mathcal{H}$ 的 VC 维至少为 2; 对任意大小为 3 的示例集 $\left\{x_{3}, x_{4}, x_{5}\right\}$ ，不妨设 $x_{3}<x_{4}<x_{5}$，则 $\mathcal{H}$ 中不存在任何假设 $h_{[a, b]}$ 能实现对分结果 $\left\{\left(x_{3},+\right),\left(x_{4},-\right),\left(x_{5},+\right)\right\}$。于是，$\mathcal{H}$ 的 VC 维为 2.




例 12.2 二维实平面上的线性划分：令 $\mathcal{H}$ 表示二维实平面上所有线性划 分构成的集合，$\mathcal{X}=\mathbb{R}^{2}$。由图 12.1可知，存在大小为 $3$ 的示例集可被 $\mathcal{H}$ 打散, 但不存在大小为 $4$ 的示例集可被 $\mathcal{H}$ 打散。于是，二维实平面上所有线性划分构 成的假设空间 $\mathcal{H}$ 的 VC 维为 3.


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190708/LQBF91YVEcaO.png?imageslim">
</p>


由定义 12.7可知，VC维与增长函数有密切联系，引理 12.2给出了二者之 间的定量关系[Sauer, 1972]:

亦称“Sauer 引理”.

**引理 12.2** 若假设空间 $\mathcal{H}$ 的 VC 维为 $d$，则对任意 $m \in \mathbb{N}$ 有

$$
\Pi_{\mathcal{H}}(m) \leqslant \sum_{i=0}^{d}\left(\begin{array}{c}{m} \\ {i}\end{array}\right)\tag{12.24}
$$

**证明** 由数学归纳法证明。当 $m=1$ , $d=0$ 或 $d=1$ 时，定理成立. 假设定理对 $(m-1, d-1)$ 和 $(m-1, d)$ 成立。令 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$ ，$D^{\prime}=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m-1}\right\}$：

$$
\mathcal{H}_{ | D}=\left\{\left(h\left(\boldsymbol{x}_{1}\right), h\left(\boldsymbol{x}_{2}\right), \ldots, h\left(\boldsymbol{x}_{m}\right)\right) | h \in \mathcal{H}\right\}
$$
$$
\mathcal{H}_{ | D^{\prime}}=\left\{\left(h\left(\boldsymbol{x}_{1}\right), h\left(\boldsymbol{x}_{2}\right), \ldots, h\left(\boldsymbol{x}_{m-1}\right)\right) | h \in \mathcal{H}\right\}
$$


任何假设 $h \in \mathcal{H}$ 对 $\boldsymbol{x}_{m}$ 的分类结果或为 $+1$ ，或为 $-1$，因此任何出现在 $\mathcal{H}_{ | D^{\prime}}$ 中的串都会在 $\mathcal{H}_{ | D}$ 中出现一次或两次。令 $\mathcal{H}_{D^{\prime} | D}$ 表示在 $\mathcal{H}_{ | D}$ 中出现两次的 $\mathcal{H}_{ | D^{\prime}}$ 中串组成的集合，即

$$
\begin{aligned} \mathcal{H}_{D^{\prime} | D}=\{ &\left(y_{1}, y_{2}, \ldots, y_{m-1}\right) \in \mathcal{H}_{ | D^{\prime}} | \exists h, h^{\prime} \in \mathcal{H} \\ &\left(h\left(\boldsymbol{x}_{i}\right)=h^{\prime}\left(\boldsymbol{x}_{i}\right)=y_{i}\right) \wedge\left(h\left(\boldsymbol{x}_{m}\right) \neq h^{\prime}\left(\boldsymbol{x}_{m}\right)\right), 1 \leqslant i \leqslant m-1 \} \end{aligned}
$$

考虑到 $\mathcal{H}_{D^{\prime} | D}$ 中的串在 $\mathcal{H}_{ | D}$ 中出现了两次，但在 $\mathcal{H}_{ | D^{\prime}}$ 中仅出现了一次， 有

$$
\left|\mathcal{H}_{ | D}\right|=\left|\mathcal{H}_{ | D^{\prime}}\right|+\left|\mathcal{H}_{D^{\prime} | D}\right|\tag{12.25}
$$

$D^{\prime}$ 的大小为 $m-1$，由假设可得

$$
\left|\mathcal{H}_{ | D^{\prime}}\right| \leqslant \Pi_{\mathcal{H}}(m-1) \leqslant \sum_{i=0}^{d}\left(\begin{array}{c}{m-1} \\ {i}\end{array}\right)\tag{12.26}
$$

令 $Q$ 表示能被 $\mathcal{H}_{D^{\prime} | D}$ 打散的集合，由 $\mathcal{H}_{D^{\prime} | D}$ 定义可知 $Q \cup\left\{\boldsymbol{x}_{m}\right\}$ 必能被 $\mathcal{H}_{ | D}$ 打散。由于 $\mathcal{H}$ 的 VC 维为 $d$，因此 $\mathcal{H}_{D^{\prime} | D}$ 的 VC 维最大为 $d-1$，于是有

$$
\left|\mathcal{H}_{D^{\prime} | D}\right| \leqslant \Pi_{\mathcal{H}}(m-1) \leqslant \sum_{i=0}^{d-1}\left(\begin{array}{c}{m-1} \\ {i}\end{array}\right)\tag{12.27}
$$

由式(12.25)~(12.27)可得

$$
\begin{aligned}\left|\mathcal{H}_{ | D}\right| & \leqslant \sum_{i=0}^{d}\left(\begin{array}{c}{m-1} \\ {i}\end{array}\right)+\sum_{i=0}^{d-1}\left(\begin{array}{c}{m-1} \\ {i}\end{array}\right) \\ &=\sum_{i=0}^{d}\left(\left(\begin{array}{c}{m-1} \\ {i}\end{array}\right)+\left(\begin{array}{c}{m-1} \\ {i-1}\end{array}\right)\right) \\ &=\sum_{i=0}^{d}\left(\begin{array}{c}{m} \\ {i}\end{array}\right) \end{aligned}
$$

由集合 D 的任意性，引理 12.2得证.

从引理 12.2可计算出増长函数的上界：

推论 12.2 若假设空间 $\mathcal{H}$ 的 VC 维为 $d$，则对任意整数 $m \geqslant d$ 有

$$
\Pi_{\mathcal{H}}(m) \leqslant\left(\frac{e \cdot m}{d}\right)^{d}\tag{12.28}
$$

证明

$$
\begin{aligned} \Pi_{\mathcal{H}}(m) & \leqslant \sum_{i=0}^{d}\left(\begin{array}{c}{m} \\ {i}\end{array}\right) \\ & \leqslant \sum_{i=0}^{d}\left(\begin{array}{c}{m} \\ {i}\end{array}\right)\left(\frac{m}{d}\right)^{d-i} \\ &=\left(\frac{m}{d}\right)^{d} \sum_{i=0}^{d}\left(\begin{array}{c}{m} \\ {i}\end{array}\right)\left(\frac{d}{m}\right)^{i} \\ & \leqslant\left(\frac{m}{d}\right)^{d} \sum_{i=0}^{m}\left(\begin{array}{c}{m} \\ {i}\end{array}\right)\left(\frac{d}{m}\right)^{i} \\ &=\left(\frac{m}{d}\right)^{d}\left(1+\frac{d}{m}\right)^{m} \\ & \leqslant\left(\frac{e \cdot m}{d}\right)^{d} \end{aligned}
$$


根据推论 12.2和定理 12.2可得基于 VC 维的泛化误差界：

定理 12.3 若假设空间 $\mathcal{H}$ 的 VC 维为 $d$，则对任意 $m>d$, $0<\delta<1$ 和 $h \in \mathcal{H}$ 有

$$
P\left(E(h)-\widehat{E}(h) \leqslant \sqrt{\frac{8 d \ln \frac{2 e m}{d}+8 \ln \frac{4}{\delta}}{m}}\right) \geqslant 1-\delta\tag{12.29}
$$

证明  令 $4 \Pi_{\mathcal{H}}(2 m) \exp \left(-\frac{m \epsilon^{2}}{8}\right) \leqslant 4\left(\frac{2 e m}{d}\right)^{d} \exp \left(-\frac{m \epsilon^{2}}{8}\right)=\delta$ ，解得

$$
\epsilon=\sqrt{\frac{8 d \ln \frac{2 e m}{d}+8 \ln \frac{4}{\delta}}{m}}
$$

带入定理 12.2 ，于是定理 12.3 得证。

由定理 12.3 可知，式(12.29) 的泛化误差界只与样例数目 $m$ 有关，收敛速率 $O\left(\frac{1}{\sqrt{m}}\right)$，与数据分布 $\mathcal{D}$ 和样例集 $D$ 无关。因此，基于 VC 维的泛化误差界是分布无关(distribution-free) 、数据独立 (data- independent) 的.

令 $h$ 表示学习算法 $\mathfrak{L}$ 输出的假设，若 $h$ 满足

$$
\widehat{E}(h)=\min _{h^{\prime} \in \mathcal{H}} \widehat{E}\left(h^{\prime}\right)\tag{12.30}
$$

则称 $\mathfrak{L}$ 为满足经验风险最小化 (Empirical Risk Mini ization ，简称 ERM)则的算法。我们有下面的定理:

**定理 12.4** 任何 VC 维有限的假设空间 $\mathcal{H}$ 都是(不可知)PAC 可学习的.

**证明** 假设 $\mathfrak{L}$ 为满足经验风险最小化原则的算法 ，$h$ 为学习算法 $\mathfrak{L}$ 输出的假设。令 $g$ 表示 $\mathcal{H}$ 中具有最小泛化误差的假设，即

$$
E(g)=\min _{h \in \mathcal{H}} E(h)\tag{12.31}
$$

令：

$$
\delta^{\prime}=\frac{\delta}{2}
$$
$$
\sqrt{\frac{\left(\ln 2 / \delta^{\prime}\right)}{2 m}}=\frac{\epsilon}{2}\tag{12.32}
$$

由推论 12.1 可知

$$
\widehat{E}(g)-\frac{\epsilon}{2} \leqslant E(g) \leqslant \widehat{E}(g)+\frac{\epsilon}{2}
$$


至少以 $1-\delta / 2$ 的概率成立。令

$$
\sqrt{\frac{8 d \ln \frac{2 e m}{d}+8 \ln \frac{4}{\delta^{\prime}}}{m}}=\frac{\epsilon}{2}\tag{12.34}
$$

则由定理 12.3 可知

$$
P\left(E(h)-\widehat{E}(h) \leqslant \frac{\epsilon}{2}\right) \geqslant 1-\frac{\delta}{2}
$$

从而可知

$$
\begin{aligned} E(h)-E(g) & \leqslant \widehat{E}(h)+\frac{\epsilon}{2}-\left(\widehat{E}(g)-\frac{\epsilon}{2}\right) \\ &=\widehat{E}(h)-\widehat{E}(g)+\epsilon \\ & \leqslant \epsilon \end{aligned}
$$

以至少 $1-\delta$ 的概率成立。由式(12.32) (12.34) 可以解出 $m$ ，再由 $\mathcal{H}$ 的任意性可知定理 12.4 得证。




# 相关

- 《机器学习》周志华
