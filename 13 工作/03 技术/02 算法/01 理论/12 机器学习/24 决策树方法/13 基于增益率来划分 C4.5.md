

### 增益率

#### 信息增益对可取数目较多的属性是有偏好的

看起来，上面的以信息增益进行划分的方式是没有问题的，但是实际上，还是存在一个严重的问题的：

如果，有一个属性，每个样本值在这个属性上的属性值都不同，那么会发生什么？

我们可以很容易计算出这个属性对应的信息增益，由于每个属性值都不同，因此如果按照这个属性来划分，那么每个节点只有一个样本，这个节点的信息熵就是 0，也就是说，这种划分的信息增益就是 0.998-0=0.998 。

我们很容易就想到了这种情况造成的后果，由于这个信息增益肯定是大于以别的属性为划分的信息增益的，所以，我们肯定会选择这个属性进行划分，但是，一旦按照这个属性进行划分，整个划分过程也就完结了，因为每个节点只有一个样本了。

很容易感觉到，这样的划分肯定是不具有泛化能力的，是无法对新样本进行有效预测的。

这就是按照信息增益进行划分的时候一个潜在的坑。

实际上，信息增益准则对可取值数目较多的属性是有所偏好的，<span style="color:red;">的确</span>，为了减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法不直接使用信息增益，而是使用 "增益率" (gain ratio) 来选择最优划分属性。

#### 为了减少这种偏好带来的影响，使用增益率

我们定义增益率为：

$$
\operatorname{Gain\_ratio}(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}\tag{4.3}
$$

> $$
> Gain-ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
> $$
> [解析]：基于信息增益的缺点，$C4.5$ 算法不直接使用信息增益，而是使用一种叫增益率的方法来选择最优特征进行划分，对于样本集 $D$ 中的离散特征 $a$ ，增益率为
> $$
> Gain-ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
> $$
> 其中，
> $$
> IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
> $$
> IV(a) 是特征 a 的熵。
>
> 增益率对特征值较少的特征有一定偏好，因此 $C4.5$ **算法选择特征的方法是先从候选特征中选出信息增益高于平均水平的特征，再从这些特征中选择增益率最高的**。

其中：

$$
\mathrm{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}\tag{4.4}
$$

称为属性 $a$ 的 "固有值" (intrinsic value) 。属性 $a$ 的可能取值数目越多(即 $V$ 越大)，则 $IV(a)$ 的值通常会越大。<span style="color:red;">这个固有值是怎么定的？为什么这么计算？</span>

比如，对于西瓜数据集 2.0，有

- IV(触感) = 0.874 (V = 2)
- IV(色泽) = 1.580 (V = 3)
- IV(编号) = 4.088 (V = 17)

但是，同样需要注意的是：增益率准则对可取值数目较少的属性是有所偏好的。<span style="color:red;">好吧，这个之前没有注意到。</span>

因此 ， C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。<span style="color:red;">好吧，有点精，怪不得 C4.5 这么强，整合了信息增益与增益率两者的优点。嗯，nice。不过启发式是什么意思？</span>

<span style="color:red;">我之前还以为 C4.5 就是直接对应的增益率，没想到是对信息增益和增益率的整合，做得好。</span>












# 相关

- 《机器学习》周志华
