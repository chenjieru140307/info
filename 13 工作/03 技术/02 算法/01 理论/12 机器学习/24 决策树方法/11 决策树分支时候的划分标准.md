

## 我们想要的是什么结果



我们知道，决策树的目标是从一组样本数据中，根据不同的特征和属性，建立一棵树形的分类结构。我们既希望它能拟合训练数据，达到良好的分类效果，同时又希望控制其复杂度，使得模型具有一定的泛化能力。

对于一个特定的问题，决策树的选择可能有很多种。比如，在场景描述中，如果女孩把会写代码这一属性放在根结点考虑，可能只需要很简单的一个树结构就能完成分类，如图所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190330/R9Xs8dU7aUoz.png?imageslim">
</p>

从若干不同的决策树中选取最优的决策树是一个 NP 完全问题，在实际中我们通常会采用启发式学习的方法去构建一棵满足启发式条件的决策树。<span style="color:red;">什么是 NP 完全问题？为什么是一个 NP 完全问题？</span>


由之前决策树的生成过程可以知道，决策树生成的关键就是如何选择最优划分属性。<span style="color:red;">是的这个很关键。</span>

首先，要判断什么样的划分是最优的，首先我们要看下我们想要的是什么结果。

一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的 "纯度" (purity) 越来越高。<span style="color:red;">是的，但是仅仅这样还是不够的吧？</span>

<span style="color:red;">从纯度讲到下面的衡量标准太突兀了吧？</span>

## 衡量划分好坏的几种标准

事实上，我们主要有三种方式来衡量划分的优劣：

- 信息增益       对应 ID3 决策树学习算法
- 信息增益率     对应 C4.5 决策树算法
- 基尼系数       对应 CART 决策树





# 相关

- 《机器学习》周志华
