---
title: 81 SVM 超平面上的投影仍然是线性可分的吗
toc: true
date: 2019-08-27
---

## 在空间上线性可分的两类点，分别向 SVM 分类的超平面上做投影，这些点在超平面上的投影仍然是线性可分的吗？

<span style="color:red;">没明白这个问题？感觉肯定是不是线性可分的吧？</span>

考察 SVM 模型推导的基础知识。

首先明确下题目中的概念，线性可分的两类点，即通过一个超平面可以将两类点完全分开，如图所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190330/dyAGeQyT2aTL.png?imageslim">
</p>

假设绿色的超平面（对于二维空间来说，分类超平面退化为一维直线）为 SVM 算法计算得出的分类面，那么两类点就被完全分开。我们想探讨的是：将这两类点向绿色平面上做投影，在分类直线上得到的黄棕两类投影点是否仍然线性可分，如图所示：<span style="color:red;">嗯。</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190330/tpKg3QjsTTKj.png?imageslim">
</p>



显然一眼望去，这些点在分类超平面（绿色直线）上相互间隔，并不是线性可分的。考虑一个更简单的反例，设想二维空间中只有两个样本点，每个点各属于一类的分类任务，此时 SVM 的分类超平面（直线）就是两个样本点连线的中垂线，两个点在分类面（直线）上的投影会落到这条直线上的同一个点，自然不是线性可分的。<span style="color:red;">嗯。是的，这个反例很清楚。</span>

但实际上，对于任意线性可分的两组点，它们在 SVM 分类的超平面上的投影都是线性不可分的。<span style="color:red;">为什么呢？</span>

这听上去有些不可思议，我们不妨从二维情况进行讨论，再推广到高维空间中。

由于 SVM 的分类超平面仅由支持向量决定（之后会证明这一结论），我们可以考虑一个只含支持向量 SVM 模型场景。使用反证法来证明。假设存在一个 SVM 分类超平面使所有支持向量在该超平面上的投影依然线性可分，如图 3.11所示。根据简单的初等几何知识不难发现，图中 AB 两点连线的中垂线所组成的超平面（绿色虚线）是相较于绿色实线超平面更优的解，这与之前假设绿色实线超平面为最优的解相矛盾。考虑最优解对应的绿色虚线，两组点经过投影后，并不是线性可分的。<span style="color:red;">是的，这个也是一个反证法，如果是线性可分的，那么对于这条分界线来说，他们实际上是要比原先的分界线更好的，也就是说，支持点到这条直线的更远，分的更合理。</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190330/W8L7j5SylDrh.png?imageslim">
</p>


我们的证明目前还有不严谨之处，即我们假设了仅有支持向量的情况，会不会在超平面的变换过程中支持向量发生了改变，原先的非支持向量和支持向量发生了转化呢？下面我们证明 SVM 的分类结果仅依赖于支持向量。考虑 SVM 推导中的 KKT 条件要求，<span style="color:red;">这些没看懂，要从头推导下。</span>

$$\triangledown _\omega L(\omega^*,\beta^*,\alpha^*)=\omega^*-\sum_{i=1}^{N}\alpha_i^*y_ix_i=0 \tag{3.1}$$

$$\triangledown_{\beta}L(\omega^*,\beta^*,\alpha^*)=-\sum_{i=1}^{N}\alpha_i^+y_i=0 \tag{3.2}$$

$$\alpha_i^*g_i(\omega^*)=0,i=1,...,N \tag{3.3}$$

$$g_i(\omega^*)\leq 0,i=1,...,N \tag{3.4}$$

$$\alpha_i^*\geq 0,i=1,...,N \tag{3.5}$$


结合式（3.3）和式（3.4）两个条件不难发现，当 $g_i(w^*)<0$ 时，必有 $\alpha_i^*=0$，将这一结果与拉格朗日对偶优化问题的公式相比较：

$$L(\omega^*,\alpha^*,\beta^*)=\frac{1}{2}\omega^{*2}+\sum_{i=1}^{N}\alpha_i^*g_i(w^*) \tag{3.6}$$

其中：

$$g_i(\omega^*)=-y_i(\omega^*\cdot x_i+\beta^*)+1 \tag{3.7}$$


可以看到，除支持向量外，其他系数均为 0，因此 SVM 的分类结果与仅使用支持向量的分类结果一致，说明 SVM 的分类结果仅依赖于支持向量，这也是 SVM 拥有极高运行效率的关键之一。

于是，我们证明了对于任意线性可分的两组点，它们在 SVM 分类的超平面上的投影都是线性不可分的。


实际上，该问题也可以通过凸优化理论中的超平面分离定理（Separating Hyperplane Theorem，SHT）更加轻巧地解决。<span style="color:red;">这个定理内容总结下。</span>

该定理描述的是，对于不相交的两个凸集，存在一个超平面，将两个凸集分离。对于二维的情况，两个凸集间距离最短两点连线的中垂线就是一个将它们分离的超平面。

借助这个定理，我们可以先对线性可分的这两组点求各自的凸包。不难发现，SVM求得的超平面就是两个凸包上距离最短的两点连线的中垂线，也就是 SHT 定理二维情况中所阐释的分类超平面。根据凸包的性质容易知道，凸包上的点要么是样本点，要么处于两个样本点的连线上。因此，两个凸包间距离最短的两个点可以分为三种情况：<span style="color:red;">没有很明白。</span>

- 两边的点均为样本点，如图 3.12（a）所示；
- 两边的点均在样本点的连线上，如图 3.12（b）所示；
- 一边的点为样本点，另一边的点在样本点的连线上，如图 3.12（c）所示。

两个凸包上距离最短的两个点对应的三种情况：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190330/PVLDpWpYuhE0.png?imageslim">
</p>

从几何上分析即可知道，无论哪种情况两类点的投影均是线性不可分的。


至此，我们从 SVM 直观推导和凸优化理论两个角度揭示了题目的真相。其实，在机器学习中还有很多这样看上去显而易见，细究起来却不可思议的结论。面对每一个小问题，我们都应该从数学原理出发，细致耐心地推导，对一些看似显而易见的结论抱有一颗怀疑的心，才能不断探索，不断前进，一步步攀登机器学习的高峰。








## 相关

- 《百面机器学习》
