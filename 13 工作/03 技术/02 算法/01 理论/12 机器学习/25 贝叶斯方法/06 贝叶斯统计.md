
# 贝叶斯统计


至此我们已经讨论了频率派统计(frequentist statistics)方法和基于估计单一值 $\boldsymbol{\theta}$ 的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有可能的 $\boldsymbol{\theta}$ 。后者属于贝叶斯统计(Bayesian statistics)的范畴。<span style="color:red;">嗯嗯，好好理解下。</span>

正如第 5.4.1节中讨论的，频率派的视角是真实参数 $\boldsymbol{\theta}$ 是未知的定值，而点估计 $\hat{\boldsymbol{\theta}}$ 是考虑数据集上函数(可以看作随机的)的随机变量。

贝叶斯统计的视角完全不同。贝叶斯统计用概率反映知识状态的确定性程度。数据集能够被直接观测到，因此不是随机的。另一方面，真实参数 $\boldsymbol{\theta}$ 是未知或不确定的，因此可以表示成随机变量。

在观察到数据前，我们将 $\boldsymbol{\theta}$ 的已知知识表示成先验概率分布(prior probability distribution)，$p(\boldsymbol{\theta})$ (有时简单地称为“先验”)。一般而言，机器学习实践者会选择一个相当宽泛的(即，高熵的)先验分布，以反映在观测到任何数据前参数 $\boldsymbol{\theta}$ 的高度不确定性。例如，我们可能会假设先验 $\boldsymbol{\theta}$ 在有限区间中均匀分布。许多先验偏好于“更简单”的解(如小幅度的系数，或是接近常数的函数)。<span style="color:red;">嗯。</span>

现在假设我们有一组数据样本 $\left\{x^{(1)}, \ldots, x^{(m)}\right\}$，通过贝叶斯规则结合数据似然 $p\left(x^{(1)}, \ldots, x^{(m)} | \boldsymbol{\theta}\right)$ 和先验，可以恢复数据对我们关于 $\boldsymbol{\theta}$ 信念的影响：

$$
p\left(\boldsymbol{\theta} | x^{(1)}, \ldots, x^{(m)}\right)=\frac{p\left(x^{(1)}, \ldots, x^{(m)} | \boldsymbol{\theta}\right) p(\boldsymbol{\theta})}{p\left(x^{(1)}, \ldots, x^{(m)}\right)}\tag{5.67}
$$


在贝叶斯估计常用的情景下，先验开始是相对均匀的分布或高熵的高斯分布，观测数据通常会使后验的熵下降，并集中在参数的几个可能性很高的值。

相对于最大似然估计，贝叶斯估计有两个重要区别。第一，不像最大似然方法预测时使用 $\boldsymbol{\theta}$ 的点估计，贝叶斯方法使用 $\boldsymbol{\theta}$ 的全分布。例如，在观测到 $m$ 个样本后，下一个数据样本 $x^{(m+1)}$ 的预测分布如下：<span style="color:red;">嗯。</span>

$$
p\left(x^{(m+1)} | x^{(1)}, \ldots, x^{(m)}\right)=\int p\left(x^{(m+1)} | \boldsymbol{\theta}\right) p\left(\boldsymbol{\theta} | x^{(1)}, \ldots, x^{(m)}\right) d \boldsymbol{\theta}\tag{5.68}
$$


这里，每个具有正概率密度的 $\boldsymbol{\theta}$ 的值有助于下一个样本的预测，其中贡献由后验密度本身加权。在观测到数据集 $\left\{x^{(1)}, \ldots, x^{(m)}\right\}$ 之后，如果我们仍然非常不确定 $\boldsymbol{\theta}$ 的值，那么这个不确定性会直接包含在我们所做的任何预测中。<span style="color:red;">厉害呀，感觉理解的不深。要再看下。</span>

在第 5.4节中，我们已经探讨频率派方法解决给定点估计 $\boldsymbol{\theta}$ 的不确定性的方法是评估方差，估计的方差评估了观测数据重新从观测数据中采样后，估计可能如何变化。对于如何处理估计不确定性的这个问题，贝叶斯派的答案是积分，这往往会防止过拟合。当然，积分仅仅是概率法则的应用，使贝叶斯方法容易验证，而频率派机器学习基于相当特别的决定构建了一个估计，将数据集里的所有信息归纳到一个单独的点估计。<span style="color:red;">没明白。</span>

贝叶斯方法和最大似然方法的第二个最大区别是由贝叶斯先验分布造成的。先验能够影响概率质量密度朝参数空间中偏好先验的区域偏移。实践中，先验通常表现为偏好更简单或更光滑的模型。对贝叶斯方法的批判认为，先验是人为主观判断影响预测的来源。<span style="color:red;">嗯。</span>

当训练数据很有限时，贝叶斯方法通常泛化得更好，但是当训练样本数目很大时，通常会有很大的计算代价。<span style="color:red;">为什么。</span>

### 示例：贝叶斯线性回归

我们使用贝叶斯估计方法学习线性回归的参数。在线性回归中，我们学习从输入向量 $\boldsymbol{x} \in \mathbb{R}^{n}$ 预测标量 $y \in \mathbb{R}$ 的线性映射。该预测由向量 $\boldsymbol{w} \in \mathbb{R}^{n}$ 参数化：

$$
\hat{y}=\boldsymbol{w}^{\top} \boldsymbol{x}\tag{5.69}
$$

给定一组 $m$ 个训练样本 $\left(\boldsymbol{X}^{(\text { train })}, \boldsymbol{y}^{(\text { train })}\right)$ ，我们可以表示整个训练集对 $y$ 的预测：

$$
\hat{\boldsymbol{y}}^{(\text { train })}=\boldsymbol{X}^{(\text { train })} \boldsymbol{w}\tag{5.70}
$$


表示为 $\boldsymbol{y}^{(\text { train })}$ 上的高斯条件分布，我们得到：

$$
\begin{aligned}
p\left(\boldsymbol{y}^{(\text { train })} | \boldsymbol{X}^{(\text { train })}, \boldsymbol{w}\right)&=\mathcal{N}\left(\boldsymbol{y}^{(\text { train })} ; \boldsymbol{X}^{(\text { train })} \boldsymbol{w}, \boldsymbol{I}\right)\\&\propto \exp \left(-\frac{1}{2}\left(\boldsymbol{y}^{(\text { train })}-\boldsymbol{X}^{(\text { train })} \boldsymbol{w}\right)^{\top}\left(\boldsymbol{y}^{\text { train } )}-\boldsymbol{X}^{(\text { train })} \boldsymbol{w}\right)\right)
\end{aligned}\tag{5.71}
$$



其中，我们根据标准的 MSE 公式假设 $y$ 上的高斯方差为 $1$。在下文中，为减少符号负担，我们将 $\left(\boldsymbol{X}^{(\text { train })}, \boldsymbol{y}^{(\text { train })}\right)$ 简单表示为 $(\boldsymbol{X}, \boldsymbol{y})$。

为确定模型参数向量 $\boldsymbol{w}$ 的后验分布，我们首先需要指定一个先验分布。先验应该反映我们对这些参数取值的信念。虽然有时将我们的先验信念表示为模型的参数很难或很不自然，但在实践中我们通常假设一个相当广泛的分布来表示 $\boldsymbol{\theta}$ 的高度不确定性。实数值参数通常使用高斯作为先验分布：

$$
p(\boldsymbol{w})=\mathcal{N}\left(\boldsymbol{w} ; \boldsymbol{\mu}_{0}, \boldsymbol{\Lambda}_{0}\right) \propto \exp \left(-\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{\mu}_{0}\right)^{\top} \boldsymbol{\Lambda}_{0}^{-1}\left(\boldsymbol{w}-\boldsymbol{\mu}_{0}\right)\right)\tag{5.73}
$$


其中，$\boldsymbol{\mu}_{0}$ 和 $\boldsymbol{\Lambda}_{0}$ 分别是先验分布的均值向量和协方差矩阵。<span style="color:red;">嗯。</span>  除非有理由使用协方差矩阵的特定结构，我们通常假设其为对角协方差矩阵 $\boldsymbol{\Lambda}_{0}=\operatorname{diag}\left(\boldsymbol{\lambda}_{0}\right)$ 。

确定好先验后，我们现在可以继续确定模型参数的后验分布。


$$
\begin{aligned}
p(\boldsymbol{w} | \boldsymbol{X}, \boldsymbol{y}) &\propto p(\boldsymbol{y} | \boldsymbol{X}, \boldsymbol{w}) p(\boldsymbol{w})\\&\propto \exp \left(-\frac{1}{2}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{w})^{\top}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{w})\right) \exp \left(-\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{\mu}_{0}\right)^{\top} \boldsymbol{\Lambda}_{0}^{-1}\left(\boldsymbol{w}-\boldsymbol{\mu}_{0}\right)\right)\\&\propto \exp \left(-\frac{1}{2}\left(-2 \boldsymbol{y}^{\top} \boldsymbol{X} \boldsymbol{w}+\boldsymbol{w}^{\top} \boldsymbol{X}^{\top} \boldsymbol{X} \boldsymbol{w}+\boldsymbol{w}^{\top} \boldsymbol{\Lambda}_{0}^{-1} \boldsymbol{w}-2 \boldsymbol{\mu}_{0}^{\top} \boldsymbol{\Lambda}_{0}^{-1} \boldsymbol{w}\right)\right)
\end{aligned}\tag{5.74}
$$

现在我们定义 $\boldsymbol{\Lambda}_{m}=\left(\boldsymbol{X}^{\top} \boldsymbol{X}+\mathbf{\Lambda}_{0}^{-1}\right)^{-1}$ 和 $\boldsymbol{\mu}_{m}=\boldsymbol{\Lambda}_{m}\left(\boldsymbol{X}^{\top} \boldsymbol{y}+\mathbf{\Lambda}_{0}^{-1} \boldsymbol{\mu}_{0}\right)$。使用这些新的变量，我们发现后验可改写为高斯分布：

$$
\begin{aligned}
p(\boldsymbol{w} | \boldsymbol{X}, \boldsymbol{y}) &\propto \exp \left(-\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{\mu}_{m}\right)^{\top} \boldsymbol{\Lambda}_{m}^{-1}\left(\boldsymbol{w}-\boldsymbol{\mu}_{m}\right)+\frac{1}{2} \boldsymbol{\mu}_{m}^{\top} \boldsymbol{\Lambda}_{m}^{-1} \boldsymbol{\mu}_{m}\right)\\&\propto \exp \left(-\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{\mu}_{m}\right)^{\top} \boldsymbol{\Lambda}_{m}^{-1}\left(\boldsymbol{w}-\boldsymbol{\mu}_{m}\right)\right)
\end{aligned}\tag{5.77}
$$



分布的积分必须归一这个事实意味着要删去所有不包括参数向量 $\boldsymbol{w}$ 的项。式(3.23)显示了如何标准化多元高斯分布。

检查此后验分布可以让我们获得贝叶斯推断效果的一些直觉。大多数情况下，我们设置 $\boldsymbol{\mu}_{0}=0$ 。如果我们设置 $\mathbf{\Lambda}_{0}=\frac{1}{\alpha} \boldsymbol{I}$ ，那么 $\mu_{m}$ 对 $\boldsymbol{w}$ 的估计就和频率派带权重衰减惩罚 $\alpha \boldsymbol{w}^{\top} \boldsymbol{w}$ 的线性回归的估计是一样的。一个区别是若 $\alpha$ 设为 $0$，则贝叶斯估计是未定义的——我们不能将贝叶斯学习过程初始化为一个无限宽的 $\boldsymbol{w}$ 先验。更重要的区别是，贝叶斯估计会给出一个协方差矩阵，表示 $\boldsymbol{w}$ 所有不同值的可能范围，而不仅是估计 $\mu_{m}$。

<span style="color:red;">没有很明白。</span>

## 最大后验(MAP)估计

原则上，我们应该使用参数 $\boldsymbol{\theta}$ 的完整贝叶斯后验分布进行预测，但单点估计常常也是需要的。

希望使用点估计的一个常见原因是，对于大多数有意义的模型而言，大多数涉及贝叶斯后验的计算是非常棘手的，点估计提供了一个可行的近似解。我们仍然可以让先验影响点估计的选择来利用贝叶斯方法的优点，而不是简单地回到最大似然估计。一种能够做到这一点的合理方式是选择最大后验 (Maximum A Posteriori,MAP) 点估计。MAP 估计选择后验概率最大的点(或在 $\boldsymbol{\theta}$ 是连续值的更常见情况下，概率密度最大的点)：

$$
\begin{aligned}
\boldsymbol{\theta}_{\mathrm{MAP}}&=\underset{\boldsymbol{\theta}}{\arg \max } p(\boldsymbol{\theta} | \boldsymbol{x})\\&=\underset{\boldsymbol{\theta}}{\arg \max } \log p(\boldsymbol{x} | \boldsymbol{\theta})+\log p(\boldsymbol{\theta})
\end{aligned}\tag{5.79}
$$

我们可以认出式(5.79)右边的 $\log p(\boldsymbol{x} | \boldsymbol{\theta})$ 对应着标准的对数似然项，$\log p(\boldsymbol{\theta})$ 对应着先验分布。

例如，考虑具有高斯先验权重 $\boldsymbol{w}$ 的线性回归模型。如果先验是 $\mathcal{N}\left(\boldsymbol{w} ; \mathbf{0}, \frac{1}{\lambda} I^{2}\right)$，那么式 (5.79) 的对数先验项正比于熟悉的权重衰减惩罚 $\lambda \boldsymbol{w}^{\top} \boldsymbol{w}$，加上一个不依赖于 $\boldsymbol{w}$ 也不会影响学习过程的项。因此，具有高斯先验权重的 MAP 贝叶斯推断对应着权重衰减。

正如全贝叶斯推断，MAP 贝叶斯推断的优势是能够利用来自先验的信息，这些信息无法从训练数据中获得。该附加信息有助于减少最大后验点估计的方差(相比于 ML 估计)。然而，这个优点的代价是增加了偏差。

许多正规化估计方法，例如权重衰减正则化的最大似然学习，可以被解释为贝叶斯推断的 MAP 近似。这个适应于正则化时加到目标函数的附加项对应着 $\log p(\boldsymbol{\theta})$ 。并非所有的正则化惩罚都对应着 MAP 贝叶斯推断。例如，有些正则化项可能不是一个概率分布的对数。还有些正则化项依赖于数据，当然也不会是一个先验概率分布。

MAP 贝叶斯推断提供了一个直观的方法来设计复杂但可解释的正则化项。例如，更复杂的惩罚项可以通过混合高斯分布作为先验得到，而不是一个单独的高斯分布(Nowlan and Hinton,1992)。<span style="color:red;">再看下，再补充下。</span>




# 相关

- 《深度学习》花书
