

实际上，我们不仅能把错误率这样的监督学习目标作为度量学习的优化目标，还能在度量学习中引入领域知识。

例如，若已知某些样本相似、某些样本不相似，则可定义“必连”(must-link)约束集合 $\mathcal{M}$ 与“勿连”(caimot-link)约束集合 $\mathcal{C}$ ，$\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \in \mathcal{M}$  表示 $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{j}$ 相似，$\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{k}\right) \in \mathcal{C}$ 表示 $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{k}$ 不相似。 显然，我们希望相似的样本之间距离较小，不相似的样本之间距离较大，于是可通过求解下面这个凸优化问题获得适当的度量矩阵 $\mathbf{M}$ [Xing et al., 2003]:

$$
\begin{array}{c}{\min _{\mathbf{M}} \sum_{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) \in \mathcal{M}}\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{\mathrm{M}}^{2}} \\ {\text { s.t. } \sum_{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{k}\right) \in \mathcal{M}}\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\right\|_{\mathbf{M}}^{2} \geqslant 1} \\ {\mathbf{M} \succeq 0}\end{array}
$$

其中约束 $\mathbf{M} \succeq 0$ 表明 $\mathbf{M}$ 必须是半正定的。式(10.39)要求在不相似样本间的距离不小于 1 的前提下，使相似样本间的距离尽可能小。


不同的度量学习方法针对不同目标获得“好”的半正定对称距离度量矩阵 $\mathbf{M}$ ，若 $\mathbf{M}$ 是一个低秩矩阵，则通过对 $\mathbf{M}$ 进行特征值分解，总能找到一组正交基，其正交基数目为矩阵 $\mathbf{M}$ 的秩 $\operatorname{rank}(\mathbf{M})$ , 小于原属性数 $d$ 。于是，度量学习学得的结果可衍生出一个降维矩阵 $P\in\mathbb{R}^{d\times rank(M)}$ ，能用于降维之目的。









# 相关

- 《机器学习》周志华
