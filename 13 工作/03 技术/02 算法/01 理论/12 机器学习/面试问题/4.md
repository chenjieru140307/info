
深度学习（计算机视觉）面试中问题（一）



前言：
博主参加了很多面试，应聘岗位是计算机视觉（深度学习方向），现将问到我的一些问题，总结如下，回答的有哪些不对，麻烦指正，大家也可以自己去查答案。特别说一下：面试时一定要把自己项目用到的东西涉及到的东西，都要搞懂，并且有目的性引导面试官问你准备的问题。

问题：
1、深度学习要这么深？
答：1、一个直观的解释，从模型复杂度角度。如果我们能够增强一个学习模型的复杂度，那么它的学习能力能够提升。如何增加神经网络的复杂度呢？要么变宽，即增加隐层网络神经元的个数；要么变深，即增加隐层的层数。当变宽的时候，只不过是增加了一些计算单元，增加了函数的个数，在变深的时候不仅增加了个数，还增加了函数间的嵌入的程度。

2、深度学习可以通过多个 layer 的转换学习更高维度的特征来解决更加复杂的任务。

3、那现在我们为什么可以用这样的模型？有很多因素，第一我们有了更大的数据；第二我们有强力的计算设备；第三我们有很多有效的训练技巧

4、像在 ZFNet 网络中已经体现，特征间存在层次性，层次更深，特征不变性强，类别分类能力越强，要学习复杂的任务需要更深的网络

2、如何解决数据不平衡问题？
答：1、利用重采样中的下采样和上采样，对小数据类别采用上采用，通过复制来增加数据，不过这种情况容易出现过拟合，建议用数据扩增的方法，对原有数据集进行翻转，旋转，平移，尺度拉伸，对比度，亮度，色彩变化来增加数据。对大数据类别剔除一些样本量。数据增强具体代码：常用数据增强方法代码

2、组合不同的重采样数据集：假设建立十个模型，选取小数据类 1000 个数据样本，然后将大数据类别 10000 个数据样本分为十份，每份为 1000 个，并训练十个不同的模型。

3、更改分类器评价指标： 在传统的分类方法中，准确率是常用的指标。 然而在不平衡数据分类中，准确率不再是恰当的指标，采用精准率即查准率 P：真正例除以真正例与假正例之和。召回率即查全率 F。真正例除以真正例与假反例之和。或者 F1 分数查全率和查准率加权平衡=2*P*R/(P+R)。

3、对于训练集与验证集测试集分布不同的处理办法
1、若训练集与验证集来自不同分布，比如一个网络爬虫获取的高清图像，一个是手机不清晰图像，人工合成图像，比如不清晰图像，亮度高的图像。

2、两种来源的数据一个来源数据大比如 20 万张，一个来源数据小，如五千张小数据集是我们优化目标，一种情况是将两组数据合并在一起，然后随机分配到训练验证测试集中好处是，三个数据集来自同一分布。缺点：瞄准目标都是大数据那一类的数据，而不是我们的目标小数据集。另外一种情况是训练集全部用大数据集，开发与测试集都是小数据集数据，优点：瞄准目标，坏处是不同分布。

3、分析偏差和方差方法和同一分布的方法不一样，加一个训练开发集（从训练集留出一部分数据）。总共四个数据集，训练集、训练开发集、开发集、测试集。看训练开发集的准确率与训练集验证集的区别来判别式方差还是数据分布不匹配的造成的误差。具体看如下链接：https://blog.csdn.net/koala_tree/article/details/78319908

4、如何改善训练模型的效果呢？
答：1、通过提升数据，获取良好的数据。对数据预处理；零均值 1 方差化，数据扩充或者增强，

2、诊断网络是否过拟合欠拟合。通过偏差方差。正则化解决过拟合，早停法遏制过拟合。

3、通过学习率，激活函数的选择，改善网络全连接层个数啊层数啊，优化算法，随机梯度，RMSprop，动量，adam，使用 batchnormlization.

3、权值初始化 Xavier 初始化，保持输入与输出端方差一致，避免了所有输出都趋向于 0；







5、如何解决梯度爆炸与消失。
-答：1、预训练加微调 - 梯度剪切、权重正则（针对梯度爆炸） -

2、使用不同的激活函数 -

3、使用 batchnorm -

4、使用残差结构 -

5、使用 LSTM 网络



6、你做过其他与职位申请相关项目吗，解释现在的硕士研究内容，有什么效果吗？
答：解答建议。自己做的事情和学的任何技能能够与申请的岗位建立联系。

7、为什么要使用许多小卷积核(如 3x 3 )而不是几个大卷积核？
这在 VGGNet 的原始论文中得到了很好的解释。原因有二：首先，您可以使用几个较小的核而不是几个较大的核来获得相同的感受野并捕获更多的空间上下文，但是使用较小的内核时，您使用的参数和计算量较少。其次，因为使用更小的核，您将使用更多的滤波器，您将能够使用更多的激活函数，从而使您的 CNN 学习到更具区分性的映射函数。



8、为什么在图像分割中 CNNs 通常具有编码器-解码器结构？
编码器 CNN 基本上可以被认为是特征提取网络，而解码器使用该信息通过“解码”特征并放大到原始图像大小来预测图像分割区域。

9、为什么我们对图像使用卷积而不仅仅是 FC 层？
这个很有趣，因为公司通常不会问这个问题。正如你所料，我是从一家专注于计算机视觉的公司那里得到这个问题的。这个答案有两部分。首先，卷积保存、编码和实际使用来自图像的空间信息。如果我们只使用 FC 层，我们将没有相对的空间信息。其次，卷积神经网络( CNNs )具有部分内建的平移不变性，因为每个卷积核充当其自身的滤波器/特征检测器。，而且这样减少大量的参数，减轻过拟合。

10、什么是数据正则化/归一化(normalization)？为什么我们需要它？
我觉得这一点很重要。数据归一化是非常重要的预处理步骤，用于重新缩放输入的数值以适应特定的范围，从而确保在反向传播期间更好地收敛。一般来说采取的方法都是减去每个数据点的平均值并除以其标准偏差。如果我们不这样做，那么一些特征(那些具有高幅值的特征)将在 cost 函数中得到更大的加权(如果较高幅值的特征改变 1 %，则该改变相当大，但是对于较小的特征，该改变相当小)。数据归一化使所有特征的权重相等。



11、解释降维(dimensionality reduction)，降维在哪里使用，降维的好处是什么?
降维是通过获得一组基本上是重要特征的主变量来减少所考虑的特征变量的过程。特征的重要性取决于特征变量对数据信息表示的贡献程度

数据集降维的好处可以是:

( 1 )减少所需的存储空间。

( 2 )加快计算速度(例如在机器学习算法中)，更少的维数意味着更少的计算，并且更少的维数可以允许使用不适合大量维数的算法。

( 3 )将数据的维数降低到 2D 或 3D 可以允许我们绘制和可视化它，可能观察模式，给我们提供直观感受。

( 4 )太多的特征或太复杂的模型可以导致过拟合。
---------------------
作者：搞视觉的张小凡
来源：CSDN
原文：https://blog.csdn.net/comway_Li/article/details/82532573
版权声明：本文为博主原创文章，转载请附上博文链接！


博主在前面一篇博客，已经把面试问到的问题叙述了 11 个，接下来把最近遇到的问题拿出来分享，回答的的不对，麻烦指正，谢谢。前面一篇博客为：深度学习面试常问问题（一）

1、1*1卷积作用。
答：1. 实现跨通道的交互和信息整合

2. 进行卷积核通道数的降维和升维

3、实现多个 feature map的线性组合，实现通道个数的变换。

4、对特征图进行一个比例缩放。

2、CNN池化层有什么作用？
答：1、减小图像尺寸，数据降维。

2、缓解过拟合。

3、保持一定程度的旋转和平移不变性，MaxPooling能保证卷积神经网络在一定范围内平移特征能得到同样的激励，具有平移不变形。

3、卷积神经网络中空洞卷积的作用是什么？
答、空洞卷积也叫扩张卷积，在保持参数个数不变的情况下增大了卷积核的感受野，同时它可以保证输出的特征映射（feature map）的大小保持不变。一个扩张率为 2 的 3×3卷积核，感受野与 5×5的卷积核相同，但参数数量仅为 9 个。

4、深度学习中常用的损失函数？
答：交叉熵损失，平方差损失，绝对值损失，Hinge Loss。具体介绍：损失函数具体介绍

5、 Sigmoid激活函数为什么会出现梯度消失？Sigmoid函数导数的最大值出现在哪个值？
答：为什么会出现梯度消失，从两方面来看，首先先看本身函数，若输入值 X 过大，sigmoid函数导数为零，第二方面：sigmoid函数求导，导数最大是等于 1/4，小于 1，经过深的网络传递就会出现梯度消失的问题。

在 x=0时导数最大。

6.faster rcnn是怎么样一个框架？
答：有一篇博客讲的挺清楚的。rcnn系列详解

7、faster rcnn,roi pooling具体是如何工作的？（如何把不同大小的框，pooling到同样的大小）
答、RoIPool首先将浮点数值的 RoI 量化成离散颗粒的特征图，然后将量化的 RoI 分成几个空间的小块（spatial bins），最后对每个小块进行 max pooling操作生成最后的结果。

8、评价指标有哪些？
答、机器学习中评价指标： Accuracy（准确率）、 Precision（查准率或者精准率）、Recall（查全率或者召回率）。

目标检测的指标：识别精度，识别速度，定位精度。

a、目标检测中衡量识别精度的指标是 mAP（mean average precision）。多个类别物体检测中，每一个类别都可以根据 recall 和 precision 绘制一条曲线，AP就是该曲线下的面积，mAP是多个类别 AP 的平均值。

b、  目标检测评价体系中衡量定位精度的指标是 IoU,IoU就是算法预测的目标窗口和真实的目标窗口的交叠（两个窗口面积上的交集和并集比值）

9、深度学习训练时网络不收敛的原因有哪些？如何解决？
答：不收敛一般都是数据不干净，学习率设置不合理，网络等问题，详细知识根据博主的经验和看的的别人一些经验整理了下，

不收敛的原因与解决办法

10、如何应对图像光照变化大？
答：1、直方图均衡化

2、对比度拉伸，或者调节

3、若受光源影响，使得图片整体色彩往一方面移动，用白平衡算法进行修正，使其发黄、发蓝、发红的照片更加趋于自然光下的图像

4、若是过爆或者过暗，可是设计阈值函数，不用全局阈值，对特定区域进行特定阈值分割。

5、若是太暗，可以采用对数变化，对数图像增强是图像增强的一种常见方法，其公式为： S = c log(r+1)，对数使亮度比较低的像素转换成亮度比较高的，而亮度较高的像素则几乎没有变化，这样就使图片整体变亮。

6、采用拉普拉斯算子增强 ， filter2D(src,dst)



11、常用的分割方法有哪些？
答：1、基于阈值的分割方法：比较常用的阂值法有大律法、最小误差法

2、基于边缘的分割方法：常见的微分算子包括 Robert 算子、Prewitt算子、Sobel算子、Laplaeian算子、Canny算子等

3、基于区域的分割方法：主要包括种子区域生长法、区域分裂合并法和分水岭法等几种类型。

4、基于图论的分割方法：Graph Cut方法

5、深度学习：语义分割等
