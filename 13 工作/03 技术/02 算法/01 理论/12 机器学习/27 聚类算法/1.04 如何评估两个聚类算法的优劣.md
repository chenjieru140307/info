
# 衡量聚类算法优劣



## 衡量聚类算法优劣的标准

不同聚类算法有不同的优劣和不同的适用条件。大致上从跟数据的属性（是否序列输入、维度），算法模型的预设，模型的处理能力上看。具体如下：

- 算法的处理能力：处理大的数据集的能力（即算法复杂度）；处理数据噪声的能力；处理任意形状，包括有间隙的嵌套的数据的能力；<span style="color:red;">什么是间隙的嵌套？</span>
- 算法是否需要预设条件：是否需要预先知道聚类个数，是否需要用户给出领域知识；
- 算法的数据输入属性：算法处理的结果与数据输入的顺序是否相关，也就是说算法是否独立于数据输入顺序；算法处理有很多属性数据的能力，也就是对数据维数是否敏感，对数据的类型有无要求。





## 以聚类问题为例，假设没有外部标签数据，如何评估两个聚类算法的优劣？



<span style="color:red;">是呀，确实是个好问题，怎么评估聚类算法的优劣呢？</span>

场景描述中的例子就是一个典型的聚类问题，从中可以看出，数据的聚类依赖于实际需求，同时也依赖于数据的特征度量以及评估数据相似性的方法。

相比于监督学习，非监督学习通常没有标注数据，模型、算法的设计直接影响最终的输出和模型的性能。

为了评估不同聚类算法的性能优劣，我们需要了解常见的数据簇的特点。<span style="color:red;">嗯。</span>

- 以中心定义的数据簇：这类数据集合倾向于球形分布，通常中心被定义为质心，即此数据簇中所有点的平均值。集合中的数据到中心的距离相比到其他簇中心的距离更近。
- 以密度定义的数据簇：这类数据集合呈现和周围数据簇明显不同的密度，或稠密或稀疏。当数据簇不规则或互相盘绕，并且有噪声和离群点时，常常使用基于密度的簇定义。<span style="color:red;">不同的密度？什么意思？相互缠绕？有噪声和离群点？</span>
- 以连通定义的数据簇：这类数据集合中的数据点和数据点之间有连接关系，整个数据簇表现为图结构。该定义对不规则形状或者缠绕的数据簇有效。<span style="color:red;">这个是什么样的？</span>
- 以概念定义的数据簇：这类数据集合中的所有数据点具有某种共同性质。<span style="color:red;">什么意思？以概念定义？</span>

由于数据以及需求的多样性，没有一种算法能够适用于所有的数据类型、数据簇或应用场景，似乎每种情况都可能需要一种不同的评估方法或度量标准。例如： K均值聚类可以用误差平方和来评估，但是基于密度的数据簇可能不是球形，误差平方和则会失效。

在许多情况下，判断聚类算法结果的好坏强烈依赖于主观解释。尽管如此，聚类算法的评估还是必需的，它是聚类分析中十分重要的部分之一。<span style="color:red;">是呀。</span>

聚类评估的任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结果的质量。这一过程又分为三个子任务。

### （1）估计聚类趋势

这一步骤是检测数据分布中是否存在非随机的簇结构。如果数据是基本随机的，那么聚类的结果也是毫无意义的。我们可以观察聚类误差是否随聚类类别数量的增加而单调变化，如果数据是基本随机的，即不存在非随机簇结构，那么聚类误差随聚类类别数量增加而变化的幅度应该较不显著，并且也找不到一个合适的 K 对应数据的真实簇数。<span style="color:red;">是呀。</span>

另外，我们也可以应用霍普金斯统计量（Hopkins Statistic）来判断数据在空间上的随机性[7]。首先，从所有样本中随机找 n 个点，记为 p1，p2，……，pn，对其中的每一个点 pi，都在样本空间中找到一个离它最近的点并计算它们之间的距离 xi，从而得到距离向量 x1，x2，……，xn；然后，从样本的可能取值范围内随机生成 n 个点，记为 $q_1,q_2,...,q_n$，对每个随机生成的点，找到一个离它最近的样本点并计算它们之间的距离，得到 y1，y2，……，yn。霍普金斯统计量 H 可以表示为：<span style="color:red;">哇塞！看到这个说明简直震惊了，感觉数学真的是厉害，不愧是靠近真理的。</span>

$$H=\frac{\sum_{i=1}^{n}y_i}{\sum_{i=1}^{n}x_i+\sum_{i=1}^{n}y_i}\tag{5.15}$$

如果样本接近随机分布，那么 $\sum_{i=1}^{n}x_i$ 和 $\sum_{i=1}^{n}y_i$ 的取值应该比较接近，即 $H$ 的值接近于 $0.5$ ；如果聚类趋势明显，则随机生成的样本点距离应该远大于实际样本点的距离，即 $\sum_{i=1}^{n}y_i >> \sum_{i=1}^{n}x_i$ ，$H$ 的值接近于 $1$。

### （2）判定数据簇数

确定聚类趋势之后，我们需要找到与真实数据分布最为吻合的簇数，据此判定聚类结果的质量。

数据簇数的判定方法有很多，例如手肘法和 Gap Statistic 方法。需要说明的是，用于评估的最佳数据簇数可能与程序输出的簇数是不同的。例如，有些聚类算法可以自动地确定数据的簇数，但可能与我们通过其他方法确定的最优数据簇数有所差别。<span style="color:red;">嗯，什么是手肘法？什么是 Gap Statistics？</span>


### （3）测定聚类质量

给定预设的簇数，不同的聚类算法将输出不同的结果，如何判定哪个聚类结果的质量更高呢？在无监督的情况下，我们可以通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。

定义评估指标可以展现面试者实际解决和分析问题的能力。<span style="color:red;">是呀。</span>事实上测量指标可以有很多种，以下列出了几种常用的度量指标，更多的指标可以阅读相关文献[8]。


**轮廓系数**

轮廓系数：给定一个点 p，该点的轮廓系数定义为:

$$s(p)=\frac{b(p)-a(p)}{max\{a(p),b(p)\}} \tag{5.16}$$

其中：

- $a(p)$ 是点 p 与同一簇中的其他点 $p'$ 之间的平均距离
- b（p）是点 p 与另一个不同簇中的点之间的最小平均距离（如果有 n 个其他簇，则只计算和点 p 最接近的一簇中的点与该点的平均距离）。
- a（p）反映的是 p 所属簇中数据的紧凑程度，
- b（p）反映的是该簇与其他临近簇的分离程度。

显然，b（p）越大，a（p）越小，对应的聚类质量越好，因此我们将所有点对应的轮廓系数 s（p）求平均值来度量聚类结果的质量。<span style="color:red;">嗯，好像是可以的，但是这个有什么优缺点呢？还是要琢磨下的。</span>



**均方根标准偏差**

均方根标准偏差（Root-mean-square standard deviation，RMSSTD）：用来衡量聚结果的同质性，即紧凑程度，定义为：

$$RMSSTD=\left\{\frac{\sum_i\sum_{x\in C_i}||x-c_i||^2}{P\sum_i(n_i-1)}\right\}^{\frac{1}{2}} \tag{5.17}$$

其中 $C_i$ 代表第 $i$ 个簇，$c_i$ 是该簇的中心，$x\in C_i$ 代表属于第 $i$ 个簇的一个样本点，$n_i$ 为第 $i$ 个簇的样本数量，$P$ 为样本点对应的向量维数。

可以看出，分母对点的维度 $P$ 做了惩罚，维度越高，则整体的平方距离度量值越大。$\sum_{i}(n_i-1)=n-NC$ ，其中 $n$ 为样本点的总数，$NC$ 为聚类簇的个数，通常 $NC<<n$ ，因此 $\sum_i (n_i-1)$ 的值接近点的总数，为一个常数。综上，RMSSTD 可以看作是经过归一化的标准差。<span style="color:red;">嗯嗯，有些不错，nice。</span>




**R方（R-Square）**

R方（R-Square）：可以用来衡量聚类的差异度，定义为：

$$RS=\frac{\sum_{x\in D}||x-c||^2-\sum_i\sum_{x\in C_i}||x-c_i||^2}{\sum_{x\in D}||x-c||^2} \tag{5.18}$$

其中 D 代表整个数据集，c代表数据集 D 的中心点，$\sum_{x\in D}||x-c||^2$ 从而代表将数据集 D 看作单一簇时的平方误差和。

与上一指标 RMSSTD 中的定义相同，$\sum_i\sum_{x\in C_i}||x-c_i||^2$ 代表将数据集聚类之后的平方误差和，所以 RS 代表了聚类之后的结果与聚类之前相比，对应的平方误差和指标的改进幅度。<span style="color:red;">嗯嗯，这个也不错，也很 nice</span>


**改进的 HubertΓ 统计**

改进的 HubertΓ统计：通过数据对的不一致性来评估聚类的差异，定义为：

$$\Gamma=\frac{2}{n(n-1)}\sum_{x\in D}\sum_{y\in D}d(x,y)d_{x\in C_i,y\in C_j}(c_i,c_j) \tag{5.19}$$

其中 $d(x,y)$ 表示点 $x$ 到点 $y$ 之间的距离，$d_{x\in C_i,y\in C_j}(c_i,c_j)$ 代表点 $x$ 所在的簇中心 $c_i$ 与点 $y$ 所在的簇中心 $c_j$ 之间的距离， $\frac{n(n-1)}{2}$ 为所有 $(x,y)$ 点对的个数，因此指标相当于对每个点对的和做了归一化处理。<span style="color:red;">嗯，是的。</span>

理想情况下，对于每个点对 $(x,y)$，如果 $d(x,y)$ 越小，对应的 $d_{x\in C_i,y\in C_j}(c_i,c_j)$ 也应该越小（特别地，当它们属于同一个聚类类簇时， $d_{x\in C_i,y\in C_j}(c_i,c_j)=0$ ）；当 $d(x,y)$ 越大时， $d_{x\in C_i,y\in C_j}(c_i,c_j)$ 的取值也应当越大，所以 $Γ$ 值越大说明聚类的结果与样本的原始距离越吻合，也就是聚类质量越高。<span style="color:red;">为什么这个值越大说明聚类与样本的原始距离越吻合？这个式子的说明是明白了，但是这句结论没大明白。再补充下。</span>


此外，为了更加合理地评估不同聚类算法的性能，通常还需要人为地构造不同类型的数据集，以观察聚类算法在这些数据集上的效果，几个常见的例子如图 5.10～图 5.14所示。

观察聚类误差是否随聚类类别数量的增加而单调变化：
<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190405/8VRtc6t0Dx7c.png?imageslim">
</p>

观察聚类误差对实际聚类结果的影响：
<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190405/DeGglJJnDNBh.png?imageslim">
</p>

观察近邻数据簇的聚类准确性：
<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190405/YFHK08c74wa0.png?imageslim">
</p>

观察数据密度具有较大差异的数据簇的聚类效果：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190405/wxqE3r4tTuC6.png?imageslim">
</p>

样本数量具有较大差异的数据簇的聚类效果：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190405/x97EmAXSMFmF.png?imageslim">
</p>



# 相关

- 《百面机器学习》
- [各种聚类算法的系统介绍和比较](https://blog.csdn.net/abc200941410128/article/details/78541273)
