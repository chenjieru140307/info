---
title: 2.15 K-means 聚类调优
toc: true
date: 2019-08-28
---

## 如何对其进行调优？

K均值算法的调优一般可以从以下几个角度出发：


**1. 数据归一化和离群点处理**

K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，因此使用 K 均值聚类算法之前通常需要对数据做预处理。

**2. 合理选择 K 值**

K值的选择是 K 均值聚类最大的问题之一，这也是 K 均值聚类算法的主要缺点。实际上，我们希望能够找到一些可行的办法来弥补这一缺点，或者说找到 K 值的合理估计方法。但是，K值的选择一般基于经验和多次实验结果。例如采用手肘法，我们可以尝试不同的 K 值，并将不同 K 值所对应的损失函数画成折线，横轴为 K 的取值，纵轴为误差平方和所定义的损失函数，如图所示：K均值算法中 K 值的选取：手肘法

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190331/64iz3OJvaHtc.png?imageslim">
</p>

由图可见，K值越大，距离和越小；并且，当 K=3时，存在一个拐点，就像人的肘部一样；当 $K\in(1,3)$ 时，曲线急速下降；当 $K>3$ 时，曲线趋于平稳。

手肘法认为拐点就是 K 的最佳值。<span style="color:red;">嗯，好的这个方法不错。应该可以在程序里直接遍历，然后比较误差平方和，然后找到手肘。不知道 sci-kit 里面有没有现成的，还是说这个方法并不现实？</span>

手肘法是一个经验方法，缺点就是不够自动化，因此研究员们又提出了一些更先进的方法，其中包括比较有名的 Gap Statistic方法[5]。<span style="color:red;">我之前有用过这个方法吗？之前好像是用的 GBDT 来看能聚成多少类的。与这个相比哪个更好？</span>

Gap Statistic方法的优点是，不再需要肉眼判断，而只需要找到最大的 Gap statistic所对应的 K 即可，因此该方法也适用于批量化作业。在这里我们继续使用上面的损失函数，当分为 K 簇时，对应的损失函数记为 $D_k$ 。

Gap Statistic 定义为：

$$Gap(K)=E(logD_k)−logD_k \tag{5.4}$$

其中 $E(logD_k)$ 是 $logD_k$ 的期望，一般通过蒙特卡洛模拟产生。<span style="color:red;">怎么产生的？怎么通过蒙特卡洛模拟产生的？是下面这段的步骤吗？</span>

我们在样本所在的区域内按照均匀分布随机地产生和原始样本数一样多的随机样本，并对这个随机样本做 K 均值，得到一个 $D_k$；重复多次就可以计算出 $E(logD_k)$ 的近似值。<span style="color:red;">什么意思？</span>

那么 $Gap(K)$ 有什么物理含义呢？它可以视为随机样本的损失与实际样本的损失之差。试想实际样本对应的最佳簇数为 $K$，那么实际样本的损失应该相对较小，随机样本损失与实际样本损失之差也相应地达到最小值，从而 $Gap(K)$ 取得最大值所对应的 $K$ 值就是最佳的簇数。根据式（5.4）计算 $K=1,2,...,9$ 所对应的 Gap Statistic，如图 5.4所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190331/b8DyJ6G9CYai.png?imageslim">
</p>

由图可见，当 K=3时， $Gap(K)$ 取值最大，所以最佳的簇数是 $K=3$。

<span style="color:red;">上面的 Gap Statistic 之前好像没看到过，而且，scikit-learn 里面有这个吗？</span>

**3. 采用核函数。**

采用核函数是另一种可以尝试的改进方向。传统的欧式距离度量方式，使得 $K$ 均值算法本质上假设了各个数据簇的数据具有一样的先验概率，并呈现球形或者高维球形分布，这种分布在实际生活中并不常见。

面对非凸的数据分布形状时，可能需要引入核函数来优化，这时算法又称为核 $K$ 均值算法，是核聚类方法的一种[6]。<span style="color:red;">核聚类有哪些种类？</span>

核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。<span style="color:red;">有些厉害，想了解下，而且，平时好像基本没听到这个，而且，怎么知道这个数据是不是可以使用核聚类呢？</span>




# 原文及引用

- 《百面机器学习》
