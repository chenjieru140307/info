

## 学习向量量化

与 $k$ 均值算法类似，“学习向量量化”(Learning Vector Quantization，简称 LVQ)也是试图找到一组原型向量来刻画聚类结构，但与一般聚类算法不同的是，LVQ 假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类.

给定样本集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$ ，每个样本 $\boldsymbol{x}_{j}$ 是由 $n$ 个属性描述的特征向量 $\left(x_{j 1} ; x_{j 2} ; \ldots ; x_{j n}\right)$，$y_{j} \in \mathcal{Y}$ 是样本 $\boldsymbol{x}_{j}$ 的类别标记. LVQ 的目标是学得一组 $n$ 维原型向量 $\left\{\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \ldots, \boldsymbol{p}_{q}\right\}$ ，每个原型向量代表一个聚类簇，簇标记 $t_{i} \in \mathcal{Y}$ 。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180628/gj0efCgKam.png?imageslim">
</p>

LVQ 算法描述如图 9.4所示：

- 算法第 1 行先对原型向量进行初始化，例如对第 $q$ 个簇可从类别标记为 $t_q$ 的样本中随机选取一个作为原型向量。
- 算法第 2~12行对原型向量进行迭代优化。在每一轮迭代中，算法随机选取一个有标记训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行相应的更新。在第 12 行中，若算法的停止条件已满足(例如已达到最大迭代轮数，或原型向量更新很小甚至不再更新)，则将当前原型向量作为最终结果返回。

显然，LVQ的关键是第 6-10行，即如何更新原型向量，直观上看，对样本 $\boldsymbol{x}_{j}$ ，若最近的原型向量 $\boldsymbol{p}_{i^{*}}$ 与 $\boldsymbol{x}_{j}$ 的类别标记相同，则令 $\boldsymbol{p}_{i^{*}}$ 向 $\boldsymbol{x}_{j}$ 的方向靠拢， 如第 7 行所示，此时新原型向量为:

$$
\boldsymbol{p}^{\prime}=\boldsymbol{p}_{i^{*}}+\eta \cdot\left(\boldsymbol{x}_{j}-\boldsymbol{p}_{i^{*}}\right)\tag{9.25}
$$

$\boldsymbol{p}^{\prime}$ 与 $\boldsymbol{x}_{j}$ 之间的距离为

$$
\begin{aligned}\left\|\boldsymbol{p}^{\prime}-\boldsymbol{x}_{j}\right\|_{2} &=\left\|\boldsymbol{p}_{i^{*}}+\eta \cdot\left(\boldsymbol{x}_{j}-\boldsymbol{p}_{i^{*}}\right)-\boldsymbol{x}_{j}\right\|_{2} \\ &=(1-\eta) \cdot\left\|\boldsymbol{p}_{i^{*}}-\boldsymbol{x}_{j}\right\|_{2} \end{aligned}\tag{9.26}
$$

令学习率 $\eta\in(0,1)$ ，则原型向量 $\boldsymbol{p}_{i^{*}}$ 在更新为 $\boldsymbol{p}^{\prime}$ 之后将更接近 $\boldsymbol{x}_{j}$ 。

类似的，若 $\boldsymbol{p}_{i^{*}}$ 与 $\boldsymbol{x}_{j}$ 的类别标记不同，则更新后的圆形向量与 $\boldsymbol{x}_{j}$ 之间的距离将增大为 $(1+\eta) \cdot\left\|\boldsymbol{p}_{i^{*}}-\boldsymbol{x}_{j}\right\|_{2}$，从而远离 $\boldsymbol{x}_{j}$ 。


在学得一组原型向量 $\left\{\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \ldots, \boldsymbol{p}_{q}\right\}$ 后，即可实现对样本空间 $\mathcal{X}$ 的簇划分。对任意样本 $\boldsymbol{x}$ ，它将被划入与其距离最近的原型向量所代表的簇中；换言之，每个原型向量 $\boldsymbol{p}_{i}$ 定义了与之相关的一个区域 $R_{i}$ ，该区域中每个样本与 $\boldsymbol{p}_{i}$ 的距离不大于它与其他原型向量 $\boldsymbol{p}_{i^{\prime}}\left(i^{\prime} \neq i\right)$ 的距离，即

$$
R_{i}=\left\{\boldsymbol{x} \in \mathcal{X} |\left\|\boldsymbol{x}-\boldsymbol{p}_{i}\right\|_{2} \leqslant\left\|\boldsymbol{x}-\boldsymbol{p}_{i^{\prime}}\right\|_{2}, i^{\prime} \neq i\right\}\tag{9.27}
$$

由此形成了对样本空间 $\mathcal{X}$ 的簇划分 $\left\{R_{1}, R_{2}, \ldots, R_{q}\right\}$ ，该划分通常称为 “Voronoi 剖分” (Voronoi tessellation).

下面我们以表 9.1 的西瓜数据集 4.0 为例来演示 LVQ 的学习过程。令 9-21 号样本的类别标记为 $c_2$ ，其他样本的类别标记为 $c_1$ 。假定 q=5，即学习目标是找到 5 个原型向量 $\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \boldsymbol{p}_{3}, \boldsymbol{p}_{4}, \boldsymbol{p}_{5}$，并假定其对应的类别标记分别为 $c_1$,$c_2$,$c_2$,$c_1$,$c_1$。


算法开始时，根据样本的类别标记和簇的预设类别标记对原型向量进行随机初始化，假定初始化为样本 $\boldsymbol{x}_{5}, \boldsymbol{x}_{12}, \boldsymbol{x}_{18}, \boldsymbol{x}_{23}, \boldsymbol{x}_{29}$。在第一轮迭代中，假定随机选取的样本为 $\boldsymbol{x}_{1}$ ，该样本与当前原型向量 $\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \boldsymbol{p}_{3}, \boldsymbol{p}_{4}, \boldsymbol{p}_{5}$ 的距离分别为 $0.283,0.506,0.434,0.260,0.032$。由于 $\boldsymbol{p}_{5}$ 与 $\boldsymbol{x}_{1}$ 距离最近且两者具有相同的类别标记 $c_2$ ，假定学习率 $\eta=0.1$ ，则 LVQ 更新 $\boldsymbol{p}_{5}$ 得到新原型向量

$$
\begin{aligned} \boldsymbol{p}^{\prime} &=\boldsymbol{p}_{5}+\eta \cdot\left(\boldsymbol{x}_{1}-\boldsymbol{p}_{5}\right) \\ &=(0.725 ; 0.445)+0.1 \cdot((0.697 ; 0.460)-(0.725 ; 0.445)) \\ &=(0.722 ; 0.442) \end{aligned}
$$

将 $p_5$ 更新为 $p'$ 后，不断重复上述过程，不同轮数之后的聚类结果如图 9.5所示.




# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
