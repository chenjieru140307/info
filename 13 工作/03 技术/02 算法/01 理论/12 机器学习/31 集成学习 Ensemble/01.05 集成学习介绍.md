---
title: 01.05 集成学习介绍
toc: true
date: 2019-10-17
---
# 集成学习介绍

先产生一组 “个体学习器” (individual learner)，再用某种策略将它们结合起来。


**Feature为主，Ensemble为后。** Feature决定了模型效果的上限，而Ensemble就是让你更接近这个上限。Ensemble讲究“好而不同”，不同是指模型的学习到的侧重面不一样。举个直观的例子，比如数学考试，A的函数题做的比B好，B的几何题做的比A好，那么他们合作完成的分数通常比他们各自单独完成的要高。

常见的Ensemble方法有 Bagging、Boosting、Stacking、Blending。

## Bagging

Bagging是将多个模型（**基学习器**）的预测结果简单地加权平均或者投票。Bagging的好处在于可以并行地训练基学习器，其中Random Forest就用到了Bagging的思想。举个通俗的例子，如下图：

![mark](http://images.iterate.site/blog/image/20191015/ceHpj8zFqQbC.png?imageslim)


老师出了两道加法题，A同学和B同学答案的加权要比A和B各自回答的要精确。

Bagging通常是没有一个明确的优化目标的，但是有一种叫Bagging Ensemble Selection的方法，它通过贪婪算法来Bagging多个模型来优化目标值。在这次比赛中，我们也使用了这种方法。

## Boosting

Boosting的思想有点像知错能改，每训练一个基学习器，是为了弥补上一个基学习器所犯的错误。其中著名的算法有AdaBoost，Gradient Boost。Gradient Boost Tree就用到了这种思想。

我在1.2.3节(错误分析)中提到Boosting，错误分析->抽取特征->训练模型->错误分析，这个过程就跟Boosting很相似。

## Stacking

Stacking 是用 **新的模型（次学习器）去学习怎么组合那些基学习器**，它的思想源自于Stacked Generalization（http://t.cn/ROj8F2g）这篇论文。如果把Bagging看作是多个基分类器的线性组合，那么Stacking就是多个基分类器的非线性组合。Stacking可以很灵活，它可以将学习器一层一层地堆砌起来，形成一个网状的结构，如下图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20191015/vPwxIDm510ty.png?imageslim">
</p>

举个更直观的例子，还是那两道加法题：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20191015/0unqU4ggr0UU.png?imageslim">
</p>

这里A和B可以看作是基学习器，C、D、E都是次学习器。

- Stage1:  A和B各自写出了答案。
- Stage2:  C和D偷看了A和B的答案，C认为A和B一样聪明，D认为A比B聪明一点。他们各自结合了A和B的答案后，给出了自己的答案。
- Stage3:  E偷看了C和D的答案，E认为D比C聪明，随后E也给出自己的答案作为最终答案。

在实现Stacking时，要注意的一点是，避免标签泄漏(Label Leak)。在训练次学习器时，需要上一层学习器对Train Data的测试结果作为特征。如果我们在Train Data上训练，然后在Train Data上预测，就会造成Label Leak。为了避免Label Leak，需要对每个学习器使用K-fold，将K个模型对Valid Set的预测结果拼起来，作为下一层学习器的输入。如下图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20191015/xFBOxW5QYAhE.png?imageslim">
</p>

由图可知，我们还需要对Test Data做预测。这里有两种选择，**可以将K个模型对Test Data的预测结果求平均，也可以用所有的Train Data重新训练一个新模型来预测Test Data**。所以在实现过程中，我们最好把每个学习器对Train Data和对Test Data的测试结果都保存下来，方便训练和预测。

对于Stacking还要注意一点，固定K-fold可以尽量避免Valid Set过拟合，也就是全局共用一份K-fold，如果是团队合作，组员之间也是共用一份K-fold。如果想具体了解为什么需要固定K-fold，请看这里（http://t.cn/ROjQL5o）。

## Blending

Blending与Stacking很类似，它们的区别可以参考这里（http://t.cn/RXy0TBE）
