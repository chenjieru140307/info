
# 可以补充进来的

- 其实以前也大概看到过，但是也没有很深的理解，嗯，还是要多补充下，深入理解下。

# 结构化概率模型

机器学习的算法经常会涉及在非常多的随机变量上的概率分布。通常，这些概率分布涉及的直接相互作用都是介于非常少的变量之间的。使用单个函数来描述整个联合概率分布是非常低效的(无论是计算上还是统计上)。<span style="color:red;">涉及的直接相互作用都是介于非常少的变量之间的，是什么意思？</span>

## 把概率分布分解成因子的乘积形式

我们可以把概率分布分解成许多因子的乘积形式，而不是使用单一的函数来表示概率分布。<span style="color:red;">嗯。</span>例如，假设我们有 3 个随机变量 $\mathrm{a}$ 、$\mathrm{b}$ 和 $\mathrm{c}$ ，并且 $\mathrm{a}$ 影响 $\mathrm{b}$ 的取值，$\mathrm{b}$ 影响 $\mathrm{c}$ 的取值，但是 $\mathrm{a}$ 和 $\mathrm{c}$ 在给定 $\mathrm{b}$ 时是条件独立的。我们可以把全部 3 个变量的概率分布重新表示为两个变量的概率分布的连乘形式：<span style="color:red;">其实，对于 $\mathrm{a}$ 和 $\mathrm{c}$ 在给定 $\mathrm{b}$ 时是条件独立的 这句话一直没有很深的理解。</span>


$$
p(\mathrm{a}, \mathrm{b}, \mathrm{c})=p(\mathrm{a}) p(\mathrm{b} | \mathrm{a}) p(\mathrm{c} | \mathrm{b})\tag{3.52}
$$

这种分解可以极大地减少用来描述一个分布的参数数量。每个因子使用的参数数目是其变量数目的指数倍。这意味着，如果我们能够找到一种使每个因子分布具有更少变量的分解方法，就能极大地降低表示联合分布的成本。<span style="color:red;">嗯，听得好像是这样，但是有点云里雾里，不透彻。为什么每个因子使用的参数数目是其变量数目的指数倍？什么是使每个因子分布具有更少变量的分解方法？什么是表示联合分布的成本？</span>

## 结构化概率模型 图模型

可以用图来描述这种分解。这里我们使用的是图论中的“图”的概念：由一些可以通过边互相连接的顶点的集合构成。

当用图来表示这种概率分布的分解时，我们把它称为结构化概率模型 (structured probabilistic model) 或者图模型 (graphical model)。

有两种主要的结构化概率模型：有向的和无向的。两种图模型都使用图，其中图的每个节点对应着一个随机变量，连接两个随机变量的边意味着概率分布可以表示成这两个随机变量之间的直接作用。<span style="color:red;">嗯，感觉说的很清楚，但是还是有点不透彻。</span>

### 有向模型


有向 (directed) 模型使用带有有向边的图，它们用条件概率分布来表示分解，就像上面的例子。特别地，有向模型对于分布中的每一个随机变量 $\mathrm{x}_{i}$ 都包含着一个影响因子，这个组成 $\mathrm{x}_{i}$ 条件概率的影响因子被称为 $\mathrm{x}_{i}$ 的父节点，记为 $P a_{\mathcal{G}}\left(\mathrm{x}_{i}\right)$。<span style="color:red;">？每一个随机变量  $\mathrm{x}_{i}$ 都包含一个影响因子，是什么意思？</span>


$$
p(\mathbf{x})=\prod_{i} p\left(\mathrm{x}_{i} | P a_{\mathcal{G}}\left(\mathrm{x}_{i}\right)\right)\tag{3.53}
$$


图 3.7 给出了一个有向图的例子以及它表示的概率分布的分解。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190515/LmUQqM4Ay63R.png?imageslim">
</p>


图 3.7 关于随机变量 a、b、c、d 和 e 的有向图模型。这幅图对应的概率分布可以分解为：

$$
p(\mathrm{a}, \mathrm{b}, \mathrm{c}, \mathrm{d}, \mathrm{e})=p(\mathrm{a}) p(\mathrm{b} | \mathrm{a}) p(\mathrm{c} | \mathrm{a}, \mathrm{b}) p(\mathrm{d} | \mathrm{b}) p(\mathrm{e} | \mathrm{c})\tag{3.54}
$$

该图模型使我们能够快速看出此分布的一些性质。例如，a 和 c 直接相互影响，但 a 和 e 只有通过 c 间接相互影响。<span style="color:red;">嗯。</span>


## 无向模型

无向 (undirected) 模型使用带有无向边的图，它们将分解表示成一组函数：不像有向模型那样，这些函数通常不是任何类型的概率分布。$\mathcal{G}$ 中任何满足两两之间有边连接的顶点的集合被称为团。无向模型中的每个团 $\mathcal{C}^{(i)}$ 都伴随着一个因子 $\phi^{(i)}\left(\mathcal{C}^{(i)}\right)$ 。这些因子仅仅是函数，并不是概率分布。每个因子的输出都必须是非负的，但是并没有像概率分布中那样要求因子的和或者积分为 $1$ 。<span style="color:red;">嗯。这些因子仅仅是函数，并不是概率分布，是什么意思？</span>


随机变量的联合概率与所有这些因子的乘积成比例 (proportional)-这意味着因子的值越大，则可能性越大。

当然，不能保证这种乘积的求和为 $1$。所以我们需要除以一个归一化常数 $Z$ 来得到归一化的概率分布，归一化常数 $Z$ 被定义为 $\phi$ 函数乘积的所有状态的求和或积分。概率分布为：

$$
p(\mathbf{x})=\frac{1}{Z} \prod_{i} \phi^{(i)}\left(\mathcal{C}^{(i)}\right)\tag{3.55}
$$

图 3.8 给出了一个无向图的例子以及它表示的概率分布的分解。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190515/puljz8SavLwS.png?imageslim">
</p>


图 3.8　关于随机变量 a、b、c、d和 e 的无向图模型。这幅图对应的概率分布可以分解为：


$$
p(\mathrm{a}, \mathrm{b}, \mathrm{c}, \mathrm{d}, \mathrm{e})=\frac{1}{Z} \phi^{(1)}(\mathrm{a}, \mathrm{b}, \mathrm{c}) \phi^{(2)}(\mathrm{b}, \mathrm{d}) \phi^{(3)}(\mathrm{c}, \mathrm{e})\tag{3.56}
$$

该图模型使我们能够快速看出此分布的一些性质。例如，$\mathrm{a}$ 和 $\mathrm{c}$ 直接相互影响，但 $\mathrm{a}$ 和 $\mathrm{e}$ 只有通过 $\mathrm{c}$ 间接相互影响。

## 需要注意的

请记住，这些图模型表示的分解仅仅是描述概率分布的一种语言。它们不是互相排斥的概率分布族。<span style="color:red;">什么意思？</span>

有向或者无向不是概率分布的特性；它是概率分布的一种特殊描述 (description) 所具有的特性，而任何概率分布都可以用这两种方式进行描述。<span style="color:red;">没有很理解，这个有向和无向只是描述的特性吗？不是概率分布本身的特性是吧？任何的概率分布都可以使用这两种描述吗？一般使用什么？什么场景下使用什么？</span>

在本书第 1 部分和第 2 部分中，我们仅仅将结构化概率模型视作一门语言，来描述不同的机器学习算法选择表示的直接的概率关系。在讨论研究课题之前，读者不需要更深入地理解结构化概率模型。在第 3 部分的研究课题中，我们将更为详尽地探讨结构化概率模型。<span style="color:red;">嗯，看到的时候补充到这里。</span>



# 相关

- 《深度学习》花书
