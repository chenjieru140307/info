
# 点估计随着样本的增加收敛到真实值

## 一致性

目前我们已经探讨了固定大小训练集下不同估计量的性质。通常，我们也会关注训练数据增多后估计量的效果。特别地，我们希望当数据集中数据点的数量 $m$ 增加时，点估计会收敛到对应参数的真实值。更形式化地，我们想要

$$
\operatorname{plim}_{m \rightarrow \infty} \hat{\theta}_{m}=\theta\tag{5.55}
$$

符号 $\operatorname{plim}$ 表示依概率收敛，即对于任意的 $\epsilon>0$ ，当 $m \rightarrow \infty$ 时，有 $P\left(\left|\hat{\theta}_{m}-\theta\right|>\epsilon \right ) \rightarrow 0$ 。式(5.55)表示的条件被称为一致性(consistency)。有时它是指弱一致性，强一致性是指几乎必然(almost sure)从 $\hat{\theta}$ 收敛到 $\theta$ 。几乎必然收敛 (almost sure convergence)是指当 $p\left(\lim _{m \rightarrow \infty} \mathbf{x}^{(m)}=\boldsymbol{x}\right)=1$ 时，随机变量序列 $\mathbf{x}^{(1)}, \quad \mathbf{x}^{(2)}, \ldots$ 收敛到 $\boldsymbol{x}$。<span style="color:red;">一致性，嗯，这个概念之前好像没有注意到过。</span>

一致性保证了估计量的偏差会随数据样本数目的增多而减少。然而，反过来是不正确的——渐近无偏并不意味着一致性。例如，考虑用包含 $m$ 个样本的数据集 $\left\{x^{(1)}, \ldots, x^{(m)}\right\}$ 估计正态分布 $\mathcal{N}\left(x ; \mu, \sigma^{2}\right)$ 的均值参数 $\mu$ 。我们可以使用数据集的第一个样本 $x^{(1)}$ 作为无偏估计量： $\hat{\theta}=x^{(1)}$ 。在该情况下，$\mathbb{E}\left(\hat{\theta}_{m}\right)=\theta$ ，所以不管观测到多少数据点，该估计量都是无偏的。然而，这不是一个一致估计，因为它不满足当 $m \rightarrow \infty$ 时，$\hat{\theta}_{m} \rightarrow \theta$。<span style="color:red;">为什么呢？</span>



# 相关

- 《深度学习》花书
