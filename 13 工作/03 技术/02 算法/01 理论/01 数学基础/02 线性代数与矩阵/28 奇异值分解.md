
# 奇异值分解

在第 2.7 节，我们探讨了如何将矩阵分解成特征向量和特征值。

还有另一种分解矩阵的方法，称为奇异值分解 (singular value decomposition,SVD)，是将矩阵分解为奇异向量 (singular vector) 和奇异值 (singular value)。



通过奇异值分解，我们会得到一些与特征分解相同类型的信息。


然而，奇异值分解有更广泛的应用。每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。

例如，非方阵的矩阵没有特征分解，这时我们只能使用奇异值分解。

回想一下，我们使用特征分解去分析矩阵 $\boldsymbol{A}$ 时，得到特征向量构成的矩阵 $\boldsymbol{V}$ 和特征值构成的向量 $\boldsymbol{\lambda}$，我们可以重新将 $\boldsymbol{A}$ 写作：

$$
\boldsymbol{A}=\boldsymbol{V} \operatorname{diag}(\boldsymbol{\lambda}) \boldsymbol{V}^{-1}\tag{2.43}
$$

奇异值分解是类似的，只不过这回我们将矩阵 $\boldsymbol{A}$ 分解成三个矩阵的乘积：

$$
\boldsymbol{A}=\boldsymbol{U D V}^{\top}\tag{2.44}
$$


假设 $\boldsymbol{A}$ 是一个 $m\times n$ 的矩阵，那么 $\boldsymbol{U}$ 是一个 $m\times m$ 的矩阵， $\boldsymbol{D}$ 是一个 $m\times n$ 的矩阵， $\boldsymbol{V}$ 是一个 $n\times n$ 矩阵。

这些矩阵中的每一个经定义后都拥有特殊的结构。矩阵 $\boldsymbol{U}$ 和 $\boldsymbol{V}$ 都定义为正交矩阵，而矩阵 $\boldsymbol{D}$ 定义为对角矩阵。注意，矩阵 $\boldsymbol{D}$ 不一定是方阵。<span style="color:red;">嗯。</span>

对角矩阵 $\boldsymbol{D}$ 对角线上的元素称为矩阵 $\boldsymbol{A}$ 的奇异值(singular value)。矩阵 $\boldsymbol{U}$ 的列向量称为左奇异向量(left singular vector)，矩阵 $\boldsymbol{V}$ 的列向量称右奇异向量(right singular vector)。

事实上，我们可以用与 $\boldsymbol{A}$ 相关的特征分解去解释 $\boldsymbol{A}$ 的奇异值分解。 $\boldsymbol{A}$ 的左奇异向量(left singular vector)是 $\boldsymbol{A} \boldsymbol{A}^{\top}$ 的特征向量。 $\boldsymbol{A}$ 的右奇异向量(right singular vector)是 $\boldsymbol{A}^{\top} \boldsymbol{A}$ 的特征向量。 $\boldsymbol{A}$ 的非零奇异值是 $\boldsymbol{A}^{\top} \boldsymbol{A}$ 特征值的平方根，同时也是特征值的平方根。<span style="color:red;">有些厉害，没有很明白。这个地方要详细补充下。</span>

SVD 最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。我们将在下一节中探讨。<span style="color:red;">什么意思？非方矩阵的求逆吗？怎么求？</span>





# 相关

- 《深度学习》花书
