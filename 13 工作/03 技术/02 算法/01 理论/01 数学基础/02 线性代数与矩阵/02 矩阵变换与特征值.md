---
title: 02 矩阵变换与特征值
toc: true
date: 2018-08-21 18:16:23
---

# 可以补充进来的


- 向量的求导要单独拿出来，把矩阵论的向量求导那一篇整合进来。
- 实对称阵不同特征值的特征向量正交，这个在一些机器学习的公式推理的时候用到很多次了，要再理解下。
- 感觉还需要整理 梳理。
- 很多地方不明白，而且为什么奇异值分解不能放在这里？而是要单开？还是说上面有一部分是属于奇异值分解的？


# 矩阵变换与特征值

数学中，字母上面加横线读作拔(a)表示求平均的意思。



# 相似变换：把矩阵看作线性映射就容易理解了


如果\(T:V\rightarrow V\)是一个线性变换，那么对于\(V\)的两组基\(\alpha \)与\(\widetilde{\alpha}=\alpha \cdot P\)，线性变换\(T\)的矩阵分别为：

\[A_{\alpha}(T)\, and \, A_{\widetilde{\alpha} }(T)=P^{-1}\cdot A_{\alpha}(T)\cdot P\]

<span style="color:red;">厉害了，这个是由线性映射与矩阵 这个知识点得到的。因为这里是 P 与 Q 是相等的。所以根据基变换下的矩阵变换公式得来的。</span>


## 方阵的相似变换


如果两个方阵 \(A\) 和 \(\widetilde{A}\) 满足，\(\widetilde(A)=P^{-1}AP\) 。那么这两个方阵就互为相似矩阵。

相似矩阵的几何意义是同一个线性变换在不同的基下的表达形式。<span style="color:red;">嗯 是的厉害。当研究对象是线性变换的时候，我们只关心矩阵在相似变换下不变的几何性质。是的，你其实不是关心两个不同的基是什么，而只是关心这种相似变换的 T。</span>


## 那么相似变换下不变的性质都有什么呢？



* 行列式(det)：<span style="color:red;">行列式是 det 吗？到这个地方才讲到行列式，行列式其实是相似变换下的不变量。因为这个不变量很重要，因此很多书把这个提到第一个来讲。为什么是不变量呢？证明如下：</span>

$$\begin{aligned}(P^{-1}AP)&=det(P^{-1})det(A)det(P)\\&=det(P^{-1})det(P)det(A)\\&=det(A)\end{aligned}$$


* 迹(trace)：<span style="color:red;">什么是迹？</span>


$$tr(AB)=tr(BA)$$

$$tr(P^{-1}AP)=tr(APP^{-1})=tr(A\cdot I)=tr(A)$$


* 秩(rank) 秩就是一个矩阵的极大线性无关组中的向量个数。




## 最重要的一个相似变换的性质：特征值


最重要的一个相似变换不变的性质是什么？是特征值，前面的三个都可以用特征值来表示。




  * 特征值：特征方程\(det(A-\lambda I)=0\)的根。如果\(det(A-\lambda I)=0\)，那么\(det(P^{-1}(A-\lambda I)P)=0\)，于是\(det(P^{-1}AP-\lambda I)=0\) **厉害**


特征值是最重要的相似不变量，利用这个相似不变量可以方便的得出上面的所有的不变量。


  * 行列式是什么？行列式就是所有特征值相乘。

  * 迹是什么？是所有特征值相加。

  * 秩是什么？是非 0 特征值的个数。


所以有了特征值的信息之后，前面的三个都比较简单了。**没想到特征值的作用这么大。**

下面简单介绍一下上面一些概念的几何意义：




  * 行列式是什么？如果 T 是从 V 到 V 的线性变换，那么取一组基你就得到矩阵，那么行列式就是 V 里面的单位体积的东西经过映射之后就得到了行列式。行列式就是体积膨胀的系数。

  * 迹的几何意义是：把这个映射看成是流体的向量场，流体的向量场所显示的流动的体积膨胀系数。

  * 秩说的是：比如 V 本来的维数是 n，映射到的子空间的维数就是秩？**没明白，在 Wiki 查一下。**




#




#




# 相和变换（二次型）


假设 \(V\) 是一个实系数线性空间，那么线性空间上的度量指的是空间中向量的内积关系 \(G(v_1,v_2)\) 。如果 \(\alpha \{\alpha_1,\cdot ,\alpha_k \}\) 是空间 V 的一组基，那么这个内积一般可以用一个对称矩阵 \(H_\alpha =[h_{ij}]_{n \times n}\) 来表示。**为什么这个内积可以用这个对称矩阵来表示？ 为什么度量是向量的内积关系？**

\[h_{ij}=G(\alpha_i,\alpha_j)\]

这个时候对于任意两个变量\(v_1,v_2\)，如果\(v_1=\alpha \cdot x_1\)，\(v_2=\alpha \cdot x_2\)，那么：

\[G(v_1,v_2)=x_1^TH_{\alpha}x_2\]

**这个\(H_\alpha \)是什么？**


## 方阵的相合变换


如果两个对称方阵 \(A\) 和 \(\widetilde{A}\) 满足 \(\widetilde{A}=P^TAP\) 。那么这两个方阵就互为相合矩阵。

相似矩阵的几何意义是同一个内积结构在不同基下的表示形式。之前介绍的相似变换是同一个线性变换在不同的基下的表示。

**这个地方怎么又说是相似矩阵了？不是相合矩阵吗？而且\(\widetilde{A}\)是什么？**






# 方阵的正交相似变换


有了相似变换和相和变换之后，才讲正交相似变换。

正交相似变换同时满足相似与相合变换的条件，也就是说它同时保持了矩阵的相似与相和不变量。

如果两个对称方阵 \(A\) 和 \(\widetilde{A}\) 满足 \(\widetilde{A}=P^TAP\) 。而且 \(P\) 是正交矩阵： \(P^T=P^{-1}\) 。那么这 \(A\) 和 \(\widetilde{A}\) 就互为正交相似。即既是相和的又是相似的。


## 方阵的正交相似标准型


任何一个对称矩阵 \(A\) 都可以正交相似于一个对角矩阵 \(D\) 。即总存在一个正交矩阵 \(P\) ，使得 \(A=P^TDP\) 。这个公式就是我们要学的 PCA，即主成分分析的数学原理。

为什么要做这些相似相和的东西？就是给定一个矩阵 A，你希望它相似一个非常简单的矩阵，而这个简单的矩阵几乎蕴含了你原来矩阵里面的所有的信息。什么是最简单的矩阵？对角矩阵就是最简单的矩阵。而这个定理告诉你，任何一个对称矩阵都可以正交相似与一个对角矩阵。就是说，这个对称矩阵从一定意义上来说都是非常简单的。然后 PCA 就是说：本来你只有这个 A，然后你通过找到 D 来简化这个问题。










# 正交阵：


若 n 阶矩阵 A 满足\(A^TA=I\)，成 A 为正交矩阵，简称正交阵。A是正交阵的充要条件：A的列(行)向量都是单位向量，且两两正交。

A是正交阵，x为向量，则\(A\cdot x\)称作正交变换。正交变换不改变向量长度。


## 两道思考题：


若 A、B都是 n 阶正交阵，那么，A×B是正交阵吗？是的。

正交阵和对称阵，能够通过何种操作获得一定意义下的联系？昨天学的协方差矩阵就是对称阵。






# 特征值和特征向量




## 定义：


A是 n 阶矩阵，若数λ和 n 维非 0 列向量 x 满足 Ax=λx，那么，数λ称为 A 的特征值，x称为 A 的对应于特征值λ的特征向量。

根据定义，立刻得到\((A-λI)x = 0\)，令关于λ 的多项式\(|A-λI|\)为 0，方程\(|A-λI|=0\)的根为 A 的特征值；将根λ_0 带入方程组(A-λI)x = 0 ，求得到的非零解，即λ_0 对应的特征向量。


## 特征值的性质：


设 n 阶矩阵\(A=(a_{ij} )\)的特征值为\(λ_1 ,λ_2 ,...λ_n \)，则




  * \(λ_1 +λ_2 +...+λ_n =a_{11} +a_{22}+…+a_{nn}\)

  * \(λ_1 \;λ_2 …λ_n =|A|\)


矩阵 A 主行列式的元素和，称作矩阵 A 的迹。


**特征值的意义，在聚类和谱聚类里面由好的实践方面的应用。后面学到回来补充下。**





## 两个点：


已知λ是方阵 A 的特征值，则：**用定义怎么解决？**




  * \(\lambda^2\) 是\(A^2\)的特征值

  * A可逆时，\(\lambda^{-1}\)是\(A^{-1}\)的特征值。




## 不同特征值 的 特征向量


设\(λ_1 ,λ_2 ,...,λ_m \)是方阵 A 的 m 个特征值，\(p_1 ,p_2 ,...,p_m\)是依次与之对应的特征向量，若\(λ_1 ,λ_2 ,...,λ_m \)各不相等，则\(p_1 ,p_2 ,...,p_m\)线性无关。

总结与思考




  * 不同特征值对应的特征向量，线性无关。

  * 若方阵 A 是对称阵呢？结论是否会加强？**什么叫线性无关的结论得到加强？嗯，加强到正交了。**


    * 协方差矩阵、二次型矩阵、无向图的邻接矩阵等都是对称阵

    * 在谱聚类中将会有所涉及







## 引理：


实对称阵的特征值是实数

设复数λ为对称阵 A 的特征值，复向量 x 为对应的特征向量，即 Ax=λx(x≠0)，现在证明附属λ一定是实数：

用\(\overline{\lambda}\) 表示\(\lambda\)的共轭复数，\(\overline{x}\) 表示 x 的共轭复向量，而 A 是实矩阵，有\(\overline{A}=A\) ，如果\(\lambda\)是实数，说明\(\overline{\lambda}\) =\(\lambda\)

证明：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/Cdh44ifbff.png?imageslim">
</p>

这个是想告诉大家是对称阵的特征值一定是实数。

而因为实数λ带入方程组(A- λ I)x=0，该方程组为实系数方程组，因此，实对称阵的特征向量只能是实向量。


## 实对称阵不同特征值的特征向量正交


证明如下：

令实对称矩阵为 A，它的两个不同的特征值\(λ_1\) ，\(λ_2\) 对应的特征向量分别是\(μ_1\) \(μ_2\) ；其中，\(λ_1\) \(λ_2\) \(μ_1\) \(μ_2\) 都是实数或是实向量。




  * 则有：\(Aμ 1 =λ_1 μ_1\) ，\( Aμ_2 =λ_2 μ_2\)

  * \((Aμ_1 )^T =(λ_1 μ_1 )^ T\) ，从而：\(μ_1^TA=λ_1 μ_1^T\)

  * 所以：\(μ_1^T Aμ_2 =λ_1 μ_1^T μ_2\)

  * 同时，\(μ_1^T Aμ_2 =μ_1^T (Aμ_2 )=μ_1^T λ_2 μ_2 = λ_2 μ_1^T μ_2\)

  * 所以，\(λ_1 μ_1^T μ_2 =λ_2 μ_1^T μ_2\)

  * 故：\((λ_1 -λ_2 ) μ_1^T μ_2 =0\)

  * 而\(λ_1 ≠λ_2 \)，所以\(μ_1^T μ_2 =0\)，即：\(μ_1\) ，\(μ_2 \)正交。




## 最终结论


设 A 为 n 阶的对称阵，则必有正交阵 P，使得：

\[P^{-1}AP=P^TAP=\Lambda\)

\(\Lambda\)是以 A 的 n 个特征值为对角元的对角阵。

一个对称阵总有一个正交阵，把它对角化。该变换称为“合同变换”，\(A\)和\(\Lambda\)互为合同矩阵。




# 二次型


含有 n 个变量的二次齐次函数，称为二次型。二次型本身跟线性代数没有关系。一个二次型对应一个对称阵，而上面说了对称阵可以由正交阵对角化，从而二次型可以化成只有 n 个变量平方项的标准型，而这个正交阵，对应着坐标系的旋转变化。**嗯也是。用的地方比如：可以将方程化简看是椭圆还是双曲线什么的。**


# 正定阵 还是不熟悉？什么是正定？什么是半正定？




## 正定阵的定义


对于 n 阶方阵 A，若任意 n 阶向量 x，都有 x^TAx>0，则称 A 是正定阵。可以看成是 1 阶的正数负数到方阵的推广。

若条件变成 x^TAx≥0，则 A 称作半正定阵，类似还有负定阵，半负定阵。


## 正定阵的判定：


对称阵 A 为正定阵；A的特征值都为正；A的顺序主子式（主对角线上的）大于 0；这三个命题等价。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/fe186Ccgii.png?imageslim">
</p>

用在那里呢？taylor展开式的时候，如果是 n 元的变量，求一阶导数，则是一个 n 阶的向量，求二阶导数就是一个 n^2的一个方阵，这个方阵判定的话就是用的正定负定。**没明白？**


## 两道题目：


给定凸锥的定义如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/dfmJJdcA1D.png?imageslim">
</p>

试证明：n阶半正定方阵的集合为凸锥。[note][凸锥](http://106.15.37.116/2018/03/31/ai-convex-optimization-optimization-and-convex-optimization/)[/note]这个是凸优化方面的一个题目。这个证明题本身没有价值，价值在于通过证明过程理解定义。考察的是半正定阵的定义。

利用定义证明：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/3hB60kL644.png?imageslim">
</p>




## 向量的导数


A为 m×n的矩阵， x为 n×1的列向量，则 Ax 为 m×1的列向量，记\(\overrightarrow{y}=A\cdot \overrightarrow{x}\)。思考：

\[\frac{\partial \overrightarrow{y} }{\partial \overrightarrow{x} }=?\]

推导：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/3i62FlCJab.png?imageslim">
</p>

**上面的，矩阵的求偏导的计算方式是什么样子的？**

理论与直接推广：向量偏导公式




  * \(\frac{\partial A \overrightarrow{x} }{\partial \overrightarrow{x} }=A^T\)

  * \(\frac{\partial A^T \overrightarrow{x} }{\partial \overrightarrow{x} }=A\)

  * \(\frac{\partial A \overrightarrow{x} }{\partial \overrightarrow{x}^T}=A\)


在线性回归中将直接使用该公式。使用的时候注意回来补充一下怎么用的。


## 标量对向量的导数


A为\(n\times n\)的矩阵，x为\(n\times 1\)的列向量，记 \(y=\overrightarrow{x}^T\cdot A\cdot \overrightarrow{x}\)

这个时候 y 是一个数。这时可得：**怎么求出来的？**

\[\frac{\partial y}{\partial \overrightarrow{x} }=\frac{\partial(\overrightarrow{x}^T\cdot A\cdot \overrightarrow{x})}{\partial \overrightarrow{x} }=(A^T+A)\cdot \overrightarrow{x}\]

如果 A 为对称阵，则有：

\[\frac{\partial y}{\partial \overrightarrow{x} }=\frac{\partial(\overrightarrow{x}^T\cdot A\cdot \overrightarrow{x})}{\partial \overrightarrow{x} }=2A\cdot \overrightarrow{x}\]


###




### 注意： **这个没讲**




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/g00JIIgdC8.png?imageslim">
</p>






# 相关

- 七月在线 机器学习
