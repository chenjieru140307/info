
# 上溢和下溢

连续数学在数字计算机上的根本困难是，我们需要通过有限数量的位模式来表示无限多的实数。<span style="color:red;">哇塞，真的是。</span>

这意味着我们在计算机中表示实数时，几乎总会引入一些近似误差。在许多情况下，这仅仅是舍入误差。舍入误差会导致一些问题，特别是当许多操作复合时，即使是理论上可行的算法，如果在设计时没有考虑最小化舍入误差的累积，在实践时也可能会导致算法失效。<span style="color:red;">哇塞，牛逼！</span>

## 下溢

一种极具毁灭性的舍入误差是下溢 (underflow)。当接近零的数被四舍五入为零时发生下溢。许多函数在其参数为零而不是一个很小的正数时才会表现出质的不同。

例如，我们通常要避免被零除 (一些软件环境将在这种情况下抛出异常，有些会返回一个非数字(not-a-number,NaN)的占位符) 或避免取零的对数 (这通常被视为 $-\infty$，进一步的算术运算会使其变成非数字) 。<span style="color:red;">嗯，不过非数字是什么？</span>

## 上溢

另一个极具破坏力的数值错误形式是上溢(overflow)。当大量级的数被近似为 $\infty$ 或 $-\infty$ 时发生上溢。进一步的运算通常会导致这些无限值变为非数字。<span style="color:red;">嗯。</span>



## 对上溢和下溢进行数值稳定

必须对上溢和下溢进行数值稳定的一个例子是 softmax 函数 (softmax function)。softmax 函数经常用于预测与 Multinoulli 分布相关联的概率，定义为：

$$
\operatorname{softmax}(\boldsymbol{x})_{i}=\frac{\exp \left(x_{i}\right)}{\sum_{j=1}^{n} \exp \left(x_{j}\right)}\tag{4.1}
$$

考虑一下当所有 $x_{i}$ 都等于某个常数 $c$ 时会发生什么。从理论分析上说，我们可以发现所有的输出都应该为 $\frac{1}{n}$ 。

从数值计算上说，当 $c$ 量级很大时，这可能不会发生：

- 如果 $c$ 是很小的负数，$\exp(c)$ 就会下溢。这意味着 softmax 函数的分母会变成 $0$，所以最后的结果是未定义的。
- 当 $c$ 是非常大的正数时，$\exp(c)$ 的上溢再次导致整个表达式未定义。

这两个困难能通过计算 $\operatorname{softmax}(z)$ 同时解决，其中 $\boldsymbol{z}=\boldsymbol{x}-\max _{i} x_{i}$ 。简单的代数计算表明，softmax 解析上的函数值不会因为从输入向量减去或加上标量而改变。<span style="color:red;">什么意思？这个地方没有明白？</span>减去 $\max _{i} x_{i}$ 导致 $\mathrm{exp}$ 的最大参数为 $0$，这排除了上溢的可能性。同样地，分母中至少有一个值为 $1$ 的项，这就排除了因分母下溢而导致被零除的可能性。



还有一个小问题。分子中的下溢仍可以导致整体表达式被计算为零。这意味着，如果我们在计算 $\log \operatorname{softmax}(x)$ 时，先计算 $softmax$ 再把结果传给 $\log$ 函数，会错误地得到 $-\infty$ 。相反，我们必须实现一个单独的函数，并以数值稳定的方式计算 $\log softmax$ 。我们可以使用相同的技巧来稳定 $\log softmax$ 函数。<span style="color:red;">嗯？什么样的单独的函数？</span>

在大多数情况下，我们没有明确地对本书描述的各种算法所涉及的数值考虑进行详细说明。在实现深度学习算法时，底层库的开发者应该牢记数值问题。<span style="color:red;">嗯，是的，想知道 tensorflow 或者 pytorch 这类的库怎么考虑这种问题的。</span>

本书的大多数读者可以简单地依赖保证数值稳定的底层库。在某些情况下，我们有可能在实现一个新的算法时自动保持数值稳定。Theano(Bergstra et al.,2010a;Bastien et al.,2012a)就是这样软件包的一个例子，它能自动检测并稳定深度学习中许多常见的数值不稳定的表达式。<span style="color:red;">哎？怎么自动检测并稳定深度学习中许多常见的数值不稳定的表达式的？想知道。</span>



# 相关

- 《深度学习》花书
