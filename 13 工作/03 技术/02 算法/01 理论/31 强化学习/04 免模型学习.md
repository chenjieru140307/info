---
title: 04 免模型学习
toc: true
date: 2018-07-01 16:08:51
---


# 免模型学习

亦称“无模型学习”


在现实的强化学习任务中，环境的转移概率、奖赏函数往往很难得知，甚至很难知道环境中一共有多少状态。若学习算法不依赖于环境建模，则称为 “免模型学习”(model-free learning)，这比有模型学习要困难得多.

## 蒙特卡罗强化学习

在免模型情形下，策略迭代算法首先遇到的问题是策略无法评估，这是由 于模型未知而导致无法做全概率展开。此时，只能通过在环境中执行选择的动 作，来观察转移的状态和得到的奖赏。受 $K$ 摇臂赌博机的启发，一种直接的策 略评估替代方法是多次“采样”，然后求取平均累积奖赏来作为期望累积奖赏 的近似，这称为蒙特卡罗强化学习。由于采样必须为有限次数，因此该方法更适 合于使用 $T$ 步累积奖赏的强化学习任务.

另一方面，策略迭代算法估计的是状态值函数 $V$，而最终的策略是通过状态-动作值函数 $Q$ 来获得。当模型已知时，从 $V$ 到 $Q$ 有很简单的转换方法，而当模型未知时，这也会出现困难。于是，我们将估计对象从 $V$ 转变为 $Q$，即估计 每一对“状态-动作”的值函数.

此外，在模型未知的情形下，机器只能是从一个起始状态(或起始状态集 合)开始探索环境，而策略迭代算法由于需对每个状态分别进行估计，因此在这 种情形下无法实现。例如探索种瓜的过程只能从播下种子开始，而不能任意选 择种植过程中的一个状态开始。因此，我们只能在探索的过程中逐渐发现各个 状态并估计各状态-动作对的值函数.

综合起来，在模型未知的情形下，我们从起始状态出发，使用某种策略进行 采样，执行该策略 $T$ 步并获得轨迹

$$
<x_{0}, a_{0}, r_{1}, x_{1}, a_{1}, r_{2}, \ldots, x_{T-1}, a_{T-1}, r_{T}, x_{T}>
$$

然后，对轨迹中出现的每一对状态-动作，记录其后的奖赏之和，作为该状态-动作对的一次累积奖赏采样值。多次采样得到多条轨迹后，将每个状态-动作对的累积奖赏采样值进行平均，即得到状态-动作值函数的估计.

可以看出，欲较好地获得值函数的估计，就需要多条不同的采样轨迹。然 而，我们的策略有可能是确定性的，即对于某个状态只会输出一个动作，若使用 这样的策略进行采样，则只能得到多条相同的轨迹。这与 $K$ 摇臂赌博机的“仅利用”法面临相同的问题，因此可借鉴探索与利用折中的办法，例如使用 $\epsilon$-贪 心法，以 $\epsilon$ 的概率从所有动作中均匀随机选取一个，以 $1-\epsilon$ 的概率选取当前最 优动作。我们将确定性的策略 $\pi$ 称为“原始策略”，在原始策略上使用 $\epsilon$-贪心法的策略记为

<center>

![](http://images.iterate.site/blog/image/20190709/AqLLvz4qfpRM.png?imageslim){ width=55% }


</center>


对于最大化值函数的原始策略 $\pi=\arg \max _{a} Q(x, a)$ ，其 $\epsilon$-贪心策略 $\pi^{\epsilon}$ 中，当前最优动作被选中的概率是 $1-\epsilon+\frac{\epsilon}{|A|}$，而每个非最优动作被选中的概率是 $\frac{\epsilon}{|A|}$，于是，每个动作都有可能被选取，而多次采样将会产生不同的采样轨迹.

与策略迭代算法类似，使用蒙特卡罗方法进行策略评估后，同样要对策 略进行改进。前面在讨论策略改进时利用了式(16.1幻揭示的单调性，通过换 入当前最优动作来改进策略。对于任意原始策略 $\pi$ ，其 $\epsilon$-贪心策略 $\pi^{\epsilon}$ 仅是将 $\epsilon$ 的概率均匀分配给所有动作，因此对于最大化值函数的原始策略 $\pi^{\prime}$，同样有 $Q^{\pi}\left(x, \pi^{\prime}(x)\right) \geqslant V^{\pi}(x)$ ，于是式(16.16)仍成立，即可以使用同样方法来进行策略改进.

图 16.10给出了上述过程的算法描述，这里被评估与被改进的是同一个策 略，因此称为“同策略”(on-policy)蒙特卡罗强化学习算法。算法中奖赏均值 采用增量式计算，每采样出一条轨迹，就根据该轨迹涉及的所有“状态-动作” 对来对值函数进行更新.

<center>

![](http://images.iterate.site/blog/image/20190709/2ahvzPVAWzJA.png?imageslim){ width=55% }


</center>

同策略蒙特卡罗强化学习算法最终产生的是 $\epsilon$-贪心策略。然而，引入 $\epsilon$-贪心是为了便于策略评估，在使用策略时并不需要 $\epsilon$-贪心；实际上我们希望改进 的是原始(非 $\epsilon$-贪心)策略。那么，能否仅在策略评估时引入 $\epsilon$-贪心，而在策略改 进时却改进原始策略呢？

这其实是可行的。不妨用两个不同的策略 $\pi$ 和 $\pi'$ 来产生采样轨迹，两者的 区别在于每个“状态-动作对”被采样的概率不同。一般的，函数 $f$ 在概率分布 $p$ 下的期望可表达为

$$
\mathbb{E}[f]=\int_{x} p(x) f(x) \mathrm{d} x
$$


可通过从概率分布 $p$ 上的采样 $\left\{x_{1}, x_{2}, \dots, x_{m}\right\}$ 来估计 $f$ 的期望，即

$$
\hat{\mathbb{E}}[f]=\frac{1}{m} \sum_{i=1}^{m} f(x)
$$

若引入另一个分布 $q$ ，则函数 $f$ 在概率分布 $p$ 下的期望也可等价地写为

$$
\mathbb{E}[f]=\int_{x} q(x) \frac{p(x)}{q*(x)} f(x) \mathrm{d} x
$$

这样基于一个分布的 采样来估计另一个分布 下的期望，称为重要性采 样(importance sampling).


上式可看作  $\frac{p(x)}{q(x)} f(x)$ 在分布 $q$ 下的期望，因此通过在 $q$ 上的采样 $\left\{x_{1}^{\prime}\right.x_{2}^{\prime}, \ldots, x_{m}^{\prime} \}$ 可估计为

$$
\hat{\mathbb{E}}[f]=\frac{1}{m} \sum_{i=1}^{m} \frac{p\left(x_{i}^{\prime}\right)}{q\left(x_{i}^{\prime}\right)} f\left(x_{i}^{\prime}\right)
$$

回到我们的问题上来，使用策略 $\pi$ 的采样轨迹来评估策略 $\pi$ ，实际上就是对累积奖赏估计期望

$$
Q(x, a)=\frac{1}{m} \sum_{i=1}^{m} r_{i}
$$

若改用策略 $\pi^{\prime}$ 的采样轨迹来评估策略 $\pi$，则仅需对累积奖赏加权，即

$$
Q(x, a)=\frac{1}{m} \sum_{i=1}^{m} \frac{P_{i}^{\pi}}{P_{i}^{\pi^{\prime}}} r_{i}
$$

其中 $P_{i}^{\pi}$ 和 $P_{i}^{\pi^{\prime}}$ 分别表示两个策略产生第 $i$ 条轨迹的概率。对于给定的一条轨 迹 $\left\langle x_{0}, a_{0}, r_{1}, \dots, x_{T-1}, a_{T-1}, r_{T}, x_{T}\right\rangle$，策略 $\pi$ 产生该轨迹的概率为

$$
P^{\pi}=\prod_{i=0}^{T-1} \pi\left(x_{i}, a_{i}\right) P_{x_{i} \rightarrow x_{i+1}}^{a_{i}}
$$



虽然这里用到了环境的转移概率 $P_{x_{i} \rightarrow x_{i+1}}^{a_{i}}$ ，但式(16.24)中实际只需两个策略概率的比值

$$
\frac{P^{\pi}}{P^{\pi^{\prime}}}=\prod_{i=0}^{T-1} \frac{\pi\left(x_{i}, a_{i}\right)}{\pi^{\prime}\left(x_{i}, a_{i}\right)}
$$


若 $\pi$ 为确定性策略而 $\pi^{\prime}$ 是 $\pi$ 的 $\epsilon$-贪心策略，则 $\pi\left(x_{i}, a_{i}\right)$ 始终为 $1$，$\pi^{\prime}\left(x_{i}, a_{i}\right)$ 为 $\frac{\epsilon}{|A|}$ 或 $1-\epsilon+\frac{\epsilon}{|A|}$，于是就能对策略 $\pi$ 进行评估了。图 16.11给出了 “异策略” (off-policy)蒙特卡罗强化学习算法的描述.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190709/sV9m75n6a98l.png?imageslim">
</p>


## 时序差分学习

蒙特卡罗强化学习算法通过考虑采样轨迹，克服了模型未知给策略估计造 成的困难。此类算法需在完成一个采样轨迹后再更新策略的值估计，而前面介 绍的基于动态规划的策略迭代和值迭代算法在每执行一步策略后就进行值函 数更新。两者相比，蒙特卡罗强化学习算法的效率低得多，这里的主要问题是 蒙特卡罗强化学习算法没有充分利用强化学习任务的 MDP 结构。时序差分 (Temporal Difference，简称 TD)学习则结合了动态规划与蒙特卡罗方法的思 想，能做到更高效的免模型学习.

蒙特卡罗强化学习算法的本质，是通过多次尝试后求平均来作为期望累积奖赏的近似，但它在求平均时是“批处理式”进行的，即在一个完整的采 样轨迹完成后再对所有的状态-动作对进行更新。实际上这个更新过程能增 量式进行。对于状态-动作对 $(x, a)$，不妨假定基于 $t$ 个采样已估计出值函数 $Q_{t}^{\pi}(x, a)=\frac{1}{t} \sum_{i=1}^{t} r_{i}$，则在得到第 $t+1$ 个采样 $r_{t+1}$ 时，类似式(16.3)，有

$$
Q_{t+1}^{\pi}(x, a)=Q_{t}^{\pi}(x, a)+\frac{1}{t+1}\left(r_{t+1}-Q_{t}^{\pi}(x, a)\right)
$$

显然，只需给 $Q_{t}^{\pi}(x, a)$ 加上增量 $\frac{1}{t+1}\left(r_{t+1}-Q_{t}^{\pi}(x, a)\right)$ 即可，更一般的，将 $\frac{1}{t+1}$ 替换为系数 $\alpha_{t+1}$，则可将增量项写作 $\alpha_{t+1}\left(r_{t+1}-Q_{t}^{\pi}(x, a)\right)$ ，在实践中通常令 $\alpha_{t}$ 为一个较小的正数值 $\alpha$ ，若将 $Q_{t}^{\pi}(x, a)$ 展开为每步累积奖赏之和，则可看出 系数之和为 $1$，即令 $\alpha_{t}=\alpha$ 不会影响 $Q_{t}$ 是累积奖赏之和这一性质。更新步长 $\alpha$ 越大，则越靠后的累积奖赏越重要.

以 $\gamma$ 折扣累积奖赏为例，利用动态规划方法且考虑到模型未知时使用状态-动作值函数更方便，由式(16.10)有

$$
\begin{aligned} Q^{\pi}(x, a) &=\sum_{x^{\prime} \in X} P_{x \rightarrow x^{\prime}}^{a}\left(R_{x \rightarrow x^{\prime}}^{a}+\gamma V^{\pi}\left(x^{\prime}\right)\right) \\ &=\sum_{x^{\prime} \in X} P_{x \rightarrow x^{\prime}}^{a}\left(R_{x \rightarrow x^{\prime}}^{a}+\gamma \sum_{a^{\prime} \in A} \pi\left(x^{\prime}, a^{\prime}\right) Q^{\pi}\left(x^{\prime}, a^{\prime}\right)\right) \end{aligned}
$$

通过增量求和可得

$$
Q_{t+1}^{\pi}(x, a)=Q_{t}^{\pi}(x, a)+\alpha\left(R_{x \rightarrow x^{\prime}}^{a}+\gamma Q_{t}^{\pi}\left(x^{\prime}, a^{\prime}\right)-Q_{t}^{\pi}(x, a)\right)\tag{16.31}
$$

> $$
> Q_{t+1}^{\pi}(x,a)=Q_{t}^{\pi}(x,a)+\alpha (R_{x\rightarrow x{}'}^{a}+\gamma Q_{t}^{\pi}(x{}',a{}')-Q_{t}^{\pi}(x,a))
> $$
>
> [推导]：对比公式 16.29
> $$
> Q_{t+1}^{\pi}(x,a)=Q_{t}^{\pi}(x,a)+\frac{1}{t+1}(r_{t+1}-Q_{t}^{\pi}(x,a))
> $$
> 以及由
> $$
> \frac{1}{t+1}=\alpha
> $$
> 可知
> $$
> r_{t+1}=R_{x\rightarrow x{}'}^{a}+\gamma Q_{t}^{\pi}(x{}',a{}')
> $$
> 而由 $\gamma$ 折扣累积奖赏可估计得到。

其中 $x^{\prime}$ 是前一次在状态 $x$ 执行动作 $a$ 后转移到的状态，$a^{\prime}$ 是策略 $\pi$ 在 $x^{\prime}$ 上选 择的动作.

将这几个英文单词的首 字母连起来.

使用式(16.31)，每执行一步策略就更新一次值函数估计，于是得到图 16.12 的算法。该算法由于每次更新值函数需知道前一步的状态(state)、前一步的动 作(action)、奖赏值(reward)、当前状态(state)、将要拔行的动作(action)，由 此得名为 Sarsa 算法[Rummery and Niranjan，1994]。显然，Sarsa 是一个同策 略算法，算法中评估(第 6 行)、执行(第 5 行)的均为 $\epsilon$-贪心策略.

将 Sarsa 修改为异策略算法，则得到图 16.13描述的 Q-学习(Q-learning)算 法[Watkins and Dayan, 1992]，该算法评估(第 6 行)的是 $\epsilon$-贪心策略，而执行(第 5行)的是原始策略.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190709/EusIb5kzSDHc.png?imageslim">
</p>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190709/hgr7lRiURgLh.png?imageslim">
</p>




# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
