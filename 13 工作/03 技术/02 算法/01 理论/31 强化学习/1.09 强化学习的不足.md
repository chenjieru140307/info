
# 可以补充进来的

- [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html) 内容阅读后补充进来
- [深度强化学习的弱点和局限](https://zhuanlan.zhihu.com/p/34089913) 也要补充进来。



# 强化学习的不足


这篇文章是对 Deep Reinforcement Learning Doesn't Work Yet 的总结

强化学习本身是一个非常通用的人工智能范式，在直觉上让人觉得非常适合用来模拟各种时序决策任务，如语音、文本类任务。当它和深度神经网络这种只要给我足够层和足够多的神经元，可以逼近任何函数的非线性函数近似模型结合在一起感觉要上天啊，无怪乎DeepMind经常号称人工智能=深度学习+强化学习。

然而Alex告诉我们别急，让我们先来审视一些问题：

1. **它的样本利用率非常低**。换言之为了让模型的表现达到一定高度需要极为大量的训练样本。

2. **最终表现很多时候不够好**。在很多任务上用非强化学习甚至非学习的其它方法，如基于模型的控制（model based control），线性二次型调节器（Linear Quadratic Regulator）等等可以获得好得多的表现。最气人的是这些模型很多时候样本利用率还高。当然这些模型有的时候会有一些假设比如有训练好的模型可以模仿，比如可以进行蒙特卡洛树搜索等等。

3. **DRL成功的关键离不开一个好的奖励函数（reward function），然而这种奖励函数往往很难设计**。在*Deep Reinforcement Learning That Matters*作者提到有时候把奖励乘以一个常数模型表现就会有天和地的区别。但奖励函数的坑爹之处还不止如此。奖励函数的设计需要保证：

4. 1. 加入了合适的先验，良好的定义了问题和在一切可能状态下的对应动作。坑爹的是模型很多时候会找到作弊的手段。Alex举的一个例子是有一个任务需要把红色的乐高积木放到蓝色的乐高积木上面，奖励函数的值基于红色乐高积木底部的高度而定。结果一个模型直接把红色乐高积木翻了一个底朝天。仔啊，你咋学坏了，阿爸对你很失望啊。
   2. 奖励函数的值太过稀疏。换言之大部分情况下奖励函数在一个状态返回的值都是0。这就和我们人学习也需要鼓励，学太久都没什么回报就容易气馁。都说21世纪是生物的世纪，怎么我还没感觉到呢？21世纪才刚开始呢。我等不到了啊啊啊啊啊。
   3. 有的时候在奖励函数上下太多功夫会引入新的偏见（bias）。
   4. 要找到一个大家都使用而又具有好的性质的奖励函数。这里Alex没很深入地讨论，但链接了一篇陶神（Terence Tao）的博客，大家有兴趣可以去看下。

5. **局部最优/探索和剥削（exploration vs. exploitation）的不当应用**。Alex举的一个例子是有一个连续控制的环境里，一个类似马的四足机器人在跑步，结果模型不小心多看到了马四脚朝天一顿乱踹后结果较好的情况，于是你只能看到四脚朝天的马了。

6. **对环境的过拟合**。DRL少有在多个环境上玩得转的。你训练好的DQN在一个Atari game上work了，换一个可能就完全不work。即便你想要做迁移学习，也没有任何保障你能成功。

7. **不稳定性**。

8. 1. 读DRL论文的时候会发现有时候作者们会给出一个模型表现随着尝试random seed数量下降的图，几乎所有图里模型表现最终都会降到0。相比之下在监督学习里不同的超参数或多或少都会表现出训练带来的变化，而DRL里运气不好可能很长时间你模型表现的曲线都没有任何变化，因为完全不work。
   2. 即便知道了超参数和随机种子，你的实现只要稍有差别，模型的表现就可以千差万别。这可能就是*Deep Reinforcement Learning That Matters*一文里John Schulman两篇不同文章里同一个算法在同一个任务上表现截然不同的原因。
   3. 即便一切都很顺利，从我个人的经验和之前同某DRL研究人员的交流来看只要时间一长你的模型表现就可能突然从很好变成完全不work。原因我不是完全确定，可能和过拟合和variance过大有关。

特别是上述第六点，几乎是灾难性的。作者提到自己实习的时候一开始实现Normalized Advantage Function (NAF)，为了找出Theano本身的bugs花了六周，这还是在NAF作者就在他旁边可以供他骚扰的情况下的结果。原因就是DRL的算法很多时候在没找好超参数的情况下就是不work的，所以你很难判断自己的代码到底有没有bug还是运气不好。

作者也回顾了DRL成功的案例，他认为DRL成功的案例其实非常少，大体包括：

1. 各类游戏：Atari Games, Alpha Go/Alpha Zero/Dota2 1v1/超级马里奥/日本将棋，其实还应该有DRL最早的成功案例，93年的西洋双陆棋（backgammon）。
2. DeepMind的跑酷机器人。
3. 为Google的能源中心节能。
4. Google的AutoML。

作者认为从这些案例里获得的经验教训是DRL可能在有以下条件的情况下更可能有好的表现，条件越多越好：

1. 数据获取非常容易，非常cheap。
2. 不要急着一上来就攻坚克难，可以从简化的问题入手。
3. 可以进行左右互搏。
4. 奖励函数容易定义。
5. 奖励信号非常多，反馈及时。

他也指出了一些未来潜在的发展方向和可能性：

1. **局部最优或许已经足够好**。未来某些研究可能会指出我们不必过于担心大部分情况下的局部最优。因为他们比起全局最优并没有差很多。
2. **硬件为王**。在硬件足够强的情况下我们或许就不用那么在乎样本利用率了，凡事硬刚就可以有足够好的表现。各种遗传算法玩起来。
3. **人为添加一些监督信号**。在环境奖励出现频次太低的情况下可以引入自我激励（intrinsic reward）或者添加一些辅助任务，比如DeepMind就很喜欢这套，之前还写了一篇[Reinforcement Learning with Unsupervised Auxiliary Tasks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.05397)。LeCun不是嫌蛋糕上的樱桃太少吗，让我们多给他点樱桃吧！
4. **更多融合基于模型的学习从而提高样本使用率**。这方面的尝试其实已经有很多了，具体可以去看Alex提到的那些工作。但还远不够成熟。
5. **仅仅把DRL用于fine-tuning**。比如最初Alpha Go就是以监督学习为主，以强化学习为辅。
6. **自动学习奖励函数**。这涉及到inverse reinforcement learning和imitation learning。
7. **迁移学习和强化学习的进一步结合**。
8. **好的先验**。
9. **有的时候复杂的任务反而更容易学习**。Alex提到的例子是DeepMind经常喜欢让模型学习很多同一环境的变种来减小对环境的过拟合。我觉得这也涉及curriculum learning，即从简单的任务开始逐步加深难度。可以说是层层递进的迁移学习。另外一个可能的解释是很多时候人觉得困难的任务和机器觉得困难的任务是相反的。比如人觉得倒水很简单，你让机器人用学习的路子去学倒水就可以很难。但反过来人觉得下围棋很简单而机器学习模型却在下围棋上把人击败了。

最后Alex总体还是非常乐观的。他说尽管现在有很多困难，使得DRL或许还不是一个强壮（robust）到所有人都可以轻易加入的研究领域并且很多时候一些问题用DRL远没有监督学习简单和表现好，但或许过几年你再回来DRL就work了也未知啊。这还是很振奋人心的。田渊栋老师也表达过类似的想法，觉得正因为这个领域还不够成熟所以还有很多机会。他们都是了不起的研究人员。

看到这篇文章我总体是非常激动的。但实话说也有些遗憾，如果去年暑假就有这篇文章的话也许我就会再慎重考虑一下到底要不要在实验室没有积累自己又离毕业和申请不远的情况下开始这样一个主题了。这是一个教训，就是开始一个领域前要对这个领域要有充分的了解，之前零零散散在网上也有了一点相关的声音，比如Karpathy就提到他在实现vanilla policy gradient的时候也遇到了很多困难。

> If it makes you feel any better, I’ve been doing this for a while and it took me last ~6 weeks to get a from-scratch policy gradients implementation to work 50% of the time on a bunch of RL problems. And I also have a GPU cluster available to me, and a number of friends I get lunch with every day who’ve been in the area for the last few years.
>
> Also, what we know about good CNN design from supervised learning land doesn’t seem to apply to reinforcement learning land, because you’re mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here.
>
> [Supervised learning] wants to work. Even if you screw something up you’ll usually get something non-random back. RL must be forced to work. If you screw something up or don’t tune something well enough you’re exceedingly likely to get a policy that is even worse than random. And even if it’s all well tuned you’ll get a bad policy 30% of the time, just because.
>
> Long story short your failure is more due to the difficulty of deep RL, and much less due to the difficulty of “designing neural networks”.

来源：[If it makes you feel any better, I've been doing this for a while and it took me...](https://link.zhihu.com/?target=https%3A//news.ycombinator.com/item%3Fid%3D13519044)

但我一开始并没有注意到。其实导师也一直有提到说他觉得我的project比较有风险，特别是他觉得现在除了Berkeley, OpenAI，DeepMind之外很少有DRL做的不错的实验室，这本身就表明这个方向可能有一些无形的门槛。现在我觉得这些可能包括计算资源和设备（机器人），对相关trick和坑了然于胸的相对资深的研究人员等等。客观上这些地方的人综合水平和工程能力也是强的让人发指，直接竞争非常困难。虽然我自己比较弱，但这些对于打算进入DRL的同学们都需要慎重考虑一下。

最后的最后还是要强推Alex的这篇文章，他列的这些点很多DRL的研究人员可能已经非常了解了，但之前没有人这样完整、有组织地介绍一遍。对于想要做DRL的同学们来说实在是福音。拙作是看完他文章后第一时间的感想和概括，对于我不够了解的有些地方就一笔带过了，或者表述也不够准确。原文很长，我在对大部分内容比较熟悉的情况下看了一个半小时，但也很有意思，还是强烈推荐。


# 相关

- [这里有一篇深度强化学习劝退文](https://zhuanlan.zhihu.com/p/33936457)
