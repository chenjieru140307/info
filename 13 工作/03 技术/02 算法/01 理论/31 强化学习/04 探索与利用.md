
# 可以补充进来的

- 还是需要补充一些的，而且，需要重新看下。

# 探索与利用

在和环境不断交互的过程中，智能体在不同的状态下不停地探索，获取不同的动作的反馈。

- 探索（Exploration）能够帮助智能体通过不断试验获得反馈，
- 利用（Exploitation）是指利用已有的反馈信息选择最好的动作。

因此如何平衡探索和利用是智能体在交互中不断学习的重要问题。<span style="color:red;">是的。</span>


强化学习，探索，利用

# 在智能体与环境的交互中，什么是探索和利用？如何平衡探索与利用？


假设我们开了一家叫 Surprise Me 的饭馆，客人来了不用点餐，而是用算法来决定该做哪道菜。具体过程为：

1. 客人 user = 1...T 依次到达饭馆。
2. 给客人推荐一道菜，客人接受则留下吃饭（Reward=1），拒绝则离开（Reward=0）。
3. 记录选择接受的客人总数 total_reward。

为了由浅入深地解决这个问题，我们先做以下三个假设。

1. 同一道菜，有时候会做得好吃一些（概率＝p），有时候会难吃一些（概率=1−p），但是并不知道概率 p 是多少，只能通过多次观测进行统计。
2. 不考虑个人口味的差异，即当菜做得好吃时，客人一定会留下（Reward=1）；当菜不好吃时，客人一定会离开（Reward=0）。
3. 菜好吃或不好吃只有客人说的算，饭馆是事先不知道的。


探索阶段：通过多次观测推断出一道菜做得好吃的概率。如果一道菜已经推荐了 k 遍（获取了 k 次反馈），就可以算出菜做得好吃的概率：

$$
\tilde{p}=\frac{1}{k} \sum_{i}^{k} \operatorname{Reward}_{i}\tag{11.6}
$$

如果推荐的次数足够多，$k$ 足够大，那么 $\tilde{p}$ 会趋近于真实的菜做得好吃的概率 $p$。<span style="color:red;">是的。</span>

利用阶段：已知所有的菜做得好吃的概率，决定该如何推荐？如果每道菜都被推荐了很多遍，就可以计算出每道菜做得好吃的概率 $\left\{\tilde{p}_{1}, \tilde{p}_{2}, \ldots, \tilde{p}_{N}\right\}$ ，于是只需推荐 $\tilde{p}$ 最大的那道菜。

探索和利用的平衡是一个经久不衰的问题。

- 一是，探索的代价是要不停地拿用户去试菜，影响客户的体验，但有助于更加准确的估计每道菜好吃的概率；
- 二是，利用会基于目前的估计拿出 “最好的” 菜来服务客户，但目前的估计可能是不准的（因为试吃的人还不够多）。<span style="color:red;">是的。</span>

那么如何平衡探索和利用呢？<span style="color:red;">是呀。</span>

可以使用 $\epsilon-greedy$ 算法，即每当客人到来时，先以 $\epsilon$ 的概率选择探索，从 $N$ 道菜中随机选择（概率为 $\epsilon/N$ ）一个让客人试吃，根据客人的反馈更新菜做得好吃的概率 $\left\{\tilde{p}_{1}, \tilde{p}_{2}, \ldots, \tilde{p}_{N}\right\}$ ；然后，以 $1-\epsilon$ 的概率选择利用，从 $N$ 道菜 $\left\{\tilde{p}_{1}, \tilde{p}_{2}, \ldots, \tilde{p}_{N}\right\}$ 中选择好吃的概率最高的菜推荐给用户。<span style="color:red;">为什么这么做是合理的？</span>




 $\epsilon-greedy$  算法也存在一些缺点，比如：

 - 在试吃次数相同的情况下，好吃和难吃的菜得到试吃的概率是一样的：有一道菜持续得到好吃的反馈，而另一道菜持续得到难吃的反馈，但在 $\epsilon-greedy$ 中，探索两道菜的概率是一样的，均为 $\epsilon/N$ ；
 - 在估计的成功概率相同的情况下，试吃次数多的和试吃次数少的菜得到再试吃的概率是一样的：假设有两道菜，第一道菜 $50$ 人当中 $30$ 个人说好，第二道菜 $5$ 个人当中 $3$ 个人说好，虽然两道菜的成功概率都是 $60％$（$30/50 = 3/5$），但显然反馈的人越多，概率估计的越准。再探索时，应该把重心放在试吃次数少的菜上。<span style="color:red;">是的。</span>

<span style="color:red;">嗯。</span>

总结一下， $\epsilon-greedy$ 生硬地将选择过程分成探索阶段和利用阶段，在探索时对所有物品以同样的概率进行探索，**并不会利用任何历史信息，包括某道菜被探索的次数和某道菜获得好吃反馈的比例**。<span style="color:red;">是的。</span>

不妨让我们忘记探索阶段和利用阶段，仔细想想如何充分地利用历史信息，找到最值得被推荐的菜。<span style="color:red;">嗯。</span>


- 观测 1：如果一道菜已经被推荐了 k 遍，同时获取了 k 次反馈，就可以算出菜做得好吃的概率：$\tilde{p}=\frac{1}{k} \sum_{i}^{k} \operatorname{Reward}_{i}$。当 $k$ 趋近正无穷时，$\tilde{p}$ 会趋近于真实的菜做得好吃的概率 $p$。

- 观测 2：现实当中一道菜被试吃的次数 $k$ 不可能无穷大，因此估计出的好吃的概率 $\tilde{p}$ 和真实的好吃的概率 $p$ 总会存在一个差值 $\Delta$ ，即 $\tilde{p}-\Delta \leqslant p \leqslant \tilde{p}+\Delta $。

基于上面两个观测，我们可以定义一个新的策略：每次推荐时，总是乐观地认为每道菜能够获得的回报是 $\tilde{p}+ {\Delta}$，这便是著名的置信区间上界（Upper Confidence Bound，UCB）算法。<span style="color:red;">嗯，是的，乐观的估计。</span>

最后只需解决一个问题：计算真实的概率和估计的概率之间的差值 $\Delta$ 。

在进入公式之前，让我们直观的理解影响 $\Delta$ 的因素：

- 对于被选中的菜，多获得一次反馈会使 $\Delta$ 变小，最终会小于其他没有被选中的菜；
- 对于没被选中的菜，$\Delta$ 会随着轮数的增大而增大，最终会大于其他被选中的菜。<span style="color:red;">这个为什么会增大？</span>


下面正式地介绍如何计算 $\Delta$ ，首先介绍 Chernoff-Hoeffding Bound：假设 $Reward_{1}, \ldots \ldots,Reward_{n}$ 是在 $[0,1]$ 之间取值的独立同分布随机变量，用 $\tilde{p}=\frac{\sum_{i} \operatorname{Reward}_{i}}{n}$ 表示样本均值，用 $p$ 表示分布的均值，那么有：


$$
P\{|\tilde{p}-p| \leqslant \Delta\} \geqslant 1-2 \mathrm{e}^{-2 n \Delta^{2}}\tag{11.8}
$$


当 $\Delta$ 取值为 $\sqrt{\frac{2 \ln T}{n}}$ 时，其中 $T$ 表示有 $T$ 个客人，$n$ 表示菜被吃过的次数，于是有：


$$
P\left\{|\tilde{p}-p| \leqslant \sqrt{\frac{2 \ln T}{n}}\right\} \geqslant 1-\frac{2}{T^{4}}\tag{11.9}
$$



这就是说 $\tilde{p}-\sqrt{\frac{2 \ln T}{n}} \leqslant p \leqslant \tilde{p}+\sqrt{\frac{2 \ln T}{n}}$ 是以 $1-\frac{2}{T^{4}}$ 的概率成立的：

- 当 T=2时，成立的概率为 0.875；
- 当 T=3时，成立的概率为 0.975；
- 当 T=4时，成立的概率为 0.992。

可以看到，当 $\Delta$ 取值为 $\sqrt{\frac{2 \ln T}{n}}$ 时，回报的均值 $\tilde{p}$ 距离真实回报 $p$ 的差距在 $\Delta$ 范围内的概率已经非常接近 $1$ 了，因此 $\Delta$ 的取值是一个非常合适的“置信区间上界”。

我们乐观地认为每道菜能够获得的回报是 $\tilde{p}{+\Delta}$，既利用到了当前回报的信息，也使用“置信区间上界”进行了探索。<span style="color:red;">嗯，计算出了回报，这样的探索就更加有效。不知道我这样的理解队不对</span>

<span style="color:red;">感觉还是要结合真实的例子来理解。最好有真实的代码看看效果。</span>



## 总结与扩展

如果读者对探索和利用问题有兴趣，可以继续探究一下基于贝叶斯思想的 Thompson Sampling，和考虑上下文信息的 LinUCB。<span style="color:red;">嗯，这两个都要整理进来。</span>


# 相关

- 《百面机器学习》

