

### Adam 方法

Adam 方法将惯性保持和环境感知这两个优点集于一身：

- 一方面，Adam 记录梯度的一阶矩（first moment），即过往梯度与当前梯度的平均，这体现了惯性保持
- 另一方面，Adam还记录梯度的二阶矩（second moment），即过往梯度平方与当前梯度平方的平均，这类似 AdaGrad 方法，体现了环境感知能力，为不同参数产生自适应的学习速率。

一阶矩和二阶矩采用类似于滑动窗口内求平均的思想进行融合，即当前梯度和近一段时间内梯度的平均值，时间久远的梯度对当前平均值的贡献呈指数衰减。

具体来说，一阶矩和二阶矩采用指数衰退平均（exponential decay average）技术，计算公式为：

$$
m_{t}=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t}\tag{7.52}
$$

$$
v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}\tag{7.53}
$$

其中 $\beta_{1}$ ，$\beta_{2}$ 为衰减系数，$m_{t}$ 是一阶矩，$v_{t}$ 是二阶矩。

如何理解一阶矩和二阶矩呢？

- 一阶矩相当于估计 $\mathbb{E}\left[g_{t}\right]$ ：由于当下梯度 $g_{t}$ 是随机采样得到的估计结果，因此更关注它在统计意义上的期望。
- 二阶矩相当于估计 $\mathbb{E}\left[g_{t}^{2}\right]$，这点与 AdaGrad 方法不同，不是 $g_{t}^{2}$ 从开始到现在的加和，而是它的期望。

它们的物理意义是：

- 当 $\left\|m_{t}\right\|$ 大且 $v_{t}$ 大时，梯度大且稳定，这表明遇到一个明显的大坡，前进方向明确；
- 当 $\left\|m_{t}\right\|$ 趋于零且 $v_{t}$ 大时，梯度不稳定，表明可能遇到一个峡谷，容易引起反弹震荡；
- 当 $\left\|m_{t}\right\|$ 大且 $v_{t}$ 趋于零时，这种情况不可能出现；
- 当 $\left\|m_{t}\right\|$ 趋于零且 $v_{t}$ 趋于零时，梯度趋于零，可能到达局部最低点，也可能走到一片坡度极缓的平地，此时要避免陷入平原（plateau）。

另外，Adam方法还考虑了 $m_t$ ，$v_t$ 在零初始值情况下的偏置矫正。

具体来说，Adam的更新公式为：

$$
\theta_{t+1}=\theta_{t}-\frac{\eta \cdot \hat{m}_{t}}{\sqrt{\hat{v}_{t}+\epsilon}}\tag{7.54}
$$


其中，$\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}$ ，$\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}$。






# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions) 原文
