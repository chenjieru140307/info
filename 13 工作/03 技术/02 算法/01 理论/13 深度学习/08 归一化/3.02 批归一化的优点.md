

# 批归一化 BN 算法的优点

下面我们来说一下 BN 算法的优点：

1. 减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数，或者采取更小的 L2 正则项约束参数；<span style="color:red;">为什么？这样的 BN 有利于泛化吗？</span>
2. 减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛；<span style="color:red;">这么厉害吗？</span>
3. 可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在)<span style="color:red;">为什么不用使用局部相应归一化了？这个 BN 和局部相应归一化看起来不相同呀？</span>
4. 破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 $1\%$ 的精度）。<span style="color:red;">？ 能够防止每批训练中某一个样本经常被挑选到吗？这个是真的吗？</span>
5. 减少梯度消失，加快收敛速度，提高训练精度。
