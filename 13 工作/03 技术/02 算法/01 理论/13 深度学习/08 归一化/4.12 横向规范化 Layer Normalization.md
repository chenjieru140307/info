
# 横向规范化 Layer Normalization

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/Q1AEdDdSji1P.png?imageslim">
</p>


层规范化就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的规范化，如图所示。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。

$$
\mu=\sum_{i} x_{i}, \quad \sigma=\sqrt{\sum_{i}\left(x_{i}-\mu\right)^{2}+\epsilon}
$$

其中 ${i}$ 枚举了该层所有的输入神经元。对应到标准公式中，四大参数 $\mu, \sigma, \mathbf{g}, \mathbf{b}$ 均为标量（BN中是向量），所有输入共享一个规范化变换。



LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小 mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。



但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。







# 相关

- [详解深度学习中的 Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
