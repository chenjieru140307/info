

### 3.6.8 什么是批归一化（Batch Normalization）

以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。

要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过 $\sigma(WX+b)$ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。

如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。

这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化 Batch Normalization（BN）。
