
# Normalization 为什么会有效？



我们以下面这个简化的神经网络为例来分析。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/mBKRSQOrvu2g.png?imageslim">
</p>

## Normalization 的权重伸缩不变性



**权重伸缩不变性（weight scale invariance）** 指的是，当权重 $\mathbf{W}$ 按照常量 $\lambda$ 进行伸缩时，得到的规范化后的值保持不变，即：

$$
\operatorname{Norm}\left(\mathbf{W}^{\prime} \mathbf{x}\right)=\operatorname{Norm}(\mathbf{W} \mathbf{x})
$$


其中 $\mathbf{W}^{\prime}=\lambda \mathbf{W}$ 。



**上述规范化方法均有这一性质**，这是因为，当权重 $\mathbf{W}$ 伸缩时，对应的均值和标准差均等比例伸缩，分子分母相抵。

$$
\begin{aligned} \operatorname{Norm}\left(\mathbf{W}^{\prime} \mathbf{x}\right)=& N o r m\left(\mathbf{g} \cdot \frac{\mathbf{W}^{\prime} \mathbf{x}-\mu^{\prime}}{\sigma^{\prime}}+\mathbf{b}\right) \\=& \operatorname{Norm}\left(\mathbf{g} \cdot \frac{\lambda \mathbf{W} \mathbf{x}-\lambda \mu}{\lambda \sigma}+\mathbf{b}\right) \\=& N o r m\left(\mathbf{g} \cdot \frac{\mathbf{W} \mathbf{x}-\mu}{\sigma}+\mathbf{b}\right)=N o r m(\mathbf{W} \mathbf{x}) \end{aligned}
$$


**权重伸缩不变性可以有效地提高反向传播的效率**。

由于

$$
\frac{\partial N o r m\left(\mathbf{W}^{\prime} \mathbf{x}\right)}{\partial \mathbf{x}}=\frac{\partial N o r m(\mathbf{W} \mathbf{x})}{\partial \mathbf{x}}
$$

因此，权重的伸缩变化不会影响反向梯度的 Jacobian 矩阵，因此也就对反向传播没有影响，避免了反向传播时因为权重过大或过小导致的梯度消失或梯度爆炸问题，从而加速了神经网络的训练。



**权重伸缩不变性还具有参数正则化的效果，可以使用更高的学习率。**

由于

$$
\frac{\partial N o r m\left(\mathbf{W}^{\prime} \mathbf{x}\right)}{\partial \mathbf{W}^{\prime}}=\frac{1}{\lambda} \cdot \frac{\partial N o r m(\mathbf{W} \mathbf{x})}{\partial \mathbf{W}}
$$

因此，下层的权重值越大，其梯度就越小。这样，参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，提高网络的泛化性能。


## Normalization 的数据伸缩不变性



**数据伸缩不变性（data scale invariance）**指的是，当数据 $\mathbf{x}$ 按照常量 $\lambda$ 进行伸缩时，得到的规范化后的值保持不变，即：

$$
\operatorname{Norm}\left(\mathbf{W} \mathbf{x}^{\prime}\right)=\operatorname{Norm}(\mathbf{W} \mathbf{x})
$$

其中 $\mathbf{x}^{\prime}=\lambda \mathbf{x}$。



**数据伸缩不变性仅对 BN、LN 和 CN 成立。**因为这三者对输入数据进行规范化，因此当数据进行常量伸缩时，其均值和方差都会相应变化，分子分母互相抵消。而 WN 不具有这一性质。



**数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择**。

对于某一层神经元 $h_{l}=f_{\mathbf{W}_{l}}\left(\mathbf{x}_{l}\right)$ 而言，展开可得

$$
h_{l}=f_{\mathbf{W}_{l}}\left(\mathbf{x}_{l}\right)=f_{\mathbf{W}_{l}}\left(f_{\mathbf{W}_{l-1}}\left(\mathbf{x}_{l-1}\right)\right)=\cdots=\mathbf{x}_{0} \prod_{k=0}^{l} \mathbf{W}_{k}
$$

每一层神经元的输出依赖于底下各层的计算结果。如果没有正则化，当下层输入发生伸缩变化时，经过层层传递，可能会导致数据发生剧烈的膨胀或者弥散，从而也导致了反向计算时的梯度爆炸或梯度弥散。



加入 Normalization 之后，不论底层的数据如何变化，对于某一层神经元 $h_{l}=f_{\mathbf{W}_{l}}\left(\mathbf{x}_{l}\right)$ 而言，其输入 $\mathbf{x}_{l}$ 永远保持标准的分布，这就使得高层的训练更加简单。从梯度的计算公式来看：

$$
\frac{\partial N o r m\left(\mathbf{W} \mathbf{x}^{\prime}\right)}{\partial \mathbf{W}}=\frac{\partial N o r m(\mathbf{W} \mathbf{x})}{\partial \mathbf{W}}
$$

数据的伸缩变化也不会影响到对该层的权重参数更新，使得训练过程更加鲁棒，简化了对学习率的选择。







# 相关

- [详解深度学习中的 Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
