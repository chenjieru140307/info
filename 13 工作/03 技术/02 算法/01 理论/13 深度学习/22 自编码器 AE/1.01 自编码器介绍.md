---
title: 1.01 自编码器介绍
toc: true
date: 2019-09-01
---
# 自编码器介绍


类似于 PCA 的作用。

自编码器是神经网络的一种，经过训练后能尝试将输入复制到输出。自编码器内部有一个隐藏层 $\boldsymbol h$，可以产生编码表示输入。

该网络可以看作由两部分组成：

- 一个由函数 $\boldsymbol h = f(\boldsymbol x)$ 表示的编码器
- 一个生成重构的解码器 $\boldsymbol r=g(\boldsymbol h)$。

下图展示了这种架构。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/3rXLEg7owIC2.png?imageslim">
</p>


> 14.1 自编码器的一般结构，通过内部表示或编码 $\boldsymbol h$ 将输入 $\boldsymbol x$ 映射到输出（称为重构）$\boldsymbol r$。自编码器具有两个组件：编码器~$f$（将 $\boldsymbol x$ 映射到 $\boldsymbol h$）和解码器~$g$（将 $\boldsymbol h$ 映射到 $\boldsymbol r$）。

如果一个自编码器只是简单地学会将处处设置为 $g(f(\boldsymbol x)) =\boldsymbol x$，那么这个自编码器就没什么特别的用处。相反，我们不应该将自编码器设计成输入到输出完全相等。这通常需要向自编码器强加一些约束，使它只能近似地复制，并只能复制与训练数据相似的输入。这些约束强制模型考虑输入数据的哪些部分需要被优先复制，因此它往往能学习到数据的有用特性。


现代自编码器将编码器和解码器的概念推而广之，将其中的确定函数推广为随机映射 $p_{\text{encoder}} (\boldsymbol h \mid \boldsymbol x)$ 和 $p_{\text{decoder}}(\boldsymbol x \mid \boldsymbol h)$。


数十年间，自编码器的想法一直是神经网络历史景象的一部分。传统自编码器被用于降维或特征学习。

近年来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿，我们将在深度生成模型中揭示更多细节。

自编码器可以被看作是前馈网络的一个特例，并且可以使用完全相同的技术进行训练，通常使用小批量梯度下降法（其中梯度基于反向传播计算）。

不同于一般的前馈网络，自编码器也可以使用再循环训练，这种学习算法基于比较原始输入的激活和重构输入的激活。相比反向传播算法，再循环算法更具生物学意义，但很少用于机器学习应用。<span style="color:red;">什么是再循环算法？</span>


自编码器并不是一个真正的无监督学习的算法，而是一个自监督的算法。自监督学习是监督学习的一个实例，其标签产生自输入数据，如图 1.16所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190525/6T9y1LhnJfCA.png?imageslim">
</p>




# 相关

- 《深度学习》花书
