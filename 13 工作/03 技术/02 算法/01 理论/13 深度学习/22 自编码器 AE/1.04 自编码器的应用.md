

# 自编码器的应用


自编码器已成功应用于降维和信息检索任务。

降维是表示学习和深度学习的第一批应用之一。它是研究自编码器早期驱动力之一。例如， {Hinton-Science2006}训练了一个栈式 RBM，然后利用它们的权重初始化一个隐藏层逐渐减小的深度自编码器，终结于 30 个单元的瓶颈。生成的编码比 30 维的 PCA 产生更少的重构误差，所学到的表示更容易定性解释，并能联系基础类别，这些类别表现为分离良好的集群。


低维表示可以提高许多任务的性能，例如分类。小空间的模型消耗更少的内存和运行时间。据 {Salakhutdinov+Hinton2007-small}和 {Torralba+Fergus+Weiss-2008}观察，许多降维的形式会将语义上相关的样本置于彼此邻近的位置。映射到低维空间所提供的线索有助于泛化。


相比普通任务，信息检索从降维中获益更多，此任务需要找到数据库中类似查询的条目。此任务不仅和其他任务一样从降维中获得一般益处，还使某些低维空间中的搜索变得极为高效。特别的，如果我们训练降维算法生成一个低维且\emph{二值}的编码，那么我们就可以将所有数据库条目在哈希表映射为二值编码向量。这个哈希表允许我们返回具有相同二值编码的数据库条目作为查询结果进行信息检索。我们也可以非常高效地搜索稍有不同条目，只需反转查询编码的各个位。这种通过降维和二值化的信息检索方法被称为语义哈希，已经被用于文本输入和图像。


通常在最终层上使用 sigmoid 编码函数产生语义哈希的二值编码。sigmoid 单元必须被训练为到达饱和，对所有输入值都接近 0 或接近 1。能做到这一点的窍门就是训练时在 sigmoid 非线性单元前简单地注入加性噪声。噪声的大小应该随时间增加。要对抗这种噪音并且保存尽可能多的信息，网络必须加大输入到 sigmoid 函数的幅度，直到饱和。


学习哈希函数的思想已在其他多个方向进一步探讨，包括改变损失训练表示的想法，其中所需优化的损失与哈希表中查找附近样本的任务有更直接的联系。



# 相关

- 《深度学习》花书
