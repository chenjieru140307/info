
# 可解释的机器学习

以深度学习为代表的各种机器学习技术方兴未艾，取得了举世瞩目的成功。机器和人类在很多复杂认知任务上的表现已在伯仲之间。然而，在解释模型为什么奏效及如何运作方面，目前学界的研究还处于非常初级的阶段。

## 解释什么：相关性和因果逻辑性之间的鸿沟。

大部分机器学习技术，尤其是基于统计的机器学习技术，高度依赖基于数据相关性习得的概率化预测和分析。相反，理性的人类决策更依赖于清楚可信的因果关系，这些因果关系由真实清楚的事实原由和逻辑正确的规则推理得出。从利用数据相关性来解决问题，过渡到利用数据间的因果逻辑来解释和解决问题，是可解释性机器学习需要完成的核心任务之一。

## 为什么需要解释：知之为知之，不知为不知，是知也。

机器学习模型基于历史数据进行分析和决策。但由于常识的缺失，机器在面对历史上未发生过或罕见的事件时，很大可能性会犯人类几乎不可能犯的低级错误。统计意义上的准确率并不能有效地刻画决策的风险，甚至在某些情况下，看似正确的概率性选择背后的原因与事实背道而驰。在可控性为首要考量目标的领域，比如医疗、核工业和航天等，理解数据决策背后所依赖的事实基础是应用机器学习的前提。对于这些领域，可解释性意味着可信和可靠。

可解释性机器学习，还是把机器学习技术与我们人类社会做深度集成的必经之路。对可解释性机器学习的需求不仅仅是对技术进步的渴求，同时包含各种非技术因素的考量，甚至包含法律法规。欧盟在 2018 年生效的 GDPR（General Data Protection Regulation）条例中明确要求，当采用机器做出针对某个体的决定时，比如自动拒绝一个在线贷款申请，该决定必须符合一定要求的可解释性。

除了产业和社会对可解释性机器学习的迫切需求，解释行为的动机同时是人类大脑内建的能力和诉求。认知神经科学先驱 Michael S. Gazzaniga在对现代认知科学影响深远的裂脑人（Split-Brain Patients）研究中得出了如下的观察和结论：“我们的大脑会不由自主地去寻求（决策的）解释和事件发生的原由。”

## 谁解释给谁：以人为中心的机器学习升级。

解释给谁听，这个问题相对清楚。简而言之，解释给人。根据受众的不同，包含只有机器学习专家可以理解的解释，也包含普通大众都可以理解的解释。

那么由谁来解释呢？理想情况下，由机器解释：机器一边解答问题，一边给出答案背后的逻辑推理过程。但是，受限于很多机器学习技术的工作原理，机器自答自释并不总是行得通。很多机器学习算法是“数据进来，模型出去”，绝大部分时候，模型最终得出的结论与输入数据之间的因果关联变得无迹可寻，模型也变成了一个“神奇的”黑箱子。

在机器自答自释尚无有效方案的阶段，支持人工审查和回溯解答过程的方案可以提供一定程度的可解释性。此时，机器学习系统中各个子模块作用机理的可解释性就变得尤为重要。对于一个大型的机器学习系统，整体的可解释性高度依赖于各个组成部分的可解释性。从目前的机器学习到可解释性机器学习的演化将是一个涉及方方面面的系统工程，需要对目前的机器学习从理论到算法，再到系统实现进行全面的改造和升级。

## 可解释性的度：起于实用，终于无穷。

不同的应用场景对机器学习可解释性的要求天然不同。某些时候，“曲高和寡”的专业解释就已足够，尤其当其解释只用作技术安全性审查时；另外一些场合，当可解释性是人机交互的一部分时，“老妪能解”的通俗解答就变得非常必要。任何技术都只在一定范围和一定程度上起作用，对于机器学习的可解释性同样如此。可解释机器学习，起于实用性的需求，终于永无止尽的不断改进中。




# 相关

- [机器学习：未来十年研究热点](https://www.msra.cn/zh-cn/news/executivebylines/tech-bylines-machine-learning)
