


# 概率 PCA 和因子分析

$$
\mathbf{h} \sim p(\boldsymbol{h})\tag{13.1}
$$

$$
\boldsymbol{x}=\boldsymbol{W h}+\boldsymbol{b}+\text { noise }\tag{13.2}
$$

概率 PCA、因子分析和其他线性因子模型是上述等式（式 13.1 和式 13.2）的特殊情况，并且仅在对观测到 $\boldsymbol x$ 之前的噪声分布和潜变量 $\boldsymbol h$ 先验的选择上有所不同。

在因子分析中，潜变量的先验是一个方差为单位矩阵的高斯分布


$$\begin{aligned}
\mathbf h \sim \mathcal N(\boldsymbol h; \mathbf{0},\boldsymbol I),
\end{aligned}$$


同时，假定在给定 $\boldsymbol h$ 的条件下观察值 $x_i$ 是条件独立的。具体来说，我们可以假设噪声是从对角协方差矩阵的高斯分布中抽出的，协方差矩阵为 $\boldsymbol psi = \text{diag}(\boldsymbol sigma^2)$，其中 $\boldsymbol sigma^2 = [\sigma_1^2,\sigma_2^2,\ldots,\sigma_n^2]^{\top}$ 表示一个向量，每个元素表示一个变量的方差。


因此，潜变量的作用是\emph{捕获}不同观测变量 $x_i$ 之间的\emph{依赖关系}。实际上，可以容易地看出 $\boldsymbol x$ 服从多维正态分布，并满足


$$\begin{aligned}
\mathbf x \sim \mathcal N(\boldsymbol x; \boldsymbol b, \boldsymbol W\boldsymbol W^{\top}+\boldsymbol psi).
\end{aligned}$$



为了将 PCA 引入到概率框架中，我们可以对因子分析模型作轻微修改，使条件方差 $\sigma_i^2$ 等于同一个值。在这种情况下，$\boldsymbol x$ 的协方差简化为 $\boldsymbol W\boldsymbol W^{\top}+\sigma^2\boldsymbol I$，这里的 $\sigma^2$ 是一个标量。
由此可以得到条件分布，如下：

$$\begin{aligned}
\mathbf x \sim \mathcal N(\boldsymbol x; \boldsymbol b, \boldsymbol W\boldsymbol W^{\top} + \sigma^2\boldsymbol I ),
\end{aligned}$$


或者等价地


$$\begin{aligned}
\mathbf x = \boldsymbol W\mathbf h + \boldsymbol b + \sigma\mathbf z,
\end{aligned}$$


其中 $\mathbf z \sim \mathcal N(\boldsymbol z;\mathbf{0},\boldsymbol I)$ 是高斯噪声。之后 {Tipping99}提出了一种迭代的~EM~算法来估计参数 $\boldsymbol W$ 和 $\sigma^2$。


这个概率 PCA 模型利用了这样一种观察现象：除了一些微小残余的重构误差（至多为 $\sigma^2$），数据中的大多数变化可以由潜变量 $\boldsymbol h$ 描述。通过{Tipping99}的研究我们可以发现，当 $\sigma \xrightarrow{} 0$ 时，概率 PCA 退化为 PCA。在这种情况下，给定 $\boldsymbol x$ 情况下 $\boldsymbol h$ 的条件期望等于将 $\boldsymbol x - \boldsymbol b$ 投影到 $\boldsymbol W$ 的 $d$ 列所生成的空间上，与 PCA 一样。

当 $\sigma\xrightarrow{} 0$ 时， 概率 PCA 所定义的密度函数在 $d$ 维的 $\boldsymbol W$ 的列生成空间周围非常尖锐。这导致模型会为没有在一个超平面附近聚集的数据分配非常低的概率。<span style="color:red;">如果某些数据实际上没有集中在超平面附近，这会导致模型为数据分配非常低的可能性。导致模型会为没有在一个超空间附近聚集的数据分配非常低的概率</span>



# 相关

- 《深度学习》花书
