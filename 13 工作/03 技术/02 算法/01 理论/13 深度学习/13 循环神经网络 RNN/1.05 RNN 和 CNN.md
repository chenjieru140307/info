
# 循环神经网络和卷积神经网络

用传统方法进行文本分类任务时，通常将一篇文章所对应的 TF-IDF 向量作为输入，其中 TF-IDF 向量的维度是词汇表的大小。

使用前馈神经网络，如卷积神经网络对文本数据建模时，一般会如何操作？使用循环神经网络对文本这种带有序列信息的数据进行建模时，相比卷积神经网络又会有什么不同？

循环神经网络，前馈神经网络

## 处理文本数据时，循环神经网络与前馈神经网络相比有什么特点？

刚才提到，传统文本处理任务的方法中一般将 TF-IDF 向量作为特征输入。显而易见，这样的表示实际上丢失了输入的文本序列中每个单词的顺序。

在神经网络的建模过程中：

- 一般的前馈神经网络，如卷积神经网络，通常接受一个定长的向量作为输入。卷积神经网络对文本数据建模时，输入变长的字符串或者单词串，然后通过滑动窗口加池化的方式将原先的输入转换成一个固定长度的向量表示，这样做可以捕捉到原文本中的一些局部特征，但是两个单词之间的长距离依赖关系还是很难被学习到。<span style="color:red;">是的。</span>
- 循环神经网络却能很好地处理文本数据变长并且有序的输入序列。它模拟了人阅读一篇文章的顺序，从前到后阅读文章中的每一个单词，将前面阅读到的有用信息编码到状态变量中去，从而拥有了一定的记忆能力，可以更好地理解之后的文本。<span style="color:red;">前面阅读的信息编码到状态变量中去。嗯，但是为什么这种编码方式是合理的？嗯，仔细看看。</span>


图 10.1展示了一个典型的循环神经网络结构[22]。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190414/52wRDE1BWKGT.png?imageslim">
</p>

由图可见，一个长度为 $T$ 的序列用循环神经网络建模，展开之后可以看作是一个 $T$ 层的前馈神经网络。其中，第 $t$ 层的隐含状态 $h_t$ 编码了序列中前 $t$ 个输入的信息，可以通过当前的输入 $x_t$ 和上一层神经网络的状态 $h_{t−1}$ 计算得到；最后一层的状态 $h_T$ 编码了整个序列的信息，因此可以作为整篇文档的压缩表示。<span style="color:red;">嗯嗯，是怎么训练出来的？</span>

以此为基础的结构可以应用于多种具体任务。

例如，在 $h_T$ 后面直接接一个 Softmax 层，输出文本所属类别的预测概率 $y$，就可以实现文本分类。 $h_t$ 和 $y$ 的计算公式为：


$$
net_{t}=U x_{t}+W h_{t-1}\tag{10.1}
$$

$$
h_{t}=f\left(\text {net}_{t}\right)\tag{10.2}
$$

$$
y=g\left(V h_{T}\right)\tag{10.3}
$$


其中 $f$ 和 $g$ 为激活函数，$U$ 为输入层到隐含层的权重矩阵，$W$ 为隐含层从上一时刻到下一时刻状态转移的权重矩阵。在文本分类任务中，$f$ 可以选取 Tanh 函数或者 ReLU 函数，$g$ 可以采用 Softmax 函数。<span style="color:red;">是的是的，这个还是与前向神经网络有些差别的。包括变量的命名。。</span>


通过最小化损失误差（即输出的 $y$ 与真实类别之间的距离），我们可以不断训练网络，使得得到的循环神经网络可以准确地预测文本所属的类别，达到分类目的。

相比于卷积神经网络等前馈神经网络，循环神经网络由于具备对序列顺序信息的刻画能力，往往能得到更准确的结果。<span style="color:red;">嗯，好的。谢谢这么 yummy 的文章</span>



# 相关

- 《百面机器学习》
- 《百面机器学习》
