---
title: 2.01 各种循环神经网络的计算
toc: true
date: 2019-09-01
---

基于图展开和参数共享的思想，我们可以设计各种循环神经网络。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/vbaHrTaPtk2L.png?imageslim">
</p>

> 10.3 计算循环网络(将 $\boldsymbol x$ 值的输入序列映射到输出值 $\boldsymbol o$ 的对应序列)训练损失的计算图。损失 $L$ 衡量每个 $\boldsymbol o$ 与相应的训练目标 $\boldsymbol y$ 的距离。当使用 softmax 输出时，我们假设 $\boldsymbol o$ 是未归一化的对数概率。损失 $L$ 内部计算 $\hat{\boldsymbol y} = \text{softmax}(\boldsymbol o)$，并将其与目标 $\boldsymbol y$ 比较。RNN输入到隐藏的连接由权重矩阵 $\boldsymbol U$ 参数化，隐藏到隐藏的循环连接由权重矩阵 $\boldsymbol W$ 参数化以及隐藏到输出的连接由权重矩阵 $\boldsymbol V$ 参数化。式 10.8 定义了该模型中的前向传播。(左)使用循环连接绘制的 RNN 和它的损失。(右)同一网络被视为展开的计算图，其中每个节点现在与一个特定的时间实例相关联。






循环神经网络中一些重要的设计模式包括以下几种：


- 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络，如图 10.3 所示。
- 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络，如图 10.4 所示。
- 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络，如图 10.5 所示。



图 10.3 是非常具有代表性的例子，我们将会在本章大部分涉及这个例子。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/dnSYuK8LS1LA.png?imageslim">
</p>

> 10.4 此类 RNN 的唯一循环是从输出到隐藏层的反馈连接。在每个时间步~$t$，输入为 $\boldsymbol x_t$，隐藏层激活为 $\boldsymbol h^{(t)}$，输出为 $\boldsymbol o^{(t)}$，目标为 $\boldsymbol y^{(t)}$，损失为 $L^{(t)}$。\emph{(左)}回路原理图。\emph{(右)}展开的计算图。这样的 RNN 没有\fig?表示的 RNN 那样强大（只能表示更小的函数集合）。\fig?中的 RNN 可以选择将其想要的关于过去的任何信息放入隐藏表示 $\boldsymbol h$ 中并且将 $\boldsymbol h$ 传播到未来。该图中的 RNN 被训练为将特定输出值放入 $\boldsymbol o$ 中，并且 $\boldsymbol o$ 是允许传播到未来的唯一信息。此处没有从 $\boldsymbol h$ 前向传播的直接连接。之前的 $\boldsymbol h$ 仅通过产生的预测间接地连接到当前。$\boldsymbol o$ 通常缺乏过去的重要信息，除非它非常高维且内容丰富。这使得该图中的 RNN 不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化，如\sec?所述。





<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/1G80vekC6syt.png?imageslim">
</p>

> 10.5 关于时间展开的循环神经网络，在序列结束时具有单个输出。这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标（如此处所示），或者通过更下游模块的反向传播来获得输出 $\boldsymbol o^{(t)}$ 上的梯度。


<span style="color:red;">10.3 和 10.4 的区别是什么？</span>

任何图灵可计算的函数都可以通过这样一个有限维的循环网络计算，在这个意义上图 10.3 和式 10.8 的循环神经网络是万能的。RNN 经过若干时间步后读取输出，这与由图灵机所用的时间步是渐近线性的，与输入长度也是渐近线性的。由图灵机计算的函数是离散的，所以这些结果都是函数的具体实现，而不是近似。RNN 作为图灵机使用时，需要一个二进制序列作为输入，其输出必须离散化以提供二进制输出。利用单个有限大小的特定 RNN 计算在此设置下的所有函数是可能的（ Siegelmann+Sontag-1995 用了 886 个单元）。图灵机的"输入"是要计算函数的详细说明(specification)，所以模拟此图灵机的相同网络足以应付所有问题。用于证明的理论 RNN 可以通过激活和权重（由无限精度的有理数表示）来模拟无限堆栈。


现在我们研究图 10.3 中 RNN 的前向传播公式。这个图没有指定隐藏单元的激活函数。我们假设使用双曲正切激活函数。此外，图中没有明确指定何种形式的输出和损失函数。我们假定输出是离散的，如用于预测词或字符的 RNN。表示离散变量的常规方式是把输出 $\boldsymbol o$ 作为每个离散变量可能值的非标准化对数概率。然后，我们可以应用 softmax 函数后续处理后，获得标准化后概率的输出向量 $\hat \boldsymbol y$。RNN 从特定的初始状态 $\boldsymbol h^{(0)}$ 开始前向传播。从 $t= 1$ 到 $t = \tau$ 的每个时间步，我们应用以下更新方程：


$$
a^{(t)}=b+W h^{(t-1)}+U x^{(t)}\tag{10.8}
$$

$$
\boldsymbol{h}^{(t)}=\tanh \left(\boldsymbol{a}^{(t)}\right)\tag{10.9}
$$

$$
\boldsymbol{o}^{(t)}=\boldsymbol{c}+\boldsymbol{V} \boldsymbol{h}^{(t)}\tag{10.10}
$$

$$
\hat{\boldsymbol{y}}^{(t)}=\operatorname{softmax}\left(\boldsymbol{o}^{(t)}\right)\tag{10.11}
$$


其中的参数的偏置向量 $\boldsymbol b$ 和 $\boldsymbol c$ 连同权重矩阵 $\boldsymbol U$、$\boldsymbol V$ 和 $\boldsymbol W$，分别对应于输入到隐藏、隐藏到输出和隐藏到隐藏的连接。这个循环网络将一个输入序列映射到相同长度的输出序列。与 $\boldsymbol x$ 序列配对的 $\boldsymbol y$ 的总损失就是所有时间步的损失之和。

例如，$L^{(t)}$ 为给定的 $\boldsymbol x^{(1)}, \dots, \boldsymbol x^{(t)}$ 后 $\boldsymbol y^{(t)}$ 的负对数似然，则

$$\begin{aligned}
& L\big( \{ \boldsymbol x^{(1)}, \dots, \boldsymbol x^{(\tau)} \}, \{ \boldsymbol y^{(1)}, \dots, \boldsymbol y^{(\tau)}  \} \big) \\
& = \sum_t L^{(t)} \\
& = - \sum_t \log p_{\text{model}} \big(  y^{(t)} \mid  \{ \boldsymbol x^{(1)}, \dots, \boldsymbol x^{(t)} \} \big) ,
\end{aligned}$$



其中 $p_{\text{model}} \big(  y^{(t)} \mid  \{ \boldsymbol x^{(1)}, \dots, \boldsymbol x^{(t)} \} \big) $ 需要读取模型输出向量 $\hat \boldsymbol y^{(t)}$ 中对应于 $y^{(t)}$ 的项。关于各个参数计算这个损失函数的梯度是计算成本很高的操作。梯度计算涉及执行一次前向传播（如在\fig?展开图中从左到右的传播），接着是由右到左的反向传播。运行时间是 $\mathcal O(\tau)$，并且不能通过并行化来降低，因为前向传播图是固有循序的；每个时间步只能一前一后地计算。前向传播中的各个状态必须保存，直到它们反向传播中被再次使用，因此内存代价也是 $\mathcal O(\tau)$。应用于展开图且代价为 $\mathcal O(\tau)$ 的反向传播算法称为通过时间反向传播。

因此隐藏单元之间存在循环的网络非常强大但训练代价也很大。我们是否有其他选择呢？



# 相关

- 《深度学习》花书
