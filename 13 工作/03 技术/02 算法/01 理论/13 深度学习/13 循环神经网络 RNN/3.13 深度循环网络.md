---
title: 3.13 深度循环网络
toc: true
date: 2019-06-05
---

# 深度循环网络

大多数 RNN 中的计算可以分解成三块参数及其相关的变换：


- 从输入到隐藏状态，
- 从前一隐藏状态到下一隐藏状态，以及
- 从隐藏状态到输出。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/vbaHrTaPtk2L.png?imageslim">
</p>

> 10.3 计算循环网络(将 $\boldsymbol x$ 值的输入序列映射到输出值 $\boldsymbol o$ 的对应序列)训练损失的计算图。损失 $L$ 衡量每个 $\boldsymbol o$ 与相应的训练目标 $\boldsymbol y$ 的距离。当使用 softmax 输出时，我们假设 $\boldsymbol o$ 是未归一化的对数概率。损失 $L$ 内部计算 $\hat{\boldsymbol y} = \text{softmax}(\boldsymbol o)$，并将其与目标 $\boldsymbol y$ 比较。RNN输入到隐藏的连接由权重矩阵 $\boldsymbol U$ 参数化，隐藏到隐藏的循环连接由权重矩阵 $\boldsymbol W$ 参数化以及隐藏到输出的连接由权重矩阵 $\boldsymbol V$ 参数化。式 10.8 定义了该模型中的前向传播。(左)使用循环连接绘制的 RNN 和它的损失。(右)同一网络被视为展开的计算图，其中每个节点现在与一个特定的时间实例相关联。

根据图 10.3 中的 RNN 架构，这三个块都与单个权重矩阵相关联。换句话说，当网络被展开时，每个块对应一个浅的变换。能通过深度 MLP 内单个层来表示的变换称为浅变换。通常，这是由学成的仿射变换和一个固定非线性表示组成的变换。

在这些操作中引入深度会有利的吗？实验证据强烈暗示理应如此。实验证据与我们需要足够的深度以执行所需映射的想法一致。读者可以参考 {Schmidhuber92,ElHihi+Bengio-nips8}或 {Jaeger2007}了解更早的关于深度 RNN 的研究。

Graves-et-al-ICASSP2013 第一个展示了将 RNN 的状态分为多层的显著好处，如图 10.13 ~\emph{(左)}。我们可以认为，在图 10.13 (a)所示层次结构中较低的层起到了将原始输入转化为对更高层的隐藏状态更合适表示的作用。Pascanu-et-al-ICLR2014 更进一步提出在上述三个块中各使用一个单独的 MLP（可能是深度的），如图 10.13 (b)所示。考虑表示容量，我们建议在这三个步中都分配足够的容量，但增加深度可能会因为优化困难而损害学习效果。在一般情况下，更容易优化较浅的架构，加入图 10.13 (b)的额外深度导致从时间步 $t$ 的变量到时间步 $t+1$ 的最短路径变得更长。例如，如果具有单个隐藏层的 MLP 被用于状态到状态的转换，那么与图 10.13 相比，我们就会加倍任何两个不同时间步变量之间最短路径的长度。然而 Pascanu-et-al-ICLR2014 认为，在隐藏到隐藏的路径中引入跳跃连接可以缓和这个问题，如图 10.13 (c)所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/LUS1Mv9OjHv0.png?imageslim">
</p>

> 10.13 循环神经网络可以通过许多方式变得更深。(a)隐藏循环状态可以被分解为具有层次的组。(b)可以向输入到隐藏，隐藏到隐藏以及隐藏到输出的部分引入更深的计算(如 MLP)。这可以延长链接不同时间步的最短路径。(c)可以引入跳跃连接来缓解路径延长的效应。



# 相关

- 《深度学习》花书
