---
title: 6.05 GRU
toc: true
date: 2019-09-03
---

### 6.13.4 Gated Recurrent Unit Recurrent Neural Networks

<span style="color:red;">重新确认下改进地方。</span>

GRUs 是一般的 RNNs 的变型版本，其主要是从以下两个方面进行改进。

1. 以语句为例，序列中不同单词处的数据对当前隐藏层状态的影响不同，越前面的影响越小，即每个之前状态对当前的影响进行了距离加权，距离越远，权值越小。<span style="color:red;">是的，不过，这种对于隐藏层的状态的影响 LSTM 不是也有吗？</span>
2. 在产生误差 error 时，其可能是由之前某一个或者几个单词共同造成，所以应当对对应的单词 weight 进行更新。GRUs 的结构如下图所示。GRUs 首先根据当前输入单词向量 word vector 以及前一个隐藏层状态 hidden state 计算出 update gate 和 reset gate。再根据 reset gate、当前 word vector 以及前一个 hidden state 计算新的记忆单元内容(new memory content)。当 reset gate 为 1 的时候， new memory content 忽略之前所有 memory content，最终的 memory 是由之前的 hidden state 与 new memory content 一起决定。<span style="color:red;">是呀，没问题呀，但是这个为什么称为改进呢？</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/TPR1e9U4K7DW.png?imageslim">
</p>
