


# 某些情况下，为了正确定义问题，添加正则化

在某些情况下，为了正确定义机器学习问题，正则化是必要的。


机器学习中许多线性模型，包括线性回归和 PCA，都依赖于对矩阵 $\boldsymbol X^\top\boldsymbol X$ 求逆。只要 $\boldsymbol X^\top\boldsymbol X$ 是奇异的，这些方法就会失效。当数据生成分布在一些方向上确实没有差异时，或因为例子较少（即相对输入特征的维数来说）而在一些方向上没有观察到方差时，这个矩阵就是奇异的。在这种情况下，正则化的许多形式对应求逆 $\boldsymbol X^\top\boldsymbol X + \alpha \boldsymbol I$。这个正则化矩阵可以保证是可逆的。

相关矩阵可逆时，这些线性问题有闭式解。没有闭式解的问题也可能是欠定的。一个例子是应用于线性可分问题的逻辑回归。如果权重向量 $\boldsymbol w$ 能够实现完美分类，那么 $2 \boldsymbol w$ 也会以更高似然实现完美分类。类似随机梯度下降的迭代优化算法将持续增加 $\boldsymbol w$ 的大小，理论上永远不会停止。在实践中，数值实现的梯度下降最终会达到导致数值溢出的超大权重，此时的行为将取决于程序员如何处理这些不是真正数字的值。

大多数形式的正则化能够保证应用于欠定问题的迭代方法收敛。例如，当似然的斜率等于权重衰减的系数时， 权重衰减将阻止梯度下降继续增加权重的大小。

使用正则化解决欠定问题的想法不局限于机器学习。同样的想法在几个基本线性代数问题中也非常有用。


我们可以使用 Moore-Penrose 求解欠定线性方程。回想 $\boldsymbol X$ 伪逆 $\boldsymbol X^+$ 的一个定义：

$$\begin{aligned}
 \boldsymbol X^+ = \lim_{\alpha \searrow 0} (\boldsymbol X^\top \boldsymbol X + \alpha \boldsymbol I)^{-1}\boldsymbol X^\top.
\end{aligned}\tag{7.29}
$$

现在我们可以将式子 7.29 看作进行具有权重衰减的线性回归。具体来说，当正则化系数趋向 0 时，式 7.29 是 $\boldsymbol{w}=\left(\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}\right)^{-1} \boldsymbol{X}^{\top} \boldsymbol{y}$ 的极限。因此，我们可以将伪逆解释为使用正则化来稳定欠定问题。



# 相关

- 《深度学习》花书
