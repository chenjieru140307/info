
# 大纲

批归一化单独拿出去了。

迁移学习的东西要放在这里吗？比如 fine-tuning


## 主要内容


- 正则化项
- 数据集增强 添加噪声 使用半监督学习 合并多个任务
- 批量归一化 BN
- early stopping
- 参数绑定和参数共享
- bagging dropout
- 对抗训练
- 切面距离，正切传播和流行正切分类器




模型的构建、训练、调试、评估

- 模型构建与评估
  - 模型评估
  - 惩罚函数
  - Dropout
- 模型训练
  - 深度学习网络训练
  - 深度学习网络调参
- 模型调试
  - 深度学习网络调试

- [深度学习调参有哪些技巧？](https://www.zhihu.com/question/25097993/answer/127667684)


## 主要内容



## 需要消化的


## 可以补充进来的

- 概念和理论性的东西还是放到机器学习理论里面。对于这些理论的在实际场景中的使用还是要在这里。







# 大纲

机器学习中的一个核心问题是设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法。在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。这些策略被统称为正则化。<span style="color:red;">嗯，这些策略统称为正则化。</span>我们将在后文看到，深度学习工作者可以使用许多不同形式的正则化策略。事实上，开发更有效的正则化策略已成为本领域的主要研究工作之一。

第五章介绍了泛化、欠拟合、过拟合、偏差、方差和正则化的基本概念。如果你不熟悉这些概念，请参考该章节再继续阅读本章。

在本章中，我们会更详细地介绍正则化，重点介绍深度模型（或组成深度模型的模块）的正则化策略。<span style="color:red;">嗯。</span>

本章中的某些章节涉及机器学习中的标准概念。如果你已经熟悉了这些概念，可以随意跳过相关章节。然而，本章的大多数内容是关于这些基本概念在特定神经网络中的扩展概念。

在第 5.2.2 节中，我们将正则化定义为"对学习算法的修改——旨在减少泛化误差而不是训练误差"。目前有许多正则化策略。有些策略向机器学习模型添加限制参数值的额外约束。有些策略向目标函数增加额外项来对参数值进行软约束。如果我们细心选择，这些额外的约束和惩罚可以改善模型在测试集上的表现。有时侯，这些约束和惩罚被设计为编码特定类型的先验知识；<span style="color:red;">什么是编码特定类型的先验知识？</span>其他时候，这些约束和惩罚被设计为偏好简单模型，以便提高泛化能力。有时，惩罚和约束对于确定欠定的问题是必要的。<span style="color:red;">什么是确定欠定的问题？</span>其他形式的正则化，如被称为集成的方法，则结合多个假说来解释训练数据。


在深度学习的背景下，大多数正则化策略都会对估计进行正则化。估计的正则化以偏差的增加换取方差的减少。一个有效的正则化是有利的"交易"，也就是能显著减少方差而不过度增加偏差。我们在第五章中讨论泛化和过拟合时，主要侧重模型族训练的 3 个情形：

- （1）不包括真实的数据生成过程 - 对应欠拟合和含有偏差的情况，
- （2）匹配真实数据生成过程，
- （3）除了包括真实的数据生成过程，还包括许多其他可能的生成过程 - 方差（而不是偏差）主导的过拟合。

正则化的目标是使模型从第三种情况转化为第二种情况。<span style="color:red;">没懂？</span>

在实践中，过于复杂的模型族不一定包括目标函数或真实数据生成过程，甚至也不包括近似过程。我们几乎从未知晓真实数据的生成过程，所以我们永远不知道被估计的模型族是否包括生成过程。然而，深度学习算法的大多数应用都是针对这样的情况，其中真实数据的生成过程几乎肯定在模型族之外。深度学习算法通常应用于极为复杂的领域，如图像、音频序列和文本，本质上这些领域的真实生成过程涉及模拟整个宇宙。<span style="color:red;">嗯。</span>从某种程度上说，我们总是持方枘（数据生成过程）而欲内圆凿（我们的模型族）。

这意味着控制模型的复杂度不是找到合适规模的模型（带有正确的参数个数）这样一个简单的事情。相反，我们可能会发现，或者说在实际的深度学习场景中我们几乎总是会发现，最好的拟合模型（从最小化泛化误差的意义上）是一个适当正则化的大型模型。<span style="color:red;">最好的拟合模型是一个适当正则化的大型模型。为什么呢？</span>

现在我们回顾几种策略，以创建这些正则化的大型深度模型。<span style="color:red;">嗯。</span>
