
# GraphSage 学习每个节点的嵌入

GraphSage 提供了解决上述问题的办法，以一种归纳的方式学习每个节点的嵌入。

具体地说，GraphSage 每个节点由其邻域的聚合 (aggregation) 表示。因此，即使图中出现了在训练过程中没有看到的新节点，它仍然可以用它的邻近节点来恰当地表示。

下面是 GraphSage算法：

![mark](http://images.iterate.site/blog/image/20190901/piXKlF7euevD.png?imageslim)

外层循环表示更新迭代的数量，而 $h ^ k_v$ 表示更新迭代 $k$ 时节点 $v$ 的潜在向量。在每次更新迭代时，$h ^ k_v$ 的更新基于一个聚合函数、前一次迭代中 $v$ 和 $v$ 的邻域的潜在向量，以及权重矩阵 $W ^ k$。

论文中提出了三种聚合函数：

1. Mean aggregator：

mean aggregator 取一个节点及其所有邻域的潜在向量的平均值。

$$
\mathbf{h}_{v}^{k} \leftarrow \sigma\left(\mathbf{W} \cdot \operatorname{MEAN}\left(\left\{\mathbf{h}_{v}^{k-1}\right\} \cup\left\{\mathbf{h}_{u}^{k-1}, \forall u \in \mathcal{N}(v)\right\}\right)\right.
$$

与原始方程相比，它删除了上面伪代码中第 5 行中的连接运算。这种运算可以看作是一种 “skip-connection”，在论文后面的部分中，证明了这在很大程度上可以提高模型的性能。

2. LSTM aggregator：


由于图中的节点没有任何顺序，因此它们通过对这些节点进行排列来随机分配顺序。

3. Pooling aggregator：


这个运算符在相邻集上执行一个 element-wise 的 pooling 函数。下面是一个 max-pooling 的示例：

$$
\operatorname{AGGREGATE}_{k}^{\mathrm{pool}}=\max \left(\left\{\sigma\left(\mathbf{W}_{\mathrm{pool}} \mathbf{h}_{u_{i}}^{k}+\mathbf{b}\right), \forall u_{i} \in \mathcal{N}(v)\right\}\right)
$$


论文使用 max-pooling 作为默认的聚合函数。

损失函数定义如下：

$$
J_{\mathcal{G}}\left(\mathbf{z}_{u}\right)=-\log \left(\sigma\left(\mathbf{z}_{u}^{\top} \mathbf{z}_{v}\right)\right)-Q \cdot \mathbb{E}_{v_{n} \sim P_{n}(v)} \log \left(\sigma\left(-\mathbf{z}_{u}^{\top} \mathbf{z}_{v_{n}}\right)\right)
$$


其中 $u$ 和 $v$ 在固定长度的 random walk 中共存，而 $v_n$ 是不与 $u$ 共存的负样本。这种损失函数鼓励距离较近的节点具有相似的嵌入，而距离较远的节点则在投影空间中被分离。通过这种方法，节点将获得越来越多的关于其邻域的信息。

GraphSage 通过聚合其附近的节点，可以为看不见的节点生成可表示的嵌入。它允许将节点嵌入应用到涉及动态图的域，其中图的结构是不断变化的。例如，Pinterest 采用了 GraphSage 的扩展版本 PinSage 作为其内容发现系统的核心。


# 相关

- [掌握图神经网络 GNN 基本，看这篇文章就够了](https://posts.careerengine.us/p/5c64eebe4337430d41ceae7a)
