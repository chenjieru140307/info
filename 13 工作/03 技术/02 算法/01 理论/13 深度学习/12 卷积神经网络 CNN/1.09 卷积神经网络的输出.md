---
title: 1.09 卷积神经网络的输出
toc: true
date: 2019-09-03
---

# 如何计算卷积神经网络输出值？

假设有一个 $5\times5$ 的图像，使用一个 $3\times3$ 的 filter 进行卷积，想得到一个 $3\times3$ 的 Feature Map，如下所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/85eGrIDIxhRb.png?imageslim">
</p>


$x_{i,j}$ 表示图像第 $i$ 行第 $j$ 列元素。$w_{m,n}$ 表示 filter​ 第 $m$ 行第 $n$ 列权重。 $w_b$ 表示 $filter$ 的偏置项。 表 $a_i,_j$ 示 feature map 第 $i$ 行第 $j$ 列元素。 $f$ 表示激活函数，这里以 $ReLU$ 函数为例。

卷积计算公式如下：

$$
a_{i,j} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b )
$$

当步长为 $1$ 时，计算 feature map 元素 $a_{0,0}​$ 如下：

$$
\begin{aligned}
a_{0,0} &= f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{0+m, 0+n} + w_b )\\
&= relu(w_{0,0} x_{0,0} + w_{0,1} x_{0,1} + w_{0,2} x_{0,2} + w_{1,0} x_{1,0} + w_{1,1} x_{1,1} + w_{1,2} x_{1,2} + w_{2,0} x_{2,0} + w_{2,1} x_{2,1} + w_{2,2} x_{2,2}) \\
&= 1 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 1 \\
&= 4
\end{aligned}
$$

其计算过程图示如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/6KKobKwqjbed.png?imageslim">
</p>


以此类推，计算出全部的 Feature Map。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/Ikz0GkyPV7Vd.png?imageslim">
</p>


当步幅为 2 时，Feature Map计算如下

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/834YzQ0tTAau.png?imageslim">
</p>


注：图像大小、步幅和卷积后的 Feature Map大小是有关系的。它们满足下面的关系：

$$
W_2 = (W_1 - F + 2P)/S + 1\\
H_2 = (H_1 - F + 2P)/S + 1
$$

其中：

- $W_2$， 是卷积后 Feature Map 的宽度；
- $W_1$ 是卷积前图像的宽度；$H_1$ 是卷积前图像的宽度。
- $F$ 是 filter 的宽度；
- $P$ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 $0$，如果 $P$ 的值是 $1$，那么就补 $1$ 圈 $0$；
- $S$ 是步幅；$H_2$ 卷积后 Feature Map 的高度；



举例：假设图像宽度 $W_1 = 5$，filter 宽度 $F=3$，Zero Padding $P=0$，步幅 $S=2$，$Z$ 则

$$
W_2 = (W_1 - F + 2P)/S + 1
= (5-3+0)/2 + 1
= 2
$$

说明 Feature Map 宽度是 2。同样，我们也可以计算出 Feature Map 高度也是 2。

如果卷积前的图像深度为 $D​$，那么相应的 filter 的深度也必须为 $D​$。深度大于 $1$ 的卷积计算公式：

$$
a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)
$$

其中，$D$ 是深度；$F$ 是 filter 的大小；$w_{d,m,n}$ 表示 filter 的第 $d$ 层第 $m$ 行第 $n$ 列权重；$a_{d,i,j}$ 表示 feature map 的第 $d$ 层第 $i$ 行第 $j$ 列像素；其它的符号含义前面相同，不再赘述。

每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。下面的图示显示了包含两个 filter 的卷积层的计算。$7*7*3$ 输入，经过两个 $3*3*3$ filter 的卷积(步幅为 $2$)，得到了 $3*3*2$ 的输出。图中的 Zero padding 是 $1$，也就是在输入元素的周围补了一圈 $0$。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/49jrJ6h5Od7R.png?imageslim">
</p>

<span style="color:red;">怎么算出来的 -5 ？</span>

以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 $3 * 3 * 3$ 的 fitler 的卷积层来说，其参数数量仅有 $(3 * 3 * 3+1) * 2 = 56$ 个，且参数数量与上一层神经元个数无关。<span style="color:red;">与上一层的神经元个数无关，为什么？这些神经元的输出到下一层是同一个 channel 吗？</span>与全连接神经网络相比，其参数数量大大减少了。
