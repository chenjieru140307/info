

# 多通道的卷积


当在神经网络的上下文中讨论卷积时，我们通常不是特指数学文献中使用的那种标准的离散卷积运算。实际应用中的函数略有不同。



首先，当我们提到神经网络中的卷积时，我们通常是指由多个并行卷积组成的运算。这是因为具有单个核的卷积只能提取一种类型的特征，尽管它作用在多个空间位置上。我们通常希望网络的每一层能够在多个位置提取多种类型的特征。

另外，输入通常也不仅仅是实值的网格，而是由一系列观测数据的向量构成的网格。例如，一幅彩色图像在每一个像素点都会有红绿蓝三种颜色的亮度。在多层的卷积网络中，第二层的输入是第一层的输出，通常在每个位置包含多个不同卷积的输出。当处理图像时，我们通常把卷积的输入输出都看作是 3 维的张量，其中一个索引用于标明不同的通道（例如红绿蓝），另外两个索引标明在每个通道上的空间坐标。软件实现通常使用批处理模式，所以实际上会使用 4 维的张量，第四维索引用于标明批处理中不同的实例，但我们为简明起见这里忽略批处理索引。

因为卷积网络通常使用多通道的卷积，所以即使使用了核翻转， 也不一定保证网络的线性运算是可交换的。只有当其中的每个运算的输出和输入具有相同的通道数时，这些多通道的运算才是可交换的。<span style="color:red;">什么意思？</span>

假定我们有一个 4 维的核张量 $\boldsymbol K$，它的每一个元素是 $K_{i,j,k,l}$，表示输出中处于通道 $i$ 的一个单元和输入中处于通道 $j$ 中的一个单元的连接强度，并且在输出单元和输入单元之间有 $k$ 行 $l$ 列的偏置。

- 假定我们的输入由观测数据 $\boldsymbol V$ 组成，它的每一个元素是 $V_{i,j,k}$，表示处在通道 $i$ 中第 $j$ 行第 $k$ 列的值。
- 假定我们的输出 $\boldsymbol Z$ 和输入 $\boldsymbol V$ 具有相同的形式。

如果输出 $\boldsymbol Z$ 是通过对 $\boldsymbol K$ 和 $\boldsymbol V$ 进行卷积而不涉及翻转 $\boldsymbol K$ 得到的，那么

$$
Z_{i,j,k} = \sum_{l,m,n} V_{l, j+m-1, k+n-1} K_{i,l,m,n},
$$

这里对所有的 $l$，$m$ 和 $n$ 进行求和是对所有（在求和式中）有效的张量索引的值进行求和。在线性代数中，向量的索引通常从 1 开始，这就是上述公式中 $-1$ 的由来。但是像 C 或 python 这类编程语言索引通常从 0 开始，这使得上述公式可以更加简洁。







# 相关

- 《深度学习》花书
