

# 池化

## 典型卷积神经网络层的组件


卷积网络中一个典型层包含三级（如下图所示）：

- 在第一级中，这一层并行地计算多个卷积产生一组线性激活响应。
- 在第二级中，每一个线性激活响应将会通过一个非线性的激活函数，例如整流线性激活函数。这一级有时也被称为探测级。
- 在第三级中，我们使用池化函数来进一步调整这一层的输出。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/pJfQv3jCqco7.png?imageslim">
</p>

> 9.7 一个典型卷积神经网络层的组件。有两组常用的术语用于描述这些层。(左)在这组术语中，卷积网络被视为少量相对复杂的层，每层具有许多"级"。在这组术语中，核张量与网络层之间存在一一对应关系。在本书中，我们通常使用这组术语。(右)在这组术语中，卷积网络被视为更多数量的简单层；每一个处理步骤都被认为是一个独立的层。这意味着不是每一"层"都有参数。


## 池化

池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。例如，最大池化函数给出相邻矩形区域内的最大值。其他常用的池化函数包括相邻矩形区域内的平均值、$L^2$ 范数以及基于据中心像素距离的加权平均函数。


不管采用什么样的池化函数，当输入作出**少量平移**时，池化能够帮助输入的表示近似不变。对于平移的不变性是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。下图用了一个例子来说明这是如何实现的。

**局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时**。例如，当判定一张图像中是否包含人脸时，我们并不需要知道眼睛的精确像素位置，我们只需要知道有一只眼睛在脸的左边，有一只在右边就行了。但在一些其他领域，保存特征的具体位置却很重要。例如当我们想要寻找一个由两条边相交而成的拐角时，我们就需要很好地保存边的位置来判定它们是否相交。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/fwTBg14hbYen.png?imageslim">
</p>

> 9.8 最大池化引入了不变性。(上)卷积层中间输出的视图。下面一行显示非线性的输出。上面一行显示最大池化的输出，每个池的宽度为三个像素并且池化区域的步幅为一个像素。(下)相同网络的视图，不过对输入右移了一个像素。下面一行的所有值都发生了改变，但上面一行只有一半的值发生了改变，这是因为最大池化单元只对周围的最大值比较敏感，而不是对精确的位置。

## 池化增加了一个先验：这一层学得的函数必须具有对少量平移的不变性

使用池化可以看作是增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性。当这个假设成立时，池化可以极大地提高网络的统计效率。

对空间区域进行池化产生了平移不变性，但当我们对分离参数的卷积的输出进行池化时，特征能够学得应该对于哪种变换具有不变性（如下图所示）。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/YJPujgOLzWhY.png?imageslim">
</p>

> 9.9 学习不变性的示例。使用分离的参数学得多个特征，再使用池化单元进行池化，可以学得对输入的某些变换的不变性。这里我们展示了用三个学得的过滤器和一个最大池化单元可以学得对旋转变换的不变性。这三个过滤器都旨在检测手写的数字 5。每个过滤器尝试匹配稍微不同方向的 5。当输入中出现 5 时，相应的过滤器会匹配它并且在探测单元中引起大的激活。然后，无论哪个探测单元被激活，最大池化单元都具有大的激活。我们在这里演示了网络如何处理两个不同的输入，这导致两个不同的探测单元被激活，然而对池化单元的影响大致相同。这个原则在 maxout 网络和其他卷积网络中更有影响。空间位置上的最大池化对于平移是天然不变的；这种多通道方法只在学习其他变换时是必要的。



## 池化综合了全部邻居的反馈

因为池化综合了全部邻居的反馈，这使得池化单元少于探测单元成为可能，我们可以通过综合池化区域的 $k$ 个像素的统计特征而不是单个像素来实现。

下图给出了一个例子。这种方法提高了网络的计算效率，因为下一层少了约 $k$ 倍的输入。当下一层的参数数目是关于那一层输入大小的函数时（例如当下一层是全连接的基于矩阵乘法的网络层时），这种对于输入规模的减小也可以提高统计效率并且减少对于参数的存储需求。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/2LiD4zhLVdyV.png?imageslim">
</p>

> 9.10 带有降采样的池化。这里我们使用最大池化，池的宽度为三并且池之间的步幅为二。这使得表示的大小减少了一半，减轻了下一层的计算和统计负担。注意到最右边的池化区域尺寸较小，但如果我们不想忽略一些探测单元的话就必须包含这个区域。

## 池化对于处理不同大小的输入具有重要作用

在很多任务中，池化对于处理不同大小的输入具有重要作用。例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定的大小，而这通常通过调整池化区域的偏置大小来实现，这样分类层总是能接收到相同数量的统计特征而不管最初的输入大小了。例如，最终的池化层可能会输出四组综合统计特征，每组对应着图像的一个象限，而与图像的大小无关。

## 应当使用哪种池化

<span style="color:red;">没怎么说好像</span>

一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导。将特征一起动态地池化也是可行的，例如，对于感兴趣特征的位置运行聚类算法。这种方法对于每幅图像产生一个不同的池化区域集合。另一种方法是先学习一个单独的池化结构，再应用到全部的图像中。

## 池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂

池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂，例如卷积玻尔兹曼机和自编码器。一些可微网络中需要的在池化单元上进行的类逆运算将在卷积生成网络中讨论。<span style="color:red;">补充下。</span>

# 相关

- 《深度学习》花书
