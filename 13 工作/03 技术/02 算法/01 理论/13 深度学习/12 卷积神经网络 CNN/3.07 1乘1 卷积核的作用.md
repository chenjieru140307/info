


# $1\times1$ 卷积作用？

NIN(Network in Network)$^{[4]}​$ 是第一篇探索 $1\times1​$ 卷积核的论文，这篇论文通过在卷积层中使用 MLP 替代传统线性的卷积核，使单层卷积层内具有非线性映射的能力，也因其网络结构中嵌套 MLP 子网络而得名 NIN。NIN 对不同通道的特征整合到 MLP 自网络中，让不同通道的特征能够交互整合，使通道之间的信息得以流通，<span style="color:red;">没明白，怎么就流通了？</span>其中的 MLP 子网络恰恰可以用 $1\times1​$ 的卷积进行代替。

GoogLeNet$^{[5]}​$ 则采用 $1\times1​$ 卷积核来减少模型的参数量。在原始版本的 Inception 模块中，由于每一层网络采用了更多的卷积核，大大增加了模型的参数量。此时在每一个较大卷积核的卷积层前引入 $1\times1​$ 卷积，可以通过分离通道与宽高卷积来减少模型参数量。<span style="color:red;">怎么通过分离通道和宽高卷积来减少模型参数量的？</span>

以图 5.2为例，在不考虑参数偏置项的情况下，若输入和输出的通道数为 $C_1=16​$，则：

- 左半边网络模块所需的参数为 $(1\times1+3\times3+5\times5+0)\times C_1\times C_1=8960​$；
- 假定右半边网络模块采用的 $1\times1​$ 卷积通道数为 $C_2=8​$ (满足 $C_1>C_2$)​，则右半部分的网络结构所需参数量为 $(1\times1\times (3C_1+C_2)+3\times3\times C_2 +5\times5\times C_2)\times C_1=5248​$。 <span style="color:red;">这个 式子为什么是这么计算的？嗯，理解了，但是为什么要满足 $C_1>C_2$？这个跟输出为 $C_1=16​$ 有差别呀？ </span>

可以在不改变模型表达能力的前提下大大减少所使用的参数量。<span style="color:red;">为什么这种变更是没有盖面模型表达能力的？</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/2b6uhdp5Kyak.png?imageslim">
</p>

> 图 5.2 Inception模块

综上所述，$1\times 1​$ 卷积的作用主要为以下两点：

- 实现信息的跨通道交互和整合。<span style="color:red;">？怎么就实现信息的跨通道交互和整合了？</span>
- 对卷积核通道数进行降维和升维，减小参数量。<span style="color:red;">只是对通道数进行升维和降维，感觉并没有减少很多参数量吧。</span>









# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)

