

### 稀疏交互（Sparse Interaction）

在传统神经网络中，网络层之间输入与输出的连接关系可以由一个权值参数矩阵来表示，其中每个单独的参数值都表示了前后层某两个神经元节点之间的交互。对于全连接网络，任意一对输入与输出神经元之间都产生交互，形成稠密的连接结构。<span style="color:red;">嗯，这么一说就知道稀疏交互是什么意思了。</span>

如图 9.15所示，神经元 $s_i$ 与输入的所有神经元 $x_j$ 均有连接。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190414/VJX71MPMNFM4.png?imageslim">
</p>


而在卷积神经网络中，卷积核尺度远小于输入的维度，这样每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重（即产生交互），我们称这种特性为稀疏交互。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190414/2nAxgtHMwUwA.png?imageslim">
</p>

如图 9.16所示。可以看到与稠密的连接结构不同，神经元 $s_i$ 仅与前一层中的 $x_{i−1}$、$x_i$ 和 $x_{i+1}$ 相连。具体来讲，假设网络中相邻两层分别具有 $m$ 个输入和 $n$ 个输出，全连接网络中的权值参数矩阵将包含 $m×n$ 个参数。对于稀疏交互的卷积网络，如果限定每个输出与前一层神经元的连接数为 $k$ ，那么该层的参数总量为 $k×n$。在实际应用中，一般 $k$ 值远小于 $m$ 就可以取得较为可观的效果；而此时优化过程的时间复杂度将会减小几个数量级，过拟合的情况也得到了较好的改善。<span style="color:red;">嗯，优化过程的时间复杂度降低，过拟合的情况也会有所改善。</span>

稀疏交互的物理意义是，通常图像、文本、语音等现实世界中的数据都具有局部的特征结构，我们可以先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。以人脸识别为例，最底层的神经元可以检测出各个角度的边缘特征（见图 9.17（a））；位于中间层的神经元可以将边缘组合起来得到眼睛、鼻子、嘴巴等复杂特征（见图 9.17（b））；最后，位于上层的神经元可以根据各个器官的组合检测出人脸的特征（见图 9.17（c））。<span style="color:red;">对于这个地方还有个问题，那边是最底层的神经元？那边是上层的神经元？</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190414/4rY92eCd7XK0.png?imageslim">
</p>



## 稀疏交互

传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。(注：译者注：这里可以粗略地理解为输入 $\times$ 参数矩阵=输出。)其中，参数矩阵中每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。

然而，卷积网络具有稀疏交互的特征。这是使核的大小远小于输入的大小来达到的。

举个例子，当处理一张图像时，输入的图像可能包含成千上万个像素点，但是我们可以通过只占用几十到上百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。


这意味着：

- 我们需要存储的参数更少，不仅减少了模型的存储需求，而且提高了它的统计效率。
- 这也意味着为了得到输出我们只需要更少的计算量。这些效率上的提高往往是很显著的。

如果有 $m$ 个输入和 $n$ 个输出，那么矩阵乘法需要 $m \times n$ 个参数并且相应算法的时间复杂度为 $O(m\times n)$（对于每一个例子）。如果我们限制每一个输出拥有的连接数为 $k$，那么稀疏的连接方法只需要 $k\times n$ 个参数以及 $O(k\times n)$ 的运行时间。在很多实际应用中，只需保持 $k$ 比 $m$ 小几个数量级，就能在机器学习的任务中取得好的表现。

稀疏连接的图形化解释如图 9.2 和 9.3 所示。在深度卷积网络中，处在网络深层的单元可能与绝大部分输入是\emph{间接}交互的，如图 9.4 所示。这允许网络可以通过只描述稀疏交互的基石来高效地描述多个变量的复杂交互。




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/pQoIDFi9lBz6.png?imageslim">
</p>

> 9.2 稀疏连接，对每幅图从下往上看。我们强调了一个输入单元 $x_3$ 以及在 $\boldsymbol s$ 中受该单元影响的输出单元。\emph{(上)}当 $\boldsymbol s$ 是由核宽度为 3 的卷积产生时，只有三个输出受到 $\boldsymbol x$ 的影响。\emph{(下)}当 $\boldsymbol s$ 是由矩阵乘法产生时，连接不再是稀疏的，所以所有的输出都会受到 $x_3$ 的影响。




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/jvNKD2kA6hPk.png?imageslim">
</p>

> 9.3 稀疏连接，对每幅图从上往下看。我们强调了一个输出单元 $s_3$ 以及 $\boldsymbol x$ 中影响该单元的输入单元。这些单元被称为 $s_3$ 的接受域。\emph{(上)}当 $\boldsymbol s$ 是由核宽度为 3 的卷积产生时，只有三个输入影响 $s_3$。\emph{(下)}当 $\boldsymbol s$ 是由矩阵乘法产生时，连接不再是稀疏的，所以所有的输入都会影响 $s_3$。<span style="color:red;">译者注：在生物中称之为"感受野"。</span>





<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/MsBNhmTginaj.png?imageslim">
</p>

> 9.4 处于卷积网络更深的层中的单元，它们的接受域要比处在浅层的单元的接受域更大。如果网络还包含类似步幅卷积（\fig?）或者池化（\sec?）之类的结构特征，这种效应会加强。这意味着在卷积网络中尽管\emph{直接}连接都是很稀疏的，但处在更深的层中的单元可以\emph{间接地}连接到全部或者大部分输入图像。





# 原文与相关

- 《百面机器学习》
- 《深度学习》花书
