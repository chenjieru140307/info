---
title: 02 权值的初始化
toc: true
date: 2019-08-31
---

##  权重 W 的初始化


利⽤用随机分布初始化 W，来消除对称性，因为对称的网络每个神经元将学习到相同的权重，如果不消除对称，那么每个神经元学到的东西是一样的。因为对单个层的每个神经元来说，输入是一样的。**是的**

那么随机初始化权重的方法具体为：

* 采用小的数值：0.001 * randn(D, H) ~ u(0, 1) 就是简单的用 0.001乘以一个随机数。

* 根据 Fan-in 的大小计算：w = randn(n) / sqrt(n) 就是使每一层的 Var 基本不变，计算如下：**没明白？什么是 Fan-in？**


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/BJE9AJ5A2I.png?imageslim">
</p>

w = randn(n) / sqrt(n)



# 相关

- 七月在线
