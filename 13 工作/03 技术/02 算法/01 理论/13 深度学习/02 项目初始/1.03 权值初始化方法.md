

## 13.5 权值初始化方法有哪些？

在深度学习的模型中，从零开始训练时，权重的初始化有时候会对模型训练产生较大的影响。良好的初始化能让模型快速、有效的收敛，而糟糕的初始化会使得模型无法训练。

目前，大部分深度学习框架都提供了各类初始化方式，其中一般常用的会有如下几种：
**1. 常数初始化(constant)**

把权值或者偏置初始化为一个常数。例如设置为 0，偏置初始化为 0 较为常见，权重很少会初始化为 0。TensorFlow中也有 zeros_initializer、ones_initializer等特殊常数初始化函数。

**2. 高斯初始化(gaussian)**

 给定一组均值和标准差，随机初始化的参数会满足给定均值和标准差的高斯分布。高斯初始化是很常用的初始化方式。特殊地，在 TensorFlow 中还有一种截断高斯分布初始化（truncated_normal_initializer），其主要为了将超过两个标准差的随机数重新随机，使得随机数更稳定。

**3. 均匀分布初始化(uniform)**

给定最大最小的上下限，参数会在该范围内以均匀分布方式进行初始化，常用上下限为（0，1）。

**4. xavier 初始化(uniform)**

在 batchnorm 还未出现之前，要训练较深的网络，防止梯度弥散，需要依赖非常好的初始化方式。xavier 就是一种比较优秀的初始化方式，也是目前最常用的初始化方式之一。其目的是为了使得模型各层的激活值和梯度在传播过程中的方差保持一致。本质上 xavier 还是属于均匀分布初始化，但与上述的均匀分布初始化有所不同，xavier 的上下限将在如下范围内进行均匀分布采样：
$$
[-\sqrt{\frac{6}{n+m}},\sqrt{\frac{6}{n+m}}]
$$
其中，n为所在层的输入维度，m为所在层的输出维度。

**6. kaiming初始化（msra 初始化）**

kaiming初始化，在 caffe 中也叫 msra 初始化。kaiming初始化和 xavier 一样都是为了防止梯度弥散而使用的初始化方式。kaiming初始化的出现是因为 xavier 存在一个不成立的假设。xavier在推导中假设激活函数都是线性的，而在深度学习中常用的 ReLu 等都是非线性的激活函数。而 kaiming 初始化本质上是高斯分布初始化，与上述高斯分布初始化有所不同，其是个满足均值为 0，方差为 2/n的高斯分布：
$$
[0,\sqrt{\frac{2}{n}}]
$$
其中，n为所在层的输入维度。

除上述常见的初始化方式以外，不同深度学习框架下也会有不同的初始化方式，读者可自行查阅官方文档。
