

![mark](http://images.iterate.site/blog/image/20190818/mvogtzkgSd6I.png?imageslim)



迁移学习指的就是，假设你手上有一些跟你现在要进行的 task 没有直接相关的 data，那你能不能用这些没有直接相关的 data 来帮助我们做一些什么事情。比如说：你现在做的是猫跟狗的 classifer，那所谓没有什么直接相关的 data 是什么意思呢？没有什么直接相关其实是有很多不同的可能。比如说 input distribution 是类似的(一样时动物的图片)，但是你的 label 是无关的(domain是类似的，task是不像的)。还有另外的一个可能是：input domain是不同的，但是 task 是相同的(猫跟狗的分类，一个招财猫图片，一个是高非狗的图片)

![mark](http://images.iterate.site/blog/image/20190818/VvSzFw6ump3P.png?imageslim)
迁移学习问的问题是：我们能不能再有一些不想关 data 的情况下，然后帮助我们现在要做的 task。





为什么我要考虑迁移学习这样的 task 呢？举例来说：在 speech recognition里面(台语的语音辨识)，台语的 data 是很少的(但是语音的 data 是很好收集的，中文，英文等)。那我们能不能用其他语音的 data 来做台语这件事情。或许在 image recongnition里面有一些是 medical images，这种 medical image其实是很少的，但是 image data是很不缺的。

![mark](http://images.iterate.site/blog/image/20190818/0eMRSLAD60fT.png?imageslim)
用不相干的 data 来做 domain 其他的 data，来帮助现在的 task，是有可能的。




我们在现实生活中，我们在不断的做迁移学习。比如说：你可能是是一名研究生，你可能想知道研究生咋样过日子，你就可以参考爆漫王里面的例子。在保漫王里面漫画家就是研究生，责编就等同于指导教授。漫画家每周画分镜去给责编看，跟责编讨论(就跟指导教授每周去汇报报告一样)，画分镜就是跑实验。目标就是投稿就等于投稿期刊。

![mark](http://images.iterate.site/blog/image/20190818/BiGR3i06X1Jj.png?imageslim)
虽然我们没有一个研究生的守则，但是从爆漫王里面我们可以知道说：身为一个研究生应该要做什么样的事情。那你可能会说：拿漫画跟学术做类比，有点不伦不类的(漫画也是拿生命来做的)。



迁移学习有很多的方法，它是很多方法的集合。下面你有可能会看到我说的 terminology 可能跟其他的有点不一样，不同的文献用的词汇其实是不一样的，有些人说算是迁移学习，有些人说不算是迁移学习，所以这个地方比较混乱，你只需要知道那个方法是什么就好了。

![mark](http://images.iterate.site/blog/image/20190818/e8wPh4L3rolV.png?imageslim)
我们现在有一个我们想要做的 task，有一些跟这个 task 有关的数据叫做 target data，有一些跟这个 task 无关的 data，这个 data 叫做 source data。这个 target data有可能是有 label 的，也有可能是没有 label 的，这个 source data有可能是有 label 的，也有可能是没有 label 的，所以现在我们就有四种可能，所以之后我们会分这四种来讨论。

## 模型微调

那现在我们假设 target data跟 source data都同时有 label 的情况下，可以的做的事情是：最常见的事情就是：fine-tuning你的 model



在现在的 task 里面，target data($x^t,y^t$)和 source data($x^s,y^s$)都是有 label 的，但是我们通常是假设说：现在 target data的数据量是非常少的(如果 target data量是很多的话，你就当做一般的 machine learning 来 train 你的 model 就好了，你也不需要做什么迁移学习)，source data是很多的。虽然 source data跟我们现在考虑的 task 没有关系，但我们想知道说：在 target data很少的情况下，有一大推不相关的 source data到底有么有可能会有帮助。

![mark](http://images.iterate.site/blog/image/20190818/ckJND3cb9xhn.png?imageslim)

如果你今天的 target data的量很少，少到只有几个 example 而已，这个就叫做 one-shot learning。这样的 task 的例子是：在语音上最典型的例子就是 speaker adaption，target data是某一个人的声音，但是这个人的声音你不太有可能有太多的 label data(可能对你的 machine 说了三句话)，但是 source data有一大堆的 audio data，它是来自于不同人的。你当然不可能直接去用 target data去 train 一个语音辨识系统，这样一定会坏掉的。所以你会希望说：这个有好几 w 小时的 source data在这个 task 里面有什么帮助。


处理方式是非常直觉的，那你的 source data直接去 train 一个 model，然后接下来 fine tune这个 model 通过 target data。可能会遇到的 challenge：source data非常的少，所以你在 target data train出一个好的 model，然后在 source data上做 train，可能就坏掉了。



## 保守训练

有一个技巧叫做：conservative training，你现在有大量的 source data，(比如说：在语音辨识里面就是很多不同 speaker 的声音)，那你拿来做 neural network。target data是某个 speaker 的声音，如果你直接拿这些去 train 的话就坏掉了。你可以在 training 的时候加一些 constraint(regularization)，让新的 model 跟旧的 model 不要差太多。你会希望新的 model 的 output 跟旧的 model 的 output 在看同一笔 data 的时候越接近越好。或者说新的 model 跟旧的 model L2-Norm差距越小越好(防止 overfitting 的情形)

![mark](http://images.iterate.site/blog/image/20190818/zdLw1fi7xAIs.png?imageslim)
## 层迁移

另外的一个方法是 layer transfer，你现在用 source data train好了一个 model，把这个 model 的某几个 layer 拿出来 copy 到新的 model 里面
。接下来用 source data只去用没有 copy 的 layer(可能你只保留一个 layer 没有 copy)，这样的好处就是 source data只需要考虑非常少的参数，这样就可以避免 overfitting 的情形。当然之后你的 source data够多了，那之后可能还是要 fine-tune整个 model。

![mark](http://images.iterate.site/blog/image/20190818/drebEjwfvWmk.png?imageslim)



哪些 layer 应该被 transfer，哪些 layer 不应该去 transfer 呢？有趣的是在不同的 task 上面需要被 transfer 的 layer 往往是不一样的。比如说在语音辨识上面，我们通常是 copy the last few layers(最后几层)。同样的发音方式，因为口腔结果略有差异，得到的声音是不一样的。neural network前几层做的事情是从这个声音讯号里面得知现在说话人的发音方式，根据发音方式就可以得到说的词汇。所以从这个角度来看，从发音方式到辨识结果，也就是 neural network后面几层是跟语者是每一关系的，所以它是可以被 copy 的。不一样的是从声音讯号到发音方式这一段可能每个人都是不一样的。

![mark](http://images.iterate.site/blog/image/20190818/sfeawQRX2FIm.png?imageslim)
所以在做语音辨识的时候，常见的做法是把 neural network的后几层是 copy。但是在 image 的时候发现是不一样的，在 image 的时候是 copy 前面几层，只 train 最后几层。

在 image 的时候你会发现数说，当你 source domain上 learn 了一 network，你 learn 到 CNN 通常前几层做的就是 deceide 最简单的事情(比如前几层做的就是 decide 有么有直线，有么有简单的几何图形)。所以在 image 上面前几层 learn 的东西，它是可以被 transfer 到其他的 task 上面。而最后几层 learn 的东西往往是没有办法 transfer 到其他的东西上面去。所以在做影像处理的时候反而是会 copy 前面几层。




这是一个 image 在 layer transfer上的实验，
120多 wimage 分成 source 跟 target，分法是按照 class 来分的(500 class归为 source data，500classes归为 target data)。横轴的意思是：我们在做迁移学习的时候 copy 了几个 layer(copy 0个 layer，就是说完全没有做迁移学习)，纵轴时候 top-1 accuracy，越高越好。

![mark](http://images.iterate.site/blog/image/20190818/brbpO87byQce.png?imageslim)




假设 source 跟 target 是没关系的，把这个 Imagenet 分为 source data跟 target data的时候，把自然界的东西通通当成 source，target都是人造的东西，这样的迁移学习会有什么样的影响。如果 source data跟 target data是差很多的，那在做迁移学习的时候，你的性能会掉的非常多(如果只是 copy 前面几个 layer 的话，性能仍然跟没有跟 copy 是持平的)。这意味着说：即使 source domain跟 target domain是非常不一样的，在 neural network的第一个 layer，他们仍然做的事情仍然可能是一样的。绿色的这条线：假设我前面几个 layer 的参数 random 会坏掉了。



接下来是多任务学习，多任务学习跟 fine tuning不同是：在 fine tuning里面我们 care target domain做的好不好，那在多任务学习里面我们同时 care target domain跟 source domain做的好不好。

![mark](http://images.iterate.site/blog/image/20190818/JjkBeuEy2WkI.png?imageslim)


其实我们今天用 deep learning base方法的话，它特别适合拿来做这种多任务学习，因为你可以说：假设有两个不同的 task 用的同样的 feature(都做影像辨识)，我 learn 一个 neural network，中间会分叉出来一部分 network 去处理 taskA，一部分 network 去处理 taskB。这么做的好处是：你的 taskA 跟 taskB 他们在前面几个 layer 会是共用的(有比较多的 data，会有比较好的性能)。这样做的前提是：这两个 task 有没有共通性，是不是可以共用前面几个 layer。

![mark](http://images.iterate.site/blog/image/20190818/efkHtFzEjKGz.png?imageslim)
还有一种是 input 没有办法确定，两个不同 task 的 input 都用不同的 neural network把它 transfer 到同一个 domain 上去，在同一个 domain 上你在 apply 不同的 neural network，一条路去做 taskA，一条路去做 taskB。如果在这样的 task 下你也迁移学习，就算 tasKA 跟 taskB 的 input 完全不一样，如果你觉得中间几个 layer 有共同的地方，你还是可以用这样的 model 架构来处理。





## 多任务学习

多任务学习一个很成功的例子就是多语言的语音辨识，假设你现在手上有一大堆不同语言的 data(法文，中文，英文等)，那你在 train 你的 model 的时候，同时可以辨识这五种不同的语言。这个 model 前面几个 layer 他们会共用参数，后面几个 layer 每一个语言可能会有自己的参数，这样做是合理的。虽然是不同的语言，但是都是人类所说的，所以前面几个 layer 它们可能是 share 同样的咨询，共用同样的参数。

![mark](http://images.iterate.site/blog/image/20190818/qfFikRo2qDLV.png?imageslim)
在 translation 你也可以拥同样的事情，假设你今天要做中翻英，也要做中翻日，你也把这两个 model 一起 train。在一起 train 的时候无论是中翻英还是中翻日，你都要把中文的 data 先做 process，那一部分 neural network就可以是两种不同语言的 data。



在过去收集了十几种语言，把它们两两之间互相做 transfer，做了一个很大 N*N的 tabel，每一个 task 都有进步。所以目前发现大部分 task，不同人类的语言就算你觉得它们不是非常像，但是它们之间都是可以 transfer。

这边举得例子是从欧洲语言去 transfer 中文，横轴是中文的 data，纵轴是 character error rate。假设你一开始用中文 train 一个 model，data很少，error rate很大，随着 data 越来越多，error rate就可以压到 30 以下。但是今天如果你有一大堆的欧洲语言，你把这些欧洲语言跟中文一起去做 multitask train，用这个欧洲语言的 data 来帮助中文 model 前面几层让它 train 更好。你会发现说：在中文 data 很少的情况下，你有做迁移学习，你就可以得到比较好的性能。随着中文 data 越多的时候，中文本身性能越好，就算是中文 100 小时借用一些从欧洲语言对这个变化也是有微幅帮助的。所以这边的好处是说：假设你做多任务学习的时候，你会发现你有 100 多个小时跟有 50 小时以内，如果你有做迁移学习的话，你只需要 1/2以下的 data 就可以跟有两倍的 data 做的一样好


常常有人会担心说：迁移学习会不会有负面的效应，这是会有可能，如果两个 task 不像的话，你的 transfer 就是 negative 的。但是有人说：总是思考两个 task 到底之间能不能 transfer，这样很浪费时间。所以就会有 progressive neural networks。

![mark](http://images.iterate.site/blog/image/20190818/4Hr5anDJw56w.png?imageslim)
###  渐进神经网络

progressive network neural其实是很新的做法(2016年的 paper)。我先 train 一个 task1，train好以后它的参数就 fix 住，那现在我们要做 task2，但是 task2 它的每一个 hidden layer都会去接前一个 task1 的某一个 hidden layer的 output。所以在 train 的时候好处就是：task1跟 task2 非常不像，首先 task1 的 data 不会去动到 task2 的 model，所以 task1 一定不会比原来更差。task2去借用 task1 的参数，但是它可以把这些参数直接设为 0，这样也不会影响 task2 的性能。task3也是做一样的事情，task3会同时从 task1 和 task2 的 hidden layer得到 information。




接下来是：假设 target data是 unlaebl，而 source data是 label 的时候我们可以做什么样的事情

![mark](http://images.iterate.site/blog/image/20190818/edKrmnLUUNPE.png?imageslim)




在 source data里面有 function 的 input 跟 output，在 target 里我们只有 function 的 input，没有 function 的 output。举例来说：我们可以说：source data是 MNIST image，target data是 MNIST-M image(MNIST image加上一些奇怪的背景)。MNIST是有 label 的，MNIST-M是没有 label 的，在这种情况下我们通常是把 source data就视作 training data，target data视作 testing data。产生的问题是：training data跟 testing data是非常 mismatch

![mark](http://images.iterate.site/blog/image/20190818/SInk7173vUDz.png?imageslim)




如果你今天直接 learn 一个 model，input是一张 image，一直 learn 下去结果可能就会坏掉。如果我们把一个 neural network当做 feature extract，neural network前面几层我们可以看做是抽 feature，后面几层可以看做 classification。我们把这个 feature 拿来看的话，我们会发现说：不同 domain data它的 feature 完全就不一样，如果把 MNIST 丢进去的话，它是蓝色的这些点(0-9，总共 10 群)，但是如果你今天是把另外一群 image 丢进去的话，你会发现抽出来的 feature 是红色的这一群。所以你会发现说：今天做 feature extract的时候，原来 source image跟 target image不在同一个位置里面，所以后面的 classifier 只能把蓝色的做好，红色部分就无能为力。

![mark](http://images.iterate.site/blog/image/20190818/b1aNmcbSGFsf.png?imageslim)



## 神经网络的领域对抗训练

所以该肿么办呢？这边希望做的事情是：前面的 feature extract 它可以把 domain 的特性去除掉，这一招较做 Domain-adversarial training。也就是 feature extract output不应该是红色跟蓝色的点分成两群，而是不同的 domain 应该混在一起(不同 domain 的特性取消掉)。

那咋样 learn 这样的 feature extract呢？，这边的做法是在后面接一下 domain classifier。把 feature extract output丢给 domain clssifier，domain classifier它也是一个 classification task，它要做的事情就是：根据 feature extract给它的 feature，判断这个 feature 来自于哪个 domain，在这个 task 里面，要分辨这些 feature 是来自 MNIST 还是来自与 MNIST-M。


有一个 generator 的 output，然后又有 discriminator，让它的架构非常像 GAN。但是跟 GAN 不一样的事情是：之前在 GAN 那个 task 里面，你的 generator 要做的事情是产生一个 image，然后骗过 discriminator，这件事很难。但是在这个 Domain-adversarial training里面，要骗过 domain classifier太简单了。肿么做呢？有一个 solution 是：不管看到什么东西，output都是 0，这样就骗过了 classifier。

![mark](http://images.iterate.site/blog/image/20190818/3frVsxh0um64.png?imageslim)

所以只是 training 这个 domain classifier是不够的，因为 feature extract可以很轻易骗过 domain classifier






所以你要在 feature extract增加它任务的难度，所以 feature extract它 output feature不仅要骗过 domain classifier还要同时让 laebl predictor做好。这个 label predictor它就吃 feature extract output，然后它的 output 就是 10 个 class。

![mark](http://images.iterate.site/blog/image/20190818/kswrli3eFlH0.png?imageslim)

所以今天你的 feature extract 不只要骗过 domain classifier，还要满足 label predictior的需求。抽出的 feature 不仅要把 domain 的特性消掉，同时还要保留原来 feature 的特性。那我们把这三个 neural 放在一起的话。实际上就是一个大型的 neural network，是一个各怀 鬼胎的 neural network(一般的 neural network整个参数想要做的事情都是一样的，要 minimize loss)，在这个 neural network里面参数是各怀鬼胎的。蓝色 label predictor做的事情是把 class 分类做的正确率越高越好，domain classifier做的事情是想正确 predict image是属于哪个 domain。feature extractor想要做的事情是：要同时 improve label predictor，同时想要 minimize domain classifier accuracy。





feature extractor 咋样陷害队友呢(domain classifier)？这件事情是很容易的，你只要加一个 gradient reversal layer就行了。也就是你在做 backpropagation 的时候(feedford和 backford)，在做 backford task的时候你的 domain classifier传给 feature extractor什么样的 value，feature extractor就把它乘上一个负号。也就是 domain classifier 告诉你说某个 value 要上升，它就故意下降。


domain classifier因为看不到真正的 image，所以它最后一定 fail 掉。因为它所能看到的东西都是 feature extractor告诉它的，所以它最后一定会无法分辨 feature extractor所抽出来的 feature 是来自哪个 domain

![mark](http://images.iterate.site/blog/image/20190818/gObMguIj21Pz.png?imageslim)
今天的问题就是 domain classifier一定要奋力的挣扎，因为它要努力去判断现在的 feature 是来自哪个 domain。

这些都是很新的 paper，值得看。




这是 paper 中的一些实验结果，做了不同的 domain transfer。

![mark](http://images.iterate.site/blog/image/20190818/h0ivxAXWJVav.png?imageslim)
我们来看一些实验结果的话，纵轴代表用不同的方法，在这四个结果里面，你会发现说：如果只用 source only的话，性能是比较差的。




接下来是 zero-shot-learning

![mark](http://images.iterate.site/blog/image/20190818/DozKY5aJhMbi.png?imageslim)

# 零样本学习

在 zero-shot-learning里面呢？跟刚才讲的 task 是一样的，source data有 label，target data每天 label。在刚才 task 里面可以把 source data当做 training data，把 target data当做 testing data，但是实际上在 zero-shot learning里面，它的 difine 又更加严格一点。它的 difine 是：今天在 source data和 target data里面，它的 task 是不一样的。



比如说在影像上面(你可能要分辨猫跟狗)，你的 source data可能有猫的 class，也有狗的 class。但是你的 target data里面 image 是草泥马的样子，在 source data里面是从来没有出现过草泥马的，如果 machine 看到草泥马，就未免有点强人所难了吧。但是这个 task 在语音上很早就有 solution 了，其实语音是常常会遇到 zero-shot learning的问题。

![mark](http://images.iterate.site/blog/image/20190818/XVRnF8UThlGl.png?imageslim)
假如我们把不同的 word 都当做一个 class 的话，那本来在 training 的时候跟 testing 的时候就有可能看到不同的词汇。你的 testing data本来就有一些词汇是在 training 的时候是没有看过的。

那在语音上我们咋样来解决这个问题呢？不要直接去辨识一段声音是属于哪一个 word，我们辨识的是一段声音是属于哪一个音标。然后我们在做一个音标跟 table 对应关系的表，这个东西也就是词典。在辨识的时候只要辨识出音标就好，再去查表说：这个音标对应到哪一个 word。这样就算有一些 word 是没有在 training data里面的，它只要在你的词典里面出现过，你的 model 可以正确辨识出声音是属于哪一个音标的话，你就可以处理这个问题。




在影像上我们可以把每一个 class 用它的 attribute 来表示，也就是说：你有一个 database，这个 database 里面会有所以不同可能的 class 跟它的特性。假设你要辨识的是动物，但是你 training data跟 testing data他们的动物是不一样的。但是你有一个 database，这个 database 告诉你说：每一种动物它是有什么样的特性。比如狗就是毛茸茸，四只脚，有尾巴；鱼是有尾巴但不是毛茸茸，没有脚。

这个 attribute 要更丰富，每一个 class 都要有不一样的 attribute(如果两个 class 有相同的 attribute 的话，方法会 fail)。那在 training 的时候，我们不直接辨识说：每一张 image 是属于哪一个 class，而是去辨识说：每一张 image 里面它具备什么样的 attribute。所以你的 neural network target就是说：看到猩猩的图，就要说：这是一个毛茸茸的动物，没有四只脚，没有尾巴。看到狗的图就要说：这是毛茸茸的动物，有四只脚，有尾巴。

![mark](http://images.iterate.site/blog/image/20190818/OWbXJj4UYtQL.png?imageslim)






那在 testing 的时候，就算今天来了你从来没有见过的 image，也是没有关系的。你今天 neural network target也不是说：input image它是哪一种动物，而是 input 这一张 image 它是具有什么样的 attribute。所以 input 你从来没有见过的动物，你只要把它的 attribute 长出来，然后你就查表看说：在 database 里面哪一种动物它的 attribute 跟你现在 model output最接近。有时可能没有一摸一样的也是没有关系的，看谁最接近，那个动物就是你要找的。

![mark](http://images.iterate.site/blog/image/20190818/iko9HtGHNg8E.png?imageslim)



那有时候你的 attribute 可能非常的复杂(attribute dimension非常大)，你可以做 attribute embedding。也就是说现在有一个 embedding space，把 training data每一个 image 都通过一个 transform，变成一个 embedding space上的一个点。然后把所有的 attribute 也都变成 embedding space上的一个点，这个 $g(*)$ 跟 $f(*)$ 都可能是 neural network，那 training 的时候希望 f 跟 g 越接近越好。那在 testing 的时候如果有一张没有看过的 image，你就可以说这张 image attribute embedding以后跟哪个 attribute 最像，那你就可以知道它是什么样的 image。
![mark](http://images.iterate.site/blog/image/20190818/IFRczGRLKf5P.png?imageslim)




image跟 attribute 都可以描述为 vector，要做的事情就是把 attribute 跟 image 都投影到同一个空间里面。也就是说：你可以想象成是对 image 的 vector，也就是图中的 x，跟 attribute 的 vector，也就是图中的 y 都做降维，然后都降到同一个 dimension。所以你把 x 通过一个 function f都变成 embedding space上的 vector，把 y 通过另外一个 function g也都变成 embedding space上的 vector。

但是咋样找这个 f 跟 g 呢？你可以说 f 跟 g 就是 neural network。input一张 image 它变成一个 vector，或者 input attribute 变成一个 vector。training target你希望说：假设我们已经知道 $y^1$ 是 $x^1$ 的 attribute，$y^2$ 是 $x^2$ 的 attribute，那你就希望说找到一个 f 跟 g，它可以让 $x^1$ 跟 $y^1$ 投影到 embedding space以后越接近越好，$x^2$ 跟 $y^2$ 投影到 embedding space以后越接近越好。

那现在把 f 跟 g 找出来了，那现在假如有一张你从来没见过的 image$x^3$ 在你的 testing data里面，它也可以透过这个 f 变成 embedding space上面的一个 vector，接下来你就可以说这个 embedding vector它跟 $y^3$ 最接近，那 $y^3$ 就是它的 attribute


又是你会遇到一个问题，如果我没有 database 呢？我根本不知道每一个动物的 attribute 是什么，肿么办呢？那你可以借用 word vector。我们知道 word vector的每一个 dimension 就代表了现在 word 某种 attribute。所以你不一定需要一个 datbase 去告诉你说：每一个动物的 attribute 是什么。假设你有一组 word vector，这组 word vector里面你知道每一个动物对应的 word vector，那你可以把你的 attribute 直接换成 word vector，再做跟刚才一样的 embedding 就结束了。
![mark](http://images.iterate.site/blog/image/20190818/uOYVHOXPJNcq.png?imageslim)



假设我们的 train 的 query 是要让 $x^n$ 通过 f、跟 $y^n$ 通过 g 之后的距离越接近越好。这样子的话是有问题的，这样你的 model 只会 learn 到说：它把所有不同的 x 跟所有不同的 y 都投影同一个点，这样子距离最好。所以你的 loss function这样定其实是不行的，所以你要稍微重新设计一下你的 loss function。前面这个 loss function只有考虑到 $x^n$ 跟 $y^n$ 越接近越好，但没有考虑
$x^n$ 跟另一个 $y^n$，它的距离应该被拉大。


max里面两个的 element 分别是 0，k-f($x^n$)跟 g($y^n$)的 inner product，加上一个 max(m不等于 n)里面的 f($x^n$)跟 g($y^m$)的 inner product。这个 k 是自己 difine 的 margin(一个 constant，在 train 的时候自己 difine)


这个 max 的两个 element 一个是 0，一个是 max$f(x^n)*g(y^m)$。它会从 0 跟这个式子中选一个最大的，所以这一项的最小值就是 0。什么时候会等于 0 呢？当你另外一项小于 0 的时候，这个 loss 就会是 0。所以今天 $k-f(x^n)*g(y^n)$ 的 inner product 加上 $max_{m\neq n}f(x^n)*g(y^m)$ 的 inner product小于 0 的时候，这一项会是 zero loss，整理一下得到下面的这个式子 $f(x^n)g(y^n)-max_{m\neq n}f(x^n)*g(y^m)$ 的 inner product小于 k 的时候是 zero loss。这一项也和解释为：当 $f(x^n)$ 跟 $g(y^n)$ 的 inner product大于另外一项(y不是 $y^n$ 里面找一个 m，这个 $y^m$ 跟 $x^n$ 是最接近的)


如果 $x^n$ 跟 $y^n$ 之间的 inner product大过所有其它的 $y^m$ 跟 $x^n$ 之间的 inner product，而且要大过一个 margin k。

![mark](http://images.iterate.site/blog/image/20190818/TrIbgaDGmUjE.png?imageslim)


还有另外一个简单的 Zero-Shot learning的方法叫做 convex combination of semantic embedding。这个方法是说：我们也不要做什么 learning，假设我们现在有一个语音辨识系统，有一个 word vector，这两个是从网络上下载下来的，就可以做这件事情。

我把一张图丢到 neural network里面去，它的 output 没有办法决定是哪一个 class，但它觉得有 0.5的几率是 lion，有 0.5的几率是 tiger。接下来你在去找 lion 跟 tiger 的 word vector，然后把 lion 跟 tiger 的 word vector得到新的 vector(用 1:1的比例混合,0.5V(tiger)+0.5V(lion))，那你再看哪一个 word 的 vector 跟这个混合之后的结果最接近。假设是 liger 最接近，那这个东西就是 liger(狮虎)

![mark](http://images.iterate.site/blog/image/20190818/z23hQhcfaT2X.png?imageslim)



以下是这个的实验结果，也是蛮惊人的。我们来比一下人类跟机器的差别，第一张图，CNN判别说是 sea lion(海狮)，DeViSE没有得到好的结果，ConSE判别为各种 sea lion。
![mark](http://images.iterate.site/blog/image/20190818/lgz47kPEhwNt.png?imageslim)




在 training 的时候，machine看过如何把英文翻译成韩文，知道咋样把韩文翻译为英文，知道咋样把英文翻译为日文，知道咋样把日文翻译为英文。但是它从来没有看过日文翻译韩文的 data，但是可以翻，但是它从来没有看过韩文翻译日文的 data，但是可以翻。

![mark](http://images.iterate.site/blog/image/20190818/3uR0BFV3usWJ.png?imageslim)
为什么 zero-shot在这个 task 上是可行的呢？如果你今天用同一个 model 做了不同语言之间的 translation 以后，machine可以学到的事情是：对不同语言的 input 句子都可以 project 到同一个 space 上面





我们现在根据我们 learn 好得 translation，那个 translation 有一个 encoder，它会把你 input 的句子变成 vector，decoder根据这个 vector 解回一个句子，就是翻译的结果。那今天我们把不同语言都丢到这个 encoder 里面让它变成 vector 的话，那这些不同语言的不同句子在这个 space 上面有什么不一样的关系呢？

它发现说今天有日文、英文、韩文这三个句子，这三个句子讲的是同一件事情，通过 encoder embedding以后再 space 上面其实是差不多的位置。在左边这个图上面不同的颜色代表说：不同语言的用一个意思。所以你这样说：machine发明了一个新语言也是可以接受的，如果你把这个 embedding space当做一个新的语言的话。machine做的是：发现可一个 sequence language，每一种不同的语言都先要先转成它知道的 sequence language，在用这个 sequence language转为另外一种语言。

![mark](http://images.iterate.site/blog/image/20190818/bDO1hyPG32ha.png?imageslim)
所以今天就算是某一个翻译 task ，你的 input 语言和 output 语言 machine 没有看过，它也可以透过这种自己学出来的 sequence language来做 translation。



一些 paper 给予参考。
![mark](http://images.iterate.site/blog/image/20190818/BUjNlnyzbVVo.png?imageslim)



# 自我学习 & 自我聚类


target data有 label,source data没有 label 的状况叫做 self-taught learning。target label没有 label，source data也没有 label 的状况叫做 self-taught clustering。

![mark](http://images.iterate.site/blog/image/20190818/6yRphVDOBqnH.png?imageslim)





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
