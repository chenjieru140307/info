


# 学成近似推断



我们已经看到了推断可以被视作一个增加函数 $\mathcal L$ 值的优化过程。显式地通过迭代方法（比如不动点方程或者基于梯度的优化算法）来进行优化的过程通常是代价很高且耗时巨大的。通过学习一个近似推断，许多推断算法避免了这种代价。具体地说，我们可以将优化过程视作将一个输入 $\boldsymbol v$ 投影到一个近似分布 $q^* = \arg\max_q\  \mathcal L(\boldsymbol v,q)$ 的一个 $f$ 的函数。一旦我们将多步的迭代优化过程看作是一个函数，我们可以用一个近似函数为 $\hat{f}(\boldsymbol v;{\boldsymbol \theta})$ 的神经网络来近似它。



## 醒眠算法

<span style="color:red;">wake_sleep 翻译成  醒眠</span>

训练一个可以用 $\boldsymbol v$ 来推断 $\boldsymbol h$ 的模型的一个主要难点在于我们没有一个监督训练集来训练模型。给定一个 $\boldsymbol v$，我们无法获知一个合适的 $\boldsymbol h$。从 $\boldsymbol v$ 到 $\boldsymbol h$ 的映射依赖于模型族的选择，并且在学习过程中随着 ${\boldsymbol \theta}$ 的改变而变化。醒眠算法~通过从模型分布中抽取 $\boldsymbol v$ 和 $\boldsymbol h$ 的样本来解决这个问题。例如，在有向模型中，这可以通过执行从 $\boldsymbol h$ 开始并在 $\boldsymbol v$ 结束的原始采样来高效地完成。然后这个推断网络可以被训练来执行反向的映射：预测哪一个 $\boldsymbol h$ 产生了当前的 $\boldsymbol v$。<!-- %这种方法的主要缺点是我们将只能够训练推断网络在模型下具有高概率的 $\boldsymbol v$ 值。 -->这种方法的主要缺点是我们将只能在那些在当前模型上有较高概率的 $\boldsymbol v$ 值上训练推断网络。在学习早期，模型分布与数据分布偏差较大，因此推断网络将不具有在类似数据的样本上学习的机会。



在\sec?中，我们看到睡眠做梦在人类和动物中作用的一个可能解释是，做梦可以提供蒙特卡罗训练算法用于近似无向模型中对数配分函数负梯度的负相样本。生物做梦的另一个可能解释是它提供来自 $p(\boldsymbol h,\boldsymbol v)$ 的样本，这可以用于训练推断网络在给定 $\boldsymbol v$ 的情况下预测 $\boldsymbol h$。在某些意义上，这种解释比配分函数的解释更令人满意。如果蒙特卡罗算法仅使用梯度的正相运行几个步骤，然后仅对梯度的负相运行几个步骤，那么结果通常不会很好。人类和动物通常连续清醒几个小时，然后连续睡着几个小时。这个时间表如何支持无向模型的蒙特卡罗训练尚不清楚。然而，基于最大化 $\mathcal L$ 的学习算法可以通过长时间调整改进 $q$ 和长期调整 ${\boldsymbol \theta}$ 来实现。如果生物做梦的作用是训练网络来预测 $q$，那么这解释了动物如何能够保持清醒几个小时（它们清醒的时间越长，$\mathcal L$ 和 $\log p(\boldsymbol v)$ 之间的差距越大， 但是 $\mathcal L$ 仍然是下限）并且睡眠几个小时（生成模型本身在睡眠期间不被修改）， 而不损害它们的内部模型。当然，这些想法纯粹是猜测性的，没有任何确定的证据表明做梦实现了这些目标之一。做梦也可以通过从动物的过渡模型（用来训练动物策略）采样合成经验来服务于强化学习而不是概率建模。也许睡眠可以服务于一些机器学习社区尚未发现的其他目的。





## 学成推断的其他形式



这种学成近似推断策略已经被应用到了其他模型中。{Salakhutdinov+Larochelle-2010}证明了在学成推断网络中的单遍传递相比于在深度玻尔兹曼机中的迭代均值场不动点方程能够得到更快的推断。其训练过程是基于运行推断网络的，然后运行一步均值场来改进其估计，并训练推断网络来输出这个更精细的估计以代替其原始估计。



我们已经在\sec?中看到，预测性的稀疏分解模型训练一个浅层编码器网络，从而预测输入的稀疏编码。这可以被看作是自编码器和稀疏编码之间的混合。为模型设计概率语义是可能的，其中编码器可以被视为执行学成近似~MAP~推断。由于其浅层的编码器，PSD不能实现我们在均值场推断中看到的单元之间的那种竞争。然而，该问题可以通过训练深度编码器实现学成近似推断来补救，如 ISTA 技术~。



近来学成近似推断已经成为了变分自编码器形式的生成模型中的主要方法之一 。在这种优美的方法中，不需要为推断网络构造显式的目标。反之，推断网络仅仅被用来定义 $\mathcal L$，然后调整推断网络的参数来增大 $\mathcal L$。我们将在\sec?中详细介绍这种模型。


我们可以使用近似推断来训练和使用很多不同的模型。其中许多模型将在下一章中描述。





# 相关

- 《深度学习》花书
