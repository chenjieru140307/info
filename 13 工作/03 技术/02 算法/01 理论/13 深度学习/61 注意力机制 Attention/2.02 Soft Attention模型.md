---
title: 2.02 Soft Attention模型
toc: true
date: 2019-09-29
---


# Soft Attention模型


要了解深度学习中的注意力模型，就不得不先谈 Encoder-Decoder框架，因为目前大多数注意力模型附着在 Encoder-Decoder框架下，当然，**其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架**，这点需要注意。

## 没有注意力的时候

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/4GTawCuydbtP.png?imageslim">
</p>


上图中展示的 Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。

为什么说它注意力不集中呢？

请观察下目标句子 Target 中每个单词的生成过程如下：





$$
\begin{aligned} \mathbf{y}_{1} &=\mathbf{f}(\mathbf{C}) \\ \mathbf{y}_{2} &=\mathbf{f}\left(\mathbf{C}, \mathbf{y}_{1}\right) \\ \mathbf{y}_{3} &=\mathbf{f}\left(\mathbf{C}, \mathbf{y}_{1}, \mathbf{y}_{2}\right) \end{aligned}
$$





其中 $f$ 是 Decoder 的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子 Source 的语义编码 $C$ 都是一样的，没有任何区别。

而语义编码 C 是由句子 Source 的每个单词经过 Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是 y3，其实句子 Source 中任意单词对生成某个目标单词 yi 来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。

这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。

如果拿机器翻译来解释这个分心模型的 Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder 框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。

在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。

**没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。**

上面的例子中，如果引入 Attention 模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：

（Tom,0.3）(Chase,0.2) (Jerry,0.5)

每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。

同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词 $y_i$ 的时候，原先都是相同的中间语义表示 $C$ 会被替换成根据当前生成单词而不断变化的 $C_i$。

理解 Attention 模型的关键就是这里，即 **由固定的中间语义表示 $C$ 换成了根据当前输出单词来调整成加入注意力模型的变化的 $C_i$。**

增加了注意力模型的 Encoder-Decoder框架理解起来如下图所示。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/fIqbojP9RDTS.png?imageslim">
</p>

> 引入注意力模型的 Encoder-Decoder框架



即生成目标句子单词的过程成了下面的形式：



$$
\begin{array}{l}{\mathbf{y}_{1}=\mathbf{f} \mathbf{1}\left(\mathbf{C}_{1}\right)} \\ {\mathbf{y}_{2}=\mathbf{f} \mathbf{1}\left(\mathbf{C}_{2}, \mathbf{y}_{1}\right)} \\ {\mathbf{y}_{3}=\mathbf{f} \mathbf{1}\left(\mathbf{C}_{3}, \mathbf{y}_{1}, \mathbf{y}_{2}\right)}\end{array}
$$

而每个 $C_i$ 可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：

$$
\begin{array}{l}{\mathbf{C}_{汤姆}=\mathbf{g}\left(0.6 * \mathbf{f} 2\left(\text { Tom }\right), \mathbf{0} .2 * \mathbf{f} 2(\text { Chase }), \mathbf{0} .2 * \mathbf{f} 2\left(\text {Jerry }\right)\right)} \\ {\mathbf{C}_{追逐}=\mathbf{g}\left(0.2 * \mathrm{f} 2\left(\text { Tom }\right), \mathbf{0} .7 * \mathbf{f} 2(\text { Chase }), \mathbf{0} .1 * \mathbf{f} 2\left(\text { Jerry }\right)\right)} \\ {\mathbf{C}_{杰瑞}=\mathbf{g}\left(\mathbf{0} .3 * \mathrm{f} 2\left(\text { Tom }\right), \mathbf{0} . \mathbf{2} * \mathbf{f} 2(\text { Chase }), \mathbf{0} .5 * \mathbf{f} 2(\text { Jerry }))\right.}\end{array}
$$

其中：

- $f_2$ 函数代表 Encoder 对输入英文单词的某种变换函数，比如如果 Encoder 是用的 RNN 模型的话，这个 $f_2$ 函数的结果往往是某个时刻输入 $x_i$ 后隐层节点的状态值；
- $g$ 代表 Encoder 根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，$g$ 函数就是对构成元素加权求和，即下列公式：

$$
\boldsymbol{c}_{i}=\sum_{j=1}^{L_{x}} \boldsymbol{a}_{i j} \boldsymbol{h}_{j}
$$

其中：

- $L_x$ 代表输入句子 Source 的长度
- $a_{ij}$ 代表在 Target 输出第 $i$ 个单词时 Source 输入句子中第 $j$ 个单词的注意力分配系数
- $h_j$ 则是 Source 输入句子中第 $j$ 个单词的语义编码。

假设下标 $i$ 就是上面例子所说的“汤姆”，那么 $L_x$ 就是 3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是 0.6,0.2,0.2，所以 $g$ 函数本质上就是个加权求和函数。

如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示 $C_i$ 的形成过程类似图 4。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/xJnqMK8lqvUx.png?imageslim">
</p>

> Attention的形成过程

## 注意力分配的概率分布

这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道 Attention 模型所需要的输入句子单词注意力分配概率分布值呢？

就是说“汤姆”对应的输入句子 Source 中各个单词的概率分布：(Tom,0.6)(Chase,0.2)(Jerry,0.2) 是如何得到的呢？

为了便于说明，我们假设对开始的非 Attention 模型的 Encoder-Decoder框架进行细化，Encoder采用 RNN 模型，Decoder也采用 RNN 模型，这是比较常见的一种模型配置，则转换为下图。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/rFGcGegoOI8T.png?imageslim">
</p>

> RNN作为具体模型的 Encoder-Decoder框架

那么用下图可以较为便捷地说明注意力分配概率分布值的通用计算过程。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/rVCa2qIm3CFM.png?imageslim">
</p>

> 注意力分配概率计算

对于采用 RNN 的 Decoder 来说，在时刻 $i$，如果要生成 $y_i$ 单词，我们是可以知道 Target 在生成 $y_i$ 之前的时刻 $i-1$ 时，隐层节点 $i-1$ 时刻的输出值 $H_{i-1}$ 的，而我们的目的是要计算生成 $y_i$ 时输入句子中的单词“Tom”、“Chase”、“Jerry”对 $y_i$ 来说的注意力分配概率分布，那么可以用 Target 输出句子 $i-1$ 时刻的隐层节点状态 $H_{i-1}$ 去一一和输入句子 Source 中每个单词对应的 RNN 隐层节点状态 $h_j$ 进行对比，即通过函数 $F(hj,Hi-1)$ 来获得目标单词 $y_i$ 和每个输入单词对应的对齐可能性，这个 $F$ 函数在不同论文里可能会采取不同的方法，然后函数 $F$ 的输出经过 Softmax 进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。<span style="color:red;">没有很明白。</span>

绝大多数 Attention 模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在 $F$ 的定义上可能有所不同。

下图可视化地展示了在英语-德语翻译系统中加入 Attention 机制后，Source和 Target 两个句子每个单词对应的注意力分配概率分布。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/5laokOFxKD6L.png?imageslim">
</p>


> 英语-德语翻译的注意力概率分布

## 理解 Attention 模型的物理含义

上述内容就是经典的 Soft Attention模型的基本思想，那么怎么理解 Attention 模型的物理含义呢？

一般在自然语言处理应用里会把 Attention 模型看作是输出 Target 句子中某个单词和输入 Source 句子每个单词的对齐模型，这是非常有道理的。

目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/k33VHkPcjWuT.png?imageslim">
</p>


> Google 神经网络机器翻译系统结构图

上图所示即为 Google 于 2016 年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了 60%，其架构就是上文所述的加上 Attention 机制的 Encoder-Decoder框架，主要区别无非是其 Encoder 和 Decoder 使用了 8 层叠加的 LSTM 模型。


# 相关

- [深度学习中的注意力模型（2017版）](https://zhuanlan.zhihu.com/p/37601161)
