

# 推断和近似推断




<!-- %我们可以使用概率模型的主要方法之一是提出关于变量如何相互关联的问题。  -->
解决变量之间如何相互关联的问题是我们使用概率模型的一个主要方式。给定一组医学测试，我们可以询问患者可能患有什么疾病。在一个潜变量模型中，我们可能需要提取能够描述可观察变量 $\mathbf v$ 的特征 $\mathbb E[\mathbf h \mid \mathbf v]$。有时我们需要解决这些问题来执行其他任务。我们经常使用最大似然的准则来训练我们的模型。由于


$$\begin{aligned}
\log p(\boldsymbol v) = \mathbb E_{\mathbf h \sim p(\mathbf h\mid \boldsymbol v)} [\log p(\boldsymbol h,\boldsymbol v) -  \log p(\boldsymbol h\mid\boldsymbol v)],
\end{aligned}$$


学习过程中，%为了执行学习规则，我们经常需要计算 $p(\mathbf h\mid\boldsymbol v)$。所有这些都是推断问题的例子，其中我们必须预测给定其他变量的情况下一些变量的值，或者在给定其他变量值的情况下预测一些变量的概率分布。


<!-- %不幸的是，对于大多数有趣的深度模型来说，这些推断问题都是难以处理的，即使我们使用结构化图模型来简化它们。 -->不幸的是，对于大多数有趣的深度模型来说，即使我们使用结构化图模型来简化这些推断问题，它们仍然是难以处理的。图结构允许我们用合理数量的参数来表示复杂的高维分布，但是用于深度学习的图并不满足这样的条件，从而难以实现高效地推断。



我们可以直接看出，计算一般图模型的边缘概率是\#P-hard的。复杂性类别\#P是复杂性类别 NP 的泛化。NP中的问题只需确定其中一个问题是否有解决方案，并找到一个解决方案（如果存在）就可以解决。\#P中的问题需要计算解决方案的数量。为了构建最坏情况的图模型，我们可以设想一下我们在 3-SAT问题中定义二值变量的图模型。我们可以对这些变量施加均匀分布。然后我们可以为每个子句添加一个二值潜变量，来表示每个子句是否成立。然后，我们可以添加另一个潜变量，来表示所有子句是否成立。这可以通过构造一个潜变量的缩减树来完成，树中的每个结点表示其他两个变量是否成立，从而不需要构造一个大的团。该树的叶是每个子句的变量。树的根表示整个问题是否成立。由于子句的均匀分布，缩减树根结点的边缘分布表示子句有多少比例是成立的。虽然这是一个设计的最坏情况的例子，NP-hard图确实会频繁地出现在现实世界的场景中。



这促使我们使用近似推断。在深度学习中，这通常涉及变分推断，其中通过寻求尽可能接近真实分布的近似分布 $q(\mathbf h\mid\mathbf v)$ 来逼近真实分布 $p(\mathbf h\mid\boldsymbol v)$。这个技术将在\chap?中深入讨论。





# 相关

- 《深度学习》花书
