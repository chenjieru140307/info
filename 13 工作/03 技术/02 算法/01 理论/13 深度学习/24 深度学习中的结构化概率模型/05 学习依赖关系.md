

# 学习依赖关系



良好的生成模型需要准确地捕获所观察到的或"可见"变量 $\mathbf v$ 上的分布。通常 $\mathbf v$ 的不同元素彼此高度依赖。在深度学习中，最常用于建模这些依赖关系的方法是引入几个潜在或"隐藏"变量 $\mathbf h$。然后，该模型可以捕获任何对（变量 $\mathrm v_i$ 和 $\mathrm v_j$ 间接依赖可以通过 $\mathrm v_i$ 和 $\mathbf h$ 之间直接依赖和 $\mathbf h$ 和 $\mathrm v_j$ 直接依赖捕获)之间的依赖关系。



如果一个良好的关于 $\mathbf v$ 的模型不包含任何潜变量，那么它在贝叶斯网络中的每个节点需要具有大量父节点或在马尔可夫网络中具有非常大的团。仅仅表示这些高阶相互作用的成本就很高了，首先从计算角度上考虑，存储在存储器中的参数数量是团中成员数量的指数级别，接着在统计学意义上，因为这些指数数量的参数需要大量的数据来准确估计。



当模型旨在描述直接连接的可见变量之间的依赖关系时，通常不可能连接所有变量，因此设计图模型时需要连接那些紧密相关的变量，并忽略其他变量之间的作用。机器学习中有一个称为结构学习的领域专门讨论这个问题。{koller-book2009}是一个不错的结构学习参考资料。大多数结构学习技术基于一种贪婪搜索的形式。它们提出了一种结构，对具有该结构的模型进行训练，然后给出分数。该分数奖励训练集上的高精度并对模型的复杂度进行惩罚。然后提出添加或移除少量边的候选结构作为搜索的下一步。搜索向一个预计会增加分数的新结构发展。


使用潜变量而不是自适应结构避免了离散搜索和多轮训练的需要。可见变量和潜变量之间的固定结构可以使用可见单元和隐藏单元之间的直接作用，从而建模可见单元之间的间接作用。<!-- %从而使得可见单元之间间接作用。 -->使用简单的参数学习技术，我们可以学习到一个具有固定结构的模型，这个模型在边缘分布 $p(\boldsymbol v)$ 上拥有正确的结构。



潜变量除了发挥本来的作用，即能够高效地描述 $p(\mathbf v)$ 以外，还具有另外的优势。新变量 $\mathbf h$ 还提供了 $\mathbf v$ 的替代表示。例如，如 \sec? 所示，高斯混合模型学习了一个潜变量，这个潜变量对应于输入样本是从哪一个混合体中抽出。这意味着高斯混合模型中的潜变量可以用于做分类。我们可以看到\chap?中简单的概率模型如稀疏编码，是如何学习可以用作分类器输入特征或者作为流形上坐标的潜变量的。其他模型也可以使用相同的方式，但是更深的模型和具有多种相互作用方式的模型可以获得更丰富的输入描述。许多方法通过学习潜变量来完成特征学习。通常，给定 $\mathbf v$ 和 $\mathbf h$，实验观察显示 $\mathbb E[\mathbf h\mid\mathbf v]$ 或 ${\arg\max}_{\boldsymbol h}\ p(\boldsymbol h,\boldsymbol v)$ 都是 $\boldsymbol v$ 的良好特征映射。





# 相关

- 《深度学习》花书
