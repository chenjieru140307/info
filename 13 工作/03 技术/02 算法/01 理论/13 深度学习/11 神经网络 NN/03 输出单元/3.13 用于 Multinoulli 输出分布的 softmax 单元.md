
# 用于 Multinoulli 输出分布的 softmax 单元

任何时候当我们想要表示一个具有 $n$ 个可能取值的离散型随机变量的分布时，我们都可以使用 softmax 函数。它可以看作是 sigmoid  函数的扩展，其中 sigmoid  函数用来表示二值型变量的分布。


softmax 函数最常用作分类器的输出，来表示 $n$ 个不同类上的概率分布。比较少见的是，softmax 函数可以在模型内部使用，例如如果我们想要在某个内部变量的 $n$ 个不同选项中进行选择。<span style="color:red;">内部变量的 $n$ 个不同选项中进行选择，这个什么时候会用到？嗯，想知道，真的会吧 softmax 用在模型内部吗？ </span>

在二值型变量的情况下，我们希望计算一个单独的数

$$
\hat{y} = P(y=1\mid\boldsymbol x).
$$

因为这个数需要处在 0 和 1 之间，并且我们想要让这个数的对数可以很好地用于对数似然的基于梯度的优化，我们选择去预测另外一个数 $z=\log \hat{P}(y=1\mid\boldsymbol x)$。对其指数化和归一化，我们就得到了一个由 sigmoid  函数控制的 Bernoulli 分布。

为了推广到具有 $n$ 个值的离散型变量的情况，我们现在需要创造一个向量 $\hat{\boldsymbol y}$，它的每个元素是 $\hat{y}_i = P(y=i\mid\boldsymbol x)$。我们不仅要求每个 $\hat{y}_i$ 元素介于 0 和 1 之间，还要使得整个向量的和为 1，使得它表示一个有效的概率分布。用于 Bernoulli 分布的方法同样可以推广到 Multinoulli 分布。首先，线性层预测了未归一化的对数概率：<span style="color:red;">嗯，预测未归一化的对数概率这个知道了。但是是怎么控制使线性层预测未归一化的对数概率的？</span>

$$
\boldsymbol z = \boldsymbol W^\top \boldsymbol h+\boldsymbol b,
$$

其中 $z_i=\log \hat{P}(y=i\mid\boldsymbol x)$。softmax函数然后可以对 $z$ 指数化和归一化来获得需要的 $\hat{\boldsymbol y}$。最终，softmax函数的形式为：<span style="color:red;">怎么进行指数化的？</span>

$$
\text{softmax}(\boldsymbol z)_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}.
$$


和 logistic sigmoid 一样，当使用最大化对数似然训练 softmax 来输出目标值 $\mathrm y$ 时，使用指数函数工作地非常好。这种情况下，我们想要最大化 $\log P(\mathrm y =i; \boldsymbol z)=\log \text{softmax}(\boldsymbol z)_i$。将 softmax 定义成指数的形式是很自然的因为对数似然中的 log 可以抵消 softmax 中的 exp：<span style="color:red;">什么叫把 softmax 定义成指数的形式？</span>

$$
\log \text{softmax}(\boldsymbol z)_i = z_i - \log \sum_j \exp(z_j).
$$



式 6.30 中的第一项表示输入 $z_i$ 总是对代价函数有直接的贡献。因为这一项不会饱和，所以即使 $z_i$ 对式 6.30 的第二项的贡献很小，学习依然可以进行。当最大化对数似然时，第一项鼓励 $z_i$ 被推高，而第二项则鼓励所有的 $\boldsymbol z$ 被压低。<span style="color:red;">嗯。</span>为了对第二项 $\log \sum_j \exp(z_j)$ 有一个直观的理解，注意到这一项可以大致近似为 $\max_j z_j$。<span style="color:red;">为什么可以这样近似。</span><span style="color:blue;">哦，后面这句有说的。</span>这种近似是基于对任何明显小于 $\max_j z_j$ 的 $z_k$，$\exp(z_k)$ 都是不重要的。我们能从这种近似中得到的直觉是，负对数似然代价函数总是强烈地惩罚最活跃的不正确预测。<span style="color:red;">是的，负对数似然是在惩罚最活跃的不正确预测的样本。</span>如果正确答案已经具有了 softmax 的最大输入，那么 $-z_i$ 项和 $\log\sum_j \exp(z_j) \approx \max_j z_j = z_i$ 项将大致抵消。<span style="color:red;">是的。</span>这个样本对于整体训练代价贡献很小，这个代价主要由其他未被正确分类的样本产生。<span style="color:red;">是的。挺厉害的。</span>

到目前为止我们只讨论了一个例子。总体来说，未正则化的最大似然会驱动模型去学习一些参数，而这些参数会驱动 softmax 函数来预测在训练集中观察到的每个结果的比率：

$$
\text{softmax}(\boldsymbol z(\boldsymbol x; \boldsymbol \theta))_i \approx \frac{\sum_{j=1}^m \bm{1}_{y^{(j)}=i, \boldsymbol x^{(j)} = \boldsymbol x}  }{ \sum_{j=1}^{m} \bm{1}_{\boldsymbol x^{(j)} = \boldsymbol x} }.
$$

<span style="color:red;">这个式子有点厉害，感觉有点清楚又有点模糊。</span>

因为最大似然是一致的估计量，所以只要模型族能够表示训练的分布，这就能保证发生。在实践中，有限的模型能力和不完美的优化将意味着模型只能近似这些比率。<span style="color:red;">嗯，只能近似这些比率。</span>

除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习。特别是，平方误差对于 softmax 单元来说是一个很差的损失函数，即使模型做出高度可信的不正确预测，也不能训练模型改变其输出。要理解为什么这些损失函数可能失败，我们需要检查 softmax 函数本身。

像 sigmoid 一样，softmax 激活函数可能会饱和。sigmoid 函数具有单个输出，当它的输入极端负或者极端正时会饱和。对于 softmax 的情况，它有多个输出值。当输入值之间的差异变得极端时，这些输出值可能饱和。当 softmax 饱和时，基于 softmax 的许多代价函数也饱和，除非它们能够转化饱和的激活函数。


为了说明 softmax 函数对于输入之间差异的响应，观察到当对所有的输入都加上一个相同常数时 softmax 的输出不变：<span style="color:red;">嗯。</span>

$$
\text{softmax}(\boldsymbol z) = \text{softmax}(\boldsymbol z+c).
$$

使用这个性质，我们可以导出一个数值方法稳定的 softmax 函数的变体：

$$
\text{softmax}(\boldsymbol z) = \text{softmax}(\boldsymbol z- \max_i z_i).
$$

变换后的形式允许我们在对 softmax 函数求值时只有很小的数值误差，即使是当 $z$ 包含极正或者极负的数时。观察 softmax 数值稳定的变体，可以看到 softmax 函数由它的变量偏离 $\max_i z_i$ 的量来驱动。<span style="color:red;">没懂。</span>

当其中一个输入是最大（$z_i = \max_i z_i$）并且 $z_i$ 远大于其他的输入时，相应的输出 $\text{softmax}(\boldsymbol z)_i$ 会饱和到 1。当 $z_i$ 不是最大值并且最大值非常大时，相应的输出 $\text{softmax}(\boldsymbol z)_i$ 也会饱和到 0。这是 sigmoid  单元饱和方式的一般化，并且如果损失函数不被设计成对其进行补偿，那么也会造成类似的学习困难。

softmax 函数的变量 $\boldsymbol z$ 可以通过两种方式产生。最常见的是简单地使神经网络较早的层输出 $\boldsymbol z$ 的每个元素，就像先前描述的使用线性层 $\boldsymbol z={W}^\top\boldsymbol h+\boldsymbol b$。虽然很直观，但这种方法是对分布的过度参数化。$n$ 个输出总和必须为 1 的约束意味着只有 $n-1$ 个参数是必要的；第 $n$ 个概率值可以通过 1 减去前面 $n-1$ 个概率来获得。因此，我们可以强制要求 $\boldsymbol z$ 的一个元素是固定的。例如，我们可以要求 $z_n=0$。事实上，这正是 sigmoid  单元所做的。定义 $P(y=1\mid\boldsymbol x)=\sigma(z)$ 等价于用二维的 $\boldsymbol z$ 以及 $z_1=0$ 来定义 $P(y=1\mid\boldsymbol x)=\text{softmax}(\boldsymbol z)_1$。无论是 $n-1$ 个变量还是 $n$ 个变量的方法，都描述了相同的概率分布，但会产生不同的学习机制。在实践中，无论是过度参数化的版本还是限制的版本都很少有差别，并且实现过度参数化的版本更为简单。<span style="color:red;">看到这一段，平时有用过限制的版本吗？</span>

从神经科学的角度看，有趣的是认为 softmax 是一种在参与其中的单元之间形成竞争的方式：softmax 输出总是和为 1，所以一个单元的值增加必然对应着其他单元值的减少。这与被认为存在于皮质中相邻神经元间的侧抑制类似。在极端情况下（当最大的 $a_i$ 和其他的在幅度上差异很大时），它变成了赢者通吃的形式（其中一个输出接近 1，其他的接近 0）。<span style="color:red;">嗯。</span>

"softmax" 的名称可能会让人产生困惑。这个函数更接近于 argmax 函数而不是 max 函数。"soft"这个术语来源于 softmax 函数是连续可微的。"argmax"函数的结果表示为一个 one-hot 向量（只有一个元素为 1，其余元素都为 0 的向量），不是连续和可微的。<span style="color:red;">"argmax"函数的结果为什么是表示为一个 one-hot 向量？</span>softmax 函数因此提供了 argmax 的"软化"版本。max 函数相应的软化版本是 $\text{softmax}(\boldsymbol z)^\top \boldsymbol z$。可能最好是把 softmax 函数称为 "softargmax"，但当前名称已经是一个根深蒂固的习惯了。<span style="color:red;">没明白。</span>







# 相关

- 《深度学习》花书
