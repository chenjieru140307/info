

### 3.4.11 交叉熵代价函数定义及其求导推导

<span style="color:red;">讲的挺好的，看明白了。手推一遍</span>

神经元的输出就是 $\mathrm{a}=\sigma(\mathrm{z})$，其中 $z=\sum w_{j}i_{j}+b​$ 是输入的带权和。

$$
C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)]
$$

对数图像如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190727/8jFSWsGxDbuo.png?imageslim">
</p>


​其中 $n$ 是训练数据的总数，求和是在所有的训练输⼊ $x$ 上进⾏的， $y$ 是对应的目标输出。<span style="color:red;">$y$ 的值是多少？</span>

表达式是否解决学习缓慢的问题并不明显。实际上，甚至将这个定义看做是代价函数也不是显⽽易⻅的！<span style="color:red;">是呀。</span>

在解决学习缓慢前，我们来看看交叉熵为何能够解释成⼀个代价函数。

将交叉熵看做是代价函数有两点原因。

1. 它是非负的， $C > 0$。可以看出：式子中的求和中的所有独立的项都是负数的，因为对数函数的定义域是 $(0，1)$，并且求和前面有⼀个负号，所以结果是非负。<span style="color:red;">？这个跟定义域有什么关系？</span>
2. 如果对于所有的训练输⼊ $x$，神经元实际的输出接近目标值，那么交叉熵将接近 $0$。<span style="color:red;">是的，因为 $a$ 是 $(0,1)$ ，因此，$ln(a)$ 就是 $(-\infty,0)$ ，而 $y$ 的值是 $0$,$1$ ，所以 $C$ 接近 $0$。</span>

> 解释下为什么神经元的输出接近目标值，交叉熵将接近 $0$ ：假设在这个例⼦中， $y = 0$ ⽽ $a \approx 0$。这是我们想到得到的结果。我们看到公式中第⼀个项就消去了，因为 $y = 0$，⽽第二项实际上就是 $− ln(1 − a) \approx 0$。反之， $y = 1$ ⽽ $a \approx 1$。
>
> 所以在实际输出和目标输出之间的差距越小，最终的交叉熵的值就越低了。（这里假设输出结果不是 0，就是 1，实际分类也是这样的）

综上所述，交叉熵是非负的，在神经元达到很好的正确率的时候会接近 $0$。这些其实就是我们想要的代价函数的特性。<span style="color:red;">为什么非负的是一个很好的特性？</span>所以，交叉熵就是很好的选择了。其实这些特性也是二次代价函数具备的。<span style="color:red;">二次代价函数是什么？均方误差吗？</span>但是交叉熵代价函数有⼀个⽐二次代价函数更好的特性就是它避免了学习速度下降的问题。为了弄清楚这个情况，我们来算算交叉熵函数关于权重的偏导数。我们将 $a={\varsigma}(z)$ 代⼊到公式中应用两次链式法则，得到：

$$
\begin{eqnarray}\frac{\partial C}{\partial w_{j}}&=&-\frac{1}{n}\sum \frac{\partial }{\partial w_{j}}[ylna+(1-y)ln(1-a)]\\&=&-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]*\frac{\partial a}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})*\frac{\partial a}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)})\frac{\partial \varsigma(z)}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)}){\varsigma}'(z)x_{j}\end{eqnarray}
$$

​	根据 $\varsigma(z)=\frac{1}{1+e^{-z}}$ 的定义，和⼀些运算，我们可以得到 ${\varsigma}'(z)=\varsigma(z)(1-\varsigma(z))$。化简后可得：

$$
\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum x_{j}({\varsigma}(z)-y)
$$

这是⼀个优美的公式。它告诉我们权重学习的速度受到 $\varsigma(z)-y$，也就是输出中的误差的控制。<span style="color:red;">嗯，挺好。</span>

更大的误差，更快的学习速度。这是我们直觉上期待的结果。特别地，这个代价函数还避免了像在二次代价函数中类似方程中 ${\varsigma}'(z)$ 导致的学习缓慢。当我们使用交叉熵的时候，${\varsigma}'(z)$ 被约掉了，所以我们不再需要关⼼它是不是变得很小。<span style="color:red;">嗯。</span>这种约除就是交叉熵带来的特效。

实际上，这也并不是非常奇迹的事情。我们在后面可以看到，交叉熵其实只是满⾜这种特性的⼀种选择罢了。<span style="color:red;">还有别的选择吗？</span>

根据类似的方法，我们可以计算出关于偏置的偏导数。我这⾥不再给出详细的过程，你可以轻易验证得到：

$$
\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\varsigma}(z)-y)​
$$


再⼀次, 这避免了二次代价函数中类似 ${\varsigma}'(z)$ 项导致的学习缓慢。

<span style="color:red;">不错不错。</span>
