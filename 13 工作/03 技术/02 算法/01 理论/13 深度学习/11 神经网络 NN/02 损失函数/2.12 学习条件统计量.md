

### 1.2 学习条件统计量

有时我们并不是想学习一个完整的概率分布 $p(\boldsymbol y\mid\boldsymbol x; \boldsymbol \theta)$，而仅仅是想学习在给定 $\boldsymbol x$ 时 $\boldsymbol y$ 的某个条件统计量。

例如，我们可能有一个预测器 $f(\boldsymbol x; \boldsymbol \theta)$，我们想用它来预测 $\boldsymbol y$ 的均值。<span style="color:red;">什么是 $\boldsymbol y$ 的均值？</span>如果我们使用一个足够强大的神经网络，我们可以认为这个神经网络能够表示一大类函数中的任何一个函数 $f$，这个类仅仅被一些特征所限制，例如连续性和有界，而不是具有特殊的参数形式。从这个角度来看，我们可以把代价函数看作是一个泛函而不仅仅是一个函数。泛函是函数到实数的映射。我们因此可以将学习看作是选择一个函数而不仅仅是选择一组参数。我们可以设计代价泛函在我们想要的某些特殊函数处取得最小值。<span style="color:red;">怎么设计代价泛函？跟代价函数有什么区别吗？</span>例如，我们可以设计一个代价泛函，使它的最小值处于一个特殊的函数上，这个函数将 $\boldsymbol x$ 映射到给定 $\boldsymbol x$ 时 $\boldsymbol y$ 的期望值。<span style="color:red;">？怎么做到的？</span>对函数求解优化问题需要用到变分法这个数学工具，我们将在第 19.4.2 节中讨论。理解变分法对于理解本章的内容不是必要的。目前，只需要知道变分法可以被用来导出下面的两个结果。<span style="color:red;">什么是变分法？为什么可以解决这个问题？</span>


我们使用变分法导出的第一个结果是解优化问题

$$
f^* = \underset{f}{\arg \min }  \ \mathbb E_{\mathbf x, \mathbf y \sim  p_\text{data}} ||\boldsymbol y-f(\boldsymbol x)||^2
$$


得到

$$
f^*(\boldsymbol x) = \mathbb E_{\mathbf y\sim p_\text{data}(\boldsymbol y|\boldsymbol x)} [\boldsymbol y],
$$

<span style="color:red;">上面这个式子怎么得到的？</span>

要求这个函数处在我们要优化的类里。换句话说，如果我们能够用无穷多的、来源于真实的数据生成分布的样本进行训练，最小化均方误差代价函数将得到一个函数，它可以用来对每个 $\boldsymbol x$ 的值预测出 $\boldsymbol y$ 的均值。<span style="color:red;">什么是预测出 $\boldsymbol y$ 的均值？</span>

不同的代价函数给出不同的统计量。第二个使用变分法得到的结果是<span style="color:red;">为什么这个是变分法得到的结果？</span>

$$
f^* = \underset{f}{\arg \min } \ \mathbb E_{\mathbf x, \mathbf y \sim  p_\text{data}} ||\boldsymbol y - f(\boldsymbol x)||_1
$$

将得到一个函数可以对每个 $\boldsymbol x$ 预测 $\boldsymbol y$ 取值的**中位数**，只要这个函数在我们要优化的函数族里。这个代价函数通常被称为平均绝对误差。<span style="color:red;">中位数？为什么是预测中位数？</span>

可惜的是，均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。这就是为什么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了，即使是在没必要估计整个 $p(\boldsymbol y\mid\boldsymbol x)$ 分布时。<span style="color:red;">嗯，这句感觉很有用，但是理解不深。</span>







# 相关

- 《深度学习》花书
