

## 根据问题 1 中定义的损失函数，推导各层参数更新的梯度计算公式。

回顾之前给出的定义：

- 第 $(l)$ 层的参数为 $W^{(l)}$ 和 $b^{(l)}$；
- 每一层的线性变换为 $z^{(l)}=\boldsymbol{W}^{(l)} \boldsymbol{x}^{(l)}+\boldsymbol{b}^{(l)}$ ；
- 输出为 $\boldsymbol{a}^{(l)}=f\left(z^{(l)}\right)$ ，其中 $f$ 为非线性激活函数（如 Sigmoid、Tanh、ReLU等）；
- $a^{(l)}$ 直接作为下一层的输入，即 $\boldsymbol{x}^{(l+1)}=\boldsymbol{a}^{(l)}$ 。


我们可以利用批量梯度下降法来优化网络参数。

梯度下降法中每次迭代对参数 $W$（网络连接权重）和 $b$（偏置）进行更新：<span style="color:red;">嗯。</span>

$$
W_{i j}^{(l)}=W_{i j}^{(l)}-\alpha \frac{\partial}{\partial W_{i j}^{(l)}} J(\boldsymbol{W}, \boldsymbol{b})\tag{9.14}
$$

$$
b_{i}^{(l)}=b_{i}^{(l)}-\alpha \frac{\partial}{\partial b_{i}^{(l)}} J(\boldsymbol{W}, \boldsymbol{b})\tag{9.15}
$$

其中 $\alpha$ 为学习速率，控制每次迭代中梯度变化的幅度。

问题的核心为求解 $\frac{\partial}{\partial W_{i j}^{(l)}} J(\boldsymbol{W}, \boldsymbol{b})$ 与 $\frac{\partial}{\partial b^{(l)}} J(\boldsymbol{W}, \boldsymbol{b})$ 。<span style="color:red;">嗯，是的</span>

为得到递推公式，我们还需要计算损失函数对隐含层的偏导：

$$
\frac{\partial}{\partial z_{i}^{(l)}} J(\boldsymbol{W}, \boldsymbol{b})=\sum_{j=1}^{s_{l+1}}\left(\frac{\partial J(\boldsymbol{W}, \boldsymbol{b})}{\partial z_{j}^{(l+1)}} \frac{\partial z_{j}^{(l+1)}}{\partial z_{i}^{(l)}}\right)\tag{9.16}
$$


其中 $s_{l+1}$ 为第 $l+1$ 层的节点数。<span style="color:red;">嗯。</span>

OK，我们看式（9.16）的后面一点的部分，可以写成：

$$
\frac{\partial z_{j}^{(l+1)}}{\partial z_{i}^{(l)}}=\frac{\partial\left(W_{i j}^{(l)} x^{(l+1)}+b_{j}^{(l+1)}\right)}{\partial z_{i}^{(l)}}\tag{9.17}
$$

其中 $b^{(l+1)}$ 与 $z_{i}^{(l)}$ 无关可以省去，<span style="color:red;">是的。</span>

$x^{(l+1)}=a^{(l)}=f\left(z^{(l)}\right)$ ，因此式（9.17）可写为：

$$
\frac{\partial z_{j}^{(l+1)}}{\partial z_{i}^{(l)}}=W_{i j}^{(l)} f^{\prime}\left(z_{i}^{(l)}\right)\tag{9.18}
$$

<span style="color:red;">是的。</span>

OK，我们继续看式（9.16），$\frac{\partial}{\partial z_{i}^{(l)}} J(\boldsymbol{W}, \boldsymbol{b})$ 可以看作损失函数在第 $l$ 层第 $i$ 个节点产生的残差量，记为 $\delta_{i}^{(l)}$ ，然后，我们将上面的 （9.18）代入（9.16），这时，我们可以将递推公式可以表示为：<span style="color:red;">是的。但是什么是残差来着？补充下。</span>

$$
\delta_{i}^{(l)}=\left(\sum_{j=1}^{s_{l+1}} W_{i j}^{(l)} \delta_{j}^{(l+1)}\right) f^{\prime}\left(z_{i}^{(l)}\right)\tag{9.19}
$$


OK，我们回到（9.14），此时，损失对参数函数的梯度可以写为：

$$
\begin{aligned}\frac{\partial}{\partial W_{i j}^{(l)}} J(\boldsymbol{W}, \boldsymbol{b})&=\frac{\partial J(\boldsymbol{W}, \boldsymbol{b})}{\partial z_{j}^{(l+1)}} \frac{\partial z_{j}^{(l+1)}}{\partial W_{i j}^{(l)}}\\&=\delta_{i}^{(l+1)} x_{j}^{(l+1)}
\\&=\delta_{i}^{(l+1)} a_{j}^{(l)} \frac{\partial}{\partial b_{i}^{(l)}} J(W, b)\\&=\delta_{i}^{(l+1)}\end{aligned}\tag{9.21}
$$

<span style="color:red;">哎？上面的 $x_{j}^{(l+1)}=a_{j}^{(l)} \frac{\partial}{\partial b_{i}^{(l)}} J(W, b)$ 没明白？ 而且，好像 $a_{j}^{(l)} \frac{\partial}{\partial b_{i}^{(l)}} J(W, b)$ 就等于 1 了？为什么？</span>


OK，上面的式子已经说明了这个梯度等于后一层的残差。<span style="color:red;">这一句是我理解的。。不知道对不对。</span>OK，下面我们针对两种不同的损失函数计算最后一层的残差 $\delta^{(L)}$ ；得到 $\delta^{(L)}$ 之后，其他层的残差 $\delta^{(L-1)}$，...， $\delta^{(1)}$ 可以根据上面得到的递推公式（9.19）来计算。<span style="color:red;">这个地方再补充下，到底是怎么计算的？</span>

为了简化起见，这里暂时忽略 Batch 样本集合和正则化项的影响，重点关注这两种损失函数产生的梯度。<span style="color:red;">嗯。</span>


**平方误差损失：**

$$
J(\boldsymbol{W}, \boldsymbol{b})=\frac{1}{2}\left\|\boldsymbol{y}-\boldsymbol{a}^{(L)}\right\|^{2}=\frac{1}{2}\left\|\boldsymbol{y}-f\left(z_{j}^{(L)}\right)\right\|^{2}\tag{9.22}
$$

由于 $\delta_{i}^{(l)}=\frac{\partial}{\partial z_{i}^{(l)}} J(\boldsymbol{W}, \boldsymbol{b})$ ，因此：

$$
\delta^{(L)}=-\left(y-a^{(L)}\right) f^{\prime}\left(z^{(L)}\right)\tag{9.23}
$$

<span style="color:red;">能理解，但是为什么求导的结果是这个？</span>

**交叉熵损失：**

$$
J(\boldsymbol{W}, \boldsymbol{b})=-\sum_{k=1}^{n} y_{k} \ln a_{k}^{(L)}=-\sum_{k=1}^{n} y_{k} \ln f\left(z_{k}^{(L)}\right)\tag{9.24}
$$

在分类问题中，$y_{k}$ 仅在一个类别 $k$ 时取值为 $1$，其余为 $0$。设实际的类别为 $\tilde{k}$，则：<span style="color:red;">嗯。</span>


$$
J(\boldsymbol{W}, \boldsymbol{b})=-\ln a_{\tilde{k}}^{(L)}\tag{9.25}
$$

由于 $\delta_{i}^{(l)}=\frac{\partial}{\partial z_{i}^{(l)}} J(\boldsymbol{W}, \boldsymbol{b})$ ，因此：

$$
\delta^{(L)}=-\frac{f^{\prime}\left(z_{\tilde{k}}^{(L)}\right)}{f\left(z_{\tilde{k}}^{(L)}\right)}\tag{9.26}
$$

<span style="color:red;">上面这个式子的结果再补充下，详细步骤，不要跳步。</span>

$f$ 取 SoftMax 激活函数时，$f^{\prime}(x)=f(x)(1-f(x))$，因此：

$$
\delta^{(L)}=f\left(z_{k}^{(L)}\right)-1=a_{\tilde{k}}^{(L)}-1\tag{9.27}
$$


<span style="color:red;">总体看下来还是很顺畅的。不过还是需要补充下。</span>
