---
title: 7.13 递归地使用链式法则来实现反向传播
toc: true
date: 2019-08-31
---

# 递归地使用链式法则来实现反向传播


使用链式规则，我们可以直接写出某个标量关于计算图中任何产生该标量的节点的梯度的代数表达式。


## 实际计算时候的额外考虑

然而，实际在计算机中计算该表达式时会引入一些额外的考虑。

具体来说，许多子表达式可能在梯度的整个表达式中重复若干次。

任何计算梯度的程序都需要选择是存储这些子表达式还是重新计算它们几次。


下图给出了一个例子来说明这些重复的子表达式是如何出现的。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190712/KNtb9RLtN23c.png?imageslim">
</p>

> 计算梯度时导致重复子表达式的计算图。


令 $w \in \mathbb R$ 为图的输入。我们对链中的每一步使用相同的操作函数 $f: \mathbb R \to \mathbb R$，这样 $x=f(w), y=f(x), z=f(y)$。

则：

$$
\begin{aligned}
& \frac{\partial z}{\partial w}\\
=& \frac{\partial z}{\partial y} \frac{\partial y}{\partial x} \frac{\partial x}{\partial w}\\
=& f'(y)f'(x)f'(w)\\
=& f'(f(f(w))) f'(f(w)) f'(w).
\end{aligned}
$$


第三行建议我们采用的实现方式是，仅计算 $f(w)$ 的值一次并将它存储在变量 $x$ 中。这是反向传播算法所采用的方法。

式第四行提出了一种替代方法，其中子表达式 $f(w)$ 出现了不止一次。在替代方法中，每次只在需要时重新计算 $f(w)$。

当存储这些表达式的值所需的存储较少时，式第三行的反向传播方法显然是较优的，因为它减少了运行时间。然而，式第四行也是链式法则的有效实现，并且当存储受限时它是有用的。<span style="color:red;">没看懂。</span>



因此，在某些情况下，计算两次相同的子表达式纯粹是浪费。在复杂图中，可能存在指数多的这种计算上的浪费，使得简单的链式法则不可实现。在其他情况下，计算两次相同的子表达式可能是以较高的运行时间为代价来减少内存开销的有效手段。<span style="color:red;">嗯。</span>

## 先给出一个版本的反向传播算法

我们首先给出一个版本的反向传播算法，它指明了梯度的直接计算方式（算法 6.2 以及相关的正向计算的算法 6.1），按照它实际完成的顺序并且递归地使用链式法则。

我们可以直接执行这些计算或者将算法的描述视为用于计算反向传播的计算图的符号表示。然而，这些公式并没有明确地操作和构造用于计算梯度的符号图。这些公式将在后面的一般化的反向传播中给出，其中我们还推广到了包含任意张量的节点。

首先考虑描述如何计算单个标量 $u^{(n)}$（例如训练样本上的损失函数）的计算图。我们想要计算这个标量对 $n_i$ 个输入节点 $u^{(1)}$ 到 $u^{(n_i)}$ 的梯度。换句话说，我们希望对所有的 $i\in\{1,2,\ldots,n_i\}$ 计算 $\frac{\partial u^{(n)}}{\partial u^{(i)}}$。在使用反向传播计算梯度来实现参数的梯度下降时，$u^{(n)}$ 将对应单个或者小批量实例的代价函数，而 $u^{(1)}$ 到 $u^{(n_i)}$ 则对应于模型的参数。


我们假设图的节点已经以一种特殊的方式被排序，使得我们可以一个接一个地计算他们的输出，从 $u^{(n_i+1)}$ 开始，一直上升到 $u^{(n)}$。如算法 6.1 中所定义的，每个节点 $u^{(i)}$ 与操作 $f^{(i)}$ 相关联，并且通过对以下函数求值来得到

$$
u^{(i)} = f(\mathbb A^{(i)}),
$$

其中 $\mathbb A^{(i)}$ 是 $u^{(i)}$ 所有父节点的集合。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190712/DSL8coEiGU0k.png?imageslim">
</p>

该算法详细说明了前向传播的计算，我们可以将其放入图 $\mathcal G$ 中。为了执行反向传播，我们可以构造一个依赖于 $\mathcal G$ 并添加额外一组节点的计算图。这形成了一个子图 $\mathcal B$，它的每个节点都是 $\mathcal G$ 的节点。$\mathcal B$ 中的计算和 $\mathcal G$ 中的计算顺序完全相反，而且 $\mathcal B$ 中的每个节点计算导数 $\frac{\partial u^{(n)}}{\partial u^{(i)}}$ 与前向图中的节点 $u^{(i)}$ 相关联。这通过对标量输出 $u^{(n)}$ 使用链式法则来完成：

$$
\frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i:j \in Pa(u^{(i)})} \frac{\partial u^{(n)} }{ \partial u^{(i)} } \frac{ \partial u^{(i)} }{ \partial u^{(j)} }
$$

这在算法 6.2 中详细说明。子图 $\mathcal B$ 恰好包含每一条对应着 $\mathcal G$ 中从节点 $u^{(j)}$ 到节点 $u^{(i)}$ 的边。从 $u^{(j)}$ 到 $u^{(i)}$ 的边对应着计算 $\frac{\partial u^{(i)}}{\partial u^{(j)}}$。另外，对于每个节点都要执行一个内积，内积的一个因子是对于 $u^{j}$ 子节点 $u^{(i)}$ 的已经计算的梯度，另一个因子是对于相同子节点 $u^{(i)}$ 的偏导数 $\frac{\partial u^{(i)}}{\partial u^{(j)}}$ 组成的向量。总而言之，执行反向传播所需的计算量与 $\mathcal G$ 中的边的数量成比例，其中每条边的计算包括计算偏导数（节点关于它的一个父节点的偏导数）以及执行一次乘法和一次加法。下面，我们将此分析推广到张量值节点，这只是在同一节点中对多个标量值进行分组并能够更高效地实现。


反向传播算法被设计为减少公共子表达式的数量而不考虑存储的开销。具体来说，它大约对图中的每个节点执行一个 Jacobian 乘积。这可以从算法 6.2 中看出，反向传播算法访问了图中的节点 $u^{(j)}$ 到节点 $u^{(i)}$ 的每条边一次，以获得相关的偏导数 $\frac{\partial u^{(i)}}{\partial u^{(j)}}$ 。反向传播因此避免了重复子表达式的指数爆炸。然而，其他算法可能通过对计算图进行简化来避免更多的子表达式，或者也可能通过重新计算而不是存储这些子表达式来节省内存。我们将在描述完反向传播算法本身后再重新审视这些想法。<span style="color:red;">嗯。</span>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190712/gYh3zYy7ICFG.png?imageslim">
</p>






# 相关

- 《深度学习》花书
