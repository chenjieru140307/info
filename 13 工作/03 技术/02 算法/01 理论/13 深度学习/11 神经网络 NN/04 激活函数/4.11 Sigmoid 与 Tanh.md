
# 可以补充进来的

- Sigmoid 的推导过程补充下。

# Sigmoid 与 Tanh



## Sigmoid

Sigmoid 激活函数的形式为，

$$
f(z)=\frac{1}{1+e^{-z}}\tag{9.3}
$$

其值域为 $(0,1)$。

对应的导函数为：

$$
f^{\prime}(z)=f(z)(1-f(z))\tag{9.4}
$$


函数图像如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/URVjdLLrU1FD.png?imageslim">
</p>

### 对两个不同点的梯度的说明

Sigmoid 函数曲线如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/4hydKl8lFODO.jpg?imageslim">
</p>


如上图所示，对 0.88 和 0.98 两个点进行比较：
​
- 假设目标是收敛到 1.0。0.88离目标 1.0比较远，梯度比较大，权值调整比较大。0.98离目标 1.0比较近，梯度比较小，权值调整比较小。调整方案合理。
- 假如目标是收敛到 0。0.88离目标 0 比较近，梯度比较大，权值调整比较大。0.98离目标 0 比较远，梯度比较小，权值调整比较小。调整方案不合理。
​
原因：在使用 sigmoid 函数的情况下, 初始的代价（误差）越大，导致训练越慢。<span style="color:red;">嗯，初始的代价误差越大，导致训练越慢。还是有些没有理解。</span>




## Tanh

Tanh 激活函数的形式为：

$$
f(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\tag{9.5}
$$

值域为 $(-1,1)$

对应的导函数为：

$$
f^{\prime}(z)=1-(f(z))^{2}\tag{9.6}
$$


函数图像如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/ViAgM2OV8sqr.png?imageslim">
</p>

### sigmoid 函数


这个在逻辑回归里面经常用，它输出是 0~1 ，因此可以把输出的值看作是概率值，而概率值可以作为一个判别的标准。比如有了概率之后，我们通常可以给定一个分界线，比如 0.5 ，这样就可以来判定这个样本是正样本还是负样本。

以前这个是最常用的激活函数，但是现在一般只用在输出层，中间层很少使用。。这是因为它有两个缺点：




  1. sigmoid 函数的两头过于平坦，导致如果 x 大于 5 时候，对 sigmoid 函数求导的话基本是 0，这样在应用链式法则进行求导的时候，导数非常容易是 0，会导致 [梯度消失](http://106.15.37.116/2018/04/25/gradient-explosions-and-gradients-disappear/)。


  2. 而且，它的输出值是 0~1 ，没有负数，这样是不对称的，因此这会导致比如，输入的时候我还是有负值的，但是我下一层的输出就全是大于 0 的了。**但是这个具体会有什么影响？**




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/m01KiAC5Ai.png?imageslim">
</p>

### tanh 函数


这个比 sigmoid 好一点


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/LEfeEB067C.png?imageslim">
</p>




## logistic sigmoid 与双曲正切函数


在引入整流线性单元之前，大多数神经网络使用 logistic sigmoid 激活函数：<span style="color:red;">嗯。</span>

$$
g(z) = \sigma(z)
$$

或者是双曲正切激活函数

$$
g(z) = \text{tanh}(z).
$$

这些激活函数紧密相关，因为 $\text{tanh}(z)=2\sigma(2z)-1$。

我们已经看过 sigmoid  单元作为输出单元用来预测二值型变量取值为 1 的概率。与分段线性单元不同，sigmoid 单元在其大部分定义域内都饱和——当 $z$ 取绝对值很大的正值时，它们饱和到一个高值，当 $z$ 取绝对值很大的负值时，它们饱和到一个低值，并且仅仅当 $z$ 接近 0 时它们才对输入强烈敏感。sigmoid 单元的广泛饱和性会使得基于梯度的学习变得非常困难。因为这个原因，现在不鼓励将它们用作前馈网络中的激活函数。当使用一个合适的代价函数来抵消 sigmoid 的饱和性时，它们作为输出单元可以与基于梯度的学习相兼容。<span style="color:red;">嗯嗯，好的。</span>

当必须要使用 sigmoid  激活函数时，双曲正切激活函数通常要比 logistic sigmoid 函数表现更好。<span style="color:red;">什么时候一定要使用 sigmoid 激活函数？是下一段说的这些场景吗？</span>在 $\text{tanh}(0)=0$ 而 $\sigma(0)=\frac{1}{2}$ 的意义上，它更像是单位函数。因为 $\text{tanh}$ 在 0 附近与单位函数类似，训练深层神经网络 $\hat{y}=\boldsymbol w^\top \text{tanh}(\boldsymbol U^\top \text{tanh}(\boldsymbol V^\top \boldsymbol x))$ 类似于训练一个线性模型 $\hat{y}= \boldsymbol w^\top \boldsymbol U^\top \boldsymbol V^\top \boldsymbol x$，只要网络的激活能够被保持地很小。这使得训练 $\text{tanh}$ 网络更加容易。<span style="color:red;">嗯。没有很明白。</span>


sigmoid 激活函数在除了前馈网络以外的情景中更为常见。循环网络、许多概率模型以及一些自编码器有一些额外的要求使得它们不能使用分段线性激活函数，<span style="color:red;">为什么这些场景不能使用分段线性激活函数？</span>并且使得 sigmoid 单元更具有吸引力，尽管它存在饱和性的问题。


## 为什么 Tanh 收敛速度比 Sigmoid 快？

首先看如下两个函数的求导：

$$
\tanh ^{\prime}(x)=1-\tanh (x)^{2} \in(0,1)
$$

$$
s^{\prime}(x)=s(x) *(1-s(x)) \in\left(0, \frac{1}{4}\right]
$$

由上面两个公式可知 $tanh(x)$ 梯度消失的问题比 $sigmoid$ 轻，所以 Tanh 收敛速度比 Sigmoid 快。<span style="color:red;">什么叫比 $sigmoid$ 轻？是看的导数的值域吗？</span>




## 还有的一个问题

输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如 Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时 loss 函数中的 log 操作能够抵消其梯度消失的影响[1]）、LSTM里的 gate 函数。


# 相关

- 《百面机器学习》
- 《深度学习》花书
