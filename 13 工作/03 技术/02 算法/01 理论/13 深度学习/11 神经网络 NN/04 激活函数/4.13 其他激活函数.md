


## 其他激活函数


也存在许多其他种类的激活函数，但它们并不常用。

一般来说，很多种类的可微函数都表现得很好。许多未发布的激活函数与流行的激活函数表现得一样好。为了提供一个具体的例子，作者在 MNIST 数据集上使用 $\boldsymbol h=\cos(\boldsymbol W\boldsymbol x+\boldsymbol b)$ 测试了一个前馈网络，并获得了小于 $1\%$ 的误差率，这可以与更为传统的激活函数获得的结果相媲美。在新技术的研究和开发期间，通常会测试许多不同的激活函数，并且会发现许多标准方法的变体表现非常好。这意味着，通常新的激活函数类型只有在被明确证明能够提供显著改进时才会被发布。新的激活函数类型如果与已有的激活函数表现大致相当的话，那么它们是非常常见的，不会引起别人的兴趣。<span style="color:red;">嗯，大致相当的变体吸引力不是那么大。不过，这些变体存在的意义是什么呢？感觉变体本身还没有得到一个概括性的东西，比如泛函啥的。</span>

列出文献中出现的所有激活函数类型是不切实际的。我们只对一些特别有用和独特的类型进行强调。

其中一种是完全没有激活函数 $g(z)$。也可以认为这是使用单位函数作为激活函数的情况。我们已经看过线性单元可以用作神经网络的输出。它也可以用作激活函数。如果神经网络的每一层都仅由线性变换组成，那么网络作为一个整体也将是线性的。然而，神经网络的一些层是纯线性也是可以接受的。考虑具有 $n$ 个输入和 $p$ 个输出的神经网络层 $\boldsymbol h=g(\boldsymbol W^\top \boldsymbol x+\boldsymbol b)$。我们可以用两层来代替它，一层使用权重矩阵 $\boldsymbol U$，另一层使用权重矩阵 $\boldsymbol V$。如果第一层没有激活函数，那么我们对基于 $\boldsymbol W$ 的原始层的权重矩阵进行因式分解。分解方法是计算 $\boldsymbol h=g(\boldsymbol V^\top \boldsymbol U^\top \boldsymbol x+\boldsymbol b)$。如果 $U$ 产生了 $q$ 个输出，那么 $\boldsymbol U$ 和 $V$ 一起仅包含 $(n+p)q$ 个参数，而 $\boldsymbol W$ 包含 $np$ 个参数。如果 $q$ 很小，这可以在很大程度上节省参数。这是以将线性变换约束为低秩的代价来实现的，但这些低秩关系往往是足够的。线性激活函数因此提供了一种减少网络中参数数量的有效方法。<span style="color:red;">这个真的是有效的吗？有副作用吗？平时有这样搭建网络的吗？而且，q 不能很小吧？</span>


softmax 单元是另外一种经常用作输出的单元（如 6.2.2.3 节中所描述的），但有时也可以用作激活函数。softmax单元很自然地表示具有 $k$ 个可能值的离散型随机变量的概率分布，所以它们可以用作一种开关。这些类型的激活函数通常仅用于明确地学习操作内存的高级结构中，将在 10.12 节中描述。<span style="color:red;">哇塞，用作开关，有些厉害。嗯，什么是用于明确的学习操作内存的高级结构？补充下。</span>

其他一些常见的激活函数类型包括：

- 径向基函数：$h_i = \exp \left (-\frac{1}{\sigma_i^2}|| \boldsymbol W_{:,i}-\boldsymbol x||^2 \right )$。这个函数在 $\boldsymbol x$ 接近模板 $\boldsymbol W_{:,i}$ 时更加活跃。因为它对大部分 $\boldsymbol x$ 都饱和到 0，因此很难优化。
- $\textbf{softplus}$ 函数：$g(a)=\zeta(a)=\log(1+e^a)$。这是整流线性单元的平滑版本，由 {Dugas01}引入用于函数近似，由 {Nair-2010-small}引入用于无向概率模型的条件分布。{Glorot+al-AI-2011-small}比较了 softplus 和整流线性单元，发现后者的结果更好。通常不鼓励使用 softplus函数。softplus表明激活函数类型的性能可能是非常反直觉的——因为它处处可导或者因为它不完全饱和，人们可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。<span style="color:red;">嗯，为什么是反直觉的，为什么会反直觉。想了解下。</span>
- 硬双曲正切函数：它的形状和 $\text{tanh}$ 以及整流线性单元类似，但是不同于后者，它是有界的，$g(a)=\max(-1, \min(1,a))$。它由 {Collobert04}引入。


激活函数的设计仍然是一个活跃的研究领域，许多有用的激活函数类型仍有待发现。<span style="color:red;">还有很多吗？到底有多少？</span>




# 相关

- 《深度学习》花书
