---
title: 4.12 ReLU
toc: true
date: 2019-08-31
---


# ReLU

ReLU激活函数的形式为：

$$
f(z)=\max (0, z)\tag{9.7}
$$

值域为 $[0,+\infty)$；

对应的导函数为：

$$
f^{\prime}(z)=\left\{\begin{array}{l}{1, z>0} \\ {0, z \leqslant 0}\end{array}\right.\tag{9.8}
$$



函数图像如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/JXUQebXcMFfq.png?imageslim">
</p>


ReLU 也有的一个问题：

非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是 Sigmoid，它的导数在 x 为比较大的正值和比较小的负值时都会接近于 0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为 0，因此处处饱和，无法作为激活函数。ReLU在 x>0时导数恒为 1，因此对于再大的正值也不会饱和。但同时对于 x<0，其梯度恒为 0，这时候它也会出现饱和的现象（在这种情况下通常称为 dying ReLU）。Leaky ReLU[3]和 PReLU[4]的提出正是为了解决这一问题。


## Leaky ReLU

Leak Relu 激活函数

函数定义为：

$$
f(x) =  \left\{
\begin{aligned}
ax, \quad x<0 \\
x, \quad x>0
\end{aligned}
\right.
$$

值域为 $(-\infty,+\infty)$。

图像如下（$a = 0.5$）：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/NvHfkJ7nPi7X.png?imageslim">
</p>


### ReLU 和 Leaky ReLU


ReLU（Rectified linear unit）是现在最常用的，在卷积网络中基本上是标配。

它有下面几个特点：




  * 不存在 saturate（饱和）区域


  * 收敛速度比 sigmoid / tanh 函数快，这个是在实践中证明了的。


  * 计算高效简单


但是它也有一个缺点：


  * 如果你的输入不巧全部在 ReLU 的左侧，即 Dead Area，那么这个神经元就挂掉了，再也没法激活，在后向传播时，相应的神经元的参数都不会更新。但是呢，这个发生的概率很小，因为是一批一批训练的。所以虽然可能很脆弱，但是大家都在用。


所以，为了能够使它不至于挂掉，改进出了 Leaky ReLU：


  * 在小于 0 的时候，斜率为很小的一个比如 0.1，这样在反向传播的时候也能少许的改变一下权重。




![](http://images.iterate.site/blog/image/180728/Hm8c961KBG.png?imageslim){ width=55% }




### ELU


所有的 LU 相关的都是从 ReLU 出来的，这个 ELU 也是对 ReLU 的左侧修改了下。

**需要补充**![](http://images.iterate.site/blog/image/180728/9A9glHe4ig.png?imageslim){ width=55% }



## 使用 ReLu 激活函数的优点？

- 在区间变动很大的情况下，ReLu 激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。
- sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和 Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。<span style="color:red;">什么是梯度弥散？跟梯度消失和梯度爆炸有什么关系吗？为什么 ReLU 没有梯度弥散？</span>
- 需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。<span style="color:red;">不训练跟稀疏性有关系吗？稀疏不是指的这个吧？</span>


## 怎样理解 Relu（< 0 时）是非线性激活函数？

Relu 激活函数图像如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/4GwSuR2CcXNG.png?imageslim">
</p>



根据图像可看出具有如下特点：

- 单侧抑制；
- 相对宽阔的兴奋边界；<span style="color:red;">什么叫相对广阔的兴奋边界？相对于谁？有多广阔？</span>
- 稀疏激活性；
  - ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。
  - 因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。
  - **稀疏激活性**：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $x<0$ 时，ReLU 硬饱和，而当 $x>0$ 时，则不存在饱和问题。ReLU 能够在 $x>0$ 时保持梯度不衰减，从而缓解梯度消失问题。<span style="color:red;">什么意思？这个稀疏激活是一件好事吗？保持梯度不衰减为什么只是缓解了梯度消失？还会有梯度消失吗？</span>


## 为什么有不可微的点还是可以用呢？

整流线性单元  $g(z)=\max\{0, z\}$  并不是在所有的输入点上都是可微的，在 $z=0$ 处不可微。这似乎使得 $g$ 对于基于梯度的学习算法无效。

在实践中，梯度下降对这些机器学习模型仍然表现得足够好。部分原因是神经网络训练算法通常不会达到代价函数的局部最小值，而是仅仅显著地减小它的值，<span style="color:red;">不会到 0 吗？</span>

如图 4.3 所示。因为我们不再期望训练能够实际到达梯度为 $\bm{0}$ 的点，所以代价函数的最小值对应于梯度未定义的点是可以接受的。<span style="color:red;">什么意思？</span>不可微的激活函数通常只在少数点上不可微。一般来说，函数 $g(z)$ 具有左导数和右导数，左导数定义为紧邻在 $z$ 左边的函数的斜率，右导数定义为紧邻在 $z$ 右边的函数的斜率。只有当函数在 $z$ 处的左导数和右导数都有定义并且相等时，函数在 $z$ 点处才是可微的。

神经网络中用到的函数通常对左导数和右导数都有定义。在 $g(z)=\max\{0,z\}$ 的情况下，在 $z=0$ 处的左导数是 0，右导数是 1。神经网络训练的软件实现通常返回左导数或右导数的其中一个，而不是报告导数未定义或产生一个错误。这可以通过观察到在数字计算机上基于梯度的优化总是会受到数值误差的影响来启发式地给出理由。当一个函数被要求计算 $g(0)$ 时，底层值真正为 0 是不太可能的。相对的，它可能是被舍入为 0 的一个小量 $\epsilon$。在某些情况下，理论上有更好的理由，但这些通常对神经网络训练并不适用。重要的是，在实践中，我们可以放心地忽略下面描述的激活函数激活函数的不可微性。<span style="color:red;">嗯，这一段讲的很好呀，好。</span>



## 整流线性单元及其扩展


整流线性单元使用激活函数 $g(z)=\max\{0, z\}$。

整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。这使得只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数处处为 1。这意味着相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用。<span style="color:red;">为什么相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用？</span>

整流线性单元通常作用于仿射变换之上：

$$
\boldsymbol h = g(\boldsymbol W^\top \boldsymbol x + \boldsymbol b).
$$

当初始化仿射变换的参数时，可以将 $\boldsymbol b$ 的所有元素设置成一个小的正值，例如 0.1。这使得整流线性单元很可能初始时就对训练集中的大多数输入呈现激活状态，并且允许导数通过。

有很多整流线性单元的扩展存在。大多数这些扩展的表现比得上整流线性单元，并且偶尔表现得更好。<span style="color:red;">。。偶尔表现得更好</span>

整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活为零的样本。整流线性单元的各种扩展保证了它们能在各个位置都接收到梯度。<span style="color:red;">嗯，这种保证是有必要的吗？什么场景是有必要的？</span>

整流线性单元的三个扩展基于当 $z_i<0$ 时使用一个非零的斜率 $\alpha_i$：$h_i =g(\boldsymbol z, \boldsymbol \alpha)_i = \max(0, z_i) + \alpha_i \min(0, z_i)$。

- 绝对值整流固定 $\alpha_i=-1$ 来得到 $g(z)=|z|$。<span style="color:red;">为何要这样？$g(z)=|z|$？这感觉有点突兀。。</span>它用于图像中的对象识别，其中寻找在输入照明极性反转下不变的特征是有意义的。<span style="color:red;">嗯，那么它只是用在某一层吗？还是所有的层？</span>整流线性单元的其他扩展比这应用地更广泛。
- 渗漏整流线性单元将 $\alpha_i$ 固定成一个类似 0.01的小值，
- 参数化整流线性单元或者 $\textbf{PReLU}$ 将 $\alpha_i$ 作为学习的参数。<span style="color:red;">。。怎么学习 $\alpha_i$ ？补充下。</span>


maxout 单元进一步扩展了整流线性单元。maxout 单元将 $\boldsymbol z$ 划分为每组具有 $k$ 个值的组，而不是使用作用于每个元素的函数 $g(z)$ 。<span style="color:red;">怎么划分为具有 $k$ 个值的组的？怎么定 $k$ 的？怎么划分的？ </span>每个 maxout 单元则输出每组中的最大元素：<span style="color:red;">这个是个什么操作？</span>

$$
g(\boldsymbol z)_i = \underset{j\in \mathbb G^{(i)}}{\max} z_j
$$

这里 $\mathbb G^{(i)}$ 是组 $i$ 的输入索引集 $\{(i-1)k+1, \ldots, ik\}$。这提供了一种方法来学习对输入 $\boldsymbol x$ 空间中多个方向响应的分段线性函数。<span style="color:red;">？没明白？</span>

maxout 单元可以学习具有多达 $k$ 段的分段线性的凸函数。maxout 单元因此可以视为学习激活函数本身而不仅仅是单元之间的关系。使用足够大的 $k$，maxout单元可以以任意的精确度来近似任何凸函数。特别地，具有两块的 maxout 层可以学习实现和传统层相同的输入 $\boldsymbol x$ 的函数，这些传统层可以使用整流线性激活函数、绝对值整流、渗漏整流线性单元 或参数化整流线性单元，或者可以学习实现与这些都不同的函数。maxout 层的参数化当然也将与这些层不同，所以即使是 maxout 学习去实现和其他种类的层相同的 $\boldsymbol x$ 的函数这种情况下，学习的机理也是不一样的。<span style="color:red;">感觉非常厉害，但是还是理解的不够。</span>

每个 maxout 单元现在由 $k$ 个权重向量来参数化，而不仅仅是一个，所以 maxout 单元通常比整流线性单元需要更多的正则化。<span style="color:red;">是呀。</span>如果训练集很大并且每个单元的块数保持很低的话，它们可以在没有正则化的情况下工作得不错。

<span style="color:red;">这个 maxout 单元有使用过吗？</span>

maxout 单元还有一些其他的优点。在某些情况下，要求更少的参数可以获得一些统计和计算上的优点。具体来说，如果由 $n$ 个不同的线性过滤器描述的特征可以在不损失信息的情况下，用每一组 $k$ 个特征的最大值来概括的话，那么下一层可以获得 $k$ 倍更少的权重数 。<span style="color:red;">？没明白，什么是线性过滤器？什么是用每一组 $k$ 个特征的最大值来概括？</span>

因为每个单元由多个过滤器驱动，maxout 单元具有一些冗余来帮助它们抵抗一种被称为灾难遗忘的现象，这个现象是说神经网络忘记了如何执行它们过去训练的任务。<span style="color:red;">什么是灾难遗忘？</span>

整流线性单元和它们的这些扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。使用线性行为更容易优化的一般性原则同样也适用于除深度线性网络以外的情景。循环网络可以从序列中学习并产生状态和输出的序列。当训练它们时，需要通过一些时间步来传播信息，当其中包含一些线性计算（具有大小接近 1 的某些方向导数）时，这会更容易。作为性能最好的循环网络结构之一，LSTM 通过求和在时间上传播信息，这是一种特别直观的线性激活。它将在第 10.10 节中进一步讨论。

<span style="color:red;">maxout 真的有在使用吗？要怎么用？</span>


# 相关

- 《百面机器学习》
- 《深度学习》花书
