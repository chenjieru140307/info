---
title: 8.11 反向传播算法
toc: true
date: 2018-06-27 08:44:44
---

# 反向传播算法

反向传播 error BackPropagation BP

反向传播是迄今最成功的神经网络学习算法。

## 说明如下

给定训练集 $D=\left\{\left(\boldsymbol{x}_{1}, \boldsymbol{y}_{1}\right)\right.\left(\boldsymbol{x}_{2}, \boldsymbol{y}_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, \boldsymbol{y}_{m}\right) \}$ ，$\boldsymbol{x}_{i} \in \mathbb{R}^{d}$ ，$\boldsymbol{y}_{i} \in \mathbb{R}^{l}$ ，即输入示例由 $d$ 个属性描述，输出 $l$ 维实值向量。


为便于讨论，下图给出了一个拥有 $d$ 个输入神经元、$l$ 个输出神经元、$q$ 个隐层神经元的多层前馈网络结构，其中：

- 输出层第 $j$ 个神经元的阈值 用 $\theta_j$ 表示
- 隐层第 $h$ 个神经元的阈值用 $\gamma_h$  表示
- 输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间的连接权为 $v_{ih}$
- 隐层第 $h$ 个神经元与输出层第 $j$ 个神经元之间的连接权为 $w_{hj}$

记：

- 隐层第 $h$ 个神经元接收到的输入为 $\alpha_h=\sum_{i=1}^{d}v_{ih}x_i$
- 输出层第 $j$ 个神经元接收到的输入为 $\beta_j=\sum_{h=1}^{q}w_{hj}b_h$ ，其中 $b_h$ 为隐层第 $h$ 个神经元的输出

假设隐层和输出层神经元都使用 Sigmoid 函数：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180626/giFDElh243.png?imageslim">
</p>

对训练例 $\left(\boldsymbol{x}_{k}, \boldsymbol{y}_{k}\right)$ ，假定神经网络的输出为 $\hat{\boldsymbol{y}}_{k}=\left(\hat{y}_{1}^{k}, \hat{y}_{2}^{k}, \ldots, \hat{y}_{l}^{k}\right)$，即

$$
\hat{y}_{j}^{k}=f\left(\beta_{j}-\theta_{j}\right)\tag{5.3}
$$

则网络在 $\left(\boldsymbol{x}_{k}, \boldsymbol{y}_{k}\right)$ 上的均方误差为:


$$
E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(\hat{y}_{j}^{k}-y_{j}^{k}\right)^{2}\tag{5.4}
$$



上图的网络中有 $(d + $l$ + 1)q + l$ 个参数需确定：

- 输入层到隐层的 $d\times q$ 个权值
- 隐层到输出层的 $q\times l$ 个权值
- $q$ 个隐层神经元的阈值
- $l$ 个输出层神经元的阈值

BP是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计，任意参数 $v$ 的更新估计式为：

$$
v \leftarrow v+\Delta v\tag{5.5}
$$

下面我们以上图中隐层到输出层的连接权 $w_{hj}$ 为例来进行推导.

## 推导

BP 算法基于梯度下降(gradient descent)策略，以目标的负梯度方向对参数进行调整。对式(5.4)的误差 $E_k$，给定学习率 $\eta$ ，有


$$
\Delta w_{h j}=-\eta \frac{\partial E_{k}}{\partial w_{h j}}\tag{5.6}
$$


注意到 $w_{hj}$ 先影响到第 $j$ 个输出层神经元的输入值 $\beta_j$ ，再影响到其输出值 $\hat{y}_j^k$ ， 然后影响到 $E_k$，有

$$
\frac{\partial E_{k}}{\partial w_{h j}}=\frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \cdot \frac{\partial \beta_{j}}{\partial w_{h j}}\tag{5.7}
$$



根据 $\beta_j$ 的定义，显然有

$$
\frac{\partial \beta_{j}}{\partial w_{h j}}=b_{h}\tag{5.8}
$$


图 5.2 中的 Sigmoid 函数有一个很好的性质：

<span style="color:red;">这个性质的推导要补充下。</span>


$$
f^{\prime}(x)=f(x)(1-f(x))\tag{5.9}
$$


于是根据式 (5.4) 和 (5.3)，有

$$
\begin{aligned} g_{j} &=-\frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \\ &=-\left(\hat{y}_{j}^{k}-y_{j}^{k}\right) f^{\prime}\left(\beta_{j}-\theta_{j}\right) \\ &=\hat{y}_{j}^{k}\left(1-\hat{y}_{j}^{k}\right)\left(y_{j}^{k}-\hat{y}_{j}^{k}\right) \end{aligned}\tag{5.10}
$$


将式 (5.10) 和 (5.8) 代入式 (5.7)，再代入式 (5.6)，就得到了 BP 算法中关于 $w_{hj}$ 的更新公式

$$
\Delta w_{h j}=\eta g_{j} b_{h}\tag{5.11}
$$


类似可得

$$
\Delta \theta_{j}=-\eta g_{j}\tag{5.12}
$$
$$
\Delta v_{i h}=\eta e_{h} x_{i}\tag{5.13}
$$

$$
\Delta \gamma_{h}=-\eta e_{h}\tag{5.14}
$$

> $$\Delta \theta_j = -\eta g_j$$
> [推导]：因为
> $$\Delta \theta_j = -\eta \cfrac{\partial E_k}{\partial \theta_j}$$
> 又
> $$
> \begin{aligned}
> \cfrac{\partial E_k}{\partial \theta_j} &= \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot\cfrac{\partial \hat{y}_j^k}{\partial \theta_j} \\
> &= (\hat{y}_j^k-y_j^k) \cdot f’(\beta_j-\theta_j) \cdot (-1) \\
> &= -(\hat{y}_j^k-y_j^k)f’(\beta_j-\theta_j) \\
> &= g_j
> \end{aligned}
> $$
> 所以
> $$\Delta \theta_j = -\eta \cfrac{\partial E_k}{\partial \theta_j}=-\eta g_j$$

> $$\Delta v_{ih} = \eta e_h x_i$$
> [推导]：因为
> $$\Delta v_{ih} = -\eta \cfrac{\partial E_k}{\partial v_{ih}}$$
> 又
> $$
> \begin{aligned}
> \cfrac{\partial E_k}{\partial v_{ih}} &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot \cfrac{\partial b_h}{\partial \alpha_h} \cdot \cfrac{\partial \alpha_h}{\partial v_{ih}} \\
> &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot \cfrac{\partial b_h}{\partial \alpha_h} \cdot x_i \\
> &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot f’(\alpha_h-\gamma_h) \cdot x_i \\
> &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot w_{hj} \cdot f’(\alpha_h-\gamma_h) \cdot x_i \\
> &= \sum_{j=1}^{l} (-g_j) \cdot w_{hj} \cdot f’(\alpha_h-\gamma_h) \cdot x_i \\
> &= -f’(\alpha_h-\gamma_h) \cdot \sum_{j=1}^{l} g_j \cdot w_{hj}  \cdot x_i\\
> &= -b_h(1-b_h) \cdot \sum_{j=1}^{l} g_j \cdot w_{hj}  \cdot x_i \\
> &= -e_h \cdot x_i
> \end{aligned}
> $$
> 所以
> $$\Delta v_{ih} = -\eta \cdot -e_h \cdot x_i=\eta e_h x_i$$


> $$\Delta \gamma_h= -\eta e_h$$
> [推导]：因为
> $$\Delta \gamma_h = -\eta \cfrac{\partial E_k}{\partial \gamma_h}$$
> 又
> $$
> \begin{aligned}
> \cfrac{\partial E_k}{\partial \gamma_h} &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot \cfrac{\partial b_h}{\partial \gamma_h} \\
> &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot f’(\alpha_h-\gamma_h) \cdot (-1) \\
> &= -\sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot w_{hj} \cdot f’(\alpha_h-\gamma_h)\\
> &=e_h
> \end{aligned}
> $$
> 所以
> $$\Delta \gamma_h= -\eta e_h$$

式 (5.13) 和 (5.14) 中


$$
\begin{aligned} e_{h} &=-\frac{\partial E_{k}}{\partial b_{h}} \cdot \frac{\partial b_{h}}{\partial \alpha_{h}} \\ &=-\sum_{j=1}^{l} \frac{\partial E_{k}}{\partial \beta_{j}} \cdot \frac{\partial \beta_{j}}{\partial b_{h}} f^{\prime}\left(\alpha_{h}-\gamma_{h}\right) \\ &=\sum_{j=1}^{l} w_{h j} g_{j} f^{\prime}\left(\alpha_{h}-\gamma_{h}\right) \\ &=b_{h}\left(1-b_{h}\right) \sum_{j=1}^{l} w_{h j} g_{j} \end{aligned}\tag{5.15}
$$




学习率 $\eta\in (0,1)$ 控制着算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛速度又会过慢。有时为了做精细调节，可令式 (5.11) 与 (5.12) 使用 $\eta_1$ ，式 (5.13) 与 (5.14) 使用 $\eta_2$，两者未必相等.


图 5.8 给出了 BP 算法的工作流程。

对每个训练样例，BP算法执行以下操作：

- 先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；
- 然后计算输出层的误差(第 4-5行)，再将误差逆向传播至隐层神经元(第 6 行)，最后根据隐层神经元的误差来对连接权和阈值进行调整(第 7 行)
- 该迭代过程循环进行，直到达到某些停止条件为止，例如训练误差已达到一个很小的值。

图 5.9 给出了在 2 个属性、5个样本的西瓜数据上，随着训练轮数的增加，网络参数和分类边界的变化情况.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180626/ED5kmckjme.png?imageslim">
</p>
<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180626/Ffb1jifACi.png?imageslim">
</p>


需注意的是，BP算法的目标是要最小化训练集 $D$ 上的累积误差：

$$
E=\frac{1}{m} \sum_{k=1}^{m} E_{k}\tag{5.16}
$$

但我们上面介绍的“标准 BP 算法”每次仅针对一个训练样例更新连接权和阈值，也就是说，图 5.8中算法的更新规则是基于单个的 $E_k$ 推导而得。

如果类似地推导出基于累积误差最小化的更新规则，就得到了累积反向传播(accumulated error backpropagation)算法。累积 BP 算法与标准 BP 算法都很常用。

一般来说，标准 BP 算法每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新的效果可能出现“抵消”现象因此，为了达到同样的累积误差极小点，标准 BP 算法往往需进行更多次数的迭代。累积 BP 算法直接针对累积误差最小化，它在读取整个训练集 $d$ 一遍后才对参数进行更新, 其参数更新的频率低得多。但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这时标准 BP 往往会更快获得较好的解，尤其是在训练集 $D$ 非常大时更明显。



# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
