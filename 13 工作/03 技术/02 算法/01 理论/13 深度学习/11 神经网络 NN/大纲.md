
# 大纲

8.01 相关的计算拆分到反向传播里。



（9）归一化（normalization）：这个是最近才出来的概念，对应的激活函数是 SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如 Batch Normalization[9]。




一、神经网络基础部分
No1 wide_deep模型论文：

关于神经元、全连接网络之类的基础结构，想必每个 AI 学者都有了解。那么你是否真的了解全连接网络中深层与浅层的关系呢？来看看 wide_deep模型吧。这篇论文会使你对全连接有个更深刻的理解。

关于该模型的更多介绍可以参考论文：

https://arxiv.org/pdf/1606.07792.pdf

在 wide_deep模型中，wide模型和 deep 模型具有各自不同的分工。

—wide模型：一种浅层模型。它通过大量的单层网络节点，实现对训练样本的高度拟合性。它的缺点是泛化能力很差。

—deep模型：一种深层模型。它通过多层的非线性变化，使模型具有很好的泛化性。它的缺点是拟合度欠缺。

将二者结合起来——用联合训练方法共享反向传播的损失值来进行训练—可以使两个模型综合优点，得到最好的结果。

No2 wide_deep模型论文：

为什么 Adam 被广泛使用？光会用可不行，还得把原理看懂。这样出去喷一喷，才会显得更有面子。

Adam的细节请参阅论文《Adam: A Method for Stochastic Optimization》，该论文的链接网址是：

https://arxiv.org/pdf/1412.6980v8.pdf

No3 Targeted Dropout模型论文：

你还再用普通的 Dropout 吗？我已经开始用 Targeted Dropout了。比你的又快，又好。你不知道吧，赶紧学习一下。

Targeted Dropout不再像原有的 Dropout 那样按照设定的比例随机丢弃部分节点，而是对现有的神经元进行排序，按照神经元的权重重要性来丢弃节点。这种方式比随机丢弃的方式更智能，效果更好。更多理论见以下论文：

https://openreview.net/pdf?id=HkghWScuoQ


## 主要内容


- 感知机
- 损失函数
- 输出单元
- 激活函数
- 神经网络架构
- 梯度下降
- 反向传播


## 可以补充进来的

- [​神经网络求解新思路：OpenAI用线性网络计算非线性问题](https://36kr.com/p/5096070)
- [推荐｜“寻根问祖”深度学习只需六段代码](https://www.jianshu.com/p/83a838c14f77)




- [神经网络为什么可以（理论上）拟合任何函数？](https://www.zhihu.com/question/268384579/answer/428891447)

- [ICLR 最佳论文作者张驰原演讲全文：理解深度学习，为何我们需要重新思考泛化问题？](http://news.ifeng.com/a/20170426/50997786_0.shtml)


## 需要消化的
