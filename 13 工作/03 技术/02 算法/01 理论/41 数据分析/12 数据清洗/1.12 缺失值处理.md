
# 可以补充进来的

- 查看缺失值的方法补充下。
- 建模法要补充下。

# 缺失值处理

缺失值在实际数据中是不可避免的问题，有的人看到有缺失的数据就直接删除了，有的人直接赋予 0 值或者某一个特殊的值，那么到底该怎么处理呢？

对于不同的数据场景应该采取不同的策略，首先应该判断缺失值的分布情况。


## 我们常用的方法有以下几种：

1. 直接删除：适合缺失值数量较小，并且是随机出现的，删除它们对整体数据影响不大的情况
2. 使用一个全局常量填充：譬如将缺失值用“Unknown”等填充。缺点：效果不一定好，因为算法可能会把它识别为一个新的类别，一般很少用。<span style="color:red;">可以用吗？</span>
3. 使用均值或中位数代替：优点：不会减少样本信息，处理简单。缺点：当缺失数据不是随机数据时会产生偏差。对于正常分布的数据可以使用均值代替，如果数据是倾斜的，使用中位数可能更好。
4. 插补法
    1. 随机插补法：从总体中随机抽取某个样本代替缺失样本
    2. 多重插补法：通过变量之间的关系对缺失数据进行预测，利用蒙特卡洛方法生成多个完整的数据集，在对这些数据集进行分析，最后对分析结果进行汇总处理。<span style="color:red;">怎么利用蒙特卡洛方法生成多个完整数据集的？</span>
    3. 热平台插补：指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补。优点：简单易行，准去率较高。缺点：变量数量较多时，通常很难找到与需要插补样本完全相同的样本。但我们可以按照某些变量将数据分层，在层中对缺失值实用均值插补
    4. 拉格朗日差值法和牛顿插值法（简单高效，数值分析里的内容，数学公式以后再补）<span style="color:red;">怎么实现的？</span>
5. 建模法：可以用回归、使用贝叶斯形式化方法的基于推理的工具或决策树归纳确定。例如，利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。<span style="color:red;">具体是怎么实现的？</span>


以上方法各有优缺点，具体情况要根据实际数据分分布情况、倾斜程度、缺失值所占比例等等来选择方法。<span style="color:red;">什么是倾斜程度？</span>

一般而言，建模法是比较常用的方法，它根据已有的值来预测缺失值，准确率更高。<span style="color:red;">嗯，补充下。</span>



## 例子


sklearn 里有一个工具 Imputer 可以对缺失值进行插补。Imputer 类可以对缺失值进行均值插补、中位数插补或者某行/列出现的频率最高的值进行插补，也可以对不同的缺失值进行编码。**并且支持稀疏矩阵。**

```
import numpy as np
from sklearn.preprocessing import Imputer
#用均值插补缺失值
imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
imp.fit([[1, 2], [np.nan, 3], [7, 6]])
Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)
X = [[np.nan, 2], [6, np.nan], [7, 6]]
print(imp.transform(X))
[[ 4.          2.        ]
 [ 6.          3.666...]
 [ 7.          6.        ]]

#对稀疏矩阵进行缺失值插补
import scipy.sparse as sp
X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])
imp = Imputer(missing_values=0, strategy='mean', axis=0)
imp.fit(X)
Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)
X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])
print(imp.transform(X_test))
[[ 4.          2.        ]
 [ 6.          3.666...]
 [ 7.          6.        ]]
```

在稀疏矩阵中，缺失值被编码为 0 存储为矩阵中，这种格式是适合于缺失值比非缺失值多得多的情况。此外，Imputer类也可以用于 Pipeline 中



　　Imputor类的参数：*class* `sklearn.preprocessing.``Imputer`(*missing_values='NaN'*, *strategy='mean'*, *axis=0*, *verbose=0*, *copy=True*)[
](https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/preprocessing/imputation.py#L64)

> **missing_values** : int或"NaN"，默认 NaN（String类型）
>
> **strategy** : string, 默认为 mean，可选则 mean、median、most_frequent
>
> **axis** :int, 默认为 0（axis = 0，对列进行插值；axis= 1，对行进行插值）
>
> **verbose** : int, 默认为 0
>
> **copy** : boolean, 默认为 True
>
> 　　True：会创建一个 X 的副本
>
> 　　False：在任何合适的地方都会进行插值。
>
> 　　但是以下四种情况，计算设置的 copy = Fasle，也会创建一个副本：
>
> 　　1.X不是浮点型数组
>
> 　　2.X是稀疏矩阵，而且 miss_value = 0
>
> 　　3.axis= 0，X被编码为 CSR 矩阵
>
> 　　　　　　 4.axis= 1，X被编码为 CSC 矩阵


举个实例(在用随机森林算法之前先用 Imputer 类进行处理)：


```py
import numpy as np

from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import Imputer
from sklearn.cross_validation import cross_val_score

rng = np.random.RandomState(0)

dataset = load_boston()
X_full, y_full = dataset.data, dataset.target
n_samples = X_full.shape[0]
n_features = X_full.shape[1]

# Estimate the score on the entire dataset, with no missing values
estimator = RandomForestRegressor(random_state=0, n_estimators=100)
score = cross_val_score(estimator, X_full, y_full).mean()
print("Score with the entire dataset = %.2f" % score)

# Add missing values in 75% of the lines
missing_rate = 0.75
n_missing_samples = np.floor(n_samples * missing_rate)
missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,
                                      dtype=np.bool),
                             np.ones(n_missing_samples,
                                     dtype=np.bool)))
rng.shuffle(missing_samples)
missing_features = rng.randint(0, n_features, n_missing_samples)

# Estimate the score without the lines containing missing values
X_filtered = X_full[~missing_samples, :]
y_filtered = y_full[~missing_samples]
estimator = RandomForestRegressor(random_state=0, n_estimators=100)
score = cross_val_score(estimator, X_filtered, y_filtered).mean()
print("Score without the samples containing missing values = %.2f" % score)

# Estimate the score after imputation of the missing values
X_missing = X_full.copy()
X_missing[np.where(missing_samples)[0], missing_features] = 0
y_missing = y_full.copy()
estimator = Pipeline([("imputer", Imputer(missing_values=0,
                                          strategy="mean",
                                          axis=0)),
                      ("forest", RandomForestRegressor(random_state=0,
                                                       n_estimators=100))])
score = cross_val_score(estimator, X_missing, y_missing).mean()
print("Score after imputation of the missing values = %.2f" % score)
```

结果：

```
Score with the entire dataset = 0.56
Score without the samples containing missing values = 0.48
Score after imputation of the missing values = 0.55
```



# 相关

- [机器学习基础与实践（一）----数据清洗](https://www.cnblogs.com/charlotte77/p/5606926.html)
