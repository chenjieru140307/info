
在探究不同范数的正则化表现之前，我们需要说明一下：

在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常 **只对权重** 做惩罚而不对偏置做正则惩罚。因为：

- 精确拟合偏置所需的数据通常比拟合权重少得多。<span style="color:red;">嗯。</span>每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。
- 另外，正则化偏置参数可能会导致明显的欠拟合。因此，我们使用向量 $\boldsymbol w$ 表示所有应受范数惩罚影响的权重，而向量 $\boldsymbol \theta$ 表示所有参数(包括 $\boldsymbol w$ 和无需正则化的参数)。<span style="color:red;">嗯。</span>


# 相关

- 《深度学习》花书
