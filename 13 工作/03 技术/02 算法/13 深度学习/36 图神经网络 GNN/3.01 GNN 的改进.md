
# GNN改进&变种

对 GNN 的改进分为如下三种：

- 图类型改进；
- 传播步骤改进；
- 训练方法改进；

## 图类型

原始 GNN 的输入图是带有标记信息的节点和无向边。以下是几种不同的图类型：

- 有向图：信息更紧密；
- 异质图：包含几种不同的节点，最简单的处理方式是 one-hot feature vector；
- 带边信息的图：每个边也有信息、权值和类型，我们可以将边也变成节点，或者当传播时在不同的边上使用不同的权重矩阵；



## 传播步骤的改进

对 GNN 而言，传播步骤是非常重要的，它可以获得节点（边）的隐藏状态。传播步骤使用的方法通常是不同的聚合函数（在每个节点的邻居收集信息）和特定的更新函数（更新节点隐藏状态）。

### 卷积操作

在图上的卷积操作通常可以分为光谱方法和非光谱方法。

- 光谱方法：光谱方法在图的光谱表示上运行，学习的过滤器是基于拉普拉斯特征权重的，因此与图的结构紧密相关，难以泛化；
- 非光谱方法：直接在图上定义卷积，对紧密相近的节点进行操作，主要的挑战就是非光谱方法在不同大小的邻居上的定义和保持 CNN 的局部变量，非光谱方法具有点分类和图分类两种；


### Gate闸门机制

在 GNN 中使用门限机制是为了减少限制并改善长期的图结构上的信息传递。

### Attention注意力机制


注意力机制已经成功的应用于基于时序的任务，例如机器翻译、机器阅读等。GAT在传播步骤使用了注意力机制，会通过节点的邻居来计算节点的隐藏状态，通过自注意策略。

### Skip connection

许多机器学习的应用都会使用多层神经网络，然而多层神经网络不一定更好，因为误差会逐层累积，最直接定位问题的方法，残差网络，是来自于计算机视觉。即使使用了残差网络，多层 GCN 依旧无法像 2 层 GCN 一样表现良好。

有一种方法是使用高速路 GCN（Highway GCN），它像高速路网络一样使用逐层门限。

## 训练方法的改进


原始的图神经网络在训练和优化步骤有缺陷，它需要完整的图拉普拉斯，对大图而言计算力消耗大。更多的，层 L 上的节点嵌入是递归计算的，通过嵌入它的所有 L-1层的邻居。因此，单层节点是成倍增长的，因此对节点的计算消耗巨大。且 GCN 是对每个固定图进行独立训练的，因此泛化能力不好。

以下是几种改善方式：

1. GraphSAGE：
    - 将全图拉普拉斯替换为可学习聚合函数，是使用信息传递并生长到未见节点的关键，GraphSAGE还使用了邻居采样来避免接收域爆炸；
2. FastGCN：
    - FastGCN对采样算法做了更深的改进，FastGCN为每层直接采样接受域，而非对每个节点进行邻居采样；
3. control-variate based stochastic approximation：
    - 使用节点的历史激励作为控制随机数，此方法限制接受域为 1 跳邻居，但使用历史隐藏状态作为可接受最优化方法；
4. Co-Training GCN and Self-Training GCN：
    - 用于解决 GCN 需要许多额外的有标记数据及卷积过滤器的局部特征的限制，因此使用了此方法来扩大训练数据集，Co-Training方法为训练数据找到最近的邻居，Self-Training则采用了类似 boosting 的方法。


# 相关

- [GNN图神经网络综述](https://blog.csdn.net/qq_34911465/article/details/88524599)
