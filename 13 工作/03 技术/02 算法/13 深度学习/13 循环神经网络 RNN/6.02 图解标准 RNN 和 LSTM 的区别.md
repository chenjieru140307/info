

### 6.11.2 图解标准 RNN 和 LSTM 的区别

​	所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层，如下图所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/YBxm7aruD4IX.png?imageslim">
</p>

<span style="color:red;">嗯，理解，不过，这个 A 是一个神经元，那么一个层要怎么表示？而且 h 到底是输出给谁？</span>

​	LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/TFMApF799vve.png?imageslim">
</p>

<span style="color:red;">嗯，以前看这个有点云里雾里，现在明白了，知道数据的流通是什么样的，但是，为什么这个结构是合理的？怎么构造出来的？</span>


注：上图图标具体含义如下所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/KVeAuD5zxml8.png?imageslim">
</p>

上图中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。
