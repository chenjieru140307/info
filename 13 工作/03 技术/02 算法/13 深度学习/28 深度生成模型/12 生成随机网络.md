
## 生成随机网络

生成随机网络(generative stochastic network,GSN)(Bengio et al.,2014)是去噪自编码器的推广，除可见变量(通常表示为 x)之外，在生成马尔可夫链中还包括潜变量 h。

GSN由两个条件概率分布参数化，指定马尔可夫链的一步。

(1)p(x(k)|h(k))指示在给定当前潜在状态下如何产生下一个可见变量。这种“重建分布”也可以在去噪自编码器、RBM、DBN和 DBM 中找到。
(2)p(h(k)|h(k-1),x(k-1))指示在给定先前的潜在状态和可见变量下如何更新潜在状态变量。

去噪自编码器和 GSN 不同于经典的概率模型(有向或无向)，它们自己参数化生成过程，而不是通过可见和潜变量的联合分布的数学形式。相反，后者如果存在则隐式地定义为生成马尔可夫链的稳态分布。存在稳态分布的条件是温和的，并且需要与标准 MCMC 方法相同的条件(见第 17.3节)。这些条件是保证链混合的必要条件，但它们可能被某些过渡分布的选择(例如，如果它们是确定性的)所违反。

我们可以想象 GSN 不同的训练准则。由 Bengio et al.(2014)提出和评估的只对可见单元上对数概率的重建，如应用于去噪自编码器。通过将 x(0)=x夹合到观察到的样本并且在一些后续时间步处使生成 x 的概率最大化，即最大化 logp(x(k)=x|h(k))，其中给定 x(0)=x后，h(k)从链中采样。为了估计相对于模型其他部分的 logp(x(k)=x|h(k))的梯度，Bengio　et al.(2014)使用了在第 20.9节中介绍的重参数化技巧。

回退训练过程(在第 20.11.3节中描述)可以用来改善训练 GSN 的收敛性(Bengio et al.,2014)。

## 判别性 GSN

GSN的原始公式(Bengio et al.,2014)用于无监督学习和对观察数据 x 的 p(x)的隐式建模，但是我们可以修改框架来优化 p(y|x)。

例如，Zhou and Troyanskaya(2014)以如下方式推广 GSN：只反向传播输出变量上的重建对数概率，并保持输入变量固定。他们将这种方式成功应用于建模序列(蛋白质二级结构)，并在马尔可夫链的转换算子中引入(一维)卷积结构。重要的是要记住，对于马尔可夫链的每一步，我们需要为每个层生成新序列，并且该序列用于在下一时间步计算其他层的值(例如下面一个和上面一个)的输入。

因此，马尔可夫链确实不只是输出变量(与更高层的隐藏层相关联)，并且输入序列仅用于条件化该链，其中反向传播使得它能够学习输入序列如何条件化由马尔可夫链隐含表示的输出分布。因此这是在结构化输出中使用 GSN 的一个例子。

Z-hrer and Pernkopf(2014)引入了一个混合模型，通过简单地添加(使用不同的权重)监督和非监督成本即 y 和 x 的重建对数概率，组合了监督目标(如上面的工作)和无监督目标(如原始的 GSN)。Larochelle and Bengio(2008a)以前在 RBM 中就提出了这样的混合标准，他们展示了在这种方案下分类性能的提升。




# 相关

- 《深度学习》花书
