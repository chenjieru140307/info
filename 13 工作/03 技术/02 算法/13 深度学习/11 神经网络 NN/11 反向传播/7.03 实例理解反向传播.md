

# 实例理解反向传播

​	一个典型的三层神经网络如下所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/EaXwVkLoNOi8.png?imageslim">
</p>


其中 Layer $L_1$ 是输入层，Layer $L_2$ 是隐含层，Layer $L_3$ 是输出层。

假设输入数据集为 $D={x_1, x_2, ..., x_n}$，输出数据集为 $y_1, y_2, ..., y_n$。

如果输入和输出是一样，即为自编码模型。

如果原始数据经过映射，会得到不同于输入的输出，假设有如下的网络层：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/wtbSyzAewFPY.png?imageslim">
</p>


输入层包含神经元 $i_1, i_2$，偏置 $b_1$；隐含层包含神经元 $h_1, h_2$，偏置 $b_2$，输出层为  $o_1, o_2$，$w_i$ 为层与层之间连接的权重，激活函数为 $sigmoid$ 函数。对以上参数取初始值，如下图所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/XgYPPtehYMsG.png?imageslim">
</p>


其中：

- 输入数据 $i1=0.05, i2 = 0.10$
- 输出数据 $o1=0.01, o2=0.99$;
- 初始权重 $w1=0.15, w2=0.20, w3=0.25,w4=0.30, w5=0.40, w6=0.45, w7=0.50, w8=0.55$
- 目标：给出输入数据 $i1,i2$ ( $0.05$ 和 $0.10$ )，使输出尽可能与原始输出 $o1,o2$，( $0.01$ 和 $0.99$)接近。

**前向传播**

1. 输入层 --> 输出层

计算神经元 $h1​$ 的输入加权和：

$$
net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1\\
net_{h1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775
$$

神经元 $h1$ 的输出 $o1$ ：（此处用到激活函数为 sigmoid 函数）：

$$
out_{h1} = \frac{1}{1 + e^{-net_{h1}}} = \frac{1}{1 + e^{-0.3775}} = 0.593269992
$$

同理，可计算出神经元 $h2$ 的输出 $o1$：

$$
out_{h2} = 0.596884378
$$


2. 隐含层--> 输出层：  　　

计算输出层神经元 $o1$ 和 $o2​$ 的值：

$$
net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1
$$

$$
net_{o1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967
$$

$$
out_{o1} = \frac{1}{1 + e^{-net_{o1}}} = \frac{1}{1 + e^{1.105905967}} = 0.75136079
$$

这样前向传播的过程就结束了，我们得到输出值为 $[0.75136079 ,  0.772928465]$，与实际值 $[0.01 , 0.99]​$ 相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。

**反向传播**

1. 计算总误差

总误差：(这里使用 Square Error)

$$
E_{total} = \sum \frac{1}{2}(target - output)^2
$$

但是有两个输出，所以分别计算 $o1$ 和 $o2$ 的误差，总误差为两者之和：

$$
E_{o1} = \frac{1}{2}(target_{o1} - out_{o1})^2
= \frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083
$$

$$
E_{o2} = 0.023560026
$$

$$
E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109
$$

2. 隐含层 --> 输出层的权值更新：

以权重参数 $w5​$ 为例，如果我们想知道 $w5​$ 对整体误差产生了多少影响，可以用整体误差对 $w5​$ 求偏导求出：（链式法则）

$$
\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial w5}
$$

下面的图可以更直观的看清楚误差是怎样反向传播的：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/wCrEFnu1nCob.png?imageslim">
</p>

<span style="color:red;">求解的过程补充下。之前好像在别的地方看到过这个。</span>
