

# 反向传播和其他的微分算法


当我们使用前馈神经网络接收输入 $\boldsymbol x$ 并产生输出 $\hat{\boldsymbol y}$ 时，信息通过网络向前流动。输入 $\boldsymbol x$ 提供初始信息，然后传播到每一层的隐藏单元，最终产生输出 $\hat{\boldsymbol y}$。这称之为前向传播。


在训练过程中，前向传播可以持续向前直到它产生一个标量代价函数 $J(\boldsymbol \theta)$。

反向传播算法，经常简称为 $backprop$，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

计算梯度的解析表达式是很直观的，但是数值化地求解这样的表达式在计算上的代价可能很大。反向传播算法使用简单和廉价的程序来实现这个目标。

## 一些误解


- 反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习。
- 此外，反向传播经常被误解为仅适用于多层神经网络，但是原则上它可以计算任何函数的导数（对于一些函数，正确的响应是报告函数的导数是未定义的）。特别地，我们会描述如何计算一个任意函数 $f$ 的梯度 $\nabla_{\boldsymbol x} f(\boldsymbol x, \boldsymbol y)$，其中 $\boldsymbol x$ 是一组变量，我们需要它们的导数，而 $\boldsymbol y$ 是函数的另外一组输入变量，但我们并不需要它们的导数。在学习算法中，我们最常需要的梯度是代价函数关于参数的梯度，即 $\nabla_{\boldsymbol \theta} J(\boldsymbol \theta)$。许多机器学习任务需要计算其他导数，来作为学习过程的一部分，或者用来分析学得的模型。反向传播算法也适用于这些任务，不局限于计算代价函数关于参数的梯度。通过在网络中传播信息来计算导数的想法非常普遍，它还可以用于计算诸如多输出函数 $f$ 的 Jacobian 的值。我们这里描述的是最常用的情况，其中 $f$ 只有单个输出。<span style="color:red;">怎么用来计算多输出函数 $f$ 的 Jacobian 的值？</span>






# 相关

- 《深度学习》花书
