
# Attention 机制的本质思想

如果把 Attention 机制从上文讲述例子中的 Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂 Attention 机制的本质思想。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/0jCbdaSBQQM9.png?imageslim">
</p>

> Attention机制的本质思想

## 一种理解 对 Source 中元素的 Value 记性加权求和

我们可以这样来看待 Attention 机制（参考上图）：

将 Source 中的构成元素想象成是由一系列的`<Key,Value>`数据对构成，此时给定 Target 中的某个元素 Query，通过计算 Query 和各个 Key 的相似性或者相关性，得到每个 Key 对应 Value 的权重系数，然后对 Value 进行加权求和，即得到了最终的 Attention 数值。

所以本质上 Attention 机制是对 Source 中元素的 Value 值进行加权求和，而 Query 和 Key 用来计算对应 Value 的权重系数。即可以将其本质思想改写为如下公式：

$$
\text { Attention (Query, Source) }=\sum_{i=1}^{L_{x}} \text { Similarity (Query, Key_{i} ) } * \text { Value_{i} }
$$

其中，$L_x=||Source||$ 代表 Source 的长度，公式含义即如上所述。

上文所举的机器翻译的例子里，因为在计算 Attention 的过程中，Source中的 Key 和 Value 合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。

当然，从概念上理解，把 Attention 仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的 Value 值上，即权重代表了信息的重要性，而 Value 是其对应的信息。

## 另一种理解 软寻址

从上图可以引出另外一种理解，也可以将 Attention 机制看作一种软寻址（SoftAddressing）：

Source可以看作存储器内存储的内容，元素由地址 Key 和值 Value 组成，当前有个 Key=Query的查询，目的是取出存储器中对应的 Value 值，即 Attention 数值。通过 Query 和存储器内元素 Key 的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个 Key 地址都会取出内容，取出内容的重要性根据 Query 和 Key 的相似性来决定，之后对 Value 进行加权求和，这样就可以取出最终的 Value 值，也即 Attention 值。

所以不少研究人员将 Attention 机制看作软寻址的一种特例，这也是非常有道理的。

至于 Attention 机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：

- 第一个过程是根据 Query 和 Key 计算权重系数，
- 第二个过程根据权重系数对 Value 进行加权求和。

而第一个过程又可以细分为两个阶段：

- 第一个阶段根据 Query 和 Key 计算两者的相似性或者相关性；
- 第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将 Attention 的计算过程抽象为如下图展示的三个阶段。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/ULm1W8d2tPMv.png?imageslim">
</p>


> 三阶段计算 Attention 过程

在第一个阶段，可以引入不同的函数和计算机制，根据 Query 和某个 $Key_i$，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量 Cosine 相似性或者通过再引入额外的神经网络来求值，即如下方式：

- 点积： $\left.\text { Similarity (Query, Key }_{i}\right)=$ Query $\cdot K e y_{i}$
- Cosine 相似性：$\text {Similarity} (Query, Key_i ) =\frac{\text { Query} \cdot Key_{i} }{ \| \text { Query} \| \cdot \| Key_i\| }$
- MLP 网络：$\text{Similarity} (Query, Key_{i} ) =\operatorname{MLP}\left(\text { Query, } K e y_{i}\right)$



第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似 SoftMax 的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为 1 的概率分布；另一方面也可以通过 SoftMax 的内在机制更加突出重要元素的权重。

即一般采用如下公式计算：



$$
a_{i}=\operatorname{Softmax}\left(\operatorname{Sim}_{i}\right)=\frac{e^{sim _{i}}}{\sum_{j=1}^{L_{x}} e^{\operatorname{sim}_{j}}}
$$




第二阶段的计算结果 $a_i$ 即为 $value_i$ 对应的权重系数，然后进行加权求和即可得到 Attention 数值：

$$
\text { Attention (Query, Source) }=\sum_{i=1}^{L_{x}} a_{i} \cdot \text { Value }_{i}
$$

通过如上三个阶段的计算，即可求出针对 Query 的 Attention 数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。


# 相关

- [深度学习中的注意力模型（2017版）](https://zhuanlan.zhihu.com/p/37601161)
