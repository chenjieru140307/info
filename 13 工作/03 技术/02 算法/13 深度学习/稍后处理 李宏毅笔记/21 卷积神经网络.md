
![mark](http://images.iterate.site/blog/image/20190818/qXLVIXW3znXa.png?imageslim)

## 为什么用 CNN
![mark](http://images.iterate.site/blog/image/20190818/UkF2WHoiykUU.png?imageslim)

我们都知道 CNN 常常被用在影像处理上，如果你今天用 CNN 来做影像处理，当然也可以用一般的 neural network来做影像处理，不一定要用 CNN。比如说你想要做影像的分类，那么你就是 training 一个 neural network,input一张图片，那么你就把这张图片表示成里面的 pixel，也就是很长很长的 vector。output就是(假如你有 1000 个类别，output就是 1000 个 dimension)dimension。那我相信根据刚才那堂课内容，若给你一组 training data你都可以描作出来。

![mark](http://images.iterate.site/blog/image/20190818/mJC5wWdBgUoe.png?imageslim)

但是呢，我们现在会遇到的问题是这样的，实际上我们在 training neural network时，我们会期待说：在 network 的 structure 里面，每一个 neural 就是代表了一个最基本的 classifier，事实在文件上根据训练的结果，你有可能会得到很多这样的结论。举例来说：第一层的 neural 是最简单的 classifier，它做的事情就是 detain 有没有绿色出现，有没有黄色出现，有没有斜的条纹。

第二个 layer 是做比这个更复杂的东西，根据第一个 layer 的 output，它看到直线横线就是窗框的一部分，看到棕色纹就是木纹，看到斜条纹+灰色可能是很多的东西(轮胎的一部分等等)

再根据第二个 hidden layer的 outpost，第三个 hidden layer会做更加复杂的事情。

但现在的问题是这样的，当我们一般直接用 fully connect feedforward network来做影像处理的时候，往往我们会需要太多的参数，举例来说，假设这是一张 100 *100的彩色图(一张很小的 imgage)，你把这个拉成一个 vector，(它有多少个 pixel)，它有 100 *100 3的 pixel。
如果是彩色图的话，每个 pixel 需要三个 value 来描述它，就是 30000 维(30000 dimension)，那 input vector假如是 30000dimension，那这个 hidden layer假设是 1000 个 neural，那么这个 hidden layer的参数就是有 30000 *1000，那这样就太多了。那么 CNN 做的事就是简化 neural network的架构。我们把这里面一些根据人的知识，我们根据我们对影像就知道，某些 weight 用不上的，我们一开始就把它滤掉。不是用 fully connect feedforward network，而是用比较少的参数来做影像处理这件事。所以 CNN 比一般的 DNN 还要简单的。

等一下我们讲完会觉得发现说：你可能觉得 CNN 运作很复杂，但事实上它的模型是要比 DNN 还要更简单的。我们就是用 power-knowledge 去把原来 fully connect layer中一些参数拿掉就成了 CNN。

### Small region

![mark](http://images.iterate.site/blog/image/20190818/hsGMx0FDi4Dy.png?imageslim)

我们先来讲一下，为什么我们有可能把一些参数拿掉(为什么可以用比较少的参数可以来做影像处理这件事情)

这里有几个观察，第一个是在影像处理里面，我们说第一层的 hidden layer那些 neural 要做的事就是侦测某一种 pattern，有没有某一种 patter 出现。大部分的 pattern 其实要比整张的 image 还要小，对一个 neural 来说，假设它要知道一个 image 里面有没有某一个 pattern 出现，它其实是不需要看整张 image，它只要看 image 的一小部分。

举例来说，假设我们现在有一张图片，第一个 hidden layer的某一种 neural 的工作就是要侦测有没有鸟嘴的存在(有一些 neural 侦测有没有爪子的存在，有没有一些 neural 侦测有没有翅膀的存在，有没有尾巴的存在，合起来就可以侦测图片中某一只鸟)。假设有一个 neural 的工作是要侦测有没有鸟嘴的存在，那并不需要看整张图，其实我们只需要给 neural 看着一小红色方框的区域(鸟嘴)，它其实就可以知道说，它是不是一个鸟嘴。对人来说也是一样，看这一小块区域这是鸟嘴，不需要去看整张图才知道这件事情。所以，每一个 neural 连接到每一个小块的区域就好了，不需要连接到整张完整的图。

### Same Patterns

![mark](http://images.iterate.site/blog/image/20190818/qgUhnh97GdHm.png?imageslim)

第二个观察是这样子，同样的 pattern 在 image 里面，可能会出现在 image 不同的部分，但是代表的是同样的含义，它们有同样的形状，可以用同样的 neural，同样的参数就可以把 patter 侦测出来。

比如说，这张图里面有一张在左上角的鸟嘴，在这张图里面有一个在中央的鸟嘴，但是你并不需要说：我们不需要去训练两个不同的 detector，一个专门去侦测左上角的鸟嘴，一个去侦测中央有没有鸟嘴。如果这样做的话，这样就太冗了。我们不需要太多的冗源，这个 nerual 侦测左上角的鸟嘴跟侦测中央有没有鸟嘴做的事情是一样的。我们并不需要两个 neural 去做两组参数，我们就要求这两个 neural 用同一组参数，就样就可以减少你需要参数的量

### Subsampling
![mark](http://images.iterate.site/blog/image/20190818/D45pWAQVw86a.png?imageslim)

第三个是：我们知道一个 image 你可以做 subsampling，你把一个 image 的奇数行，偶数列的 pixel 拿掉，变成原来十分之一的大小，它其实不会影响人对这张 image 的理解。对你来说：这张 image 跟这张 image 看起来可能没有太大的差别。是没有太大的影响的，所以我们就可以用这样的概念把 image 变小，这样就可以减少你需要的参数。

## CNN架构

![mark](http://images.iterate.site/blog/image/20190818/XXncWW7uMHoB.png?imageslim)

所以整个 CNN 的架构是这样的，首先 input 一张 image 以后，这张 image 会通过 convolution layer，接下里做 max pooling这件事，然后在做 convolution，再做 max pooling这件事。这个 process 可以反复无数次，反复的次数你觉得够多之后，(但是反复多少次你是要事先决定的，它就是 network 的架构(就像你的 neural 有几层一样)，你要做几层的 convolution，做几层的 Max Pooling，你再定 neural 架构的时候，你要事先决定好)。你做完决定要做的 convolution 和 Max Pooling以后，你要做另外一件事，这件事情叫做 flatten，再把 flatten 的 output 丢到一般 fully connected feedforward network，然后得到影像辨识的结果。

![mark](http://images.iterate.site/blog/image/20190818/R5Jg31vwAD8B.png?imageslim)

我们刚才讲基于三个对影像处理的观察，所以设计了 CNN 这样的架构。

第一个观察是，要生成一个 pattern，不要看整张的 image，你只需要看 image 的一小部分。第二是，通用的 pattern 会出现在一张图片的不同的区域。第三个是，我们可以做 subsampling

前面的两个 property 可以用 convolution 来处理掉，最后的 property 可以用 Max Pooling这件事来处理。等一下我们要介绍每一个 layer 再做的事情，我们就先从 convolution 开始看起。

## Convolution

### Propetry1

![mark](http://images.iterate.site/blog/image/20190818/JKFjnE8nzBuV.png?imageslim)

假设现在我们的 network 的 input 是一张 6*6的 Image，如果是黑白的，一个 pixel 就只需要用一个 value 去描述它，1就代表有涂墨水，0就代表没有涂到墨水。那在 convolution layer里面，它由一组的 filter，(其中每一个 filter 其实就等同于是 fully connect layer里面的一个 neuron)，每一个 filter 其实就是一个 matrix(3 *3)，这每个 filter 里面的参数(matrix里面每一个 element 值)就是 network 的 parameter(这些 parameter 是要学习出来的，并不是需要人去设计的)

每个 filter 如果是 3* 3的 detects 意味着它就是再侦测一个 3 *3的 pattern(看 3 *3的一个范围)。在侦测 pattern 的时候不看整张 image，只看一个 3 *3的范围内就可以决定有没有某一个 pattern 的出现。这个就是我们考虑的第一个 Property


### Propetry2

![mark](http://images.iterate.site/blog/image/20190818/DyJeVge1SwYo.png?imageslim)

这个 filter 咋样跟这个 image 运作呢？首先第一个 filter 是一个 3* 3的 matrix，把这个 filter 放在 image 的左上角，把 filter 的 9 个值和 image 的 9 个值做内积，两边都是 1,1,1(斜对角)，内积的结果就得到 3。(移动多少是事先决定的)，移动的距离叫做 stride(stride等于多少，自己来设计)，内积等于-1。stride等于 2，内积等于-3。我们先设 stride 等于 1。


![mark](http://images.iterate.site/blog/image/20190818/xLIVb3vnYKLQ.png?imageslim)

你把 filter 往右移动一格得到-1，再往右移一格得到-3，再往右移动一格得到-1。接下里往下移动一格，得到-3。以此类推(每次都移动一格)，直到你把 filter 移到右下角的时候，得到-1(得到的值如图所示)

经过这件事情以后，本来是 6 *6的 matrix，经过 convolution process就得到 4 *4的 matrix。如果你看 filter 的值，斜对角的值是 1,1,1。所以它的工作就是 detain1 有没有 1,1,1(连续左上到右下的出现在这个 image 里面)。比如说：出现在这里(如图所示蓝色的直线)，所以这个 filter 就会告诉你：左上跟左下出现最大的值



就代表说这个 filter 要侦测的 pattern，出现在这张 image 的左上角和左下角，这件事情就考虑了 propetry2。同一个 pattern 出现在了左上角的位置跟左下角的位置，我们就可以用 filter 1侦测出来，并不需要不同的 filter 来做这件事。


![mark](http://images.iterate.site/blog/image/20190818/0OjHftni5frK.png?imageslim)

在一个 convolution layer 里面会有很多的 filter(刚才只是一个 filter 的结果)，那另外的 filter 会有不同的参数(图中显示的 filter2)，它也做跟 filter1 一模一样的事情，在 filter 放到左上角再内积得到结果-1，依次类推。你把 filter2 跟 input image做完 convolution 之后，你就得到了另一个 4*4的 matrix，红色 4 *4的 matrix 跟蓝色的 matrix 合起来就叫做 feature map，看你有几个 filter，你就得到多少个 image(你有 100 个 filter，你就得到 100 个 4 *4的 image)


![mark](http://images.iterate.site/blog/image/20190818/KXmUaPF3vo1B.png?imageslim)

 刚才举的例子是一张黑白的 image，所以 input 是一个 matrix。若今天换成彩色的 image，彩色的 image 是由 RGB 组成的，所以，一个彩色的 image 就是好几个 matrix 叠在一起，就是一个立方体。如果要处理彩色 image，这时候 filter 不是一个 matrix，filter而是一个立方体。如果今天是 RGB 表示一个 pixel 的话，那 input 就是 3*6 *6，那 filter 就是 3 *3 *3。

 在做 convolution 的话，就是将 filter 的 9 个值和 image 的 9 个值做内积(不是把每一个 channel 分开来算，而是合在一起来算，一个 filter 就考虑了不同颜色所代表的 channel)



##  convolution和 fully connected之间的关系

![mark](http://images.iterate.site/blog/image/20190818/RMnFp4SJzOUA.png?imageslim)

convolution就是 fully connected layer把一些 weight 拿掉了。经过 convolution 的 output 其实就是一个 hidden layer的 neural 的 output。如果把这两个 link 在一起的话，convolution就是 fully connected拿掉一些 weight 的结果。

![mark](http://images.iterate.site/blog/image/20190818/11vYCrqsRdIb.png?imageslim)

我们在做 convolution 的时候，我们 filter1 放到左上角(先考虑 filter1)，然后做 inner product，得到内积为 3，这件事情就等同于把 6* 6的 image 拉直(变成如图所示)。然后你有一个 neural 的 output 是 3，这个 neural 的 output 考虑了 9 个 pixel，这 9 个 pixel 分别就是编号(1,2,3,7,8,9,13,14,15)的 pixel。这个 filter 做 inner product以后的 output 3就是某个 neuron output 3时，就代表这个 neuron 的 weight 只连接到(1,2,3,7,8,9,13,14,15)。这 9 个 weight 就是 filter matrix里面的 9 个 weight(同样的颜色)

在 fully connected中，一个 neural 应该是连接在所有的 input(有 36 个 pixel 当做 input，这个 neuron 应连接在 36 个 input 上)，但是现在只连接了 9 个 input(detain一个 pattern，不需要看整张 image，看 9 个 input 就好)，这样做就是用了比较少的参数了。



![mark](http://images.iterate.site/blog/image/20190818/AO99MNLK10N3.png?imageslim)

将 stride=1(移动一格)做内积得到另外一个值-1，假设这个-1是另外一个 neural 的 output，这个 neural 连接到 input 的(2,3,4，8,9,10,14，15,16)，同样的 weight 代表同样的颜色。在 9 个 matrix

当我们做这件事情就意味说：这两个 neuron 本来就在 fully connect里面这两个 neural 本来是有自己的 weight，当我们在做 convolution 时，首先把每一个 neural 连接的 wight 减少，强迫这两个 neural 共用一个 weight。这件事就叫做 shared weight，当我们做这件事情的时候，我们用的这个参数就比原来的更少。



## Max pooling

![mark](http://images.iterate.site/blog/image/20190818/tUd74a9480BF.png?imageslim)

![mark](http://images.iterate.site/blog/image/20190818/euKho3WQIVaS.png?imageslim)

相对于 convolution 来说，Max Pooling是比较简单的。我们根据 filter 1得到 4*4的 maxtrix，根据 filter2 得到另一个 4 *4的 matrix，接下来把 output ，4个一组。每一组里面可以选择它们的平均或者选最大的都可以，就是把四个 value 合成一个 value。这个可以让你的 image 缩小。

![mark](http://images.iterate.site/blog/image/20190818/oKzH9nR8cXuU.png?imageslim)

假设我们选择四个里面的 max vlaue保留下来，这样可能会有个问题，把这个放到 neuron 里面，这样就不能够微分了，但是可以用微分的办法来处理的


![mark](http://images.iterate.site/blog/image/20190818/c4xPS6Y3bKyl.png?imageslim)

做完一个 convolution 和一次 max pooling，就将原来 6 * 6的 image 变成了一个 2 *2的 image。这个 2 *2的 pixel 的深度 depend 你有几个 filter(你有 50 个 filter 你就有 50 维)，得到结果就是一个 new image but smaller，一个 filter 就代表了一个 channel。



![mark](http://images.iterate.site/blog/image/20190818/ug2MoybBrzVp.png?imageslim)

这件事可以 repeat 很多次，通过一个 convolution + max pooling就得到新的 image。它是一个比较小的 image，可以把这个小的 image，做同样的事情，再次通过 convolution + max pooling，将得到一个更小的 image。


这边有一个问题：第一次有 25 个 filter，得到 25 个 feature map，第二个也是由 25 个 filter，那将其做完是不是要得到`$25^2$`的 feature map。其实不是这样的！



假设第一层 filter 有 2 个，第二层的 filter 在考虑这个 imput 时是会考虑深度的，并不是每个 channel 分开考虑，而是一次考虑所有的 channel。所以 convolution 有多少个 filter，output就有多少个 filter(convolution有 25 个 filter，output就有 25 个 filter。只不过，这 25 个 filter 都是一个立方体)


## Flatten

![mark](http://images.iterate.site/blog/image/20190818/sy47TcqkMiPn.png?imageslim)

flatten就是 feature map拉直，拉直之后就可以丢到 fully connected feedforward netwwork，然后就结束了。




## CNN in Keras

![mark](http://images.iterate.site/blog/image/20190818/QdChAUNnJoRi.png?imageslim)

唯一要改的是：network structure和 input format，本来在 DNN 中 input 是一个 vector，现在是 CNN 的话，会考虑 input image的几何空间的，所以不能给它一个 vector。应该 input 一个 tensor(高维的 vector)。为什么要给三维的 vector？因为 image 的长宽高各是一维，若是彩色的话就是第三维。所以要给三维的 tensor

**model.add(Convolution2D**( **25, 3, 3**)


25代表有 25 个 filter，3 *3代表 filter 是一个 3 *3的 matrix


**Input_shape=(28,28,1)**

假设我要做手写数字辨识，input是 28 *28的 image，每个 pixel 都是单一颜色。所以 input_shape是(1,28,28)。如果是黑白图为 1(blacj/white)，如果是彩色的图时为 3(每个 pixel 用三个值来表述)。



**MaxPooling2D**(( 2, 2 ))

2,2表示把 2*2的 feature map里面的 pixel 拿出来，选择 max value



![mark](http://images.iterate.site/blog/image/20190818/6z2ncBlANAbI.png?imageslim)

假设我们 input 一个 1 *28 * 28的 image，你就可以写 model.add(Convolution2D( 25, 3, 3, Input_shape=(28,28,1)))。通过 convplution 以后得到 output 是 25 *26 26(25个 filter，通过 3 *3得到 26 * 26)。然后做 max pooling，2 *2一组选择 max value得到 25 *13 * 13

然后在做一次 convolution，假设我在这选 50 个 filter，每一个 filter 是 3 *3时，那么现在的 channel 就是 50。13 *13的 image 通过 3 *3的 filter，就成 11 *11，然后通过 2 *2的 Max Pooling，变成了 50 *5 *5



在第一个 convolution layer里面，每一个 filter 有 9 个参数，在第二个 convolution layer里面，虽然每一个 filter 都是 3 *3，但不是 3 *3个参数，因为它 input channel 是 25 个，所以它的参数是 3 *3 *25(225)。

![mark](http://images.iterate.site/blog/image/20190818/t7Wrd7usJfvr.png?imageslim)

通过两次 convolution，两次 Max Pooling，原来是 1 *28 *28变为 50 *5 *5。flatten的目的就是把 50 *5 *5拉直，拉直之后就成了 1250 维的 vector，然后把 1250 维的 vector 丢到 fully connected。


## CNN学到了什么?
![mark](http://images.iterate.site/blog/image/20190818/4NdPm33bMTkH.png?imageslim)

很多人常会说：deep learning就是一个黑盒子，然后你 learn 以后你不知道它得到了什么，所以有很多人不喜欢用这种方法。但还有很多的方法分析的，比如说我们今天来示范一下咋样分析 CNN，它到底学到了什么。

分析 input 第一个 filter 是比较容易的，因为一个 layer 每一个 filter 就是一个 3*3的 mmatrix，对应到 3 *3的范围内的 9 个 pixel。所以你只要看到这个 filter 的值就可以知道说：它在 detain 什么东西，所以第一层的 filter 是很容易理解的，但是你没有办法想要它在做什么事情的是第二层的 filter。在第二层我们也是 3 *3的 filter 有 50 个，但是这些 filter 的 input 并不是 pixel(3 *3的 9 个 input 不是 pixel)。而是做完 convolution 再做 Max Pooling的结果。所以这个 3 *3的 filter 就算你把它的 weight 拿出来，你也不知道它在做什么。另外这个 3 *3的 filter 它考虑的范围并不是 3 *3的 pixel(9个 pixel)，而是比 9 个 pxiel 更大的范围。不要这 3 *3的 element 的 input是做完 convolution 再加 Max Pooling的结果。所以它实际上在 image 上看到的范围，是比 3 *3还要更大的。那我们咋样来分析一个 filter 做的事情是什么呢，以下是一个方法。


我们知道现在做第二个 convolution layer里面的 50 个 filter，每一个 filter 的 output 就是一个 matrix(11*11的 matrix)。假设我们现在把第 k 个 filter 拿出来，它可能是这样子的(如图)，每一个 element 我们就叫做`$a_{ij}^k$`(上标是说这是第 k 个 filter，i,j代表在这个 matrix 里面的第 i row和第 j column)。

接下来我们定义一个东西叫做："Degree of the activation of the k-th filter"，我们定义一个值代表说：现在第 k 个 filter 有多被 active(现在的 input 跟第 k 个 filter 有多 match)，第 k 个 filter 被启动的 Degree 定义成：这个 11*11的 matrix里面全部的 element的 summation。(input一张 image，然后看这个 filter output的这个 11 *11的值全部加起来，当做是这个 filter 被 active 的程度)


截下来我们要做的事情是这样子的：我们想知道第 k 个 filter 的作用是什么，所以我们想要找一张 image，这张 image 它可以让第 k 个 filter 被 active 的程度最大。

假设 input 一张 image，我们称之为 X，那我们现在要解的问题就是：找一个 x，它可以让我们现在定义的 activation Degree `$a^k$`最大，这件事情要咋样做到呢？其实是用 gradient ascent你就可以做到这件事(minimize使用 gradient descent，maximize使用 gradient ascent)

这是事还是蛮神妙的，我们现在是把 X 当做我们要找的参数用 gradient ascent做 update，原来在 train CNN network neural的时候，input是固定的，model的参数是你需要用 gradient descent找出来的，用 gradient descent找参数可以让 loss 被 minimize。但是现在立场是反过来的，现在在这个 task 里面，model的参数是固定的，我们要让 gradient descent 去 update 这个 X，可以让这个 activation function的 Degree of the activation是最大的。





![mark](http://images.iterate.site/blog/image/20190818/19KGelyn38Ur.png?imageslim)

这个是得到的结果，如果我们随便取 12 个 filter 出来，每一个 filter 都去找一张 image，这个 image 可以让那个 filter 的 activation 最大。现在有 50 个 filter，你就要去找 50 张 image，它可以让这些 filter 的 activation 最大。我就随便取了前 12 个 filter，可以让它最 active 的 image 出来(如图)。

这些 image 有一个共同的特征就是：某种纹路在图上不断的反复。比如说第三张 image，上面是有小小的斜条纹，意味着第三个 filter 的工作就是 detain 图上有没有斜的条纹。那不要忘了每一个 filter 考虑的范围都只是图上一个小小的范围。所以今天一个图上如果出现小小的斜的条纹的话，这个 filter 就会被 active，这个 output 的值就会比较大。那今天如果让图上所有的范围通通都出现这个小小的斜条纹的话，那这个时候它的 Degree activation会是最大的。(因为它的工作就是侦测有没有斜的条纹，所以你给它一个完整的数字的时候，它不会最兴奋。你给它都是斜的条纹的时候，它是最兴奋的)

所以你就会发现：每一个 filter 的工作就是 detain 某一张 pattern。比如说：第三图 detain 斜的线条，第四图是 detain 短的直线条，等等。每一个 filter 所做的事情就是 detain 不同角度的线条，如果今天 input 有不同角度的线条，你就会让某一个 activation function，某一个 filter 的 output 值最大


### 分析全连接层

![mark](http://images.iterate.site/blog/image/20190818/LDUTT2Dlrmch.png?imageslim)

在做完 convolution 和 Max Pooling以后，要做一件事情叫做 flatten，把 flatten 的结果丢到 neural network里面去。那我们想要知道：在这个 neural network里面，每一个 neural 的工作是什么。

我们要做的事情是这样的：定义第 j 个 neural，它的 output 叫做`$a_j$`。接下来我们要做事情就是：找一张 image(用 gradient ascent的方法找一张 X)，这个 image X你把它丢到 neural network里面去，它可以让`$a_j$`的值被 maximize。找到的结果就是这样的(如图)

如图是随便取前 9 个 neural 出来，什么样的图丢到 CNN 里面可以让这 9 个 neural 最被 active output的值最大，就是这 9 张图(如图)




这些图跟刚才所观察到图不太一样，在刚在的 filter 观察到的是类似纹路的图案，在整张图上反复这样的纹路，那是因为每个 filter 考虑是图上一个小小的 range(图上一部分 range)。现在每一个 neural，在你做 flatten 以后，每个 neural 的工作就是去看整张图，而不是是去看图的一小部分。


![mark](http://images.iterate.site/blog/image/20190818/IY9pNfR2zIvD.png?imageslim)

那今天我们考虑是 output 呢？(output就是 10 维，每一维对应一个 digit)我们把某一维拿出来，找一张 image 让那个维度 output 最大。那我们会得到咋样的 image 呢？你可以想象说：每一个 output，每一个 dimension 对应到某一个数字。

现在我们找一张 image，它可以让对应在数字 1 的 output 最大，那么那张 image 显然就像看起来是数字 1。你可以期待说：我们可以用这个方法让 machine 自动画出数字。

但是实际上我们得到的结果是这样子的，每一张图分别代表数字 0-9。也就是说：我们到 output layer对应到 0 那个 neuron，其实是这样的(如图)，以此类推。你可能会有疑惑，为什么是这样子的，是不是程序有 bug。为了确定程序没有 bug，再做了一个实验是：我把每张 image(如图)都丢到 CNN 里面，然后看它 classifier 的结果是什么。CNN确定就说：这个是 1，这个是，...，这个是 8。CNN就觉得说：你若拿这张 image train出来正确率有 98 的话，就说：这个就是 8。所以就很神奇

这个结果在很多的地方有已经被观察到了，今天的这个 neuron network它所学到东西跟我们人类是不太一样的(它所学到的东西跟我们人类想象和认知不一样的)。你可以查看这个链接的 paper(如图)


[相关的 paper](https://www.youtube.com/watch?v=M2IebCN9H)


### 让图更像数字

![mark](http://images.iterate.site/blog/image/20190818/jjR1NrQKbAE4.png?imageslim)

我们有没有办法让这个图看起来更像数字呢？想法是这样的：一张图是不是数字我们有一些基本的假设，比如说：这些就算你不知道它是什么数字(显然它不是 digit)，人类手写出来的就不是这个样子。所以我们应该对 x 做 constraint，我们告诉 machine，有些 x 可能会使 y 很大但不是数字。我们根据人的 power-knowledge就知道，这些 x 不可能是一些数字。那么我们可以加上咋样的 constraint 呢？(图中白色的亮点代表的是有墨水的，对一个 digit 来说，图白的部分其实是有限的，对于一个数字来说，一整张图的某一个小部分会有笔画，所以我们应该对这个 x 做一些限制)

假设 image 里面的每一个 pixel 用`$x_{ij}$`来表示，(每一个 image 有 28 *28的 pixel)我们把所有 image 上`$i,j$`的值取绝对值后加起来。如果你熟悉 machine learning的话，这一项就是 L1-regularization。然后我们希望说：在找一个 x 可以让`$y^i$`最大的同时让`$|x_{ij}|$`的 summation 越小越好。也就是我们希望找出的 image，大部分的地方是没有涂颜色的，只有非常少的部分是有涂颜色的。如果我们加上 constraint 以后我们得到的结果是这样的(如右图所示)，跟左边的图比起来，隐约可以看出来它是一个数字(得到的结果看起来像数字)

你可能会有一个问题，绝对值咋样去微分，下堂课会讲到

你如果加上一些额外的 constraint，比如说：你希望相邻的 pixel
是同样的颜色等等，你应该可以得到更好的结果。不过其实有更多很好的方法可以让 machine generate数字

## Deep Dream

![mark](http://images.iterate.site/blog/image/20190818/6B1Io7yBEEE1.png?imageslim)

其实上述的想法就是 Deep Dream的精神，Deep Dream是说：如果你给 machine 一张 image，它会在这张 image 里加上它看到的东西。咋样做这件事情呢？你先找一张 image，然后将这张 image 丢到 CNN 中，把它的某一个 hidden layer拿出来(vector)，它是一个 vector(假设这里是：[3.9, -1.5, 2.3...])。接下来把 postitive dimension值调大，把 negative dimension值调小(正的变的更正，负的变得更负)。你把这个(调节之后的 vector)当做是新的 image 的目标(把 3.9的值变大，把-1.5的值变得更负，2.3的值变得更大。然后找一张 image(modify image)用 GD 方法，让它在 hidden layer output是你设下的 target)。这样做的话就是让 CNN 夸大化它所看到的东西，本来它已经看到某一个东西了，你让它看起来更像它原来看到的东西。本来看起来是有一点像东西，它让某一个 filter 有被 active，但是你让它被 active 的更剧烈(夸大化看到的东西)。






![mark](http://images.iterate.site/blog/image/20190818/QztCrXIFKyIm.png?imageslim)

如果你把这张 image 拿去做 Deep Dream的话，你看到的结果是这样子的。右边有一只熊，这个熊原来是一个石头(对机器来说，这个石头有点像熊，它就会强化这件事情，所以它就真的变成了一只熊)。Deep Dream还有一个进阶的版本，叫做 Deep Style

## Deep style

![mark](http://images.iterate.site/blog/image/20190818/DytDF5DPggj2.png?imageslim)

今天 input 一张 image，input一张 image，让 machine 去修改这张图，让它有另外一张图的风格 (类似于风格迁移)

![mark](http://images.iterate.site/blog/image/20190818/r395b4T0k5NH.png?imageslim)


得到的结果就是这样子的

![mark](http://images.iterate.site/blog/image/20190818/h9yVf7kwlAzS.png?imageslim)

[这里给一个 reference 给参考](https://arxiv.org/abs/158.06576)

其中做法的精神是这样的：原来的 image 丢给 CNN，然后得到 CNN 的 filter 的 output，CNN的 filter 的 output 代表这张 image 有什么 content。接下来你把呐喊这张图也丢到 CNN 里面，也得到 filter 的 output。我们并不在意一个 filter ，而是在意 filter 和 filter 之间
的 convolution，这个 convolution 代表了这张 image 的 style。

接下来你用同一个 CNN 找一张 image，这张 image 它的 content 像左边这张相片，但同时这张 image 的 style 像右边这张相片。你找一张 image 同时可以 maximize 左边的图，也可以 maximize 右边的图。那你得到的结果就是像最底下的这张图。用的就是刚才讲的 gradient ascent的方法找一张 image，然后 maximize 这两张图，得到就是底下的这张图。


## CNN的应用

### 围棋
![mark](http://images.iterate.site/blog/image/20190818/Ho6oTyHlbIcg.png?imageslim)

我们现在 CNN 已经在很多不同的应用上，而不是只有影像处理上。比如：CNN现在有一个很知名的应用，就用用在下围棋上面。为什么 CNN 可以用来下围棋上面呢？

我们知道如果让 machine 来下围棋，你不见得需要用 CNN。其实一般的 topic neuron network也可以帮我们做到这件事情。你只要 learn 一个 network(也就是找一个 function)，它的 input 是棋盘，output是棋盘上的位置。也就是说：你根据这个棋盘的盘式，如果你下一步要落子的话，你落子的位置其实就可以让 machine 学会。

所以你用 Fully-connected feedforward network也可以帮我们做到让 machine 下围棋这件事情。也就是你只要告诉 input 是一个 19 *19的 vector，每一个 vector 的 dimension 对应到棋盘上面的每一个位置。machine就可以学会下围棋了。
如果那个位置有一个黑子的话就是 1，如果有一个白子的话就是-1，反之就是 0。


但是我们这边采用 CNN 的话，我们会得到更好的 performance。我们之前举的例子是把 CNN 用在影像上面，也就是 input 是 matrix(也就是把 19*19的 vector 表示成 19 *19的 matrix)，然后当做一个 image 来看，然后让它 output 下一步落子的位置就结束了。


![mark](http://images.iterate.site/blog/image/20190818/TqitqEMbXzr8.png?imageslim)

告诉 machine 说：看到落子在“5之五”，CNN的 output 就是在“天元”的地方是 1，其他地方是 0。看到“5之五”和“天元”都有子，CNN的 output 就是在“五之 5”的地方是 1，其他地方是 0。这个是 supervised 部分

![mark](http://images.iterate.site/blog/image/20190818/S170L7Eok8V2.png?imageslim)

现在大家都说“AlphaGo”，都是懂懂的样子。但是自从“AlphaGo”用了 CNN 以后，大家都觉得说：CNN应该很厉害。所以如果你没有用 CNN 来处理你的问题，别人就会问你为什么不用 CNN 来处理问题(比如说：面试的时候)，CNN不是比较强吗


什么时候应该用 CNN 呢？image必须有该有的那些特性，在 CNN 开头就有说：根据那三个观察，所以设计出了 CNN 这样的架构。在处理 image 时是特别有效的。为什么这样的架构也同样可以用在围棋上面(因为围棋有一些特性跟影像处理是非常相似的)

第一个是：在 image 上面，有一些 pattern 是要比整张 image 还要小的多的(比如：鸟喙是要比整张的 image 要小的多)，只需要看那一小的部分就知道那是不是鸟喙。在围棋上也有同样的现象，如图所示，一个白子被三个黑子围住(这就是一个 pattern)，你现在只需要看这一小小的范围，就可以知道白子是不是没“气”了，不需要看整个棋盘才能够知道这件事情，这跟 image 是有同样的性质。


在“AlphaGo”里面它的第一个 layer filter其实就是用 5*5的 filter，显然做这个设计的人觉得说：围棋最基本的 pattern 可能都是在 5 *5的范围内就可以被侦测出来，不需要看整个棋牌才能知道这件事情。

接下来我们说 image 还有一个特性：同样的 pattern 会出现在不同的 regions，而他们代表的是同样的意义，在围棋上可能也会有同样的现象。像如图这个 pattern 可以出现在左上角，也可以出现在右下角，它们都代表了同样的意义。所以你可以用同一个 pattern 来处理在不同位置的同样的 pattern。所以对围棋来说，是有这两个特性的。


### AlphaGo

![mark](http://images.iterate.site/blog/image/20190818/Cz4rxvDodye7.png?imageslim)

但是没有办法让我想通的地方就是第三点，我们可以对一个 image 做 subsampling，把 image 变为原来的 1/4的大小，但是也不会影响你看这张图的样子。因为基于这个观察，所以有 Max Pooling这个 layer。但是对围棋来说，你可以做这件事情吗？你可以丢到奇数行偶数类，这样它还是同一个盘式吗，显然不是的，这个让我相当的困扰。

“AlphaGo”里面有用了 Max Pooling这个架构，或许这是一个弱点。可以针对这个弱点去攻击它，击败它。但是“AlphaGo”(比李世石还强)，没有这个显而易见的弱点


有一天我突然领悟到“AlphaGo”的 CNN 架构里面有什么特别的地方(“AlphaGo”Paper的附录)，在“AlphaGo”Paper里面只说了一句：用 CNN 架构，但它没有在正文里仔细描述 CNN 的架构，会不会实际上 CNN 架构里有什么特别的玄机呢？

在“AlphaGo”Paper的附录里面，描述了 neuron network structure，它的 input 是一个 19 *19 *48的 image。19 *1是可以理解，因为棋盘就是 19 *19。48是咋样来的呢？对于“AlphaGo”来说，它把每一个位置都用 48 个 value 来描述。这里面的 value 包括：我们只要在一个位置来描述有没有白子，有没有黑子；还加上了 domain-knowledge(不只是说：有没有黑子或者白子，还会看这个位置是不是出于没“气”的状态，等等)

如果读完这段你会发现：第一个 layer 有做 zero pads。也就是说：把原来 19*19的 image 外围补上更多的 0，让它变成 23 *23的 image。

第一个 hidden layer用的是 5*5 filter(总共有 k 个 filter)，k的值在 Paper 中用的是 192(k=192)；stride设为 1；使用 RLU activation function等等。

然后你就会发现“AlphaGo”是没有用 Max Pooling，所以这个 neuron network的架构设计就是“运用之妙，存乎一心”。虽然在 image 里面我们都会用 Max Pooling这个架构，但是针对围棋的特性来设计 neuron network的时候，我们是不需要 Max Pooling这个架构的，所以在“AlphaGo”里面没有这个架构.






### 语音

![mark](http://images.iterate.site/blog/image/20190818/7KA2NDj6r3qj.png?imageslim)

CNN也可以用在其它的 task 里面，比如说：CNN也用在影像处理上。如图是一段声音，你可以把一段声音表示成 Spectrogram(横轴是时间，纵轴是那段时间里面声音的频率)，红色代表：在那段时间里那一频率的 energy 比较大。

这张 image 其实是我说“你好”，然后看到的 Spectrogram。有通过训练的人，看这张 image，就知道这句话的内容是什么。

人既然可以看这个 image 就可以知道是什么样的声音讯号，那我们也可以让机器把这个 Spectrogram 当做一张 image。然后用 CNN 来判断：input一张 image，它是对应什么样的声音讯号(单位可能是 phone)。但是神奇的地方是：CNN里面的时候，在语音上，我们通常只考虑在 frequency 方向上移动的 filter。也就是说：我们的 filter 是长方形的，其中宽是跟 image 的宽是一样的，我们在移动 filter 的时候，我们移这个方向(如图所示)

如果把 filter 向时间的方向移动的话，结果是没有太大的帮助。这样的原因是：在语音里面，CNN的 output 还会接其他的东西(比如:LSTM)，所以在向时间方向移动是没有太多的帮助。

为什么在频率上的 filter 会有帮助呢？我们用 filter 的目的是：为了 detain 同样的 pattern 出现在不同的 range，我们都可以用同一个的 filter detain出来。那在声音讯号上面，男生跟女生发同样的声音(同样说“你好”)，Spectrogram看起来是非常不一样的，它们的不同可能只是频率的区别而已(男生的“你好”跟女生的“你好”，它们的 pattern 其实是一样的)


所以今天我们把 filter 在 frequency direction移动是有效的。当我们把 CNN 用在 application 时，你永远要想一想，这个 application 的特性是什么，根据那个 application 的特性来 design network的 structure


### 文本
![mark](http://images.iterate.site/blog/image/20190818/IEfMvkvcm15i.png?imageslim)

[相关的 paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.703.6858&rep=rep1&type=pdf)

我们知道 CNN 耶可以用在文字处理上面，这个是从 paper 截下来的图。在文字处理上面，假设你要做的是：让 machine 侦测这个 word sequence代表的是 positive 还是 negative。首先 input 一个 word sequence，你把 word sequence里面的每一个 word 都用一个 vector 来表示。这边的每一个 vector 代表 word 本身的 sementic，如果两个 word 含义越接近的话，那它们的 vector 在高维的空间上就越接近，这个就叫做“wordembedding”(每一个 word 用 vector 来表示)。


当你把每一个 word 用 vector 来表示的时候，你把 sentence 所有的 word 排在一起，它就变成一张 image。你可以把 CNN 套用在这个 image 上面。

当我们把 CNN 用在文字处理上的时候，你的 filter 其实是这个样子的(如图所示)。它的高跟 image 是一样的，你把 filter 沿着句子里面词汇的顺序来移动，然后你就会得到一个 vector。不同的 filter 就会得到不同的 vector，然后 Max Pooling，然后把 Max Pooling的结果丢到 fully connect里面，就会得到最后的结果。在文字处理上，filter只在时间的序列上移动，不会在“embedding dimension”这个方向上移动。如果你有做过类似的 task(文字处理)，知道“embedding dimension”指的是什么，你就会知道在“embedding dimension”反向上移动是没有帮助的，因为在 word embedding里面每一个 dimension 的含义其实是独立的。所以当我们如果使用 CNN 的时候，你会假设说：第二个 dimension 跟第一个 dimension 有某种特别的关系；第四个 dimension 跟第五个 dimension 有某种特别的关系。这个关系是重复的(这个 pattern 出现在不同的位置是同样的意思)。但是在 word embedding里面，不同 dimension 是独立的(independent)。所以在 embedding dimension移动是没有意义的，所以你在做文字处理的时候，你只会在 sentence 顺序上移动 filter，这个是另外的例子。

## Reference
![mark](http://images.iterate.site/blog/image/20190818/99Bm29XSJb3l.png?imageslim)

如果你想知道更多 visualization 事情的话，以上是一些 reference。


![mark](http://images.iterate.site/blog/image/20190818/1WqluREKCztU.png?imageslim)

如果你想要用 Degree 的方法来让 machine 自动产生一个 digit，这件事是不太成功的，但是有很多其它的方法，可以让 machine 画出非常清晰的图。这里列了几个方法，比如说：PixelRNN，VAE，GAN来給参考.





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
