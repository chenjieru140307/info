

## keras 是什么

![mark](http://images.iterate.site/blog/image/20190818/0KI6aQdwb6Td.png?imageslim)

Keras 是一个用 python 编写的高级神经网络 API，它能够以 TensorFlow, CNTK, 或者 Theano 作为后端运行。


## 示例

以手写数字识别为例
### 步骤 1：定义模型
![mark](http://images.iterate.site/blog/image/20190818/VPxXvyBTxxwk.png?imageslim)


neural network是长什么样的，在 keras 首先定义 model 是 sequential
```python
model = sequential()
```

- 第 1 个隐藏层
	- 你看要你的 neural 长什么样子，自己就决定长什么样子，举例，这里 hidden layer 有两个 layer，每个 layer 都有 500 Neural。已经定义了一个 model，然后 model.add，加一个 Fully connect laye(这里用 Dense 表示)，然后 input，output
	- 然后增加一个 activation(激活函数)，将 sigmoid 当做 activation(也可以使用其他的当做 activation)
```
model.add(activation('sigmoid'))
```
- 第 2 个隐藏层
	- 这个 layer 的 input 就是上一个 layer 的 output，不用说 input 是 500Neural，keras自己知道

- 输出层：
	- output为 10dimension
	- activation为 softmax



### 步骤 2：模型评估

![mark](http://images.iterate.site/blog/image/20190818/Wl2N4rkvcTQE.png?imageslim)
- 评估模型的好坏


compile 编译
```python
model.compile()
```
定义一个 loss 是什么(不同的场合，需要不同的 loss function)
```python
loss = ('cateqorical crossentropy')  #损失函数
```
```python
optimizer #优化器
```
```python
metrics #指标
```

### 步骤 3：最佳模型

#### 3.1 Configuration
![mark](http://images.iterate.site/blog/image/20190818/DCX7AGbtdyak.png?imageslim)


```python
model.compile = (loss = 'categorical crossentropy', optimizer = 'adam')
```
- optimizer后面可以跟不同的方式，这些方式都是 GD，只是用的 learning rate不同，有一些 machine 会自己决定 learning rate
#### 3.2 寻找最优网络参数

![mark](http://images.iterate.site/blog/image/20190818/trNbTKo6JOiF.png?imageslim)
- 给定四个输入, x_train, y_train, batch_size, nb_epoch
- 训练数据就是一张一张的图片, 每张图片对应的标签就是数字
- Two dimension matrix(X_train)，第一个 dimension 代表你有多少个 example，第二个 dimension 代表你有多少个 pixel
- Two dimension matrix(y_train)，第一个 dimension 代表你有多少个 training example，第二个 dimension 代表 label(黑色的为数字，从 0 开始计数)

##### mini-batch 的原理详解
keras model参数`batch_size`和`nb_epoch`
![mark](http://images.iterate.site/blog/image/20190818/ji956OK8kHEA.png?imageslim)

我们在做梯度下降和深度学习时，我们并不是真的最小化总损失，我们会把训练数据随机分成几个 mini-batch。
具体步骤：
- 随机初始化神经网络的参数 (跟梯度下降一样)
- 先随机选择第一个 batch 出来，对选择出来的 batch 里面 total loss, 计算偏微分，根据 ${L}'$ 去更新参数
- 然后随机选择第二个 batch ，对第一个选择出来的 batch 里面 total loss, 计算偏微分，根据 ${L}''$ 更新参数
- 反复上述过程，直到把所有的 batch 都统统过一次，一个 epoch 才算结束。
注意：假设今天有 100 个 batch 的话，就把这个参数更新 100 次，把所有的 batch 都遍历过叫做一个 epoch。
```
 model.fit(x_train, y_train, batch_size =100, nb_epoch = 20)
```
1. 这里的 batch_size代表一个 batch 有多大(就是把 100 个 example，放到一个 batch 里)
2. nb_epoch等于 20 表示对每个 batch 重复 20 次

##### 使用 mini-batch的原因：Speed
![mark](http://images.iterate.site/blog/image/20190818/2dNRg7903gHD.png?imageslim)
![mark](http://images.iterate.site/blog/image/20190818/BV1KKVlRCbAk.png?imageslim)

- batch-szie不同时，一个 epoch 所需的时间是不一样的（上图用 batch size=1是 166s，当 batch size=10是 17s）
- batch =10相比于 batch=1，较稳定
- Speed-- why minni batch is faster than stochastic GD(为什么批量梯度下降比随机梯度下降要快)
  因为利用计算机的平行运算，之前也提到过矩阵运算会使计算速度快很多。
- 很大的 batch size会导致很差的表现（不能设置太大也不能设置太小）
![mark](http://images.iterate.site/blog/image/20190818/6AduSWPfx9ua.png?imageslim)
用随机梯度下降的时候两个矩阵 x 是分开计算的，当用 mini batch的时候，直接是用两个 x 合并在一起，一起计算得到 $Z^1$ 和 $Z^2$，对 GPU 来说上面运算时间是下面运算时间的两倍，这就是为什么我们用上 mini batch和 GPU 的时候速度会加快的原理。但是如果你用了 GPU 没用 mini batch的话，那也达不到加速的效果。

### 模型保存和使用

```python
#case1：测试集正确率
score = model.evaluate(x_test,y_test)
print("Total loss on Testing Set:", score[0])
print("Accuracy of Testing Set:", score[1])

#case2：模型预测
result = model。predict(x_test)
```
![mark](http://images.iterate.site/blog/image/20190818/Nlf4OnGMMN1c.png?imageslim)





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
