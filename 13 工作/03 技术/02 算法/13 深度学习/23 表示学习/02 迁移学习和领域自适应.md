

# 迁移学习和领域自适应

迁移学习和领域自适应指的是利用一个情景（例如，分布 $P_1$）中已经学到的内容去改善另一个情景（比如分布 $P_2$）中的泛化情况。

这点概括了上一节提出的想法，即在无监督学习任务和监督学习任务之间转移表示。

在迁移学习中，学习器必须执行两个或更多个不同的任务，但是我们假设能够解释 $P_1$ 变化的许多因素和学习 $P_2$ 需要抓住的变化相关。

这通常能够在监督学习中解释，输入是相同的，但是输出不同的性质。例如，我们可能在第一种情景中学习了一组视觉类别，比如猫和狗，然后在第二种情景中学习一组不同的视觉类别，比如蚂蚁和黄蜂。如果第一种情景（从 $P_1$ 采样）中具有非常多的数据，那么这有助于学习到能够使得从 $P_2$ 抽取的非常少样本中快速泛化的表示。许多视觉类别 **共享** 一些低级概念，比如边缘、视觉形状、几何变化、光照变化的影响等等。一般而言，当存在对不同情景或任务有用特征时，并且这些特征对应多个情景出现的潜在因素，迁移学习、多任务学习（\sec?）和领域自适应可以使用表示学习来实现。如\fig?所示，这是具有共享底层和任务相关上层的学习框架。




然而，有时不同任务之间共享的不是输入的语义，而是输出的语义。例如，语音识别系统需要在输出层产生有效的句子，但是输入附近的较低层可能需要识别相同音素或子音素发音的非常不同的版本（这取决于说话人）。在这样的情况下，共享神经网络的上层（输出附近）和进行任务特定的预处理是有意义的，如\fig?所示。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/IxDX6khPLzDV.png?imageslim">
</p>

> 15.2 多任务学习或者迁移学习的架构示例。输出变量 $\mathbf y$ 在所有的任务上具有相同的语义；输入变量 $\mathbf x$ 在每个任务（或者，比如每个用户）上具有不同的意义（甚至可能具有不同的维度），图上三个任务为 $\mathbf x^{(1)}$，$\mathbf x^{(2)}$，$\mathbf x^{(3)}$。底层结构（决定了选择方向）是面向任务的，上层结构是共享的。底层结构学习将面向特定任务的输入转化为通用特征。



在领域自适应的相关情况下，在每个情景之间任务（和最优的输入到输出的映射）都是相同的，但是输入分布稍有不同。例如，考虑情感分析的任务，如判断一条评论是表达积极的还是消极的情绪。网上的评论有许多类别。在书、视频和音乐等媒体内容上训练的顾客评论情感预测器，被用于分析诸如电视机或智能电话的消费电子产品的评论时，领域自适应情景可能会出现。可以想象，存在一个潜在的函数可以判断任何语句是正面的、中性的还是负面的，但是词汇和风格可能会因领域而有差异，使得跨域的泛化训练变得更加困难。简单的无监督预训练（去噪自编码器）已经能够非常成功地用于领域自适应的情感分析。




一个相关的问题是概念漂移，我们可以将其视为一种迁移学习，因为数据分布随时间而逐渐变化。概念漂移和迁移学习都可以被视为多任务学习的特定形式。"多任务学习"这个术语通常指监督学习任务，而更广义的迁移学习的概念也适用于无监督学习和强化学习。



在所有这些情况下，我们的目标是利用第一个情景下的数据，提取那些在第二种情景中学习时或直接进行预测时可能有用的信息。表示学习的核心思想是相同的表示可能在两种情景中都是有用的。两个情景使用相同的表示，使得表示可以受益于两个任务的训练数据。



如前所述，迁移学习中无监督深度学习已经在一些机器学习比赛中取得了成功~。这些比赛中的某一个实验配置如下。首先每个参与者获得一个第一种情景（来自分布 $P_1$）的数据集，其中含有一些类别的样本。参与者必须使用这个来学习一个良好的特征空间（将原始输入映射到某种表示），使得当我们将这个学成变换用于来自迁移情景（分布 $P_2$）的输入时，线性分类器可以在很少标注样本上训练、并泛化得很好。这个比赛中最引人注目的结果之一是，学习表示的网络架构越深（在第一个情景 $P_1$ 中的数据使用纯无监督方式学习），在第二个情景（迁移）$P_2$ 的新类别上学习到的曲线就越好。对于深度表示而言，迁移任务只需要少量标注样本就能显著地提升泛化性能。


迁移学习的两种极端形式是一次学习和零次学习，有时也被称为零数据学习。只有一个标注样本的迁移任务被称为一次学习；没有标注样本的迁移任务被称为零次学习。


因为第一阶段学习出的表示就可以清楚地分离出潜在的类别，所以一次学习~是可能的。在迁移学习阶段，仅需要一个标注样本来推断表示空间中聚集在相同点周围许多可能测试样本的标签。这使得在学成的表示空间中，对应于不变性的变化因子已经与其他因子完全分离，在区分某些类别的对象时，我们可以学习到哪些因素具有决定意义。


考虑一个零次学习情景的例子，学习器已经读取了大量文本，然后要解决对象识别的问题。如果文本足够好地描述了对象，那么即使没有看到某对象的图像，也能识别出该对象的类别。例如，已知猫有四条腿和尖尖的耳朵，那么学习器可以在没有见过猫的情况下猜测该图像中是猫。




只有在训练时使用了额外信息，零数据学习~和零次学习~才是有可能的。我们可以认为零数据学习场景包含三个随机变量：传统输入 $\boldsymbol x$，传统输出或目标 $\boldsymbol y$，以及描述任务的附加随机变量 $T$。该模型被训练来估计条件分布 $p(\boldsymbol y \mid \boldsymbol x, T)$，其中 $T$ 是我们希望执行的任务的描述。在我们的例子中，读取猫的文本信息然后识别猫，输出是二元变量 $y$，$y=1$ 表示"是"，$y=0$ 表示"不是"。任务变量 $T$ 表示要回答的问题，例如"这个图像中是否有猫？"如果训练集包含和 $T$ 在相同空间的无监督对象样本，我们也许能够推断未知的 $T$ 实例的含义。在我们的例子中，没有提前看到猫的图像而去识别猫，所以拥有一些未标注文本数据包含句子诸如"猫有四条腿"或"猫有尖耳朵"，对于学习非常有帮助。



零次学习要求 $T$ 被表示为某种形式的泛化。例如，$T$ 不能仅是指示对象类别的 one-hot编码。通过使用每个类别词的词嵌入表示，{Socher-2013}提出了对象类别的分布式表示。



我们还可以在机器翻译中发现一种类似的现象~：我们已经知道一种语言中的单词，还可以学到单一语言语料库中词与词之间的关系；另一方面，我们已经翻译了一种语言中的单词与另一种语言中的单词相关的句子。即使我们可能没有将语言 $X$ 中的单词 $A$ 翻译成语言 $Y$ 中的单词 $B$ 的标注样本，我们也可以泛化并猜出单词 $A$ 的翻译，这是由于我们已经学习了语言 $X$ 和 $Y$ 单词的分布式表示，并且通过两种语言句子的匹配对组成的训练样本，产生了关联于两个空间的链接（可能是双向的）。如果联合学习三种成分（两种表示形式和它们之间的关系），那么这种迁移将会非常成功。


零次学习是迁移学习的一种特殊形式。同样的原理可以解释如何能执行多模态学习，学习两种模态的表示，和一种模态中的观察结果 $\boldsymbol x$ 与另一种模态中的观察结果 $\boldsymbol y$ 组成的对  $(\boldsymbol x, \boldsymbol y)$ 之间的关系（通常是一个联合分布）。通过学习所有的三组参数（从 $\boldsymbol x$ 到它的表示、从 $\boldsymbol y$ 到它的表示，以及两个表示之间的关系），一个表示中的概念被锚定在另一个表示中，反之亦然，从而可以有效地推广到新的对组。这个过程如图 15.3 所示。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/0gE6Ygr8VcTi.png?imageslim">
</p>
> 15.3 两个域 $\boldsymbol x$ 和 $\boldsymbol y$ 之间的迁移学习能够进行零次学习。标注或未标注样本 $\boldsymbol x$ 可以学习表示函数 $f_{\boldsymbol x}$。同样地，样本 $\boldsymbol y$ 也可以学习表示函数 $f_{\boldsymbol y}$。上图中 $f_{\boldsymbol x}$ 和 $f_{\boldsymbol y}$ 旁都有一个向上的箭头，不同的箭头表示不同的作用函数。并且箭头的类型表示使用了哪一种函数。$\boldsymbol h_{\boldsymbol x}$ 空间中的相似性度量表示 $\boldsymbol x$ 空间中任意点对之间的距离，这种度量方式比直接度量 $\boldsymbol x$ 空间的距离更好。同样地，$\boldsymbol h_{\boldsymbol y}$ 空间中的相似性度量表示 $\boldsymbol y$ 空间中任意点对之间的距离。这两种相似函数都使用带点的双向箭头表示。标注样本（水平虚线）$(\boldsymbol x, \boldsymbol y)$ 能够学习表示 $f_{\boldsymbol x}(\boldsymbol x)$ 和表示 $f_{\boldsymbol y}(\boldsymbol y)$ 之间的单向或双向映射（实双向箭头），以及这些表示之间如何锚定。零数据学习可以通过以下方法实现。像 $\boldsymbol x_{\text{test}}$ 可以和单词 $\boldsymbol y_{\text{test}}$ 关联起来，即使该单词没有像，仅仅是因为单词表示 $f_{\boldsymbol y}(\boldsymbol y_{\text{test}})$ 和像表示 $f_{\boldsymbol x}(\boldsymbol x_{\text{test}})$ 可以通过表示空间的映射彼此关联。这种方法有效的原因是，尽管像和单词没有匹配成队，但是它们各自的特征向量 $f_{\boldsymbol x}(\boldsymbol x_{\text{test}})$ 和 $f_{\boldsymbol y}(\boldsymbol y_{\text{test}})$ 互相关联。




# 相关

- 《深度学习》花书
