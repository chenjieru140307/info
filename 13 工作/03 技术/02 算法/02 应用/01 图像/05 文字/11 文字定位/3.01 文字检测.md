

## 基于深度学习的文字检测

根据版面是否有先验信息（卡片的矩形区域、证件的关键字段标识）以及文字自身的复杂性（如水平文字、多角度），图像可划分为受控场景（如身份证、营业执照、银行卡）和非受控场景（如菜单、门头图），如下图所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190915/2by3pP4jbVXS.png?imageslim">
</p>

> 受控场景与非受控场景



考虑到这两类场景的特点不同，我们借鉴不同的检测框架。由于受控场景文字诸多约束条件可将问题简化，因此利用在通用目标检测领域广泛应用的 Faster R-CNN框架进行检测。而对于非受控场景文字，由于形变和笔画宽度不一致等原因，目标轮廓不具备良好的闭合边界，我们需要借助图像语义分割来标记文字区域与背景区域。

1. 受控场景的文字检测

对于受控场景（如身份证），我们将文字检测转换为对关键字目标（如姓名、身份证号、地址）或关键条目（如银行卡号）的检测问题。基于 Faster R-CNN的关键字检测流程如下图所示。为了保证回归框的定位精度，同时提升运算速度，我们对原有框架和训练方式进行了微调。

- 考虑到关键字或关键条目的类内变化有限，网络结构只采用了 3 个卷积层。
- 训练过程中提高正样本的重叠率阈值。
- 根据关键字或关键条目的宽高比范围来适配 RPN 层 Anchor 的宽高比。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190915/vslQIcrQS3FM.png?imageslim">
</p>

> 基于 Faster R-CNN的 OCR 解决方案



Faster R-CNN框架由 RPN（候选区域生成网络）和 RCN（区域分类网络）两个子网络组成。RPN通过监督学习的方法提取候选区域，给出的是无标签的区域和粗定位结果。RCN引入类别概念，同时进行候选区域的分类和位置回归，给出精细定位结果。训练时两个子网络通过端到端的方式联合优化。

下图以银行卡卡号识别为例，给出了 RPN 层和 RCN 层的输出。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190915/lIKY4gge4OQg.png?imageslim">
</p>

> 基于 Faster R-CNN的银行卡卡号检测



对于人手持证件场景，由于证件目标在图像中所占比例过小，直接提取微小候选目标会导致一定的定位精度损失。为了保证高召回和高定位精度，可采用由粗到精的策略进行检测。首先定位卡片所在区域位置，然后在卡片区域范围内进行关键字检测，而区域定位也可采用 Faster R-CNN框架，如下图所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190915/LaDUuJgBibxL.png?imageslim">
</p>

> 由粗到精的检测策略



2. 非受控场景的文字检测

对于菜单、门头图等非受控场景，由于文字行本身的多角度且字符的笔画宽度变化大，该场景下的文字行定位任务挑战很大。由于通用目标检测方法的定位粒度是回归框级，此方法适用于刚体这类有良好闭合边界的物体。然而文字往往由一系列松散的笔画构成，尤其对于任意方向或笔画宽度的文字，仅以回归框结果作为定位结果会有较大偏差。另外刚体检测的要求相对较低，即便只定位到部分主体（如定位结果与真值的重叠率是 50%），也不会对刚体识别产生重大影响，而这样的定位误差对于文字识别则很可能是致命的。

为了实现足够精细的定位，我们利用语义分割中常用的全卷积网络（FCN）来进行像素级别的文字/背景标注，整体流程如下图所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190915/GjYtx8QGJX8y.png?imageslim">
</p>

> 基于全卷积网络的文字检测


多尺度全卷积网络通过对多个阶段的反卷积结果的融合，实现了全局特征和局部特征的联合，进而达到了由粗到精的像素级别标注，适应于任意非受控场景（门头图、菜单图片）。

基于多尺度全卷积网络得到的像素级标注，通过连通域分析技术可得到一系列连通区域（笔划信息）。但由于无法确定哪些连通域属于同一文字行，因此需要借助单链聚类技术来进行文字行提取。至于聚类涉及的距离度量，主要从连通域间的距离、形状、颜色的相似度等方面提取特征，并通过度量学习自适应地得到特征权重和阈值，如下图所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190915/iq5pcWEdkjPN.png?imageslim">
</p>

> 基于全卷积网络的图像语义分割



下图分别给出了在菜单和门头图场景中的全卷积网络定位效果。第二列为全卷积网络的像素级标注结果，第三列为最终文字检测结果。可以看出，全卷积网络可以较好地应对复杂版面或多角度文字定位。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190915/hOBvLpG5OFAE.png?imageslim">
</p>

> 基于 FCN 的文字定位结果



# 相关

- [深度学习在 OCR 中的应用](https://tech.meituan.com/2018/06/29/deep-learning-ocr.html)
