---
title: 3.12 YOLOv2
toc: true
date: 2019-09-03
---

# 可以补充进来的

- [重温yolo v2](https://zhuanlan.zhihu.com/p/40659490)

# YOLOv2




YOLOv2改进了 YOLOv1 的网络结构，除加入时下热门的批量归一化层以及残差网络结构以外，还针对性的训练了一个高分辨率的分类网络（448x448）然后利用该网络训练检测网络，单单通过提升输入的分辨率，mAP获得了 4%的提升。YOLOv1利用单个 grid cell拼接成全连接层完成边框的预测，导致丢失较多的空间信息，定位不准，作者在这一版本中进行了优化改进：



1.借鉴了 Faster R-CNN中的 Anchor 思想，但是作者在实践中发现用基于规则选择的 Anchor 效果并没有得到提升，实验中作者对 Pascal VOC和 COCO 的数据集进行了统计分析（聚类分析）选择针对行的 Anchor 的尺寸性能的到了明显提升。



![mark](http://images.iterate.site/blog/image/20190905/v7X0YSQLiLmg.png?imageslim)


2.作者在使用 anchor boxes时发现模型收敛不稳定，尤其是在早期迭代的时候。大部分的不稳定现象出现在预测 box 的 (x,y) 坐标的优化上。因此作者就没有采用直接预测 offset 的方法，而使用了预测相对于 grid cell的坐标位置的办法，利用 logistic 函数把 ground truth归一化到 0 到 1 之间，坐标值被归一化后，模型优化会更稳定。

![mark](http://images.iterate.site/blog/image/20190905/XaFelnavRdff.png?imageslim)


**YOLOv2 有哪些创新点？**

YOLOv1 虽然检测速度快，但在定位方面不够准确，并且召回率较低。<span style="color:red;">召回率较低是什么。</span>为了提升定位准确度，改善召回率，YOLOv2 在 YOLOv1 的基础上提出了几种改进策略，如下图所示，可以看到，一些改进方法能有效提高模型的 mAP。

1. 大尺度预训练分类
2. New Network：Darknet-19
3. 加入 anchor

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/fa1iKObU7gde.png?imageslim">
</p>


**YOLOv2 介绍**

**（1）Batch Normalization**

YOLOv2 中在每个卷积层后加 Batch Normalization(BN) 层，去掉 dropout。BN 层可以起到一定的正则化效果，能提升模型收敛速度，防止模型过拟合。YOLOv2 通过使用 BN 层使得 mAP 提高了 2%。

**（2）High Resolution Classifier**

目前的大部分检测模型都会使用主流分类网络（如 vgg、resnet）在 ImageNet 上的预训练模型作为特征提取器,

而这些分类网络大部分都是以小于 256x256 的图片作为输入进行训练的，低分辨率会影响模型检测能力。YOLOv2 将输入图片的分辨率提升至 448x448，为了使网络适应新的分辨率，YOLOv2 先在 ImageNet 上以 448x448 的分辨率对网络进行 10 个 epoch 的微调，让网络适应高分辨率的输入。通过使用高分辨率的输入，YOLOv2 的 mAP 提升了约 4%。

**（3）Convolutional With Anchor Boxes**

YOLOv1 利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。YOLOv2 去掉了 YOLOv1 中的全连接层，使用 Anchor Boxes 预测边界框，同时为了得到更高分辨率的特征图，YOLOv2 还去掉了一个池化层。由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLOv2 通过缩减网络，使用 416x416 的输入，模型下采样的总步长为 32，最后得到 13x13 的特征图，然后对 13x13 的特征图的每个 cell 预测 5 个 anchor boxes，对每个 anchor box 预测边界框的位置信息、置信度和一套分类概率值。使用 anchor
boxes 之后，YOLOv2 可以预测 13x13x5=845 个边界框，模型的召回率由原来的 81% 提升到 88%，mAP 由原来的 69.5% 降低到 69.2%。召回率提升了 7%，准确率下降了 0.3%。

**（4）Dimension Clusters**

在 Faster R-CNN 和 SSD 中，先验框都是手动设定的，带有一定的主观性。YOLOv2 采用 k-means 聚类算法对训练集中的边界框做了聚类分析，选用 boxes 之间的 IOU 值作为聚类指标。综合考虑模型复杂度和召回率，最终选择 5 个聚类中心，得到 5 个先验框，发现其中中扁长的框较少，而瘦高的框更多，更符合行人特征。通过对比实验，发现用聚类分析得到的先验框比手动选择的先验框有更高的平均 IOU 值，这使得模型更容易训练学习。<span style="color:red;">嗯。</span>

**（5）New Network：Darknet-19**

YOLOv2 采用 Darknet-19，其网络结构如下图所示，包括 19 个卷积层和 5 个 max pooling 层，主要采用 3x3 卷积和 1x1 卷积，这里 1x1 卷积可以压缩特征图通道数以降低模型计算量和参数，每个卷积层后使用 BN 层以加快模型收敛同时防止过拟合。最终采用 global avg pool 做预测。采用 YOLOv2，模型的 mAP 值没有显著提升，但计算量减少了。<span style="color:red;">嗯。</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/YeNfvjCCr5NG.png?imageslim">
</p>


**（6）Direct location prediction**

Faster R-CNN 使用 anchor boxes 预测边界框相对先验框的偏移量，由于没有对偏移量进行约束，每个位置预测的边界框可以落在图片任何位置，会导致模型不稳定，加长训练时间。YOLOv2 沿用 YOLOv1 的方法，根据所在网格单元的位置来预测坐标，则 Ground Truth 的值介于 0 到 1 之间。网络中将得到的网络预测结果再输入 sigmoid 函数中，让输出结果介于 0 到 1 之间。设一个网格相对于图片左上角的偏移量是 cx,cy。先验框的宽度和高度分别是 pw 和 ph，则预测的边界框相对于特征图的中心坐标 (bx，by) 和宽高 bw、bh 的计算公式如下图所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/mMMY8t3dkNpY.png?imageslim">
</p>


YOLOv2 结合 Dimention Clusters, 通过对边界框的位置预测进行约束，使模型更容易稳定训练，这种方式使得模型的 mAP 值提升了约 5%。

**（7）Fine-Grained Features**

YOLOv2 借鉴 SSD 使用多尺度的特征图做检测，提出 pass through 层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度检测。YOLOv2 提取 Darknet-19 最后一个 max pool 层的输入，得到 26x26x512 的特征图。经过 1x1x64 的卷积以降低特征图的维度，得到 26x26x64 的特征图，然后经过 pass through 层的处理变成 13x13x256 的特征图（抽取原特征图每个 2x2 的局部区域组成新的 channel，即原特征图大小降低 4 倍，channel 增加 4 倍），再与 13x13x1024 大小的特征图连接，变成 13x13x1280 的特征图，最后在这些特征图上做预测。使用 Fine-Grained Features，YOLOv2 的性能提升了 1%.

**（8）Multi-Scale Training**

YOLOv2 中使用的 Darknet-19 网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。YOLOv2 采用多尺度输入的方式训练，在训练过程中每隔 10 个 batches，重新随机选择输入图片的尺寸，由于 Darknet-19 下采样总步长为 $32$，输入图片的尺寸一般选择 $32$ 的倍数 $\{320,352,…,608\}$。采用 Multi-Scale Training，可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP 值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高 mAP 值，但速度有所下降。

YOLOv2 借鉴了很多其它目标检测方法的一些技巧，如 Faster R-CNN 的 anchor boxes, SSD 中的多尺度检测。除此之外，YOLOv2 在网络设计上做了很多 tricks，使它能在保证速度的同时提高检测准确率，Multi-Scale Training 更使得同一个模型适应不同大小的输入，从而可以在速度和精度上进行自由权衡。

**YOLOv2 的训练**

YOLOv2 的训练主要包括三个阶段。

1. 先在 ImageNet 分类数据集上预训练 Darknet-19，此时模型输入为 $224\times 224$，共训练 160 个 epochs。
2. 将网络的输入调整为 $448\times 448$，继续在 ImageNet 数据集上 finetune 分类模型，训练 10 个 epochs，此时分类模型的 top-1 准确度为 76.5%，而 top-5 准确度为 93.3%。
3. 修改 Darknet-19 分类模型为检测模型，并在检测数据集上继续 finetune 网络。

网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling 层以及 softmax 层，并且新增了三个 $3\times 3 \times 2014$ 卷积层，同时增加了一个 passthrough 层，最后使用 $1\times 1$ 卷积层输出预测结果。






# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
