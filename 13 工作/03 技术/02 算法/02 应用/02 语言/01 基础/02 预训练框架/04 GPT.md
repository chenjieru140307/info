---
title: 04 GPT
toc: true
date: 2019-09-29
---
# 从 Word Embedding到 GPT

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/2k0S6bLfgL7K.png?imageslim">
</p>


GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。

GPT也采用两阶段过程：

- 第一个阶段是利用语言模型进行预训练，
- 第二阶段通过 Fine-tuning的模式解决下游任务。

上图展示了 GPT 的预训练过程，其实和 ELMO 是类似的，主要不同在于两点：

- 首先，特征抽取器不是用的 RNN，而是用的 Transformer，上面提到过它的特征抽取能力要强于 RNN，这个选择很明显是很明智的；
- 其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据 $W_i$ 单词的上下文去正确预测单词 $W_i$ ， $W_i$ 之前的单词序列 Context-before称为上文，之后的单词序列 Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词 $W_i$ 同时使用了上文和下文，而 GPT 则只采用 Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到 Word Embedding中，是很吃亏的，白白丢掉了很多信息。

> 这里强行插入一段简单提下 Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer 是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前 NLP 里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到 Transformer 一跃成为踢开 RNN 和 CNN 传统特征提取器，荣升头牌，大红大紫。
>
> Transformer在未来应该会逐渐替代掉 RNN 成为主流的 NLP 工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正 RNN 结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说 CNN，CNN在 NLP 里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获 NLP 的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显 Transformer 同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过 RNN 和 CNN。

好了，题外话结束，我们再回到主题，接着说 GPT。上面讲的是 GPT 如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和 ELMO 的方式大有不同。


## GPT 训练好后如何使用

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/OW7UTxe8GzIz.png?imageslim">
</p>


上图展示了 GPT 在第二阶段如何使用。

首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向 GPT 的网络结构看齐，把任务的网络结构改造成和 GPT 的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化 GPT 的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行 Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起图像领域如何做预训练的过程？对，这跟那个模式是一模一样的。

这里引入了一个新问题：对于 NLP 各种花样的不同任务，怎么改造才能靠近 GPT 的网络结构呢？


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/FzmwU5aSxTPa.png?imageslim">
</p>


GPT论文给了一个改造施工图如上，其实也很简单：

- 对于分类问题，不用怎么动，加上一个起始和终结符号即可；
- 对于句子关系判断问题，比如 Entailment，两个句子中间再加个分隔符即可；
- 对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；
- 对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。

从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。



## GPT 的效果如何

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/RNGyHDm8QHbv.png?imageslim">
</p>


GPT的效果是非常令人惊艳的，在 12 个任务里，9个达到了最好的效果，有些任务性能提升非常明显。


## GPT 有什么缺点

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190927/l6chAEtKCukE.png?imageslim">
</p>


那么站在现在的时间节点看，GPT有什么值得改进的地方呢？

其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有 Bert 太多事了。

当然，即使如此 GPT 也是非常非常好的一个工作，跟 Bert 比，其作者炒作能力亟待提升。



# 相关

- [从 Word Embedding到 Bert 模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)
