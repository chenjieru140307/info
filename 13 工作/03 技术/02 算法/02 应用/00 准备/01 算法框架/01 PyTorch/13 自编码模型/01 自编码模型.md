---
title: 01 自编码模型
toc: true
date: 2019-06-20
---
# 可以补充进来的

- <span style="color:red;">说实话这个例子挺好的，效果也挺好的，尤其是对于 matplotlib 的使用。嗯。</span>


# 自编码模型

在有监督学习中，训练样本是有类别标签的。现在假设我们只有一个没有带类别标签的训练样本集合 $x^{(1)},x^{(2)}, x^{(3)},\ldots$ 其中 $x^{(i)} \in R^{n}$ 。


其中的一种算法叫作 AutoEncoder - 自编码网络。自编码神经网络是一种无监督学习算法，它使用了反向传播算法，并让目标值等于输入值。

简单的自编码是一种三层神经网络模型，包含了数据输入层、隐藏层、输出重构层，同时它是一种无监督学习模型。

在有监督的神经网络中，我们的每个训练样本是 $(x, y)$ , $y$ 一般是我们人工标注的数据。比如我们用于手写的字体分类，那么 $y$ 的取值就是 $0~9$ 之间的数值，最后神经网络设计的时候，网络的输出层是一个 10 个神经元的网络模型（比如网络输出是（0,0,1,0,0,0,0,0,0,0），那么就表示该样本标签为 2）。

然而自编码是一种无监督学习模型，我们训练数据本来是没有标签的，那么自编码是这样干的，它令每个样本的标签为 $y=x$，也就是每个样本的数据 $x$ 的标签也是 $x$。自编码就相当于自己生成标签，而且标签是样本数据本身。

下图是一个自编码神经网络的示例：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190618/IU0Mr4YjF5XF.png?imageslim">
</p>

网络中最左侧节点是输入层，最右侧一列神经元是输出层。

自编码模型内部结构：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190618/fS1iGUhjvW6I.png?imageslim">
</p>


输出层的神经元数量完全等于输入层神经元的数量。隐藏层的神经元数量少于输出层。自编码网络的作用是将输入样本压缩到隐藏层，再在输出端重建样本，即压缩和解压。自编码神经网络尝试 $h_{w,b}(x) \approx x$。

自编码网络输出层与输入层存在如下关系：$x_{\mathrm{i}}^{\wedge} \approx x_{\mathrm{i}}$。

自编码网络是将经过压缩的数据还原，在压缩的过程中，限制隐藏层的稀疏性。<span style="color:red;">怎么限制隐藏层的稀疏性？</span>神经元总是使用一个激活函数，神经元分为 “激活状态” 和 “非激活状态”，如果大部分神经元处于非激活状态，这时候隐藏层中的激活神经元是 “稀疏” 的，即稀疏性。然后目标函数为了还原数据应该使得损失尽量小，定义如下：

$$
J(W, b)=\frac{1}{m} \sum_{i=1}^{m}(\hat{x}-x)^{2}
$$

如果我们输入一张 10×10 像素的灰度图像，这样就有 100 个像素点，所以输入层和输出层的节点数量就是 100，而我们取隐藏层节点数量为 25。要求每一个输出神经元的输出值和输入图像的对应像素灰度相同，这样就会迫使隐藏层节点学习得到输入数据的压缩表示方法，迫使隐藏层要用 25 维数据重构出 100 维的数据，这样也就完成了学习压缩过程。<span style="color:red;">是的，是的，以前听到这个编码解码器的时候真的感觉太惊艳了！</span>

通常隐含层的神经元数目要比输入/输出层的少，这样做的目的是为了神经网络只学习最重要的特征并实现特征降维。<span style="color:red;">是的。</span>

它能从原数据中总结出每种类型数据的特征，如果把这些特征类型都放在一张二维的图片上，每种类型都已经被很好地用原数据的精髓区分开来。<span style="color:red;">是的。</span>如果你了解 PCA 主成分分析，再提取主要特征时，自编码和它一样，甚至超越了 PCA。换句话说，自编码可以像 PCA 一样给特征属性降维。<span style="color:red;">是的，真实厉害呀！</span>

接下来学习自编码实现。

上面，我们已经学习了自编码的原理，下面用代码来实现。


```py
import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.utils.data as Data
import torchvision
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
import numpy as np


# torch.manual_seed(1)    # reproducible

# Hyper Parameters
EPOCH = 10
BATCH_SIZE = 64
LR = 0.005         # learning rate
DOWNLOAD_MNIST = True
N_TEST_IMG = 5

# Mnist digits dataset
train_data = torchvision.datasets.MNIST(
    root='./mnist/',
    train=True,                                     # this is training data
    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to
                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]
    download=DOWNLOAD_MNIST,                        # download it if you don't have it
)

# plot one example
print(train_data.train_data.size())     # (60000, 28, 28)
print(train_data.train_labels.size())   # (60000)
plt.imshow(train_data.train_data[2].numpy(), cmap='gray')
plt.title('%i' % train_data.train_labels[2])
plt.show()

# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)
train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)


class AutoEncoder(nn.Module):
    def __init__(self):
        super(AutoEncoder, self).__init__()

        self.encoder = nn.Sequential(
            nn.Linear(28*28, 128),
            nn.Tanh(),
            nn.Linear(128, 64),
            nn.Tanh(),
            nn.Linear(64, 12),
            nn.Tanh(),
            nn.Linear(12, 3),   # compress to 3 features which can be visualized in plt
        )
        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.Tanh(),
            nn.Linear(12, 64),
            nn.Tanh(),
            nn.Linear(64, 128),
            nn.Tanh(),
            nn.Linear(128, 28*28),
            nn.Sigmoid(),       # compress to a range (0, 1)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded


autoencoder = AutoEncoder()

optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)
loss_func = nn.MSELoss()

# initialize figure
f, a = plt.subplots(2, N_TEST_IMG, figsize=(5, 2))
plt.ion()   # continuously plot

# original data (first row) for viewing
view_data = Variable(train_data.train_data[:N_TEST_IMG].view(-1, 28*28).type(torch.FloatTensor)/255.)
for i in range(N_TEST_IMG):
    a[0][i].imshow(np.reshape(view_data.data.numpy()[i], (28, 28)), cmap='gray'); a[0][i].set_xticks(()); a[0][i].set_yticks(())

for epoch in range(EPOCH):
    for step, (x, y) in enumerate(train_loader):
        b_x = Variable(x.view(-1, 28*28))   # batch x, shape (batch, 28*28)
        b_y = Variable(x.view(-1, 28*28))   # batch y, shape (batch, 28*28)
        b_label = Variable(y)               # batch label

        encoded, decoded = autoencoder(b_x)

        loss = loss_func(decoded, b_y)      # mean square error
        optimizer.zero_grad()               # clear gradients for this training step
        loss.backward()                     # backpropagation, compute gradients
        optimizer.step()                    # apply gradients

        if step % 100 == 0:
            print('Epoch: ', epoch, '| train loss: %.4f' % loss.item())

            # plotting decoded image (second row)
            _, decoded_data = autoencoder(view_data)
            for i in range(N_TEST_IMG):
                a[1][i].clear()
                a[1][i].imshow(np.reshape(decoded_data.data.numpy()[i], (28, 28)), cmap='gray')
                a[1][i].set_xticks(()); a[1][i].set_yticks(())
            plt.draw(); plt.pause(0.05)

plt.ioff()
plt.show()


# visualize in 3D plot
```

输出：

```
Files already downloaded
torch.Size([60000, 28, 28])
torch.Size([60000])
Epoch:  0 | train loss: 0.2298
Epoch:  0 | train loss: 0.0721
Epoch:  0 | train loss: 0.0624
Epoch:  0 | train loss: 0.0619
略..
Epoch:  9 | train loss: 0.0341
Epoch:  9 | train loss: 0.0376
Epoch:  9 | train loss: 0.0358
Epoch:  9 | train loss: 0.0334
```

输出图像：

<span style="color:red;">暂时没有联网，这个地方的图像联网后补充下。第一次看到这种在 matplotlib 中显示训练状态图的。感觉这个应该还挺好的，挺赞的，可以应用到很多项目中。</span>

```
```

对于上面的代码有几个问题：

<span style="color:red;">

1. 为什么使用 Tanh ？ 是有什么好处吗？使用别的可以吗？
2. 一定要是 nn.Sequential 这种形式吗？为什么要用 nn.Sequential 来包裹？之前好像没有怎么见到使用 nn.Sequential 的。
3. 为什么 forward 返回的是 encode 和 decode ？是有什么说法吗？这个跟 loss 有什么关系？自己写的话要怎么构造？
4. 对于 plt.ion() 和 plt.ioff() 挺感兴趣的，感觉这种动态图非常的直观，甚至可以直接抽取模型里面的权重画出来看下效果。厉害呀。之前不知道还可以这样，只知道 matplotlib 可以画动态图，没想到可以这样动态。厉害。要补充下。
5. 说实话，有点怀疑，3 个神经元真的可以还原成 28*28 吗？

</span>

程序简单介绍如下：

压缩过程：

输入的图片大小为 28×28 像素大小，经过第一层的隐藏层，128 个神经元经过 Tanh函数线性变换，然后再经过第二层的隐藏层，由 128 个神经元变成 64 个神经元，经过 Tanh 激活函数，进行线性变换，然后再经过第三层的隐藏层，由 64 个神经元变成 12 个神经元，经过 Tanh 激活函数，进行线性变换，然后再经过第四层的隐藏层，由 12 个神经元变成 3 个神经元，进行线性变换。上述步骤，我们完成了图片压缩步骤。下面我们来看看如何进行解压操作。

解压缩过程：

输入的图片经过压缩，从 128 个神经元压缩到 3 个神经元，现在，如何把压缩后的图片像素转换成原始图片的像素呢？首先，解压操作经过第一层的隐藏层，3 个神经元经过 Tanh 函数线性变换，由 3 个神经元变成 12 个神经元，然后再经过第二层的隐藏层，由 12 个神经元变成 64 个神经元，经过 Tanh 激活函数，进行线性变换，然后再经过第三层的隐藏层，由 64 个神经元变成 128 个神经元，经过 Tanh 激活函数，进行线性变换，然后再经过第四层的隐藏层，由 128 个神经元变成 784 个神经元，进行线性变换，得到 784 个像素值。上述步骤，我们完成了图片解压步骤。<span style="color:red;">嗯。</span>


`torch.optim.Adam()` 的参数如下：

- lr（float，可选）：学习速率（默认：1e-3）。<span style="color:red;">其实一直想知道为什么默认的学习速率是 1e-3 呢？</span>
- betas（Tuple[float,float]，可选）：用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9,0.999）。<span style="color:red;">什么是运行平均值的系数？</span>
- eps（float，可选）：为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）。<span style="color:red;">哈哈，知道了，嗯，默认是有值的。之前不知道已经有这个默认值了。</span>
- weight_decay（float，可选）：权重衰减（L2惩罚）（默认：0）。<span style="color:red;">嗯，如果我想要使用这个 L2 权重惩罚的话，我需要设置多少为好？</span>


`class torch.nn.MSELoss（size_average=True）` 参数如下：

- 如果在创建 MSELoss 实例的时候，在构造函数中传入 size_average=False，那么求出来的平方和将不会除以 n。<span style="color:red;">嗯，对于 n 为固定值的来说，除以跟不除以不是一样的吗？还是说有区别的？想知道。</span>






# 相关

- 《深度学习框架 Pytorch 快速开发与实战》
