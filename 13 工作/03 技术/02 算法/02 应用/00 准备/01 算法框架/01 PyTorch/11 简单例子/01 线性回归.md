---
title: 01 线性回归
toc: true
date: 2019-05-27
---
# 线性回归

线性回归是利用数理统计中的回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。分析按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析。

回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系的，则称为多元线性回归分析。

线性模型基本形式

线性回归属于回归算法，表达监督学习的过程。通过属性的线性组合来预测函数：


$$
f(x)=w_{1} x_{1}+w_{2} x_{2}+\cdots+w_{d} x_{d}+b
$$

一般向量形式写成：

$$
f(x)=\boldsymbol{w}^{\mathrm{T}} x+b
$$

其中的 $\boldsymbol{w}=\left(w_{1}, w_{2} ; \ldots, w_{\mathrm{d}}\right)$。

$x_{1}, x_{2}, \ldots, x_{k}$ 是一组独立的预测变量。

$w_{1}, w_{2}, \ldots, w_{k}$ 为模型从训练数据中学习到的参数，或赋予每个变量的“权值”。

$b$ 也是一个学习到的参数，这个线性函数中的常量也称为模型的偏置（Bias）。

线性回归的目标是找到一个与这些数据最为吻合的线性函数，用来预测或者分类，主要解决线性问题。在有监督学习问题中，线性回归是一种最简单的建模手段。给定一个数据点集合作为训练集，线性回归的目标是找到一个与这些数据最为吻合的线性函数。

对于 2D 数据，这样的函数对应一条直线。

一般来说，线性回归都可以通过最小二乘法求出其方程。线性回归属于监督学习，因此方法和监督学习应该是一样的，先给定一个训练集，根据这个训练集学习出一个线性函数，然后测试这个函数训练得好不好（即此函数是否足够拟合训练集数据），挑选出最好的函数（Cost Function最小）即可。Cost Function越小的函数，说明对训练数据拟合得越好。

接下来我们来重点讲讲如何用 PyTorch 写一个简单的线性回归函数。

```py
import torch
import torch.nn as nn
from torch.autograd import Variable

import numpy as np
import matplotlib.pyplot as plt

# Hyper Parameters
input_size = 1
output_size = 1
num_epochs = 10000
learning_rate = 0.001

# 散点
x_train = np.array([[2.3], [4.4], [3.7], [6.1], [7.3],
                    [2.1], [5.6], [7.7], [8.7], [4.1],
                    [6.7], [6.1], [7.5], [2.1], [7.2],
                    [5.6], [5.7], [7.7], [3.1]], dtype=np.float32)

y_train = np.array([[3.7], [4.76], [4.], [7.1], [8.6],
                    [3.5], [5.4], [7.6], [7.9], [5.3],
                    [7.3], [7.5], [8.5], [3.2], [8.7],
                    [6.4], [6.6], [7.9], [5.3]], dtype=np.float32)
plt.figure()
plt.scatter(x_train, y_train)
plt.xlabel('x_train')
plt.ylabel('y_train')
plt.show()


# 创建线性回归模型
class LinearRegression(nn.Module):
    def __init__(self, input_size, output_size):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(input_size, output_size)

    def forward(self, x):
        out = self.linear(x)
        return out


model = LinearRegression(input_size, output_size)

# 定义损失函数和优化器
criterion = nn.MSELoss()  # 均方误差
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # 随机梯度下降

# 训练模型
for epoch in range(num_epochs):
    # Convert numpy array to torch Variable
    inputs = Variable(torch.from_numpy(x_train))
    targets = Variable(torch.from_numpy(y_train))

    # Forward + Backward + Optimize
    optimizer.zero_grad()  # 清空梯度
    outputs = model(inputs)  # 计算预测输出
    loss = criterion(outputs, targets)  # 计算损失
    loss.backward()  # 反向传播
    optimizer.step()  # 优化器变更参数

    if (epoch + 1) % 5 == 0:
        print('Epoch [%d/%d], Loss: %.4f'
              % (epoch + 1, num_epochs, loss.item())) # loss.data[0] deprecated in 0.5



# 使用模型对 x 进行预测，并绘制预测图形
model.eval()  # 切换到到预测模式
predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()  # 进行预测
print(predicted)
plt.plot(x_train, y_train, 'ro') # TODO 这个地方的 'ro' 是什么意思？为什么不用 scatter？
plt.plot(x_train, predicted, label='predict')
plt.legend()
plt.show()
```

输出：

```
Epoch [5/10000], Loss: 17.3591
Epoch [10/10000], Loss: 8.6761
略...
Epoch [9960/10000], Loss: 0.3724
Epoch [9965/10000], Loss: 0.3724
Epoch [9970/10000], Loss: 0.3724
Epoch [9975/10000], Loss: 0.3724
Epoch [9980/10000], Loss: 0.3724
Epoch [9985/10000], Loss: 0.3724
Epoch [9990/10000], Loss: 0.3724
Epoch [9995/10000], Loss: 0.3724
Epoch [10000/10000], Loss: 0.3724
[[ 3.60923696]
 [ 5.37713718]
 [ 4.78783703]
 [ 6.8082943 ]
 [ 7.81852245]
 [ 3.44086552]
 [ 6.38736534]
 [ 8.15526485]
 [ 8.99712181]
 [ 5.12457943]
 [ 7.3134079 ]
 [ 6.8082943 ]
 [ 7.98689365]
 [ 3.44086552]
 [ 7.73433685]
 [ 6.38736534]
 [ 6.47155094]
 [ 8.15526485]
 [ 4.28272247]]
```


输出图像：

img1:

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190526/MoylUU9yodnp.png?imageslim">
</p>

img2:

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190526/8eES2NtuGgb2.png?imageslim">
</p>

简单说明：

- 这里的 `self.linear = nn.Linear(input_size, output_size)`，里面的两个参数都是 1，表示的是 x 是 1 维，y 也是 1 维。

- 优化函数的种类有很多，平时我们使用比较多的是 Gradient Descent Optimizer 和 Adam Optimizer 等。<span style="color:red;">将支持的各种类型和对应的应用场景都总结到这里。</span>

- 在每次反向传播的时候需要将参数的梯度归零。


# 相关

- 《深度学习框架 Pytorch 快速开发与实战》
