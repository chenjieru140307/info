---
title: 02 递归神经网络预测股价
toc: true
date: 2019-06-22
---
# 可以补充进来的

- <span style="color:red;">程序有很多地方没有深刻理解。要重看下。</span>
- 而且模型出来的数据也不对，是啥问题？样本的问题？还是模型本身的问题？后面用别的模型试下这个。


# 递归神经网络预测股价

股票市场是一个复杂的非线性动力学系统，恰好递归神经网络具有非线性映射能力，自学习能力的优点，非常适合于股票的相关性分析和预测。股票的预测是一个动态的时间建模问题，我们在预测个股的第二天涨跌情况，或者是第二天的收盘价格，只是利用过去的历史数据的开盘价、收盘价、最高价、最低价等进行建模分析。这种问题相当于一个时间序列问题，递归神经网络恰恰适合处理这种问题。

递归神经网络（RNN）是两种人工神经网络的总称。一种是时间递归神经网络（Recurrent Neural Network），另一种是结构递归神经网络（Recursive Neural Network）。时间递归神经网络的神经元间连接构成有向图，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。两者训练的算法不同，但属于同一算法变体。<span style="color:red;">之前好像只知道时间递归神经网络，不是很清楚还有结构递归神经网络。补充下。</span>

传统的神经网络模型中，我们假设所有的输入是互相独立的。从输入层到隐藏层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。循环神经网络与传统的前馈神经网络有所不同，循环神经网络的隐藏层相互连接，即一个序列当前的输出与前面的输出也有关。

循环神经网络会对每一个时刻的输入结合当前模型的状态给出一个输出。RNN 是对序列的每个元素执行同样的操作，其输出依赖于前次计算的结果。RNN 引入了定向循环，能够处理那些输入之间前后关联的问题。RNN 拥有捕获已计算节点信息的记忆能力。

LSTM 长短时模型是最适合金融数据、量化分析的神经网络模型之一，既可以用于数值预测，也可以用于趋势预测。LSTM 算法，具有 RNN 循环神经网络算法的各种优点，但因为 LSTM 模型有更好的记忆能力，所以效果更佳。利用深度神经网络中的 LSTM 的一些基本结果，对多因子模型进行尝试，以检验深度神经网络在多因子、投资领域的适用性，使得投资者能够对神经网络有更为实际的理解，并能够在投资领域有所运用。<span style="color:red;">真的能够有所应用吗？</span>

下文，我们使用 LSTM 模型分别进行第二天股价收盘价的预测与第二天股价涨跌的预测。

首先，我们进行上证指数的次日收盘价价格进行预测。数据如图所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190622/6iYCa12pEgr7.png?imageslim">
</p>


我们采用前 100 天的开盘价、收盘价、最高价、最低价数据当作输入数据，第二天到 102 天的收盘价当作输出数据进行训练。

```py
import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
import pandas as pd

# 加载所需的模块包

# 设置参数

input_size = 1
hidden_size = 100
num_layers = 10
num_classes = 1

df = pd.read_excel(r"./shanghai_index.xlsx")
df1 = df.iloc[:100, 3:7].values
df2 = df.iloc[1:101, 7].values
data_features = torch.FloatTensor(df1)
data_labels = torch.FloatTensor(df2)

xtrain = torch.unsqueeze(data_features, dim=2)
ytrain = torch.unsqueeze(data_labels, dim=0)

# x1 = Variable(data_features.view(100, 4, 1))
x = Variable(xtrain)
y = Variable(ytrain)


# 定义循环神经网络结构
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))
        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))

        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out


rnn = RNN(input_size, hidden_size, num_layers, num_classes)

# 损失函数以及优化函数


# 训练模型
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)
for epoch in range(100):  # 100000
    inputs = x
    target = y
    out = rnn(inputs)  # 前向传播
    out_reshape = out.reshape(1, 100)
    loss = criterion(out_reshape, target)  # 计算 loss
    # backward
    optimizer.zero_grad()  # 梯度归零
    loss.backward()  # 方向传播
    optimizer.step()  # 更新参数

    if (epoch + 1) % 20 == 0:
        print('Epoch[{}], loss: {:.6f}'.format(epoch + 1, loss.item()))

rnn.eval()
predict = rnn(x)
predict = predict.data.numpy()
print(predict)
```

<span style="color:red;">上面这部分程序有很多地方没有深刻理解。</span>

输出：

```
Epoch[20], loss: 0.249401
Epoch[40], loss: 0.245553
Epoch[60], loss: 0.245175
Epoch[80], loss: 0.245102
Epoch[100], loss: 0.245101
[[0.5710173]
[0.5710173]
[0.5710173]
略..
[0.5710173]
[0.5710173]
[0.5710173]
[0.5710173]]
```

<span style="color:red;">只设置了 100 个 epoch ，不过为什么输出是这个呢？</span>



训练 LSTM 模型时，在参数层面上有两个十分重要的参数可以控制模型的过拟合：Dropout 参数和在权重上施加正则项：

- Dropout 是指在每次输入时随机丢弃一些 Features，从而提高模型的鲁棒性。它的出发点是通过不停去改变网络的结构，使神经网络记住的不是训练数据本身，而是能学出一些规律性的东西。
- 正则项则是通过在计算损失函数时增加一项 L2 范数，使一些权重的值趋近于 0，避免模型对每个 Feature 强行适应与拟合，从而提高鲁棒性，也有因子选择的效果。



# 相关


- 《深度学习框架 Pytorch 快速开发与实战》
- 数据如下：https://pan.baidu.com/s/1_pSY6_qP7zzzcJQLtjK08Q 提取码：ypas
