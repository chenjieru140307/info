
10.7 小结
值得注意的是，结构化流现在（在撰写本文时）还不具备应用于生产环境的条件。然而，Spark是一个范式转移，希望能够使数据科学家和数据工程师更容易构建连续应用程序（continuous applications）。尽管在前面几节中没有明确提及，但在使用流应用程序时，你需要在设计时考虑许多潜在的问题，例如事件滞后、部分输出、故障状态恢复、分布式读取和写入等。通过使用结构化流，这些问题大部分将被抽离出来，使你更容易构建连续应用程序。
我们鼓励你尝试使用 Spark 结构化流，以便在结构化流成熟时能够轻松地构建流应用程序。Reynold Xin曾在 2016 Spark技术峰会的演讲“The Future of Real-Time in Spark”中提到：“执行流分析的最简单的方式就是无需思考它。”（http://www.slideshare.net/rxin/the-future-of-realtime-in-spark）。
更多信息，请参阅以下更多的结构化流资源：
·PySpark 2.1 Documentation：pyspark.sql.module（http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html）
·Introducing Apache Spark2.1（https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html）
·Structuring Apache Spark2.0：SQL，DataFrames，Datasets and Streaming-by Michael Armbrust（http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797）
·Structured Streaming Programming Guide（http://spark.apache.org/docs/latest/streaming-programming-guide.html）
·Structured Streaming（aka Streaming DataFrames）[SPARK-8360]（https://issues.apache.org/jira/browse/SPARK-8360）
·Structured Streaming Programming Abstraction，Semantics，and APIs Apache JIRA（https://issues.apache.org/jira/secure/attachment/12793410/StructuredStreamingProgrammingAbstractionSemanticsandAPIs-ApacheJIRA.pdf）
在下一章中，我们将向你展示如何通过编程的方式来对你的 PySpark 应用程序执行模块化、打包及提交。
