
2.3 全局作用域和局部作用域
作为一个潜在的 PySpark 用户，你需要习惯的其中一件事是 Spark 的内在并行性。即使你精通 python，但是执行 PySpark 脚本还是需要你改变一点思维模式。
Spark可以在两种模式下运行：本地的和集群的。本地运行你的 Spark 代码时，和你目前使用的 python 没有什么不同：相比任何其他部分来说，变化最有可能是语法上的，只是加上了一个交织的部分，数据和代码在不同的工作者进程之间复制。
然而，如果你不小心将相同的代码部署到集群，便可能会导致大量的困扰。这就需要了解 Spark 是怎么在集群上执行工作的任务的。
在集群模式中，提交执行任务时，任务被发送给了驱动程序节点（或者主节点）。该驱动程序节点为任务创建 DAG（见第 1 章），并且决定哪一个执行者（工作者）节点将运行特定的任务。
然后，该驱动程序指示工作者执行它们的任务，并且在结束时将结果返回给驱动程序。然而在这之前，驱动程序为每一个任务的终止做准备：驱动程序中有一组变量和方法，以便工作者在 RDD 上执行任务。
这组变量和方法在执行者的上下文中本质上是静态的，即每个执行器从驱动程序中获得一份变量和方法的副本。运行任务时，如果执行者改变这些变量或者覆盖这些方法，它不影响任何其他执行者的副本或者驱动程序的变量和方法。这可能会导致一些意想不到的行为和运行错误，这些行为和错误通常都很难被追踪到。查看 PySpark 文档更多实例的讨论：http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes。
