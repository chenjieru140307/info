
# 大数据工具联系和对比




## 可以补充进来的

* 没有看，需要整理掌握



**·大数据处理技术怎么学习呢?首先我们要学习 Java 语言和 Linux 操作系统，这两个是学习大数据的基础，学习的顺序不分前后。**

Java：大家都知道 Java 的方向有 JavaSE、JavaEE、JavaME，学习大数据要学习那个方向呢?

只需要学习 Java 的标准版 JavaSE 就可以了，像 Servlet、JSP、Tomcat、Struct、Spring、Hibernate，Mybites都是 JavaEE 方向的技术在大数据技术里用到的并不多，只需要了解就可以了，当然 Java 怎么连接数据库还是要知道的，像 JDBC 一定要掌握一下，有同学说 Hibernate 或 Mybites 也能连接数据库啊，为什么不学习一下，我这里不是说学这些不好，而是说学这些可能会用你很多时间，到最后工作中也不常用，我还没看到谁做大数据处理用到这两个东西的，当然你的精力很充足的话，可以学学 Hibernate 或 Mybites 的原理，不要只学 API，这样可以增加你对 Java 操作数据库的理解，因为这两个技术的核心就是 Java 的反射加上 JDBC 的各种使用。

Linux：因为大数据相关软件都是在 Linux 上运行的，所以 Linux 要学习的扎实一些，学好 Linux 对你快速掌握大数据相关技术会有很大的帮助，能让你更好的理解 hadoop、hive、hbase、spark等大数据软件的运行环境和网络环境配置，能少踩很多坑，学会 shell 就能看懂脚本这样能更容易理解和配置大数据集群。还能让你对以后新出的大数据技术学习起来更快。

·好说完基础了，再说说还需要学习哪些大数据技术，可以按我写的顺序学下去。#

Hadoop：这是现在流行的大数据处理平台几乎已经成为大数据的代名词，所以这个是必学的。Hadoop里面包括几个组件 HDFS、MapReduce和 YARN，HDFS是存储数据的地方就像我们电脑的硬盘一样文件都存储在这个上面，MapReduce是对数据进行处理计算的，它有个特点就是不管多大的数据只要给它时间它就能把数据跑完，但是时间可能不是很快所以它叫数据的批处理。YARN是体现 Hadoop 平台概念的重要组件有了它大数据生态体系的其它软件就能在 hadoop 上运行了，这样就能更好的利用 HDFS 大存储的优势和节省更多的资源比如我们就不用再单独建一个 spark 的集群了，让它直接跑在现有的 hadoop yarn上面就可以了。其实把 Hadoop 的这些组件学明白你就能做大数据的处理了，只不过你现在还可能对”大数据”到底有多大还没有个太清楚的概念，听我的别纠结这个。等以后你工作了就会有很多场景遇到几十 T/几百 T 大规模的数据，到时候你就不会觉得数据大真好，越大越有你头疼的。当然别怕处理这么大规模的数据，因为这是你的价值所在，让那些个搞 Javaee 的 php 的 html5 的和 DBA 的羡慕去吧。

**·记住学到这里可以作为你学大数据的一个节点。**

Zookeeper：这是个万金油，安装 Hadoop 的 HA 的时候就会用到它，以后的 Hbase 也会用到它。它一般用来存放一些相互协作的信息，这些信息比较小一般不会超过 1M，都是使用它的软件对它有依赖，对于我们个人来讲只需要把它安装正确，让它正常的 run 起来就可以了。

Mysql：我们学习完大数据的处理了，接下来学习学习小数据的处理工具 mysql 数据库，因为一会装 hive 的时候要用到，mysql需要掌握到什么层度那?你能在 Linux 上把它安装好，运行起来，会配置简单的权限，修改 root 的密码，创建数据库。这里主要的是学习 SQL 的语法，因为 hive 的语法和这个非常相似。

Sqoop：这个是用于把 Mysql 里的数据导入到 Hadoop 里的。当然你也可以不用这个，直接把 Mysql 数据表导出成文件再放到 HDFS 上也是一样的，当然生产环境中使用要注意 Mysql 的压力。

Hive：这个东西对于会 SQL 语法的来说就是神器，它能让你处理大数据变的很简单，不会再费劲的编写 MapReduce 程序。有的人说 Pig 那?它和 Pig 差不多掌握一个就可以了。

Oozie：既然学会 Hive 了，我相信你一定需要这个东西，它可以帮你管理你的 Hive 或者 MapReduce、Spark脚本，还能检查你的程序是否执行正确，出错了给你发报警并能帮你重试程序，最重要的是还能帮你配置任务的依赖关系。我相信你一定会喜欢上它的，不然你看着那一大堆脚本，和密密麻麻的 crond 是不是有种想屎的感觉。

Hbase：这是 Hadoop 生态体系中的 NOSQL 数据库，他的数据是按照 key 和 value 的形式存储的并且 key 是唯一的，所以它能用来做数据的排重，它与 MYSQL 相比能存储的数据量大很多。所以他常被用于大数据处理完成之后的存储目的地。

Kafka：这是个比较好用的队列工具，队列是干吗的?排队买票你知道不?数据多了同样也需要排队处理，这样与你协作的其它同学不会叫起来，你干吗给我这么多的数据(比如好几百 G 的文件)我怎么处理得过来，你别怪他因为他不是搞大数据的，你可以跟他讲我把数据放在队列里你使用的时候一个个拿，这样他就不在抱怨了马上灰流流的去优化他的程序去了，因为处理不过来就是他的事情。而不是你给的问题。当然我们也可以利用这个工具来做线上实时数据的入库或入 HDFS，这时你可以与一个叫 Flume 的工具配合使用，它是专门用来提供对数据进行简单处理，并写到各种数据接受方(比如 Kafka)的。

Spark：它是用来弥补基于 MapReduce 处理数据速度上的缺点，它的特点是把数据装载到内存中计算而不是去读慢的要死进化还特别慢的硬盘。特别适合做迭代运算，所以算法流们特别稀饭它。它是用 scala 编写的。Java语言或者 Scala 都可以操作它，因为它们都是用 JVM 的。

**·会这些东西你就成为一个专业的大数据开发工程师了，月薪 2W 都是小毛毛雨**

后续提高：当然还是有很有可以提高的地方，比如学习下 python，可以用它来编写网络爬虫。这样我们就可以自己造数据了，网络上的各种数据你高兴都可以下载到你的集群上去处理。

最后再学习下推荐、分类等算法的原理这样你能更好的与算法工程师打交通。















# 相关

- [我想学大数据分析，但是 0 基础，求前辈老师指点？](https://www.zhihu.com/question/53006477)
