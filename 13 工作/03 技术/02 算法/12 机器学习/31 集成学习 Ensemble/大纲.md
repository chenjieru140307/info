
# 集成学习

　　集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost，堆叠泛化（Stacked Generalization， Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest），GBDT（Gradient Boosting Decision Tree）。

## 主要内容


- 介绍
- Boosting
- Bagging
- 基分类器
- 偏差和方差的角度来解释
- 学习器的结合策略
- 多样性



## 可以补充进来的




## 后续需要消化的


Boosting 源于对的“弱学习是否等价于强学习”这个重要理论问题的构造性证明。最初的 Boosting 算法仅有理论意义，经数年努力后有了 AdaBoost。

不同集成学习方法的工作机理和理论性质往往有显著不同，例如从偏差-方差分解的角度看， Boosting 主要关注降低偏差，而 Bagging 主要关注降低方差。

MultiBoosting 等方法尝试将二者的优点加以结合。

8.2 节给出的 AdaBoost 推导源于“统计视角” (statistical view) ，此派理论认为 AdaBoost 实质上是基于加性模型(additive model) 以类似牛顿迭代法来优化指数损失函数。受此启发，通过将迭代优化过程替换为其他优化方法，产生了 GradientBoosting 、LPBoost 等变体算法。然而，这派理论产生的推论与 AdaBoost 实际行为有相当大的差别，尤其是它不能解释 AdaBoost 为什么没有过拟合这个重要现象，因此不少人认为，统计视角本身虽很有意义， 但其阐释的是一个与 AdaBoost 相似的学习过程而并非 AdaBoost 本身。“间隔理论”(margin theory) 能直观地解释这个重要现象，但过去 15 年中一直存有争论，直到最近的研究结果使它最终得以确立，并对新 型学习方法的设计给出了启示；



本章仅介绍了最基本的几种结合方法，常见的还有基于 D-S证据理论的方法、动态分类器选择、混合专家(mixture of experts)等。

本章仅介绍了成对型多样性度量。现有多样性度量都存在显著缺陷。如何理解多样性，被认为是集成学习中的圣杯问题。

对并行化集成的修剪亦称“选择性集成”(selective ensemble)，但现在一 般将选择性集成用作集成 修剪的同义语，亦称“集 成选择” (ensemble selection)。

在集成产生之后再试图通过去除一些个体学习器来获得较小的集成，称 为集成修剪(ensemble pruning)。这有助于减小模型的存储开销和预测时间开 销。早期研究主要针对序列化集成进行，减小集成规模后常导致泛化性能下降 ［Rokach, 2010a］; ［Zhou et al., 2002］揭示出对并行化集成进行修剪能在减小规 模的同时提升泛化性能，并催生了基于优化的集成修剪技术。这方面的内容可 参阅［Zhou, 2012］第 6 章。

关于聚类、半监督学习、代价敏感学习等任务中集成学习的内容，可参阅 ［Zhou, 2012］第 7〜8章。事实上，集成学习已被广泛用于几乎所有的学习任务。 著名数据挖掘竞赛 KDDCup 历年的冠军几乎都使用了集成学习。

由于集成包含多个学习器，即便个体学习器有较好的可解释性，集成仍是 黑箱模型。已有一些工作试图改善集成的可解释性，例如将集成转化为单模 型、从集成中抽取符号规则等，这方面的研究衍生出了能产生性能超越集成 的单学习器的“二次学习”(twice-learning)技术，例如 NeC4.5算法［Zhou and Jiang, 2004］。可视化技术也对改善可解释性有一定帮助。可参阅［Zhou, 2012］ 第 8 章。
