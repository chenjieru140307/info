
# 梯度提升决策树的基本原理



梯度提升决策树的核心思想是，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。<span style="color:red;">之前所有树结论和的残差？这个残差就是一个加预测值后能得到真实值的累积量。是什么意思？梯度提升决策树之前好像看到过，但是忘记了。再补充下。</span>

我们以一个视频网站的用户画像为例，为了将广告定向投放给指定年龄的用户，视频网站需要对每个用户的年龄做出预测。在这个问题中，每个样本是一个已知性别/年龄的用户，而特征则包括这个人访问的时长、时段、观看的视频的类型等。

例如：

1. 用户 A 的真实年龄是 25 岁，但第一棵决策树的预测年龄是 22 岁，差了 3 岁，即残差为 3。
2. 那么在第二棵树里我们把 A 的年龄设为 3 岁去学习，
3. 如果第二棵树能把 A 分到 3 岁的叶子节点，那两棵树的结果相加就可以得到 A 的真实年龄；
4. 如果第二棵树的结论是 5 岁，则 A 仍然存在 −2 岁的残差，第三棵树里 A 的年龄就变成 −2 岁，继续学。

这里使用残差继续学习，就是 GBDT 中 Gradient Boosted 所表达的意思。<span style="color:red;">哇塞，嗯，感觉挺有意思的。GBDT 的效果应该是挺好的吧？确认下。</span>







梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是 Boosting 算法中非常流行的模型，也是近来在机器学习竞赛、商业应用中表现都非常优秀的模型。

GBDT 非常好地体现了“从错误中学习”的理念，基于决策树预测的残差进行迭代的学习。<span style="color:red;">是的。</span>

GBDT 几乎是算法工程师的必备技能，也是机器学习面试中常考察的内容。

GBDT，CART

## GBDT 的基本原理是什么？

本章第一节提到 Bagging 和 Boosting 两大集成学习的框架。相比于 Bagging 中各个弱分类器可以独立地进行训练，Boosting 中的弱分类器需要依次生成。在每一轮迭代中，基于已生成的弱分类器集合（即当前模型）的预测结果，新的弱分类器会重点关注那些还没有被正确预测的样本。

Gradient Boosting 是 Boosting 中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。<span style="color:red;">当前模型损失函数的负梯度信息？是什么？</span>

Gradient Boosting 算法的伪代码如图 12.6所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190420/2FcjhsdHVeHR.png?imageslim">
</p>

算法 1 描述了 Gradient Boosting 算法的基本流程，在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新。



采用决策树作为弱分类器的 Gradient Boosting 算法被称为 GBDT，有时又被称为 MART（Multiple Additive Regression Tree）。GBDT 中使用的决策树通常为 CART。


用一个很简单的例子来解释一下 GBDT 训练的过程，如图 12.7所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190420/Q8PY5tJfVzMc.png?imageslim">
</p>

模型的任务是预测一个人的年龄，训练集只有 A、B、C、D 4个人，他们的年龄分别是 14、16、24、26，特征包括了 “月购物金额” “上网时长” “上网历史” 等。

- 下面开始训练第一棵树，训练的过程跟传统决策树相同，简单起见，我们只进行一次分枝。
- 训练好第一棵树后，求得每个样本预测值与真实值之间的残差。可以看到，A、B、C、D 的残差分别是 −1、1、−1、1。
- 这时我们就用每个样本的残差训练下一棵树，直到残差收敛到某个阈值以下，或者树的总数达到某个上限为止。



由于 GBDT 是利用残差训练的，在预测的过程中，我们也需要把所有树的预测值加起来，得到最终的预测结果。

GBDT 使用梯度提升（Gradient Boosting）作为训练方法，而在逻辑回归或者神经网络的训练过程中往往采用梯度下降（Gradient Descent）作为训练方法，二者之间有什么联系和区别吗？<span style="color:red;">哈哈，是的。</span>

## 梯度提升和梯度下降的区别和联系是什么？


是梯度提升算法和梯度下降算法的对比情况：

梯度提升：

- 函数空间 $F$
- $F=F_{t-1}-\rho_{t} \nabla_{F}\left.L\right|_{F=F_{t-1}}$
- $L=\sum_{i} l\left(y_{i}, F\left(x_{i}\right)\right)$

梯度下降：

- 参数空间 $W$
- $w_{t}=w_{t-1}-\rho_{t} \nabla_{w}\left.L\right|_{w=w_{i-1}}$
- $L=\sum_{i} l\left(y_{i}, f_{w}\left(w_{i}\right)\right)$



可以发现，两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新，只不过：

- 在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新。
- 而在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。<span style="color:red;">有点困了，不是很理解。</span>



## GBDT 的优点和局限性有哪些？


### 优点

1. 预测阶段的计算速度快，树与树之间可并行化计算。<span style="color:red;">是的。</span>
2. 在分布稠密的数据集上，泛化能力和表达能力都很好，这使得 GBDT 在 Kaggle 的众多竞赛中，经常名列榜首。<span style="color:red;">啥是分布稠密？</span>
3. 采用决策树作为弱分类器使得 GBDT 模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系，并且也不需要对数据进行特殊的预处理如归一化等。<span style="color:red;">嗯，好的。</span>


局限性
1. GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。<span style="color:red;">哦，这个之前真的不知道。是不是决策树这种的在高维稀疏数据集上效果都不是很好？</span>
2. GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。<span style="color:red;">为什么呢？</span>
3. 训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。<span style="color:red;">是的。训练的时候是串行的，而 Bagging 训练的时候是并行的。</span>


# 相关

- 《百面机器学习》
