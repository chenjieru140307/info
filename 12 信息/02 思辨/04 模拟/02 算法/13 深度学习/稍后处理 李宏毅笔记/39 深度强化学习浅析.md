
![mark](http://images.iterate.site/blog/image/20190818/9bh0XzJyCeKW.png?imageslim)

## 深度强化学习浅析

2015年 2 月的时候，google在 nature 上发了一篇用 reinforcement learning 的方法来玩 akari 的小游戏，然后痛鞭人类

2016的春天，又有大家都耳熟能详的 alpha go，也是可以痛鞭人类

David Silver 说 AI 就是 Reinforcement Learning加 Deep Learning
Deep Reinforcement Learning : AI = RL + DL

## 强化学习的应用场景

在 Reinforcement Learning里面会有一个 Agent 跟一个 Environment。这个 Agent 会有 Observation 看到世界种种变化，这个 Observation 又叫做 State，这个 State 指的是环境的状态，也就是你的 machine 所看到的东西。所以在这个 Reinforcement Learning领域才会有这个 XXX 做法，我们的 state 能够观察到一部分的情况，机器没有办法看到环境所有的状态，所以才会有这个 partial of state 这个想法，这个 state 其实就是 Observation。machine会做一些事情，它做的事情叫做 Action，Action会影响环境，会跟环境产生一些互动。因为它对环境造成的一些影响，它会得到 Reward，这个 Reward 告诉它，它的影响是好的还是不好的。如下图

![mark](http://images.iterate.site/blog/image/20190818/Xc48VRAPcbh9.png?imageslim)

举个例子，比如机器看到一杯水，然后它就 take 一个 action，这个 action 把水打翻了，Environment就会得到一个 negative 的 reward，告诉它不要这样做，它就得到一个负向的 reward。在 Reinforcement Learning这些动作都是连续的，因为水被打翻了，接下来它看到的就是水被打翻的状态，它会 take 另外一个 action，决定把它擦干净，Environment觉得它做得很对，就给它一个正向的 reward。机器生来的目标就是要去学习采取那些 ation，可以让 maximize reward maximize 。

接着，以 alpha go为例子，一开始 machine 的 Observation 是棋盘，棋盘可以用一个 19*19的矩阵来描述，接下来，它要 take 一个 action，这个 action 就是落子的位置。落子在不同的位置就会引起对手的不同反应，对手下一个子，Agent的 Observation 就变了。Agent看到另外一个 Observation 后，就要决定它的 action，再 take 一个 action，落子在另外一个位置。用机器下围棋就是这么个回事。在围棋这个 case 里面，还是一个蛮难的 Reinforcement Learning，在多数的时候，你得到的 reward 都是 0，落子下去通常什么事情也没发生这样子。只有在你赢了，得到 reward 是 1，如果输了，得到 reward 是-1。Reinforcement Learning困难的地方就是有时候你的 reward 是 sparse 的，只有倒数几步才有 reward。即在只有少数的 action 有 reward 的情况下去挖掘正确的 action。

对于 machine 来说，它要怎么学习下围棋呢，就是找一某个对手一直下下，有时候输有时候赢，它就是调整 Observation 和 action 之间的关系，调整 model 让它得到的 reward 可以被 maximize。

## 监督 v.s. 强化

我们可以比较下下围棋采用 Supervised 和 Reinforcement 有什么区别。如果是 Supervised 你就是告诉机器说看到什么样的态势就落在指定的位置。Supervised不足的地方就是具体态势下落在哪个地方是最好的，其实人也不知道，因此不太容易做 Supervised。用 Supervised 就是 machine 从老师那学，老师说下哪就下哪。如果是 Reinforcement 呢，就是让机器找一个对手不断下下，赢了就获得正的 reward，没有人告诉它之前哪几步下法是好的，它要自己去试，去学习。Reinforcement 是从过去的经验去学习，没有老师告诉它什么是好的，什么是不好的，machine要自己想办法，其实在做 Reinforcement 这个 task 里面，machine需要大量的 training，可以两个 machine 互相下。alpha Go 是先做 Supervised Learning，做得不错再继续做 Reinforcement Learning。

## 应用举例

### 学习一个 chat-bot

 Reinforcement Learning 也可以被用在 Learning a chat-bot。chat-bot 是 seq2seq，input 就是一句话，output 就是机器的回答。

 如果采用 Supervised ，就是告诉机器有人跟你说“hello”，你就回答“hi”。如果有人跟你说“bye bye”，你就要说“good bye”。

如果是 Reinforcement  Learning 就是让机器胡乱去跟人讲话，讲讲，人就生气了，machine就知道一句话可能讲得不太好。不过没人告诉它哪一句话讲得不好，它要自己去发掘这件事情。
![mark](http://images.iterate.site/blog/image/20190818/eSn5jUwG14YF.png?imageslim)


这个想法听起来很 crazy，但是真正有 chat-bot是这样做的，这个怎么做呢？因为你要让 machine 不断跟人讲话，看到人生气后进行调整，去学怎么跟人对话，这个过程比较漫长，可能得好几百万人对话之后才能学会。这个不太现实，那么怎么办呢，就用 Alpha Go的方式，Learning 两个 agent，然后让它们互讲的方式。

![mark](http://images.iterate.site/blog/image/20190818/tSYmL8DWuyGH.png?imageslim)

两个 chat-bot互相对话，对话之后有人要告诉它们它们讲得好还是不好。在围棋里比较简单，输赢是比较明确的，对话的话就比较麻烦，你可以让两个 machine 进行无数轮互相对话，问题是你不知道它们这聊天聊得好还是不好，这是一个待解决问题。现有的方式是制定几条规则，如果讲得好就给它 positive reward ，讲得不好就给它 negative reward，好不好由人主观决定，然后 machine 就从它的 reward 中去学说它要怎么讲才是好。后续可能会有人用 GAN 的方式去学 chat-bot。通过 discriminator 判断是否像人对话，两个 agent 就会想骗过 discriminator，即用 discriminator 自动认出给 reward 的方式。
Reinforcement  Learning 有很多应用，尤其是人也不知道怎么做的场景非常适合。

## 交互搜索

让 machine 学会做 Interactive retrieval，意思就是说有一个搜寻系统，能够跟 user 进行信息确认的方式，从而搜寻到 user 所需要的信息。直接返回 user 所需信息，它会得到一个 positive reward，然后每问一个问题，都会得到一个 negative reward。

![mark](http://images.iterate.site/blog/image/20190818/xvJNP7fCjQ2w.png?imageslim)

### 更多应用

Reinforcement  Learning 还有很多应用，比如开个直升机，开个无人车呀，也有通过 deepmind 帮助谷歌节电，也有文本生成等。现在 Reinforcement  Learning最常用的场景是电玩。现在有现成的 environment，比如 Gym,Universe。让 machine 用 Reinforcement  Learning来玩游戏，跟人一样，它看到的东西就是一幅画面，就是 pixel，然后看到画面，它要做什么事情它自己决定，并不是写程序告诉它说你看到这个东西要做什么。需要它自己去学出来。

### 例子:玩视频游戏

- Space invader

	这个游戏界面如下
![mark](http://images.iterate.site/blog/image/20190818/ShXSkUUkhQmI.png?imageslim)
	游戏的终止条件时当所有的外星人被消灭或者你的太空飞船被摧毁。

	这个游戏里面，你可以 take 的 actions 有三个，可以左右移动跟开火。怎么玩呢，machine会看到一个 observation，这个 observation 就是一幕画面。一开始 machine 看到一个 observation $s_1$，这个 $s_1$ 其实就是一个 matrix，因为它有颜色，所以是一个三维的 pixel。machine看到这个画面以后，就要决定它 take 什么 action，现在只有三个 action 可以选择。比如它 take 往右移。每次 machine take一个 action 以后，它会得到一个 reward，这个 reward 就是左上角的分数。往右移不会得到任何的 reward，所以得到的 reward $r_1 = 0$，machine 的 action 会影响环境，所以 machine 看到的 observation 就不一样了。现在 observation 为 $s_2$，machine自己往右移了，同时外星人也有点变化了，这个跟 machine 的 action 是没有关系的，有时候环境会有一些随机变化，跟 machine 无关。machine看到 $s_2$ 之后就要决定它要 take 哪个 action，假设它决定要射击并成功的杀了一只外星人，就会得到一个 reward，发现杀不同的外星人，得到的分数是不一样的。假设杀了一只 5 分的外星人，这个 observation 就变了，少了一只外星人。这个过程会一直进行下去，直到 machine 的 reward $r_T$ 进入另一个 step，这个 step 是一个 terminal step，它会让游戏结束，在这个游戏背景里面就是你被杀死。可能这个 machine 往左移，不小心碰到 alien 的子弹，就死了，游戏就结束了。从这个游戏的开始到结束，就是一个 episode，machine要做的事情就是不断的玩这个游戏，学习怎么在一个 episode 里面怎么去 maximize reward，maximize它在所有的 episode 可以得到的 total reward。在死之前杀最多的外星人同时要闪避子弹，让自己不会被杀死。

![mark](http://images.iterate.site/blog/image/20190818/lqsi633q3Sjb.png?imageslim)

## 强化学习的难点

那么 Reinforcement  Learning的难点在哪里呢？它有两个难点

- Reward delay

	第一个难点是，reward出现往往会存在 delay，比如在 space invader里面只有开火才会得到 reward，但是如果 machine 只知道开火以后就会得到 reward，最后 learn 出来的结果就是它只会乱开火。对它来说，往左往右移没有任何 reward。事实上，往左往右这些 moving，它对开火是否能够得到 reward 是有关键影响的。虽然这些往左往右的 action，本身没有办法让你得到任何 reward，但它帮助你在未来得到 reward，就像规划未来一样，machine需要有这种远见，要有这种 visual，才能把电玩玩好。在下围棋里面，有时候也是一样的，短期的牺牲可以换来最好的结果。

- Agent's actions affect the subsequent data it receives

	Agent采取行动后会影响之后它所看到的东西，所以 Agent 要学会去探索这个世界。比如说在这个 space invader里面，Agent只知道往左往右移，它不知道开火会得到 reward，也不会试着击杀最上面的外星人，就不会知道击杀这个东西可以得到很高的 reward，所以要让 machine 去 explore 它没有做过的行为，这个行为可能会有好的结果也会有坏的结果。但是探索没有做过的行为在 Reinforcement  Learning里面也是一种重要的行为。

## 强化学习的方法

Reinforcement  Learning 的方法分成两大块，一个是 Policy-based的方法，另一个是 Valued-based的方法。先有 Valued-based的方法，再有 Policy-based的方法。在 Policy-based的方法里面，会 learn 一个负责做事的 Actor，在 Valued-based的方法会 learn 一个不做事的 Critic，专门批评不做事的人。我们要把 Actor 和 Critic 加起来叫做 Actor+Critic的方法。

![mark](http://images.iterate.site/blog/image/20190818/bgxWR5fh3mc2.png?imageslim)

现在最强的方法就是 Asynchronous Advantage Actor-Critic(A3C)。Alpha Go是各种方法大杂烩，有 Policy-based的方法，有 Valued-based的方法，有 model-based的方法。下面是一些学习 deep Reinforcement  Learning的资料

- Textbook: Reinforcement Learning: An Introduction
https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html
- Lectures of David Silver
http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html (10 lectures, 1:30 each)
http://videolectures.net/rldm2015_silver_reinforcement_learning/ (Deep Reinforcement Learning )
- Lectures of John Schulman
https://youtu.be/aUrX-rP_ss4

### Policy-based 方法

先来看看怎么学一个 Actor，所谓的 Actor 是什么呢?我们之前讲过，Machine Learning 就是找一个 Function，Reinforcement Learning也是 Machine Learning 的一种，所以要做的事情也是找 Function。这个 Function 就是所谓的魔术发现，Actor就是一个 Function。这个 Function 的 input 就是 Machine 看到的 observation，它的 output 就是 Machine 要采取的 Action。我们要透过 reward 来帮我们找这个 best Function。

![mark](http://images.iterate.site/blog/image/20190818/VVNrhwxmjOPs.png?imageslim)

找个这个 Function 有三个步骤：

- Neural Network as Actor

	第一个步骤就是决定你的 Function 长什么样子，假设你的 Function 是一个 Neural Network，就是一个 deep learning。

![mark](http://images.iterate.site/blog/image/20190818/vvnJCG3czud3.png?imageslim)

	如果 Neural Network作为一个 Actor，这个 Neural Network的输入就是 observation，可以通过一个 vector 或者一个 matrix 来描述。output就是你现在可以采取的 action。举个例子，Neural Network作为一个 Actor，inpiut是一张 image，output就是你现在有几个可以采取的 action，output就有几个 dimension。假设我们在玩 Space invader，output就是可能采取的 action 左移、右移和开火，这样 output 就有三个 dimension 分别代表了左移、右移和开火。

![mark](http://images.iterate.site/blog/image/20190818/6lmhyqkmhUtq.png?imageslim)

	这个 Neural Network怎么决定这个 Actor 要采取哪个 action 呢？通常的做法是这样，你把这个 image 丢到 Neural Network里面去，它就会告诉你说每个 output dimension所对应的分数，可以采取分数最高的 action，比如说 left。用这个 Neural Network来做 Actor 有什么好处，Neural Network是可以举一反三的，可能有些画面 Machine 从来没有看过，但是 Neural Network的特性，你给它一个 image，Neural Network吐出一个 output。所以就算是它没看过的东西，它 output 可以得到一个合理的结果。Neural Network就是比较 generous。

- Goodnedd of Actor

	第二步骤就是，我们要决定一个 Actor 的好坏。在 Supervised learning中，我们是怎样决定一个 Function 的好坏呢？举个 Training Example例子来说，我们把图片扔进去，看它的结果和 target 是否像，如果越像的话这个 Function 就会越好，我们会一个 loss，然后计算每个 example 的 loss，我们要找一个参数去 minimize 这个参数。

![mark](http://images.iterate.site/blog/image/20190818/lcyI2NxmjAqz.png?imageslim)

	在 Reinforcement Learning里面，一个 Actor 的好坏的定义是非常类似的。假设我们现在有一个 Actor，这个 Actor 就是一个 Neural Network，Neural Network的参数是 $\mathbf{\theta}$，即一个 Actor 可以表示为 $\pi_\theta(s)$，它的 input 就是 Mechine 看到的 observation。那怎么知道一个 Actor 表现得好还是不好呢？我们让这个 Actor 实际的去玩一个游戏，玩完游戏得到的 total reward为 $R_\theta=\sum_{t=1}^Tr_t$，把每个时间得到的 reward 合起来，这既是一个 episode 里面，你得到的 total reward。这个 total reward才是我们需要去 maximize 的对象。我们不需要去 maximize 每个 step 的 reward，我们是要 maximize 整个游戏玩完之后的 total reward。假设我们拿同一个 Actor，每次玩的时候，$R_\theta$ 其实都会不一样的。因为两个原因，首先你 Actor 本身如果是 Policy，看到同样的场景它也会采取不同的 Action。所以就算是同一个 Actor，同一组参数，每次玩的时候你得到的 $R_\theta$ 也会不一样的。再来游戏本身也有随机性，就算你采取同一个 Action，你看到的 observation 每次也可能都不一样。所以 $R_\theta$ 是一个 Random Variable。我们做的事情，不是去 maximize 每次玩游戏时的 $R_\theta$，而是去 maximize $R_\theta$ 的期望值。这个期望值就衡量了某一个 Actor 的好坏，好的 Actor 期望值就应该要比较大。

	那么怎么计算呢，我们假设一场游戏就是一个 trajectory $\tau$
	$$ \tau = \left\{ s_1,a_1,r_1, s_2,a_2,r_2,...,s_T,a_T,r_T \right\} $$
	$\tau$ 包含了 state，看到这个 observation，take的 Action，得到的 Reward，是一个 sequence。
	$$
	R(\tau) = \sum_{n=1}^Nr_n
	$$
	$R(\tau)$ 代表在这个 episode 里面，最后得到的总 reward。当我们用某一个 Actor 去玩这个游戏的时候，每个 $\tau$ 都会有出现的几率，$\tau$ 代表从游戏开始到结束过程，这个过程有千百万种，当你选择这个 Actor 的时候，你可能只会看到某一些过程，某些过程特别容易出现，某些过程比较不容易出现。每个游戏出现的过程，可以用一个几率 $P(\tau|\theta)$ 来表示它，就是说参数是 $\theta$ 时 $\tau$ 这个过程出现的几率。那么 $R_\theta$ 的期望值为
	$$\bar{R}_\theta=\sum_\tau R(\tau)P(\tau|\theta)$$
	实际上要穷举所有的 $\tau$ 是不可能的，那么要怎么做？让 Actor 去玩 N 场这个游戏，获得 N 个过程 ${\tau^1,\tau^2,...,\tau^N}$ ，玩 N 场就好像从 $P(\tau|\theta)$ 去 Sample N个 $\tau$。假设某个 $\tau$ 它的几率特别大，就特别容易被 sample 出来。sample出来的 $\tau$ 跟几率成正比。让 Actor 去玩 N 场，相当于从 $P(\tau|\theta)$ 概率场抽取 N 个过程，可以通过 N 各 Reward 的均值进行近似，如下表达
	$$\bar{R}_\theta=\sum_\tau R(\tau)P(\tau|\theta) \approx \frac{1}{N}R(\tau^n)$$

- Pick the best function

	怎么选择最好的 function，其实就是用我们的 Gradient Ascent。我们已经找到目标了，就是最大化这个 $\bar{R}_\theta$
	$$\theta^\ast = arg \max_\theta \bar{R}_\theta$$
	其中 $\bar{R}_\theta = \sum_\tau R(\tau)P(\tau|\theta)$。就可以用 Gradient Ascent进行最大化，过程为：
	(1) 初始化 $\theta^0$
	(2) $\theta^1 \leftarrow \theta^0+\eta \triangledown \bar{R}_{\theta^0}$
	(3) $\theta^2 \leftarrow \theta^1+\eta \triangledown \bar{R}_{\theta^1}$
	(4) .......

	参数 $\theta = {w_1,w_2,...,b_1,...}$，那么 $\triangledown \bar{R}_{\theta}$ 就是 $\bar{R}_{\theta}$ 对每个参数的偏微分，如下
	$$
	\triangledown \bar{R}_{\theta} = \begin{bmatrix}
	\partial{ \bar{R}_{\theta}}/\partial w_1 \\ \partial{ \bar{R}_{\theta}}/\partial w_2
	\\ \vdots
	\\ \bar{R}_{\theta}/\partial b_1
	\\ \vdots
	\end{bmatrix}
    $$ 接下来就是实际的计算下，$\bar{R}_\theta = \sum_\tau R(\tau)P(\tau|\theta)$ 中，只有 $P(\tau|\theta)$ 跟 $\theta$ 有关系，所以只需要对 $P(\tau|\theta)$ 做 Gradient ，即 $$\nabla \bar{R}_{\theta}=\sum_{\tau} R(\tau) \nabla P(\tau | \theta)$$ 所以 $R(\tau)$ 就算不可微也没有关系，或者是不知道它的 function 也没有差，我们只要知道把 $\tau$ 放进去得到值就可以。
	接下来，为了让 $P(\tau|\theta)$ 出现，有 $$\nabla \bar{R}_{\theta}=\sum_{\tau} R(\tau) \nabla P(\tau | \theta)=\sum_{\tau} R(\tau) P(\tau | \theta) \frac{\nabla P(\tau | \theta)}{P(\tau | \theta)}$$

	由于
	$$
	\frac{\operatorname{dlog}(f(x))}{d x}=\frac{1}{f(x)} \frac{d f(x)}{d x}
	$$
	所以
	$$
	\nabla \bar{R}_{\theta}=\sum_{\tau} R(\tau) P(\tau | \theta) \frac{\nabla P(\tau | \theta)}{P(\tau | \theta)}=\sum_{\tau} R(\tau) P(\tau | \theta) \nabla \log P(\tau | \theta)
	$$
	从而可以通过抽样的方式去近似，即
	$$
	\nabla \bar{R}_{\theta}=\sum_{\tau} R(\tau) P(\tau | \theta) \nabla \log P(\tau | \theta)=\approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log P\left(\tau^{n} | \theta\right)
	$$
	即拿 $\theta$ 去玩 N 次游戏，得到 ${\tau^1,\tau^2,...,\tau^N}$，算出每次的 $R(\tau)$。接下来的问题是怎么计算 $\nabla \log P\left(\tau^{n} | \theta\right)$，因为
	$$
	P(\tau|\theta)=p\left(s_{1}\right) p\left(a_{1} | s_{1}, \theta\right) p\left(r_{1}, s_{2} | s_{1}, a_{1}\right) p\left(a_{2} | s_{2}, \theta\right) p\left(r_{2}, s_{3} | s_{2}, a_{2}\right) \cdots \\
	=p\left(s_{1}\right) \prod_{t=1}^{T} p\left(a_{t} | s_{t}, \theta\right) p\left(r_{t}, s_{t+1} | s_{t}, a_{t}\right)
	$$
	其中 $P(s_1)$ 是初始状态出现的几率，接下来根据 $\theta$ 会有某个概率在 $s_1$ 状态下采取 Action $a_1$，然后根据 $a_1,s_1$ 会得到某个 reward $r_1$，并跳到另一个 state $s_2$，以此类推。其中有些项跟 Actor 是无关的，$p\left(s_{1}\right)$ 和 $p\left(r_{t}, s_{t+1} | s_{t}, a_{t}\right)$，只有 $p\left(a_{t} | s_{t}, \theta\right)$ 跟 Actor $\pi_\theta$ 有关系。
	通过取 log，连乘转为连加，即
	$$
	\log P(\tau | \theta) =\log p\left(s_{1}\right)+\sum_{t=1}^{T} \log p\left(a_{t} | s_{t}, \theta\right)+\log p\left(r_{t}, s_{t+1} | s_{t}, a_{t}\right)
	$$
	然后对 $\theta$ 取 Gradient，删除无关项，得到
	$$
	\nabla \log P(\tau | \theta)=\sum_{t=1}^{T} \nabla \log p\left(a_{t} | s_{t}, \theta\right)
	$$
	则
	$$
	\begin{aligned} \nabla \bar{R}_{\theta} & \approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log P\left(\tau^{n} | \theta\right)=\frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \sum_{t=1}^{T_{n}} \nabla \log p\left(a_{t}^{n} | s_{t}^{n}, \theta\right) \\ &=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p\left(a_{t}^{n} | s_{t}^{n}, \theta\right) \end{aligned}
	$$
	这个式子就告诉我们，当我们在某一次 $\tau^n$ 游戏中，在 $s_t^n$ 状态下采取 $a_t^2$ 得到 $R(\tau^n)$ 是正的，我们就希望 $\theta$ 能够使 $p(a_t^n|s_t^n)$ 的概率越大越好。反之，如果 $R(\tau^n)$ 是负的，就要调整 $\theta$ 参数，能够使 $p(a_t^n|s_t^n)$ 的几率变小。注意，某个时间点的 $p\left(a_{t}^{n} | s_{t}^{n}, \theta\right)$ 是乘上这次游戏的所有 reward $R(\tau^n)$ 而不是这个时间点的 reward。假设我们只考虑这个时间点的 reward，那么就是说只有 fire 才能得到 reward，其他的 action 你得到的 reward 都是 0。Machine就只会增加 fire 的几率，不会增加 left 或者 right 的几率。最后 Learn 出来的 Agent 它就会 fire。

	接着还有一个问题，为什么要取 log 呢？

![mark](http://images.iterate.site/blog/image/20190818/oBkSR03rCM0s.png?imageslim)

	$$\nabla \log p\left(a_{t}^{n} | s_{t}^{n}, \theta\right)=\frac{\nabla  p\left(a_{t}^{n} | s_{t}^{n}, \theta\right)}{p\left(a_{t}^{n} | s_{t}^{n}, \theta\right)}$$
	那么为什么要除以 $p\left(a_{t}^{n} | s_{t}^{n}, \theta\right)$ 呢？假设某个 state s在 $\tau^{13},\tau^{15},\tau^{17},\tau^{33}$，采取不同的 action，获得不同的 reward，会偏好出现次数比较多的 action，但是次数比较多的 action 有时候并没有比较好，就像 b 出现的比较多，但是得到的 reward 并没有比较大，Machine把这项几率调高。除掉一个几率，就是对那项做了个 normalization，防止 Machine 偏好那些出现几率比较高的项。

![mark](http://images.iterate.site/blog/image/20190818/jin8oWukx8pA.png?imageslim)

	还有另外一个问题，假设 $R(\tau^n)$ 总是正的，那么会出现什么事情呢？在理想的状态下，这件事情不会构成任何问题。假设有三个 action，a,b,c采取的结果得到的 reward 都是正的，这个正有大有小，假设 a 和 c 的 $R(\tau^n)$ 比较大，b的 $R(\tau^n)$ 比较小，经过 update 之后，你还是会让 b 出现的几率变小，a,c出现的几率变大，因为会做 normalization。但是实做的时候，我们做的事情是 sampling，所以有可能只 sample b和 c，这样 b,c几率都会增加，a没有 sample 到，几率就自动减少，这样就会有问题了。

![mark](http://images.iterate.site/blog/image/20190818/13hlLUjzN248.png?imageslim)

	这样，我们就希望 $R(\tau^n)$ 有正有负这样，可以通过将 $R(\tau^n)-b$ 来避免，$b$ 需要自己设计。如下
	$$\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(R\left(\tau^{n}\right)-b\right) \nabla \log p\left(a_{t}^{n} | s_{t}^{n}, \theta\right)$$
	这样 $R(\tau^n)$ 超过 b 的时候就把几率增加，小于 b 的时候就把几率降低，从而解决了都是正的问题。

### Value-based 方法

#### Critic

Critic就是 Learn 一个 Neural Network，这个 Neural Network不做事，然后 Actor 可以从这个 Critic 中获得，这就是 Q-learning。
Critic就是 learn 一个 function，这个 function 可以告诉你说现在看到某一个 observation 的时候，这个 observation 有有多好这样。

- 根据 actor $\pi$ 评估 critic function

	这个 function 是用 Neural Network表示

- state value function $V^\pi(s)$

	这个累加的 reward 是通过观察多个 observation

![mark](http://images.iterate.site/blog/image/20190818/D6qKINyKQRdk.png?imageslim)

	那么如何估计 $V^\pi(s)$ 呢？可以采用 Monte-Carlo based approach。

- State-action value function $Q^\pi(s,a)$

	这个累加的 reward 是通过观察 observation 和 take 的 action
![mark](http://images.iterate.site/blog/image/20190818/PM5wkGRj7dqw.png?imageslim)

### Actor-Critic

 这部分留着下学期再讲
![mark](http://images.iterate.site/blog/image/20190818/kwABOgVFTI8e.png?imageslim)

 附上一张学习地图，完结撒花~
![mark](http://images.iterate.site/blog/image/20190818/CrRjbB063Bh8.png?imageslim)





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
