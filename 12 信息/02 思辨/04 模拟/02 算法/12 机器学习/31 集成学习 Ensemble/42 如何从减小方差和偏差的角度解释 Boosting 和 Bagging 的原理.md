
# 可以补充进来的


# 如何从减小方差和偏差的角度解释 Boosting 和 Bagging 的原理？



<span style="color:red;">嗯，想看下</span>

简单回答这个问题就是：

- Bagging 能够提高弱分类器性能的原因是降低了方差，
- Boosting 能够提升弱分类器性能的原因是降低了偏差。

<span style="color:red;">哈哈，厉害！</span>

为什么这么讲呢？


### Bagging

首先，Bagging 是 Bootstrap Aggregating 的简称，意思就是再抽样，然后在每个样本上训练出来的模型取平均。

假设有 $n$ 个随机变量，方差记为 $\sigma^{2}$，两两变量之间的相关性为 $\rho$，则 $n$ 个随机变量的均值 $\frac{\sum X_{i}}{n}$ 的方差为 $\rho^{*} \sigma^{2}+(1-\rho)^{*} \sigma^{2} / n$。在随机变量完全独立的情况下，$n$ 个随机变量的方差为 $\sigma^{2} / n$ ，也就是说方差减小到了原来的 $1/n$。

再从模型的角度理解这个问题，对 $n$ 个独立不相关的模型的预测结果取平均，方差是原来单个模型的 $1/n$。这个描述不甚严谨，但原理已经讲得很清楚了。

当然，模型之间不可能完全独立。为了追求模型的独立性，诸多 Bagging 的方法做了不同的改进：

- 比如在随机森林算法中，每次选取节点分裂属性时，会随机抽取一个属性子集，而不是从所有属性中选取最优属性，这就是为了避免弱分类器之间过强的相关性。<span style="color:red;">这个我想知道，随机的属性子集在真实场景中效果怎么样？</span>
- 通过训练集的重采样也能够带来弱分类器之间的一定独立性，从而降低 Bagging 后模型的方差。<span style="color:red;">训练集的重采样不是肯定会做的吗？还是说正常是不会做的？</span>



### Boosting

再看 Boosting，大家应该还记得 Boosting 的训练过程。在训练好一个弱分类器后，我们需要计算弱分类器的错误或者残差，作为下一个分类器的输入。

这个过程本身就是在不断减小损失函数，来使模型不断逼近“靶心”，使得模型偏差不断降低。

但 Boosting 的过程并不会显著降低方差。这是因为 Boosting 的训练过程使得各弱分类器之间是强相关的，缺乏独立性，所以并不会对降低方差有作用。<span style="color:red;">是的，是强相关的。</span>


### 泛化误差、偏差、方差和模型复杂度的关系

关于泛化误差、偏差、方差和模型复杂度的关系如图 12.5 所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190420/IsuLA5xXOksk.png?imageslim">
</p>

不难看出，方差和偏差是相辅相成，矛盾又统一的，二者并不能完全独立的存在。对于给定的学习任务和训练数据集，我们需要对模型的复杂度做合理的假设。如果模型复杂度过低，虽然方差很小，但是偏差会很高；如果模型复杂度过高，虽然偏差降低了，但是方差会很高。所以需要综合考虑偏差和方差选择合适复杂度的模型进行训练。<span style="color:red;">是的是的，但是要怎么综合考虑呢？嗯，还是要结合例子来看。</span>


# 相关

- 《百面机器学习》
