
6.1 包的概述
在顶层，该软件包公开了三个主要的抽象类：转换器（Transformer）、评估器（Estimator）和管道（Pipeline）。我们将用一些简短的例子来解释每个类。在本章的最后一节提供了一些更具体的模型例子。
6.1.1 转换器
转换器类，如名称所示，通常通过将一个新列附加到 DataFrame 来转换数据。
从高层次上看，当从转换器的抽象类派生时，每个新的转换器类需要实现.transform（……）方法。该方法要求传递一个要被转换的 DataFrame，该参数通常是第一个也是唯一的一个强制性参数。当然这在 ML 包的不同方法中也不尽相同：其他常见的参数有 inputCol 和 outputCol；然而，这些参数通常用一些预定义的值作为默认值，例如 inputCol 参数的默认值“features”。
在 spark.ml.feature中提供了许多转换器，我们会在此简要介绍一下（本章稍后会使用其中的一部分）：
·Binarizer：根据指定的阈值将连续变量转换为对应的二进制值。
·Bucketizer：与 Binarizer 类似，该方法根据阈值列表（分割的参数），将连续变量转换为多项值（即将连续变量离散化到指定的范围区间）。
·ChiSqSelector：对于分类目标变量（思考分类模型），此功能允许你选择预定义数量的特征（由 numTopFeatures 参数进行参数化），以便最好地说明目标的变化。如方法名所示，使用卡方（Chi-Square）检验完成选择。该方法需要两步：首先，需要.fit（……）你的数据（所以此方法可以计算卡方检验）。调用.fit（……）方法（将 DataFrame 作为参数传入）返回一个 ChiSqSelectorModel 对象，然后可以使用该对象的.transform（……）方法来转换 DataFrame。有关卡方的更多信息，请点击：http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html。
·CountVectorizer：该方法对于标记文本（例如[['Learning'，'PySpark'，'with'，'us']，['us'，'us'，'us']]）是有用的。这是一个需要两步的方法：首先，你需要用.fit（……），即从数据集中学习这些模式，然后才能使用.fit（……）方法返回的 CountVectorizerModel 对象的.transform（……）方法。对于如上所示的标记文本，该转换器的输出类似于[（4，[0，1，2，3]，[1.0，1.0，1.0，1.0]），（4，[3]，[3.0]）]。
·DCT：离散余弦变换取实数值向量，并返回相同长度的向量，但余弦函数之和在不同频率下振荡。这种转换对于提取数据或数据压缩中的一些基本频率很有用。
·ElementwiseProduct：该方法返回一个向量，其中的元素是传入该方法的向量和另一个传入参数 scalingVec 向量的乘积。例如，如果传入的向量是[10.0，3.0，15.0]，而传入的 scalingVec 为[0.99，3.30，0.66]，那么将获得如下所示的向量：[9.9，9.9，9.9]。
·HashingTF：一个哈希转换器，输入为标记文本的列表，返回一个带有计数的有预定长度的向量。摘自 PySpark 的文档：“由于使用简单的模数将散列函数转换为列索引，建议使用 2 的幂作为 numFeatures 参数；否则特征将不会均匀地映射到列。”
·IDF：该方法计算文档列表的逆向文件频率。请注意，文档需要提前用向量表示（例如，使用 HashingTF 或 CountVectorizer）。
·IndexToString：与 StringIndexer 方法对应。它使用 StringIndexerModel 对象中的编码将字符串索引反转到原始值。另外请注意，如果有时不起作用，你需要指定 StringIndexer 中的值。
·MaxAbsScaler：将数据调整到[-1.0，1.0]范围内（因此，它不会移动数据的中心）。
·MinMaxScaler：这与 MaxAbsScaler 相似，区别在于它将数据缩放到[0.0，1.0]范围内。
·NGram：此方法的输入为标记文本的列表，返回结果包含一系列 n-gram：以两个词、三个词或更多的 n 个词为一个 n-gram。例如，如果你有一个['good'，'morning'，'Robin'，'Williams']，你会得到以下输出：['good morning'，'morning Robin'，'Robin Williams']。
·Normalizer：该方法使用 p 范数将数据缩放为单位范数（默认为 L2）。
·OneHotEncoder：该方法将分类列编码为二进制向量列。
·PCA：使用主成分分析执行数据降维。
·PolynomialExpansion：执行向量的多项式展开。例如，假如你有一个如[x，y，z]的向量，则该方法将产生以下扩展：[x，x*x，y，x*y，y*y，z，x*z，y*z，z*z]。
·QuantileDiscretizer：与 Bucketizer 方法类似，但不是传递分隔参数，而是传递一个 numBuckets 参数。然后，该方法通过计算数据的近似分位数来决定分隔应该是什么。
·RegexTokenizer：这是一个使用正则表达式的字符串分词器。
·RFormula：对于狂热的 R 用户，你可以传递一个公式，如 vec～alpha*3＋beta（假设你的 DataFrame 具有 alpha 和 beta 列），它将产生给定表达式的 vec 列。
·SQLTransformer：与上一个类似，但不是类似 R 的公式，你可以使用 SQL 语法。FROM语句应该从__THIS__中选择，表示你正在访问 DataFrame。例如：SELECT alpha*3＋beta AS vec FROM__THIS__。
·StandardScaler：标准化列，使其拥有零均值和等于 1 的标准差。
·StopWordsRemover：从标记文本中删除停用词（例如“the”或“a”）。


StringIndexer：假设包含所有单词的列表都在一列，这将产生一个索引向量。
·Tokenizer（分词器）：该默认分词器将字符串转成小写，然后以空格为分隔符分词。
·VectorAssembler：这是一个非常有用的转换器，它将多个数字（包括向量）列合并为一列向量。例如，如果你的 DataFrame 中有三列：
调用的输出如下：
如下所示：
·VectorIndexer：该方法为类别列生成索引向量。它以逐列方式工作，从列中选择不同的值，排序并从映射中返回值的索引而不是原始值。
·VectorSlicer：作用于特征向量，不管是密集的还是稀疏的：给定一个索引列表，它从特征向量中提取值。
·Word2Vec：该方法将一个句子（字符串）作为输入，并将其转换为{string，vector}格式的映射，这种表示在自然语言处理中非常有用。请注意，ML包中有许多方法在其旁边有一个 E 字母；这意味着该方法目前处于 beta（或实验）阶段，有时可能会失败或产生错误的结果。请多加留意。
6.1.2 评估器
评估器可以被视为需要评估的统计模型，对你的观测对象做预测或分类。
如果从抽象的评估器类派生，新模型必须实现.fit（……）方法，该方法用给出的在 DataFrame 中找到的数据和某些默认或自定义的参数来拟合模型。
在 PySpark 中有很多评估器可用，现在我们将简要介绍 Spark 2.0中提供的模型。分类
ML包为数据科学家提供了七种分类（Classification）模型以供选择，范围覆盖了从最简单的（如逻辑回归）到更复杂的。我们将在以下章节中提供简短的描述：
·LogisticRegression：分类的基准模型。逻辑回归使用一个对数函数来计算属于特定类别的观察对象的概率。撰写本文时，PySpark ML仅支持二值分类问题。
·DecisionTreeClassifier：该分类器构建了一个决策树来预测一个观察对象的所属类别。指定 maxDepth 参数限制树的深度，minInstancePerNode确定需要进一步拆分的树节点的观察对象的最小数量，maxBins参数指定连续变量将被分割的 Bin 的最大数量，而 impurity 指定用于测量并计算来自分割的信息的度量。
·GBTClassifier：用于分类的梯度提升决策树模型。该模型属于集合模型家族：集合模型结合多个弱预测模型而形成一个强健的模型。目前，GBTClassifier模型支持二进制标签、连续特征和分类特征。
·RandomForestClassifier：该模型产生多个决策树（因此命名为森林），并使用模式输出的决策树来对观察对象进行分类。RandomForestClassifier支持二元标签和多项标签。
·NaiveBayes：基于贝叶斯定理，该模型使用条件概率理论对观测进行分类。PySpark ML中的 NaiveBayes 模型支持二元标签和多项标签。
·MultilayerPerceptronClassifier（多层感知器分类器）：模仿人类大脑本质的分类器。深深植根于人造神经网络理论，该模型是一个黑盒模型，也就是说，不容易解释模型的内部参数。该模型至少包含三个完全相连的人造神经元层（在创建模型对象时需要指定的参数）：输入层（需要和数据集中特征的数量一样）、多个隐藏层（至少一个）以及一个输出层，其神经元数量等于标签中的类别数量。输入层和隐藏层中的所有神经元都有 sigmoid 激活函数，而输出层神经元的激活函数则为 softmax。
·OneVsRest：将多分类问题简化为二分类问题。例如，在多标签的情况下，模型可以训练成多个二元逻辑回归模型。例如，如果 label＝＝2，模型将构建一个逻辑回归，它将 label＝＝2转换为 1（所有剩余的标签值将设置为 0），然后训练二元模型。所有的模型分别积分，具有最高概率的模型获胜。回归
PySpark ML软件包中有七种可用于回归（Regression）任务的模型。与分类一样，范围从一些基本的回归（如强制线性回归）到更复杂的回归：
·AFTSurvivalRegression：适合加速失效时间回归模型。它是一个参数化模型，假设其中一个特征的边际效应加速或减缓了预期寿命（或过程失败）。它非常适用于具有明确阶段的过程。
·DecisionTreeRegressor：类似于分类模型，明显不同的是其标签是连续的而不是二元（或多项）的。
·GBTRegressor：与 DecisionTreeRegressor 一样，区别在于标签的数据类型。
·GeneralizedLinearRegression：广义线性回归是具有不同内核功能（链接功能）的线性模型家族。与假设误差项的常态性的线性回归相反，GLM允许标签具有不同的误差项分布：PySpark ML包的 GeneralizedLinearRegression 模型



支持 gaussian、binomial、gamma和 poisson 家族的误差分布，它们有许多不同的链接功能。
·IsotonicRegression：这种回归拟合一个形式自由、非递减的行到数据中。对于拟合有序的和递增的观测数据集是有用的。
·LinearRegression：最简单的回归模型，它假设了特征与连续标签以及误差项的常态之间的线性关系。
·RandomForestRegressor：与 DecisionTreeRegressor 或 GBTRegressor 类似，Random-ForestRegressor适合连续的标签，而不是离散标签。聚类
聚类是一系列无监督的模型，用于查找数据中的隐含模式。PySpark ML包提供了四种当前最流行的模型：
·BisectingKMeans（二分 k 均值算法）：该算法结合了 k 均值聚类算法和层次聚类算法。最初该算法将所有观察点作为一个簇，然后将数据迭代地分解为 k 个簇。查看以下网站了解有关伪码算法的更多信息：http://minethedata.blogspot.com/2012/08/bisecting-k-means.html。
·KMeans：著名的 k 均值算法，将数据分成 k 个簇，迭代地搜索那些使每个观察点和它所属簇的质点之间距离平方和最小的那些质点。
·GaussianMixture（高斯混合模型）：该方法使用具有未知参数的 k 个高斯分布来剖析数据集。使用期望最大化算法，通过最大化对数似然函数找到高斯参数。请注意，对于带有诸多特征的数据集，由于维度所限和高斯分布的数值问题，该模型可能表现不佳。
·LDA：该模型用于自然语言处理应用程序中的主题生成。PySpark ML还提供了一个推荐模型，但我们不会在这里描述。
6.1.3 管道
PySpark ML中管道的概念用来表示从转换到评估（具有一系列不同阶段）的端到端的过程，这个过程可以对输入的一些原始数据（以 DataFrame 形式）执行必要的数据加工（转换），最后评估统计模型。管道可以是纯转换类型的，也就是说，只由数个转换器组成。
一个管道可以被认为是由一系列不同阶段组成的。在 Pipeline 对象上执行.fit（……）方法时，所有阶段按照 stages 参数中指定的顺序执行；stages参数是转换器和评估器对象的列表。管道对象的.fit（……）方法执行每个转换器的.transform（……）方法和所有评估器的.fit（……）方法。
通常，前一阶段的输出会成为下一阶段的输入：当从转换器或评估器抽象类派生时，需要实现.getOutputCol（）方法，该方法返回创建对象时指定的 outputCol 参数的值。6.2
