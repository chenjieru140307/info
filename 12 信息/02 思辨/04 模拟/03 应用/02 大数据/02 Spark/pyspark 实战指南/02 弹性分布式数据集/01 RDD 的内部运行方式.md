
2.1 RDD的内部运行方式
RDD并行操作。Spark工作原理的最大优势是：每个转换并行执行，从而大大提高速度。
数据集转换通常是惰性的。这就意味着任何转换仅在调用数据集上的操作时才执行。这有助于 Spark 优化执行。例如，分析师们通常会按以下常见步骤来熟悉一个数据集：
1.统计出某一列中不同值出现的次数。
2.选出以字母 A 开头的。
3.将结果打印在屏幕上。
听起来和之前提到的步骤一样简单，如果仅仅是以字母 A 开始的项受到关注，就没有必要对其他项的不同值进行统计了。因此，不再按照前面的要点来执行，Spark可以仅仅统计以 A 开头的项，并将结果打印在屏幕上。
我们用一段代码作为例子来解释这三个步骤。首先，使用 Spark 的方法.map（lambda v：（v，1））来映射 A 的值，然后选择那些以“A”开头的记录（使用方法.filter（lambda val：val.startswith（'A'）））。如果我们调用方法.reduceByKey（operator.add），就会减少数据集并增加每一个关键字的次数。所有的这些步骤都在改变数据集。
然后，调用方法.collect（）执行这些步骤。这一步是在我们的数据集上进行操作，最终统计出该数据集中的不同元素的次数。实际上，该操作可能会在映射之前先改变转换顺序并过滤数据，导致将较小的数据集传递给 reducer。如果你还不明白之前的指令，别担心，我们会在本章后面的部分做出详细解释。
