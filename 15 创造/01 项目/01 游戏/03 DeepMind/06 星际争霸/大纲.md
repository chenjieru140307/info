# 大纲


- 星际争霸，deepmind 是有对应的模拟环境的 [pysc2](https://github.com/deepmind/pysc2)


## 星际争霸：走向通用 AI


面对策略类电脑游戏，挑战难点不仅仅是像素点阵组成的画面，更在于高级认知水平的表现，考察 AI 能否综合对多种单位、多种要素等的分析，设计复杂的计划，并随时根据情况灵活调整计划，尤其是即时类策略游戏，被视为 AI 最难玩的游戏。<span style="color:red;">是呀</span>

星际争霸（StarCraft）就是一款这样的游戏，于 1998 年由暴雪娱乐公司发行（见图 14.19）。它的资料片母巢之战（Brood War）提供了专给 AI 程序使用的 API，激发起很多 AI 研究者的研究热情[65]。

在平台方面，DeepMind 在成功使用深度学习攻克 Atari 游戏后，宣布和暴雪公司合作，将 StarCraft II 作为新一代 AI 测试环境，发布 SC2LE 平台，开放给 AI 研究者测试他们的算法。SC2LE 平台包括暴雪公司开发的 Machine Learning API、匿名化后的比赛录像数据集、DeepMind 开发的 PySC2 工具箱和一系列简单的 RL迷你游戏[66]。Facebook 也早在 2016 年就宣布开源 TorchCraft，目的是让每个人都能编写星际争霸 AI 程序。TorchCraft 是一个能让深度学习在即时战略类游戏上开展研究的库，使用的计算框架是 Torch[67]。<span style="color:red;">嗯，再难在卡也要把它搞定，实在不行把那个 GPU 电脑装成 Windows 的。</span>


暴雪公司出品的游戏“星际争霸”：


<center>

![](http://images.iterate.site/blog/image/20190427/xhsMu13lod74.png?imageslim){ width=55% }

</center>


在算法方面，Facebook在 2016 年提出微操作任务，来定义战斗中军事单位的短时、低等级控制问题，称这些场景为微操作场景[68]。<span style="color:red;">微操作任务是什么意思？</span>为了解决微操作场景下的控制问题，他们运用深度神经网络的控制器和启发式强化学习算法，在策略空间结合使用直接探索和梯度反向传播两种方法来寻找最佳策略。<span style="color:red;">怎么做的？想更多的了解。</span>

阿里巴巴的一批人也在 2017 年参与到这场 AI 挑战赛中，提出一个多智能体协同学习的框架，通过学习一个多智能体双向协同网络，来维护一个高效的通信协议，实验显示 AI 可以学习并掌握星际争霸中的各类战斗任务[69]。<span style="color:red;">哇塞！是怎么做到的？补充到这里。</span>

一般说来，玩星际争霸有三个不同层面的决策：

- 最高层面是战略水平的决策，要求的信息观察强度不高；
- 最低层面是微操作水平的决策，玩家需要考虑每个操控单位的类型、位置及其他动态属性，大量的信息都要通过观察获取；
- 中间层面是战术水平的决策，如兵团的位置及推进方向

星际争霸的三个决策层次：

<center>

![](http://images.iterate.site/blog/image/20190427/uzvJNeWJPcIB.png?imageslim){ width=55% }

</center>

<span style="color:red;">这些决策层次是怎么互相融合的？怎么协同作用的？嗯，想更多的知道这个。</span>

可见，即时战略类游戏对 AI 来讲有着巨大的挑战，代表着智能水平测试的最高点。<span style="color:red;">是呀，厉害的。</span>

