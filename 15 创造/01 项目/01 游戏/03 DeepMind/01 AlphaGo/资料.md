
从算法上讲，AlphaGo 的成功之处在于完美集成了深度神经网络、有监督学习技术、强化学习技术和蒙特卡洛树搜索算法。<span style="color:red;">是呀，特别想看下 AlphaGo 是怎么实现的。</span>

虽然人们很早就尝试使用蒙特卡洛树搜索算法来解决棋类 AI 问题，但是 AlphaGo：

- 首先采用强化学习加深度神经网络来指导蒙特卡洛树搜索算法。
- 强化学习提供整个学习框架，设计策略网络和价值网络来引导蒙特卡洛树搜索过程；
- 深度神经网络提供学习两个网络的函数近似工具
- 策略网络的初始化权重则通过对人类棋谱的有监督学习获得。

与传统蒙特卡洛树搜索算法不同，AlphaGo 提出 “异步策略与估值的蒙特卡洛树搜索算法”，也称 APV-MCTS。在扩充搜索树方面，APV-MCTS 根据有监督训练的策略网络来增加新的边；在树节点评估方面，APV-MCTS 结合简单的 rollout 结果与当前值网络的评估结果，得到一个新的评估值。<span style="color:red;">怎么这么厉害呢！</span>

训练 AlphaGo 可分成两个阶段：

- 第一阶段，基于有监督学习的策略网络参数，使用强化学习中的策略梯度方法，进一步优化策略网络；
- 第二阶段，基于大量的自我对弈棋局，使用蒙特卡洛策略评估方法得到新的价值网络。

需要指出的是，为了训练有监督版的策略网络，在 50 核的分布式计算平台上要花大约 3 周时间，如图 14.13所示。<span style="color:red;">厉害呀，现在的网络都这么厉害吗？这样长时间的训练是怎么能保证网络最后收敛的？</span>

AlphaGo 的训练：

<center>

![](http://images.iterate.site/blog/image/20190427/MC4pdt9H61Jp.png?imageslim){ width=55% }

</center>