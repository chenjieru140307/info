---
title: 1.01 围棋AI 的发展
toc: true
date: 2019-10-11
---
# 围棋 AI 的发展

围棋人工智能大致可以分为三个阶段[1](http://blog.greenwicher.com/2016/12/18/drl-general_ai-intro/#fn.1) ：第一阶段以启发式算法为主，水平低于业余初段，代表软件即以静态势力函数为强项的[手谈](https://zh.wikipedia.org/wiki/%E9%99%88%E5%BF%97%E8%A1%8C)； 第二阶段以[蒙特卡洛树搜索算法 ](https://zh.wikipedia.org/zh-sg/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2)为代表，水平最高达到业余5段，比如说 [Zen](http://senseis.xmp.net/?ZenGoProgram) ，[Crazy Stone](https://www.remi-coulom.fr/CrazyStone/) ；第三阶段以 `深度学习` （[Deep Learning](https://en.wikipedia.org/wiki/Deep_learning)）以及 `增强学习` （[Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning)，也称强化学习）算法为突破，并战胜了人类职业九段棋手李世乭，这也就是Alpha Go的故事了。每每提到Alpha Go卓越的能力，往往归咎于深度学习的强大，但实际上增强学习算法也功不可没。这二者的结合被称之为 `深度增强学习` （[Deep Reinforcement Learning](https://deepmind.com/blog/deep-reinforcement-learning/)，DRL），最初见于DeepMind在Nature上发表的[Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html)。


# 相关

- [深度增强学习：走向通用人工智能之路](https://www.zybuluo.com/tinadu/note/629229)
