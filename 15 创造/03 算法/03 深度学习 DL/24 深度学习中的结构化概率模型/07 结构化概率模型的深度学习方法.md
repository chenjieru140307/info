---
title: 07 结构化概率模型的深度学习方法
toc: true
date: 2019-06-05
---


# 结构化概率模型的深度学习方法



<!-- %深度学习实践者通常使用与从事结构化概率模型研究的其它机器学习研究者相同的基本计算工具。 -->深度学习从业者通常与其他从事结构化概率模型研究的机器学习研究者使用相同的基本计算工具。然而，在深度学习中，我们通常对如何组合这些工具作出不同的设计决定，导致总体算法、模型与更传统的图模型具有非常不同的风格。



深度学习并不总是涉及特别深的图模型。在图模型中，我们可以根据图模型的图而不是计算图来定义模型的深度。如果从潜变量 $h_i$ 到可观察变量的最短路径是 $j$ 步，我们可以认为潜变量 $h_j$ 处于深度 $j$。我们通常将模型的深度描述为任何这样的 $h_j$ 的最大深度。这种深度不同于由计算图定义的深度。用于深度学习的许多生成模型没有潜变量或只有一层潜变量，但使用深度计算图来定义模型中的条件分布。



深度学习基本上总是利用分布式表示的思想。即使是用于深度学习目的的浅层模型（例如预训练浅层模型，稍后将形成深层模型），也几乎总是具有单个大的潜变量层。深度学习模型通常具有比可观察变量更多的潜变量。变量之间复杂的非线性相互作用通过多个潜变量的间接连接来实现。



相比之下，传统的图模型通常包含至少是偶尔观察到的变量，即使一些训练样本中的许多变量随机地丢失。传统模型大多使用高阶项和结构学习来捕获变量之间复杂的非线性相互作用。如果有潜变量，它们的数量通常很少。




潜变量的设计方式在深度学习中也有所不同。深度学习从业者通常不希望潜变量提前包含了任何特定的含义——训练算法可以自由地开发对特定数据集建模所需要的概念。在事后解释潜变量通常是很困难的，但是可视化技术可以得到它们表示的一些粗略表征。当潜变量在传统图模型中使用时，它们通常被赋予一些特定含义——比如文档的主题、学生的智力、导致患者症状的疾病等。这些模型通常由研究者解释，并且通常具有更多的理论保证，但是不能扩展到复杂的问题，并且不能像深度模型一样在许多不同背景中重复使用。



另一个明显的区别是深度学习方法中经常使用的连接类型。深度图模型通常具有大的与其他单元组全连接的单元组，使得两个组之间的相互作用可以由单个矩阵描述。传统的图模型具有非常少的连接，并且每个变量的连接选择可以单独设计。模型结构的设计与推断算法的选择紧密相关。图模型的传统方法通常旨在保持精确推断的可解性。当这个约束太强时，我们可以采用一种流行的被称为环状信念传播的近似推断算法。这两种方法通常在稀疏连接图上都有很好的效果。相比之下，在深度学习中使用的模型倾向于将每个可见单元 $\mathrm v_i$ 连接到非常多的隐藏单元 $\mathrm h_j$ 上，从而使得 $\mathbf h$ 可以获得一个 $\mathrm v_i$ 的分布式表示（也可能是其他几个可观察变量）。分布式表示具有许多优点，但是从图模型和计算复杂性的观点来看，分布式表示有一个缺点就是很难产生对于精确推断和环状信念传播等传统技术来说足够稀疏的图。结果，大规模图模型和深度图模型最大的区别之一就是深度学习中几乎从来不会使用环状信念传播。相反的，许多深度学习模型可以设计来加速 Gibbs采样或者变分推断。此外，深度学习模型包含了大量的潜变量，使得高效的数值计算代码显得格外重要。除了选择高级推断算法之外，这提供了另外的动机，用于将结点分组成层，相邻两层之间用一个矩阵来描述相互作用。这要求实现算法的单个步骤可以实现高效的矩阵乘积运算，或者专门适用于稀疏连接的操作，例如块对角矩阵乘积或卷积。




最后，图模型的深度学习方法的一个主要特征在于对未知量的较高容忍度。与简化模型直到它的每一个量都可以被精确计算不同的是，我们仅仅直接使用数据运行或者是训练，以增强模型的能力。%让模型保持了较高的自由度我们一般使用边缘分布不能计算的模型，但可以从中简单地采近似样本。%但是可以简单从中采样的模型。我们经常训练具有难以处理的目标函数的模型，我们甚至不能在合理的时间内近似，但是如果我们能够高效地获得这样一个函数的梯度估计，我们仍然能够近似训练模型。深度学习方法通常是找出我们绝对需要的最小量信息，然后找出如何尽快得到该信息的合理近似。




## 实例：受限玻尔兹曼机

受限玻尔兹曼机或者簧风琴是图模型如何用于深度学习的典型例子。RBM 本身不是一个深层模型。相反，它有一层潜变量，可用于学习输入的表示。在\chap?中，我们将看到 RBM 如何被用来构建许多的深层模型。在这里，我们举例展示了 RBM 在许多深度图模型中使用的实践：它的单元被分成很大的组，这种组称作层，层之间的连接由矩阵描述，连通性相对密集。该模型被设计为能够进行高效的 Gibbs采样，并且模型设计的重点在于以很高的自由度来学习潜变量，而潜变量的含义并不是设计者指定的。之后在\sec?，我们将更详细地再次讨论 RBM。



标准的 RBM 是具有二值的可见和隐藏单元的基于能量的模型。 其能量函数为


$$\begin{aligned}
E(\boldsymbol v,\boldsymbol h) = -\boldsymbol b^{\top}\boldsymbol v - \boldsymbol c^{\top}\boldsymbol h - \boldsymbol v^{\top}\boldsymbol W\boldsymbol h,
\end{aligned}$$


其中 $\boldsymbol b,\boldsymbol c$ 和 $\boldsymbol W$ 都是无约束、实值的可学习参数。我们可以看到，模型被分成两组单元：$\boldsymbol v$ 和 $\boldsymbol h$，它们之间的相互作用由矩阵 $\boldsymbol W$ 来描述。该模型在\fig?中以图的形式描绘。<!-- %可以看到。 -->该图能够使我们更清楚地发现，该模型的一个重要方面是在任何两个可见单元之间或任何两个隐藏单元之间没有直接的相互作用（因此称为"受限"，一般的玻尔兹曼机可以具有任意连接）。




<center>

![](http://images.iterate.site/blog/image/20190718/PRfqs3U52f3E.png?imageslim){ width=55% }

</center>
> 16.14 一个画成马尔可夫网络形式的 RBM。



对 RBM 结构的限制产生了良好的属性


$$\begin{aligned}
p(\mathbf h\mid\mathbf v) = \prod_i p(\mathrm h_i\mid \mathbf v)
\end{aligned}$$


以及


$$\begin{aligned}
p(\mathbf v\mid\mathbf h) = \prod_i p(\mathrm v_i\mid \mathbf h).
\end{aligned}$$


独立的条件分布很容易计算。对于二元的受限玻尔兹曼机，我们可以得到：


$$\begin{aligned}
p(\mathrm h_i = 1\mid\mathbf v) &= \sigma\big(\mathbf v^{\top}\boldsymbol W_{:,i} + b_i\big),\\
p(\mathrm h_i = 0\mid\mathbf v) &= 1 - \sigma\big(\mathbf v^{\top}\boldsymbol W_{:,i} + b_i\big).
\end{aligned}$$


结合这些属性可以得到高效的块吉布斯采样，它在同时采样所有 $\boldsymbol h$ 和同时采样所有 $\boldsymbol v$ 之间交替。RBM 模型通过 Gibbs采样产生的样本展示在\fig?中。

<center>

![](http://images.iterate.site/blog/image/20190718/8CKebtAlIcA5.png?imageslim){ width=55% }

</center>
> 16.15 训练好的 RBM 的样本及其权重。\emph{(左)}用 MNIST 训练模型，然后用 Gibbs采样进行采样。每一列是一个单独的 Gibbs采样过程。每一行表示另一个 $1000$ 步后 Gibbs采样的输出。连续的样本之间彼此高度相关。\emph{(右)}对应的权重向量。将本图结果与图\?中描述的线性因子模型的样本和权重相比。由于 RBM 的先验 $p(\boldsymbol h)$ 没有限制为因子，这里的样本表现得好很多。采样时 RBM 能够学习到哪些特征需要一起出现。另一方面说，RBM 后验 $p(\boldsymbol h \mid \boldsymbol v)$ 是因子的，而稀疏编码的后验并不是，所以在特征提取上稀疏编码模型表现得更好。其他的模型可以使用非因子的 $p(\boldsymbol h)$ 和非因子的 $p(\boldsymbol h \mid \boldsymbol h)$。图片经 {lisa_tutorial_rbm}允许转载。


由于能量函数本身只是参数的线性函数，很容易获取能量函数的导数。 例如，


$$\begin{aligned}
\frac{\partial}{\partial W_{i,j}} E(\mathbf v,\mathbf h) = - \mathrm v_i \mathrm h_j.
\end{aligned}$$


这两个属性，高效的 Gibbs采样和导数计算，使训练过程变得非常方便。在\chap?中，我们将看到，可以通过计算应用于这种来自模型样本的导数来训练无向模型。

训练模型可以得到数据 $\boldsymbol v$ 的表示 $\boldsymbol h$。我们经常使用 $\mathbb E_{\mathbf h\sim p(\mathbf h\mid\boldsymbol v)}[\boldsymbol h]$ 作为一组描述 $\boldsymbol v$ 的特征。



总的来说，RBM 展示了典型的图模型深度学习方法：<!-- %结合由矩阵参数化的层之间的高效相互作用通过多层潜变量完成表示学习。 -->使用多层潜变量，并由矩阵参数化层之间的高效相互作用来完成表示学习。


图模型为描述概率模型提供了一种优雅、灵活、清晰的语言。在未来的章节中，我们将使用这种语言，以其他视角来描述各种各样的深度概率模型。




# 相关

- 《深度学习》花书
