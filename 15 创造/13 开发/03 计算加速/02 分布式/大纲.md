# 大纲




如果只是用一台机器的话，将需要一年多的时间才能完成模型训练，因此我们设计了一种可以将该任务分配给 336 个 GPU 的方法，从而将总训练时间缩短至数周。随着模型规模越来越大——这项研究中最大的是 ResNeXt 101-32x48d，其参数超过了 8.61 亿个——这种分布式训练变得越来越重要。



## 可以补充的

- [分布式系统之Raft共识算法](https://juejin.im/post/5d2c6380f265da1bb80c5fdc)




分布式平台搭建


## 可以补充进来的

- [快速打造分布式深度学习训练平台](https://zhuanlan.zhihu.com/p/28629224)
