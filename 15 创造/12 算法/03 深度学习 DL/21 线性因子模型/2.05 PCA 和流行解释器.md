---
title: 2.05 PCA 和流行解释器
toc: true
date: 2019-06-05
---

# PCA的流形解释

线性因子模型，包括 PCA 和因子分析，可以理解为学习一个流形~。我们可以将概率 PCA 定义为高概率的薄饼状区域，即一个高斯分布，沿着某些轴非常窄，就像薄饼沿着其垂直轴非常平坦，但沿着其他轴是细长的，正如薄饼在其水平轴方向是很宽的一样。\fig?解释了这种现象。PCA 可以理解为将该薄饼与更高维空间中的线性流形对准。这种解释不仅适用于传统 PCA，而且适用于学习矩阵 $\boldsymbol W$ 和 $\boldsymbol V$ 的任何线性自编码器，其目的是使重构的 $\boldsymbol x$ 尽可能接近于原始的 $\boldsymbol x$。


<center>

![](http://images.iterate.site/blog/image/20190718/S0Hgx1ldmeq0.png?imageslim){ width=55% }

</center>
> 13.3 平坦的高斯能够描述一个低维流形附近的概率密度。此图表示了"流形平面"上"馅饼"的上半部分，并且这个平面穿过了馅饼的中心。正交于流形方向（指向平面外的箭头方向）的方差非常小，可以被视作是"噪声"，其他方向（平面内的箭头）的方差则很大，对应了"信号"以及降维数据的坐标系统。




编码器表示为


$$\begin{aligned}
\boldsymbol h  = f(\boldsymbol x) = \boldsymbol W^{\top} (\boldsymbol x - \boldsymbol mu).
\end{aligned}$$% 490 head



编码器计算 $h$ 的低维表示。从自编码器的角度来看，解码器负责计算重构：



$$\begin{aligned}
\hat{\boldsymbol x} = g(\boldsymbol h) = \boldsymbol b + \boldsymbol V \boldsymbol h.
\end{aligned}$$


能够最小化重构误差


$$\begin{aligned}
\mathbb E[\boldsymbol ert\boldsymbol x - \hat{\boldsymbol x}\boldsymbol ert^2]
\end{aligned}$$


的线性编码器和解码器的选择对应着 $\boldsymbol V = \boldsymbol W$，${\boldsymbol mu} = \boldsymbol b = \mathbb E[\boldsymbol x]$， $\boldsymbol W$ 的列 形成一组标准正交基，这组基生成的子空间与协方差矩阵 $\boldsymbol C$


$$\begin{aligned}
\boldsymbol C = \mathbb E[(\boldsymbol x - {\boldsymbol mu})(\boldsymbol x - {\boldsymbol mu})^{\top}]
\end{aligned}$$


的主特征向量所生成的子空间相同。在 PCA 中，$\boldsymbol W$ 的列是按照对应特征值（其全部是实数和非负数）幅度大小排序所对应的特征向量。

我们还可以发现 $\boldsymbol C$ 的特征值 $\lambda_i$ 对应了 $\boldsymbol x$ 在特征向量 $\boldsymbol v^{(i)}$ 方向上的方差。
如果 $\boldsymbol x\in \mathbb R^D$，$\boldsymbol h\in\mathbb R^d$ 并且满足 $d<D$，则（给定上述的 ${\boldsymbol mu},\boldsymbol b,\boldsymbol V,\boldsymbol W$ 的情况下）最佳的重构误差是


$$\begin{aligned}
\min \mathbb E[\boldsymbol ert \boldsymbol x - \hat{\boldsymbol x} \boldsymbol ert^2] = \sum_{i=d+1}^{D}\lambda_i.
\end{aligned}$$


因此，如果协方差矩阵的秩为 $d$，则特征值 $\lambda_{d+1}$ 到 $\lambda_{D}$ 都为 $0$，并且重构误差为 $0$。

此外，我们还可以证明上述解可以通过在给定正交矩阵 $\boldsymbol W$ 的情况下最大化 $\boldsymbol h$ 元素的方差而不是最小化重构误差来获得。


某种程度上说，线性因子模型是最简单的生成模型和学习数据表示的最简单模型。许多模型如线性分类器和线性回归模型可以扩展到深度前馈网络，而这些线性因子模型可以扩展到自编码器网络和深度概率模型，它们可以执行相同任务但具有更强大和更灵活的模型族。



# 相关

- 《深度学习》花书
