---
title: 2.04 稀疏编码
toc: true
date: 2019-06-05
---

# 稀疏编码


稀疏编码 是一个线性因子模型，已作为一种无监督特征学习和特征提取机制得到了广泛研究。严格来说，术语"稀疏编码"是指在该模型中推断 $\boldsymbol h$ 值的过程，而"稀疏建模"是指设计和学习模型的过程，但是通常这两个概念都可以用术语"稀疏编码"描述。

像大多数其他线性因子模型一样，它使用了线性的解码器加上噪声的方式获得一个 $\boldsymbol x$ 的重构，就像\eqn?描述的一样。更具体地说，稀疏编码模型通常假设线性因子有一个各向同性精度为 $\beta$ 的高斯噪声：


$$\begin{aligned}
p(\boldsymbol x\mid \boldsymbol h) = \mathcal N
(\boldsymbol x;\boldsymbol W\boldsymbol h + \boldsymbol b ,\frac{1}{\beta}\boldsymbol I).
\end{aligned}$$



分布 $p(\boldsymbol h)$ 通常选取为一个峰值很尖锐且接近 $0$ 的分布~。常见的选择包括可分解的 Laplace、Cauchy或者可分解的 Student-t分布。例如，以稀疏惩罚系数 $\lambda$ 为参数的 Laplace 先验可以表示为


$$\begin{aligned}
p(h_i) = \text{Laplace}(h_i;0,\frac{2}{\lambda}) = \frac{\lambda}{4} \text{e}^{ -\frac{1}{2}\lambda \vert h_i\vert},
\end{aligned}$$


相应的，Student-t先验分布可以表示为


$$\begin{aligned}
p(h_i)\propto \frac{1}{(1+\frac{h_i^2}{\nu})^{\frac{\nu+1}{2}}}.
\end{aligned}$$

使用最大似然的方法来训练稀疏编码模型是不可行的。相反，为了在给定编码的情况下更好地重构数据，训练过程在编码数据和训练解码器之间交替进行。稍后在\sec?中，这种方法将被进一步证明为是解决最大似然问题的一种通用的近似方法。

对于诸如 PCA 的模型，我们已经看到使用了预测 $\boldsymbol h$ 的参数化的编码器函数，并且该函数仅包括乘以权重矩阵。稀疏编码中的编码器不是参数化的编码器。相反，编码器是一个优化算法，在这个优化问题中，我们寻找单个最可能的编码值：


$$\begin{aligned}
\boldsymbol h^* = f(\boldsymbol x) = \underset{\boldsymbol h}{\arg\max}\  p(\boldsymbol h\mid\boldsymbol x).
\end{aligned}$$ % 487


结合\eqn?和\eqn?，我们得到如下的优化问题：


$$\begin{aligned}
& \underset{\boldsymbol h}{\arg\max}\  p(\boldsymbol h\mid\boldsymbol x) \\
= ~& \underset{\boldsymbol h}{\arg\max}\ \log  p(\boldsymbol h\mid\boldsymbol x)\\
= ~& \underset{\boldsymbol h}{\arg\min}\ \lambda \boldsymbol ert \boldsymbol h\boldsymbol ert_1 + \beta  \boldsymbol ert \boldsymbol x - \boldsymbol W \boldsymbol h\boldsymbol ert_2^2,
\end{aligned}$$


其中，我们扔掉了与 $\boldsymbol h$ 无关的项，并除以一个正的缩放因子来简化表达。

由于在 $\boldsymbol h$ 上施加 $L^1$ 范数，这个过程将产生稀疏的 $\boldsymbol h^*$（详见\sec?）。


为了训练模型而不仅仅是进行推断，我们交替迭代关于 $\boldsymbol h$ 和 $\boldsymbol W$ 的最小化过程。在本文中，我们将 $\beta$ 视为超参数。我们通常将其设置为 $1$，因为它在此优化问题的作用与 $\lambda$ 类似，没有必要使用两个超参数。原则上，我们还可以将 $\beta$ 作为模型的参数，并学习它。我们在这里已经放弃了一些不依赖于 $\boldsymbol h$ 但依赖于 $\beta$ 的项。要学习 $\beta$，必须包含这些项，否则 $\beta$ 将退化为 $0$。


不是所有的稀疏编码方法都显式地构建了一个 $p(\boldsymbol h)$ 和一个 $p(\boldsymbol x\mid\boldsymbol h)$。通常我们只是对学习一个带有激活值的特征的字典感兴趣，当特征是由这个推断过程提取时，这个激活值通常为 $0$。

如果我们从 Laplace 先验中采样 $\boldsymbol h$，$\boldsymbol h$ 的元素实际上为 $0$ 是一个零概率事件。生成模型本身并不稀疏，只有特征提取器是稀疏的。{Goodfeli-et-al-TPAMI-Deep-PrePrint-2013-small}描述了不同模型族中的近似推断，如尖峰和平板稀疏编码模型，其中先验的样本通常包含许多真正的 $0$。

与非参数编码器结合的稀疏编码方法原则上可以比任何特定的参数化编码器更好地最小化重构误差和对数先验的组合。另一个优点是编码器没有泛化误差。参数化的编码器必须泛化地学习如何将 $\boldsymbol x$ 映射到 $\boldsymbol h$。对于与训练数据差异很大的异常 $\boldsymbol x$，所学习的参数化编码器可能无法找到对应精确重构或稀疏的编码 $\boldsymbol h$。对于稀疏编码模型的绝大多数形式，推断问题是凸的，优化过程总能找到最优编码（除非出现退化的情况，例如重复的权重向量）。显然，稀疏和重构成本仍然可以在不熟悉的点上升，但这归因于解码器权重中的泛化误差，而不是编码器中的泛化误差。当稀疏编码用作分类器的特征提取器，而不是使用参数化的函数来预测编码值时，基于优化的稀疏编码模型的编码过程中较小的泛化误差可以得到更好的泛化能力。{Coates2011b}证明了在对象识别任务中稀疏编码特征比基于参数化的编码器（线性-sigmoid 自编码器）的特征拥有更好的泛化能力。受他们的工作启发，{Goodfeli-et-al-TPAMI-Deep-PrePrint-2013-small}表明一种稀疏编码的变体在标签极少（每类 20 个或更少标签）的情况中比相同情况下的其他特征提取器拥有更好的泛化能力。



非参数编码器的主要缺点是在给定 $\boldsymbol x$ 的情况下需要大量的时间来计算 $\boldsymbol h$，因为非参数方法需要运行迭代算法。在\chap?中讲到的参数化自编码器方法仅使用固定数量的层，通常只有一层。另一个缺点是它不直接通过非参数编码器进行反向传播，这使得我们很难采用先使用无监督方式预训练稀疏编码模型然后使用监督方式对其进行精调的方法。允许近似导数的稀疏编码模型的修改版本确实存在但未被广泛使用。

像其他线性因子模型一样，稀疏编码经常产生糟糕的样本，如\fig?所示。即使当模型能够很好地重构数据并为分类器提供有用的特征时，也会发生这种情况。这种现象发生的原因是每个单独的特征可以很好地被学习到，但是隐藏编码值的因子先验会导致模型包括每个生成样本中所有特征的随机子集。这促使人们开发更深的模型，可以在其中最深的编码层施加一个非因子分布， 与此同时也在开发一些复杂的浅度模型。




<center>

![](http://images.iterate.site/blog/image/20190718/WDNruAICjMw7.png?imageslim){ width=55% }

</center>

> 13.2 尖峰和平板稀疏编码模型上在 MNIST 数据集训练的样例和权重。\emph{(左)}这个模型中的样本和训练样本相差很大。第一眼看来，我们可能认为模型拟合得很差。\emph{(右)}这个模型的权重向量已经学习到了如何表示笔迹，有时候还能写完整的数字。因此这个模型也学习到了有用的特征。问题在于特征的因子先验会导致特征子集合随机的组合。一些这样的子集能够合成可识别的 MNIST 集上的数字。这也促进了拥有更强大潜在编码分布的生成模型的发展。

<span style="color:red;">这促进了更深层模型的发展，可以在最深层上施加 non-factorial分布，以及开发更复杂的浅层模型。</span>



# 相关

- 《深度学习》花书
