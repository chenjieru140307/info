---
title: 自动驾驶中的计算机视觉
toc: true
date: 2019-11-17
---
# 自动驾驶中的计算机视觉


在今年 6 月召开的 CVPR2018 上，德国图宾根大学及马克斯·普朗克研究所（MPI）自动视觉组负责人 Andreas Geiger 教授摘得了 PAMI Young Researcher Award，该奖项颁发给 7 年内获得博士学位且早期研究极为有潜力的研究人员。



![img](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW93s9Y6Ljqia0EicbrXtEibh99NffKqUMYmSRe6m3uRwKfg0YvH8B7AUibSUicZpo8cibeBpmXulXmUDaMw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*图中为 Andreas Geiger，图右为另一名获奖者，来自 Facebook 的青年科学家何恺明*



Geiger 的研究主要集中在用于自动驾驶系统的三维视觉理解、分割、重建、材质与动作估计等方面。他主导了自动驾驶领域著名数据集 KITTI 及多项自动驾驶计算机视觉任务的基准体系建设，KITTI 是目前最大的用于自动驾驶的计算机视觉公开数据集。



2018 年年初，Geiger 出任了北京冲浪科技有限公司 (Surfingtech) 的首席科学家，冲浪科技是一家致力于为全球自动驾驶公司提供多传感器数据解决方案的公司。7 月，机器之心在冲浪科技对 Geiger 进行了专访，谈了谈自动驾驶领域的计算机视觉任务的特点、研究前沿及 KITTI 数据集的最新进展。



机器之心：自动驾驶系统都由哪些模块组成？它们之间的依赖关系是什么？



自动驾驶系统通常有一个非常经典的、模块化的流水线。



首先是**感知模块**（perception stack），感知模块将地图、三维传感器、二维传感器中的信息给到**「世界模型」**（world model），世界模型将上述信息，汇总在一张地图中，理解每一个时刻不同的物体相对于路面、道线等的位置，预测下一刻的可选路径都有哪些。随后是一个**规划模块**（planning model），进行决策。决策的内容也是分层级的。粗粒度的决策需要决定如何从 A 点到 B 点，完成的是类似 GPS 的工作。除此之外还有诸多细粒度的决策工作，例如决定走哪一条车道，是否要暂时占用对向车道完成超车，车速应该设定为多少等。最后是**控制模块**（control module），控制模块操纵所有的控制器，有高层的控制器，比如电子稳定系统 ESP，也有最基层的控制器，比如控制每一个轮子进行加速和刹车的控制器。



机器之心：想要让一个自动驾驶系统作出正确的决策，首先要完成哪些计算机视觉任务？



首先是**车辆定位**：衡量车辆的运动并在地图中进行定位。完成这部分工作的是视觉测距（visual odometry）系统和定位（localization）系统。二者的区别是，视觉测距估计的是车辆相对于前一时间步进行的相对运动，而定位是对车辆在地图中的运动进行全局的估计。定位是可以精确到厘米级的，车辆相对于一些地图中固定的物体（例如电线杆）的距离已经是已知的，基于这些信息，车辆已经可以进行相当不错的路径规划了。



然后是**三维视觉重建**，重建范围通常在 50-80 米，具体需求视行驶速度而定。大部分 STOA 自动驾驶系统会使用激光雷达（LiDAR）进行三维重建。不过也有少部分团队试图直接从图像中恢复三维信息。由于图像中的数据相比之下更为嘈杂，因此完全基于图像的重建是一项更具有挑战性的工作。



![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW93s9Y6Ljqia0EicbrXtEibh99yE9LuVt2gsHvIicw9aImtUnwRmqSqMZmxTBl4D4wjMmUVOTbS3PRtyw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*基于单张激光雷达点云与二维图片的三维视觉重建*



除了重建之外，你也需要对车辆正前方正在发生的事有充分的理解。因此，你需要进行**物体检测**，也需要在理解物体是什么的基础上对其进行进一步的**分类**，而检测和分类会帮助预测其未来轨迹。进行检测和分类的方式是多样的，你可以给每一个物体画一个边界框（bounding box）：这是最常见的方式，但是自动驾驶需要在三维的物理世界中进行运动规划，所以你至少要需要一个三维的边界框。



更精确的是**实例分割**（instance segmentation）和**语义分割**（semantic segmentation）。当物体是形状为凹或者是隧道之类需要穿行的物体时，边界框显然是不够的。实例分割将图像里属于一些特定目标类别的每个实例的所有像素分为一类。实例分割通常在二维图像上进行，但也有三维版本，三维实例分割基本等同于物体重建。而语义分割为图像里每一个像素分配一个语义标签，同一类别的不同实例不做区分。除此之外，**全景分割**（panoptic segmentation）基本上可以视作实例分割和语义分割的结合。全景分割对那些没有实例只有整体的类别也进行区分，例如天空和植被。天空无法用一个边界框框柱，而植被需要在平时避开，但系统也需要知道在紧急情况下汽车冲上草坪并无大碍（相比之下撞上树或者行人就有很大问题了）。因此语义信息是必要的。



接下来是**运动估计**。根据之前的一帧或数帧，预估视野里的每一个点，或者说每一个物体，在下一帧的位置。一些物体，例如车辆，它们的移动是比较容易预判的，因此运动模型可以进行准确率较高的预测。而另外一些物体，例如行人，会非常突然地变更其运动轨迹，导致运动模型的建立更为艰难。即便如此，较短时间区间（2-3 秒）的动作预测，在动态物体较多的拥挤场景下的决策过程中也仍然扮演着至关重要的角色。



上述任务都是各自独立的，但是实际上，收集上述信息的系统并不是各自独立运转的。因此**情境推理**（contextual reasoning）也有助于给出更准确的预测。例如一群行人通常会同时等红灯、同时过马路，一辆车试图并线时另一辆车会刹车让路，以这些外部信息、先验知识做约束，理解复杂场景会变得更为容易。



最后，一个我认为十分重要但是并没有引起较多重视的领域是**不确定性推理**（reasoning under uncertainty）。人类感官或者车辆传感器拿到的数据中必然包含着不确定性，因此，如何准确地评估不确定性，并兼顾「最小化风险」和「完成任务」，是一个重要的话题。理想情况下，所有上述检测、分割、重建、定位任务都应该在不确定性约束下进行，系统在行进之前应该知道它可能犯哪些错误。



机器之心：如何对与自动驾驶相关的计算机视觉任务进行分类？分类的标准是什么？



以输入进行分类是一种常见做法。**按照输入的来源**，可以分为来自激光雷达、摄像头、雷达、乃至车内的其他仪表的数据。**按照输入表征**也可以进行分类，激光雷达给出的稀疏的点云和摄像头给出的密集的二维图像就是两种不同的表征，采取的算法也有所不同。按照**维度**也可以进行分类，用于三维输入的算法通常更为复杂，因为如果不采取特殊做法，三维输入会快速耗尽内存资源。



另一种分法是**按照线索**分类。线索可以分为语义线索（semantic cues）和几何线索（geometric cues），几何线索是利用多张图片通过特征匹配和三角对齐得到深度信息。但是因为这种估算的误差与距离呈平方关系，因此有很大的局限性。换言之，人类的视觉系统其实也是不适合开车的，因为我们的视觉系统只是为了在两手张开的距离内进行操作而设计的。人类在开车的时候利用语义线索弥补了这一缺陷：即使只有一张图片，理论上其中并不包含距离信息，人类仍然可以根据大量的先验知识估计其中物体的相对距离。总而言之，自动驾驶系统可以通过安装多个摄像头获得三维信息，也可以通过安装一个摄像头，但是通过强先验来预判会看到什么。理想情况下，我们希望将二者结合。



还有一种方法是**根据物体是否运动以及如何运动**进行分类。首先分为静态部分识别和运动物体识别。对于静态场景来说，有专门的标准重建算法，基于「所有的东西都是静态的」这一假设进行重构。但是事实上，我们需要从多幅拍摄于不同时间的图像中重建场景，这就需要我们设计专门的算法来处理场景中的运动物体。运动物体又可以分为刚性物体和非刚性物体。刚性物体的所有部分一起运动，可以用六自由度变化来描述它在三维空间中的运动轨迹。汽车就是一个刚性物体，而行人就属于非刚性物体，行人有胳膊、腿，彼此之间以关节相连，在运动中，每一个部分的运动轨迹大相径庭，因此刻画行人的运动需要额外的自由度。



机器之心：任务之间是否有先后顺序？是否存在一个自动驾驶系统通常采用的完成任务的流程？



首先，确实存在一些依赖关系。例如三维物体识别需要以三维信息为基础。如果你只有普通的图像，那么通常就要从三维重建开始。虽然这不是必须要做的，但是大多数研究者选择采用这个流程。也有人选择用动作（motion）来辅助识别，但是动作只是一个非常弱的线索。



三维重建也不仅仅对三维识别有帮助，它也有助于运动估计。基于 RGBD 信息的运动估计难度要小于仅基于 RGB 信息的运动估计。



而基于地图的定位也有助于行人及其他交通参与者的行为预判。例如，人行道的常见位置、红绿灯的常见位置给出了一个关于行人运动轨迹的强先验。另外，定位还能帮助你「穿墙透视」：即使路的转角被高楼大厦遮挡住了，根据地图定位，你仍然知道楼后面是马路，而马路上会有其他交通参与者存在。



机器之心：来自不同传感器的数据如何共同作为输入服务于同一模型？



总的来说，识别或者检测类任务是靠着找出物体的**形状**和**外观**进行的。不同的传感器的长处各不相同，激光雷达给出距离信息，但是很难描述外观，因为激光雷达捕捉反射率，反射率与外观并不直接相关。而图像中虽然包含丰富的外观信息，但是缺少距离信息。毫米波雷达的视野非常窄，但是可以进行远距离测距和测速。



理想情况下，你要根据不同传感器的特点去将它们生成的数据组合在一起。例如，如果你同时拥有立体视觉数据和激光雷达数据，你希望利用激光雷达的远场数据和立体视觉的近场数据。这是因为在 10-15 米范围内，立体视觉要比激光雷达精确，但是超出了这个范围，激光雷达反而更为精确。



由于不同传感器之间的校准是非常困难的，因此在每一个点选择信任哪个传感器的数据是一道世界性的难题。而结合的做法有很多，总得来说就是把不同的数据类型投射到同一个空间内。你可以用立体摄像机找到每个三维点阵里对应的像素值，把它和激光雷达数据结合，也可以把激光雷达中的深度信息投影回图片里。



![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW93s9Y6Ljqia0EicbrXtEibh99icMUjFD73AphqfNWCx025NrPBHcobFUxUMb49guUwegyyG6Wmxbg1iag/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*将二维色彩信息映射到三维点云中*



当然，这只是研究者通常采用的方法，从很多汽车制造商的角度，他们希望拥有信息冗余，让不同的传感器各自为政，作出独立的决策。这样一旦某一个传感器失灵了，其余的传感器仍然能提供决策所必要的信息。



机器之心：研究者所采用的系统和汽车制造商使用的系统有哪些不同？



如今的车辆和驾驶员辅助系统通常都存在算力非常分散化的现状。用于研究的车辆可以有一个集中的处理器，但是如今的车辆中系统的现状是，摄像头的算力只与摄像头相连，控制系统的计算单元算力也非常小，你只能把物体基本信息提交给它，无法把深度信息全部提交给它。



但是理论上，只有当你集成了所有传感器获得的信息，才能获得最佳的性能。因此这也是从当今的驾驶辅助系统到自动驾驶汽车需要完成的一个转变。



机器之心：现如今，绝大多数自动驾驶公司仍然选用不同类型、数量的传感器，开发自己的系统。不同传感器带来的差异有多大？



大多数传感器的工作原理还是相似的。例如，对于摄像头而言，大多数公司都选择对光线变化尽可能敏感的，或者说动态范围（dynamic range）尽可能大的摄像头。这是因为如今摄像头的动态范围仍然远小于人眼，例如，从阳光下驶入隧道后就要求系统迅速更换摄像头的结构以适应突如其来的黑暗。此外，世界上生产感光元件也只有屈指可数的几家，因此摄像头之间的差别并不大。



但是，即使完全相同的摄像头，安装的高度、位置不同，视野里看到的东西就会存在很大的差异。比如卡车车顶的摄像头和普通的小型车车顶的摄像头，其相对于路面的相对位置的巨大差异就决定了他们的很多配置和算法无法通用。这是一个有待解决的研究课题。



至于激光雷达，如今的大多数激光雷达都来自 Velodyne。但是现在也有越来越多新牌子涌现。同时，在传统的旋转式雷达之外也发展出了更便宜、更容易装配的固态雷达。从学习的角度，这都是需要适应的部分。



毫米波雷达更为神秘，各家都将其工作原理视为机密，拿到毫米波雷达的原始数据都很困难。



总体来讲，如今用于研究的设备配置方法都非常类似：例如把摄像头放在尽可能高的地方、尽可能在每个方向都设置摄像头等等。如今也有越来越多功能更为强大的摄像头问世，有的摄像头的分辨率已经接近人眼，有的致力于更远的焦距。



机器之心：现在的自动驾驶视觉领域都有哪些前沿问题？



一个非常重要的问题是如何处理那些分布里极少出现的稀有事件、个案。如今行之有效的自动驾驶算法都是监督算法，而我们在收集训练集的时候无法拿到大量的稀有事件标记数据。我认为，我们需要找到好的**稀有事件生成模型**，现在，稀有事件刻画的领军团队无疑是 Waymo，但是他们采用的方法是搭建一个大实验室，把各种稀有事件人工演出来。



**领域迁移**是另一个大问题，我们不希望换了环境/车之后就要重新训练模型。迁移学习能够一定程度上通过在一个数据集上训练、在另一个上面精调来解决这个问题，但是这并不是最终的解决方案。



此外，**仿真**（simulation）虽然不像前两项那么重要，也仍然有进步的空间。尤其是如何跨越从仿真到真实以及从真实到仿真的差距。仿真永远只是仿真，仿真能够表达出的维度永远无法达到真实世界的复杂程度，纹理、几何关系都过于简单了。前面谈到的稀有事件也是无法通过仿真习得的。



最后，自动驾驶终归需要作出「主观判断」，这是整个人工智能领域面对的一个挑战：系统需要在前所未见的场景中进行决策，并且决策时可能需要稍微「违背规则」，这是计算机所不擅长的事情。



机器之心：能否介绍下您在图宾根大学/马普所的自动视觉组的工作重点？



上文提到的研究我们大部分都有涉及。



我们进行一些底层的研究，例如**三维重建和三维动作预测**。我们研究生成模型，我希望能将现有的生成模型扩展到能够生成整个「世界」的模型：生成内容包括几何信息、动作信息等等。当然，整个学术界距离实现这一步还有很长的路要走，但是一旦实现，这将对仿真以及自动驾驶的训练给予极大帮助。



另一方面，我们也在研究**小数据学习**，因为数据是一个太大的问题。在 KITTI 360 数据集中，我们就做了一些尝试，例如在三维空间进行标注，然后由此获得二维的标注。对于动作估计来说更是如此，因为人类甚至不能很好地对动作数据进行标注。

还有**仿真与真实世界的连接**，例如我们近期的工作在研究如何把虚拟的物体放进真实场景中，这样我们只需要对我们真正关注的部分建模。



![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW93s9Y6Ljqia0EicbrXtEibh999IMr2BHACbpzkx6cBjClnb1bkHHlEG4QNjGMHM1oq0pr2HicOPfib8Hw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*在通过多帧三维点云建立的三维重构空间里进行一次标注，能够同时得到多张二维图像的标注*



从模型的角度来看，我们主要研究**深度学习模型和概率模型**。我们对学术界的贡献主要在于将深度学习应用在三维数据、稀疏数据等由于存储或模型能力限制而在现阶段无法获得较好效果的子领域。我们也在思考，分模块的自动驾驶系统是不是最优的解决方案？不考虑中间输出的端到端的自动驾驶模型会不会是更优的解决方案？最极端的端到端模型是用一个单独模型拿到图像之后直接进行方向盘、油门刹车的控制，但是建立这样的模型也非常难，我想二者中间某处可能存在更好的解决方案。



我们也致力于**提供基准**（benchmarking），提供一个比较不同模型的公允基础。包括 KITTI 等一系列数据集、学术会议上的研讨会和挑战赛。KITTI 数据集本身就提供了超过了 10 项挑战。



机器之心：您如何选择研究课题？



研究的终极原则是，不应以「最大化论文数量」为目标，应该以「最大化领域贡献」为目标。这也是为什么我们致力于建立基准：自动驾驶研究领域在 KITTI 的影响下发生了很多变化。



另一个原则是「更进一步」，在直接能够投入业界应用的算法之外，我们更关心那些高风险，但是也潜在具有高收益的算法：它们有很大的概率会失败，但是一旦成功，就会是跨时代的一步。



机器之心：能否简要介绍一下 KITTI 数据集？



我们一共进行了超过六小时的行驶，并且公开了三小时的行驶数据。这听起来并不多，但是相比于之前的自动驾驶公开数据集，已经有了一个非常巨大的数量上的提升。



KITTI 的优势在于我们在车上安装了各种传感器。第一代数据集包括了激光雷达、摄像头、GPS、IMU 等不同传感器的信号数据。



![img](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW93s9Y6Ljqia0EicbrXtEibh99OHiawsXHvKItd1xJK67cNNiak2V5tAvQfGIFWDlDWYVnnWqfFQzQGUEw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*KITTI 数据采集车*



我们在网上以挑战赛的形式发布了一系列任务，任务包括立体视觉、光流、场景流、二维物体识别和三维物体识别、鸟瞰图物体识别、语义分割、实体分割、道路与车道检测、单图像深度估计、深度完成（depth completion，把稀疏的激光雷达点云数据变为密集的图像）等。当前的自动驾驶系统需要的各类任务我们都有涵盖。



每个任务的训练集大小主要取决于标注难度，例如立体视觉和光流有 400 张标记数据，动作估计有超过 10,000 张标记数据，单图像深度估计有 90,000 张。



我们给出了训练集和不包含真实标签的测试集。研究者每个月可以向服务器上传他们的结果，我们则给出性能反馈。我们不向研究者提供测试集的真实标签，这在一定程度上解决了过拟合问题。



机器之心：如何选择衡量标准（evaluation metrics）？衡量标准与损失函数之间的区别是什么？



我们通常采用业界已有的衡量标准。例如，在立体视觉和光流任务中，我们计算正确与错误的像素数；在物体识别任务中，我们计算平均精度，即预测和真值之间重叠的面积与总面积的比例。



损失函数通常是我们想要实现的目标，在自动驾驶中，的确存在目标和损失函数错位的问题，我们的一些研究也在致力于弥合这种错位。例如，在光流任务里，一个左上角的代表天空的像素点真的和代表路面的像素点一样重要吗？如果不是的话，如何设置新的衡量标准？这都是我们在思考的问题。



机器之心：在数据集、衡量标准等方面，与自动驾驶相关的计算机视觉任务和通用计算机视觉任务有什么差异？



首先是**多样性程度**不一样。自动驾驶是一个非常特殊的领域，这甚至让算法起步变得更「容易」：车辆不会行驶到下水道里、不会行驶在房顶上，如果你不知道身处何方，那么预测面前的像素是「道路」，准确率其实很高。



但是另一个角度，自动驾驶和通用计算机视觉**对算法精度的要求**也是截然不同的，一个准确率 99% 的人脸识别算法已经很令人满意，但是一个准确率 99% 的自动驾驶模型大概每天都要撞翻点什么。因此自动驾驶需要注意图片里的细部，注意那些此刻距离我们很远，但是过一会儿就会出现在我们眼前的物体，通用视觉任务不需要如此。



此外，**数据收集的难度**也截然不同，通用视觉任务的数据集大部分来自互联网，但自动驾驶所需的数据不是天然存在的。也很难标注。在业界，仅仅 Mobileye 一家公司每天就有近千人在专门进行数据标注，业界的算法与学界公开的算法相比，未必有显著的优势，其性能优势主要来源于数据优势。



机器之心：KITTI 360 相比于之前的版本有哪些更新？冲浪科技在数据集的建设方面做出了哪些工作？



KITTI 360 仍然沿用了同一辆车，我们增加了一个激光雷达，给出了更多、更加精确的三维信息。我们也增加了 360 度图像，为数据扩增建立了环境地图。我们希望这能让我们更好地标记视野中的物体。



我们和冲浪科技合作开发了 KITTI 360 用到的三维标注工具（文中大部分示例图片截取自该点云数据标注平台）。自动驾驶的标注是一项十分复杂的工作，例如像素级别的语义和实例分割，通常情况下一张图像的标注要耗时 60 - 90 分钟。而利用我们共同开发的标注工具，标注时间被极大缩短，这对于实验室而言是至关重要的。



![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW93s9Y6Ljqia0EicbrXtEibh99lGibSbxRL3tIP3NnichtH5g3Txy0KFH9faO6S1kJicCoqRdPoNvgRcPLw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*标注工具界面*



今年年底我们会公开一些与三维视觉理解相关的新任务，比如图像中所有物体的识别以及方向估计，我们仍然在探讨合适的组织方式。同时我们也很关注时序连贯的分割，因为时序连贯是自动驾驶数据的固有特性。*![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)*







**关于冲浪科技：**





冲浪科技致力于为全球自动驾驶公司提供多传感器的数据解决方案，包括：无人车多传感器的融合、自动驾驶数据的采集、导入、及点云数据的可视化等方案，并且所有通过多传感器采集的自动驾驶数据可以通过冲浪科技自主研发的点云和彩色数据融合的标注平台进行标注。



冲浪科技和 Andreas Geiger 实验室有着深入的合作：目前冲浪科技参与共同讨论整个标注流程及设计，让算法和人工可以高效地结合在一起，共同开发出更完善的标注流程和工具。冲浪科技的每个数据集都由精通算法的技术专家 Geiger 设计，会针对数据集可能出现的实际问题对数据集进行规划。数据集的采集、标注、质检流程更严格，对数据集的检查需要 3 步 100% 的质检，数据准确率达到 98%。

# 相关

- [专访 | CVPR PAMI青年研究员奖得主Andreas Geiger：自动驾驶中的计算机视觉](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650746468&idx=2&sn=42d0c5f0d202a9f2ffb7cd096cc49fdf&chksm=871aea1ab06d630c12472f1ff5d8e29447f5e8c8d5ba87d2acb413787f79c5ab4a9bff5f2339&mpshare=1&scene=1&srcid=0803B0TYRnFfhhTH5kYs06yO#rd)
