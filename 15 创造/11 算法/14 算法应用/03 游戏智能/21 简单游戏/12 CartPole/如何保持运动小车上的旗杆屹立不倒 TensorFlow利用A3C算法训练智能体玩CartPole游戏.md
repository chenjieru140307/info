---
title: å¦‚ä½•ä¿æŒè¿åŠ¨å°è½¦ä¸Šçš„æ——æ†å±¹ç«‹ä¸å€’ TensorFlowåˆ©ç”¨A3Cç®—æ³•è®­ç»ƒæ™ºèƒ½ä½“ç©CartPoleæ¸¸æˆ
toc: true
date: 2019-11-17
---
# å¦‚ä½•ä¿æŒè¿åŠ¨å°è½¦ä¸Šçš„æ——æ†å±¹ç«‹ä¸å€’ TensorFlowåˆ©ç”¨A3Cç®—æ³•è®­ç»ƒæ™ºèƒ½ä½“ç©CartPoleæ¸¸æˆ



> æœ¬æ•™ç¨‹è®²è§£å¦‚ä½•ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸€ä¸ªå¯ä»¥åœ¨ CartPole æ¸¸æˆä¸­è·èƒœçš„æ¨¡å‹ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨ tf.kerasã€OpenAI è®­ç»ƒäº†ä¸€ä¸ªä½¿ç”¨ã€Œå¼‚æ­¥ä¼˜åŠ¿åŠ¨ä½œè¯„ä»·ã€ï¼ˆAsynchronous Advantage Actor Criticï¼ŒA3Cï¼‰ç®—æ³•çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡ A3C çš„å®ç°è§£å†³äº† CartPole æ¸¸æˆé—®é¢˜ï¼Œè¿‡ç¨‹ä¸­ä½¿ç”¨äº†è´ªå©ªæ‰§è¡Œã€æ¨¡å‹å­ç±»å’Œè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚



è¯¥è¿‡ç¨‹å›´ç»•ä»¥ä¸‹æ¦‚å¿µè¿è¡Œï¼š



- è´ªå©ªæ‰§è¡Œâ€”â€”è´ªå©ªæ‰§è¡Œæ˜¯ä¸€ä¸ªå¿…è¦çš„ã€ç”±è¿è¡Œå®šä¹‰çš„æ¥å£ï¼Œæ­¤å¤„çš„è¿ç®—ä¸€æ—¦ä» Python è°ƒç”¨ï¼Œå°±è¦ç«‹åˆ»æ‰§è¡Œã€‚è¿™ä½¿å¾—ä»¥ TensorFLow å¼€å§‹å˜å¾—æ›´åŠ å®¹æ˜“ï¼Œè¿˜å¯ä»¥ä½¿ç ”ç©¶å’Œå¼€å‘å˜å¾—æ›´åŠ ç›´è§‚ã€‚
- æ¨¡å‹å­ç±»â€”â€”æ¨¡å‹å­ç±»å…è®¸é€šè¿‡ç¼–å†™ tf.keras.Model å­ç±»ä»¥åŠå®šä¹‰è‡ªå·±çš„æ­£å‘ä¼ å¯¼é€šè·¯è‡ªå®šä¹‰æ¨¡å‹ã€‚ç”±äºå¯ä»¥å¼ºåˆ¶å†™å…¥å‰å‘ä¼ å¯¼ï¼Œæ¨¡å‹å­ç±»åœ¨è´ªå©ªæ‰§è¡Œå¯ç”¨æ—¶å°¤å…¶æœ‰ç”¨ã€‚
- è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚



æœ¬æ•™ç¨‹éµå¾ªçš„åŸºæœ¬å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š



- å»ºç«‹ä¸»è¦çš„æ™ºèƒ½ä½“ç›‘ç®¡
- å»ºç«‹å·¥ä½œæ™ºèƒ½ä½“
- å®ç° A3C ç®—æ³•
- è®­ç»ƒæ™ºèƒ½ä½“
- å°†æ¨¡å‹è¡¨ç°å¯è§†åŒ–



æœ¬æ•™ç¨‹é¢å‘æ‰€æœ‰å¯¹å¼ºåŒ–å­¦ä¹ æ„Ÿå…´è¶£çš„äººï¼Œä¸ä¼šæ¶‰åŠå¤ªæ·±çš„æœºå™¨å­¦ä¹ åŸºç¡€ï¼Œä½†ä¸»é¢˜ä¸­æ¶µç›–äº†é«˜çº§ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œçš„ç›¸å…³çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘å»ºè®®é˜…è¯» Voldymyr Mnih çš„ã€ŠAsynchronous Methods for Deep Reinforcement Learningã€‹ï¼ˆhttps://arxiv.org/abs/1602.01783ï¼‰ï¼Œè¿™ç¯‡æ–‡ç« å¾ˆå€¼å¾—ä¸€è¯»ï¼Œè€Œä¸”æ–‡ä¸­æ¶‰åŠåˆ°æœ¬æ•™ç¨‹é‡‡ç”¨çš„ç®—æ³•çš„å¾ˆå¤šç»†èŠ‚ã€‚



**ä»€ä¹ˆæ˜¯ Cartpoleï¼Ÿ**



Cartpole æ˜¯ä¸€ä¸ªæ¸¸æˆã€‚åœ¨è¯¥æ¸¸æˆä¸­ï¼Œä¸€æ ¹æ†é€šè¿‡éé©±åŠ¨å…³èŠ‚è¿æ¥åˆ°å°è½¦ä¸Šï¼Œå°è½¦æ²¿æ— æ‘©æ“¦çš„è½¨é“æ»‘åŠ¨ã€‚åˆå§‹çŠ¶æ€ï¼ˆæ¨è½¦ä½ç½®ã€æ¨è½¦é€Ÿåº¦ã€æ†çš„è§’åº¦å’Œæ†å­é¡¶ç«¯çš„é€Ÿåº¦ï¼‰éšæœºåˆå§‹åŒ–ä¸º +/-0.05ã€‚é€šè¿‡å¯¹è½¦æ–½åŠ  +1 æˆ– -1ï¼ˆè½¦å‘å·¦æˆ–å‘å³ç§»åŠ¨ï¼‰çš„åŠ›å¯¹è¯¥ç³»ç»Ÿè¿›è¡Œæ§åˆ¶ã€‚æ†å¼€å§‹çš„æ—¶å€™æ˜¯ç›´ç«‹çš„ï¼Œæ¸¸æˆç›®æ ‡æ˜¯é˜²æ­¢æ†å€’ä¸‹ã€‚æ†ä¿æŒç›´ç«‹è¿‡ç¨‹ä¸­çš„æ¯ä¸ªæ—¶é—´æ­¥éƒ½ä¼šå¾—åˆ° +1 çš„å¥–åŠ±ã€‚å½“æ†å€¾æ–œ 15 åº¦ä»¥ä¸Šæˆ–å°è½¦ä¸ä¸­é—´ä½ç½®ç›¸éš” 2.4 ä¸ªå•ä½æ—¶æ¸¸æˆç»“æŸã€‚

![img](https://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW8z8b8pQrp0s9dStZDzKIyLo5KBbL3t5wJeU17Q7UdcpFzG6BibcbskT94db4ZgcTd8LibIHlKAGWQw/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)



**ä»£ç **



- å®Œæ•´ä»£ç ï¼šhttps://github.com/tensorflow/models/blob/master/research/a3c_blogpost/a3c_cartpole.py
- å®‰è£…æŒ‡å—ï¼šhttps://github.com/tensorflow/models/tree/master/research/a3c_blogpost



**å»ºç«‹åŸºçº¿**



ä¸ºäº†æ­£ç¡®åˆ¤æ–­æ¨¡å‹çš„å®é™…æ€§èƒ½ä»¥åŠè¯„ä¼°æ¨¡å‹çš„åº¦é‡æ ‡å‡†ï¼Œå»ºç«‹ä¸€ä¸ªåŸºçº¿é€šå¸¸éå¸¸æœ‰ç”¨ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æœè¿”å›çš„åˆ†æ•°å¾ˆé«˜ï¼Œä½ å°±ä¼šè§‰å¾—æ¨¡å‹è¡¨ç°ä¸é”™ï¼Œä½†äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¾ˆéš¾ç¡®å®šé«˜åˆ†æ˜¯ç”±å¥½çš„ç®—æ³•è¿˜æ˜¯éšæœºè¡Œä¸ºå¸¦æ¥çš„ã€‚åœ¨åˆ†ç±»é—®é¢˜çš„æ ·ä¾‹ä¸­ï¼Œå¯ä»¥é€šè¿‡ç®€å•åˆ†æç±»åˆ«åˆ†å¸ƒä»¥åŠé¢„æµ‹æœ€å¸¸è§çš„ç±»åˆ«æ¥å»ºç«‹åŸºçº¿ã€‚ä½†æˆ‘ä»¬è¯¥å¦‚ä½•é’ˆå¯¹å¼ºåŒ–å­¦ä¹ å»ºç«‹åŸºçº¿å‘¢ï¼Ÿå¯ä»¥åˆ›å»ºéšæœºçš„æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“å¯ä»¥åœ¨æˆ‘ä»¬çš„ç¯å¢ƒä¸­åšå‡ºä¸€äº›éšæœºè¡Œä¸ºã€‚



```
class RandomAgent:
  """Random Agent that will play the specified game

    Arguments:
      env_name: Name of the environment to be played
      max_eps: Maximum number of episodes to run agent for.
  """
  def __init__(self, env_name, max_eps):
    self.env = gym.make(env_name)
    self.max_episodes = max_eps
    self.global_moving_average_reward = 0
    self.res_queue = Queue()

  def run(self):
    reward_avg = 0
    for episode in range(self.max_episodes):
      done = False
      self.env.reset()
      reward_sum = 0.0
      steps = 0
      while not done:
        # Sample randomly from the action space and step
        _, reward, done, _ = self.env.step(self.env.action_space.sample())
        steps += 1
        reward_sum += reward
      # Record statistics
      self.global_moving_average_reward = record(episode,
                                                 reward_sum,
                                                 0,
                                                 self.global_moving_average_reward,
                                                 self.res_queue, 0, steps)

      reward_avg += reward_sum
    final_avg = reward_avg / float(self.max_episodes)
    print("Average score across {} episodes: {}".format(self.max_episodes, final_avg))
    return final_avg
```



å°± CartPole è¿™ä¸ªæ¸¸æˆè€Œè¨€ï¼Œæˆ‘ä»¬åœ¨ 4000 ä¸ªå¾ªç¯ä¸­å¾—åˆ°äº† ï½20 çš„å¹³å‡å€¼ã€‚ä¸ºäº†è¿è¡Œéšæœºçš„æ™ºèƒ½ä½“ï¼Œè¦å…ˆè¿è¡Œ python æ–‡ä»¶ï¼š python a3c_cartpole.pyâ€”algorithm=randomâ€”max-eps=4000ã€‚



**ä»€ä¹ˆæ˜¯å¼‚æ­¥ä¼˜åŠ¿åŠ¨ä½œè¯„ä»·ç®—æ³•**



å¼‚æ­¥ä¼˜åŠ¿åŠ¨ä½œè¯„ä»·ç®—æ³•æ˜¯ä¸€ä¸ªéå¸¸æ‹—å£çš„åå­—ã€‚æˆ‘ä»¬å°†è¿™ä¸ªåå­—æ‹†å¼€ï¼Œç®—æ³•çš„æœºåˆ¶å°±è‡ªç„¶è€Œç„¶åœ°æ˜¾éœ²å‡ºæ¥äº†ï¼š



- å¼‚æ­¥ï¼šè¯¥ç®—æ³•æ˜¯ä¸€ç§å¼‚æ­¥ç®—æ³•ï¼Œå…¶ä¸­å¹¶è¡Œè®­ç»ƒå¤šä¸ªå·¥ä½œæ™ºèƒ½ä½“ï¼Œæ¯ä¸€ä¸ªæ™ºèƒ½ä½“éƒ½æœ‰è‡ªå·±çš„æ¨¡å‹å’Œç¯å¢ƒå‰¯æœ¬ã€‚ç”±äºæœ‰æ›´å¤šçš„å·¥ä½œæ™ºèƒ½ä½“å¹¶è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„ç®—æ³•ä¸ä»…è®­ç»ƒå¾—æ›´å¿«ï¼Œè€Œä¸”å¯ä»¥è·å¾—æ›´å¤šæ ·çš„è®­ç»ƒç»éªŒï¼Œå› ä¸ºæ¯ä¸€ä¸ªå·¥ä½œä½“çš„ç»éªŒéƒ½æ˜¯ç‹¬ç«‹çš„ã€‚
- ä¼˜åŠ¿ï¼šä¼˜åŠ¿æ˜¯ä¸€ä¸ªè¯„ä»·è¡Œä¸ºå¥½åå’Œè¡Œä¸ºè¾“å‡ºç»“æœå¦‚ä½•çš„æŒ‡æ ‡ï¼Œå…è®¸ç®—æ³•å…³æ³¨ç½‘ç»œé¢„æµ‹å€¼ç¼ºä¹ä»€ä¹ˆã€‚ç›´è§‚åœ°è®²ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥è¡¡é‡åœ¨ç»™å®šæ—¶é—´æ­¥æ—¶éµå¾ªç­–ç•¥ Ï€ é‡‡å–è¡Œä¸º a çš„ä¼˜åŠ¿ã€‚
- åŠ¨ä½œ-è¯„ä»·ï¼šç®—æ³•çš„åŠ¨ä½œ-è¯„ä»·ç”¨äº†åœ¨ç­–ç•¥å‡½æ•°å’Œä»·å€¼å‡½æ•°é—´å…±äº«å±‚çš„æ¶æ„ã€‚



**å®ƒæ˜¯å¦‚ä½•èµ·ä½œç”¨çš„ï¼Ÿ**



åœ¨æ›´é«˜çº§åˆ«ä¸Šï¼ŒA3C ç®—æ³•å¯ä»¥é‡‡ç”¨å¼‚æ­¥æ›´æ–°ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥åœ¨å›ºå®šçš„ç»éªŒæ—¶é—´æ­¥ä¸Šè¿›è¡Œæ“ä½œã€‚å®ƒå°†ä½¿ç”¨è¿™äº›ç‰‡æ®µè®¡ç®—å¥–åŠ±å’Œä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡å€¼ã€‚æ¯ä¸€ä¸ªå·¥ä½œæ™ºèƒ½ä½“éƒ½ä¼šéµå¾ªä¸‹è¿°å·¥ä½œæµç¨‹ï¼š



1. è·å–å…¨å±€ç½‘ç»œå‚æ•°
2. é€šè¿‡éµå¾ªæœ€å°åŒ–ï¼ˆt_maxï¼šåˆ°ç»ˆæçŠ¶æ€çš„æ­¥é•¿ï¼‰æ­¥é•¿æ•°çš„å±€éƒ¨ç­–ç•¥ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’
3. è®¡ç®—ä»·å€¼æŸå¤±å’Œç­–ç•¥æŸå¤±
4. ä»æŸå¤±ä¸­å¾—åˆ°æ¢¯åº¦
5. ç”¨æ¢¯åº¦æ›´æ–°å…¨å±€ç½‘ç»œ
6. é‡å¤



åœ¨è¿™æ ·çš„è®­ç»ƒé…ç½®ä¸‹ï¼Œæˆ‘ä»¬æœŸæœ›çœ‹åˆ°æ™ºèƒ½ä½“çš„æ•°é‡ä»¥çº¿æ€§é€Ÿåº¦å¢é•¿ã€‚ä½†ä½ çš„æœºå™¨å¯ä»¥æ”¯æŒçš„æ™ºèƒ½ä½“æ•°é‡å—å¯ç”¨ CPU æ ¸çš„é™åˆ¶ã€‚æ­¤å¤–ï¼ŒA3C å¯ä»¥æ‰©å±•åˆ°å¤šä¸ªæœºå™¨ä¸Šï¼Œæœ‰ä¸€äº›è¾ƒæ–°çš„ç ”ç©¶ï¼ˆåƒæ˜¯ IMPALAï¼ˆhttps://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/ï¼‰ï¼‰ç”šè‡³æ”¯æŒå®ƒæ›´è¿›ä¸€æ­¥æ‰©å±•ã€‚ä½†æ·»åŠ å¤ªå¤šæœºå™¨å¯èƒ½ä¼šå¯¹é€Ÿåº¦å’Œæ€§èƒ½äº§ç”Ÿä¸€äº›ä¸åˆ©å½±å“ã€‚å‚é˜…è¿™ç¯‡æ–‡ç« ï¼ˆhttps://arxiv.org/abs/1602.01783ï¼‰ä»¥è·å–æ›´æ·±å…¥çš„ä¿¡æ¯ã€‚



**é‡æ–°å®¡è§†ç­–ç•¥å‡½æ•°å’Œä»·å€¼å‡½æ•°**



å¦‚æœä½ å·²ç»å¯¹ç­–ç•¥æ¢¯åº¦æœ‰æ‰€äº†è§£ï¼Œé‚£ä¹ˆå°±å¯ä»¥è·³è¿‡è¿™ä¸€èŠ‚ã€‚å¦‚æœä½ ä¸çŸ¥é“ä»€ä¹ˆæ˜¯ç­–ç•¥æˆ–ä»·å€¼ï¼Œæˆ–æ˜¯æƒ³è¦å¿«é€Ÿå¤ä¹ ä¸€äº›ç­–ç•¥æˆ–ä»·å€¼ï¼Œè¯·ç»§ç»­é˜…è¯»ã€‚



ç­–ç•¥çš„æ€æƒ³æ˜¯åœ¨ç»™å®šè¾“å…¥çŠ¶æ€çš„æƒ…å†µä¸‹å‚æ•°åŒ–è¡Œä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ä¸ªç½‘ç»œæ¥äº†è§£æ¸¸æˆçš„çŠ¶æ€å¹¶å†³å®šæˆ‘ä»¬åº”è¯¥åšä»€ä¹ˆï¼Œä»¥æ­¤æ¥å®ç°è¿™ä¸ªæƒ³æ³•ã€‚å› æ­¤ï¼Œå½“æ™ºèƒ½ä½“è¿›è¡Œæ¸¸æˆæ—¶ï¼Œæ¯å½“å®ƒçœ‹åˆ°æŸäº›çŠ¶æ€ï¼ˆæˆ–æ˜¯ç›¸ä¼¼çš„çŠ¶æ€ï¼‰ï¼Œå®ƒå°±å¯ä»¥åœ¨ç»™å®šè¾“å…¥çŠ¶æ€ä¸‹è®¡ç®—å‡ºæ¯ä¸€ä¸ªå¯èƒ½çš„è¡Œä¸ºçš„æ¦‚ç‡ï¼Œç„¶åå†æ ¹æ®æ¦‚ç‡åˆ†å¸ƒå¯¹è¡Œä¸ºè¿›è¡Œé‡‡æ ·ã€‚ä»æ›´æ·±å…¥çš„æ•°å­¦è§’åº¦è¿›è¡Œåˆ†æï¼Œç­–ç•¥æ¢¯åº¦æ˜¯æ›´ä¸ºé€šç”¨çš„åˆ†æ•°å‡½æ•°æ¢¯åº¦ä¼°è®¡çš„ç‰¹ä¾‹ã€‚ä¸€èˆ¬æƒ…å†µä¸‹å°†æœŸæœ›è¡¨ç¤ºä¸º p(x | ) [f(x)]ï¼›ä½†åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå¥–åŠ±ï¼ˆä¼˜åŠ¿ï¼‰å‡½æ•°çš„æœŸæœ›ï¼Œfï¼Œåœ¨æŸäº›ç­–ç•¥ç½‘ç»œä¸­ï¼Œpã€‚ç„¶åå†ç”¨å¯¹æ•°å¯¼æ•°æ–¹æ³•ï¼Œç®—å‡ºå¦‚ä½•æ›´æ–°æˆ‘ä»¬çš„ç½‘ç»œå‚æ•°ï¼Œä½¿å¾—è¡Œä¸ºæ ·æœ¬èƒ½è·å¾—æ›´é«˜çš„å¥–åŠ±å¹¶ä»¥ âˆ‡ Ex[f(x)] =Ex[f(x) âˆ‡ log p(x)] ç»“æŸã€‚è¿™ä¸ªç­‰å¼è§£é‡Šäº†å¦‚ä½•æ ¹æ®å¥–åŠ±å‡½æ•° f åœ¨æ¢¯åº¦æ–¹å‘ä¸Šè½¬æ¢ Î¸ ä½¿å¾—åˆ†æœ€å¤§åŒ–ã€‚



ä»·å€¼å‡½æ•°åŸºæœ¬ä¸Šå°±å¯ä»¥åˆ¤æ–­æŸç§çŠ¶æ€çš„å¥½åç¨‹åº¦ã€‚ä»å½¢å¼ä¸Šè®²ï¼Œä»·å€¼å‡½æ•°å®šä¹‰äº†å½“ä»¥çŠ¶æ€ s å¼€å§‹ï¼Œéµå¾ªç­–ç•¥ p æ—¶å¾—åˆ°å¥–åŠ±çš„æœŸæœ›æ€»å’Œã€‚è¿™æ˜¯æ¨¡å‹ä¸­ã€Œè¯„ä»·ã€éƒ¨åˆ†ç›¸å…³ä¹‹å¤„ã€‚æ™ºèƒ½ä½“ä½¿ç”¨ä»·å€¼ä¼°è®¡ï¼ˆè¯„ä»·ï¼‰æ¥æ›´æ–°ç­–ç•¥ï¼ˆåŠ¨ä½œï¼‰ã€‚



**å®ç°**



æˆ‘ä»¬é¦–å…ˆæ¥å®šä¹‰ä¸€ä¸‹è¦ä½¿ç”¨çš„æ¨¡å‹ã€‚ä¸»æ™ºèƒ½ä½“æœ‰å…¨å±€ç½‘ç»œï¼Œæ¯ä¸ªå±€éƒ¨çš„å·¥ä½œä½“åœ¨å®ƒä»¬è‡ªå·±çš„ç¨‹åºä¸­æ‹¥æœ‰è¯¥ç½‘ç»œçš„çš„å‰¯æœ¬ã€‚æˆ‘ä»¬ç”¨æ¨¡å‹å­ç±»å®ä¾‹åŒ–è¯¥æ¨¡å‹ã€‚æ¨¡å‹å­ç±»ä¸ºæˆ‘ä»¬æä¾›äº†æ›´é«˜çš„çµæ´»åº¦ï¼Œè€Œä»£ä»·æ˜¯å†—ä½™åº¦ä¹Ÿæ›´é«˜ã€‚



```
public class MyActivity extends AppCompatActivity {
@Override  //override the function
    protected void onCreate(@Nullable Bundle savedInstanceState) {
       super.onCreate(savedInstanceState);
       try {
            OkhttpManager.getInstance().setTrustrCertificates(getAssets().open("mycer.cer");
            OkHttpClient mOkhttpClient= OkhttpManager.getInstance().build();
        } catch (IOException e) {
            e.printStackTrace();
        }
}
```



ä»å‰å‘ä¼ é€’ä¸­å¯ä»¥çœ‹å‡ºï¼Œæ¨¡å‹å¾—åˆ°è¾“å…¥åä¼šè¿”å›ç­–ç•¥æ¦‚ç‡ logits å’Œ valuesã€‚



**ä¸»æ™ºèƒ½ä½“â€”â€”ä¸»çº¿ç¨‹**



æˆ‘ä»¬æ¥äº†è§£ä¸€ä¸‹è¯¥æ“ä½œçš„ä¸»ä½“éƒ¨åˆ†ã€‚ä¸»æ™ºèƒ½ä½“æœ‰å¯ä»¥æ›´æ–°å…¨å±€ç½‘ç»œçš„å…±äº«ä¼˜åŒ–å™¨ã€‚è¯¥æ™ºèƒ½ä½“å®ä¾‹åŒ–äº†æ¯ä¸ªå·¥ä½œæ™ºèƒ½ä½“å°†è¦æ›´æ–°çš„å…¨å±€ç½‘ç»œä»¥åŠç”¨æ¥æ›´æ–°å®ƒçš„ä¼˜åŒ–å™¨ã€‚è¿™æ ·æ¯ä¸ªå·¥ä½œæ™ºèƒ½ä½“å’Œæˆ‘ä»¬å°†ä½¿ç”¨çš„ä¼˜åŒ–å™¨å°±å¯ä»¥å¯¹å…¶è¿›è¡Œæ›´æ–°ã€‚A3C å¯¹å­¦ä¹ ç‡çš„ä¼ é€’æ˜¯å¾ˆæœ‰å¼¹æ€§çš„ï¼Œä½†é’ˆå¯¹ Cart Pole æˆ‘ä»¬è¿˜æ˜¯è¦ç”¨å­¦ä¹ ç‡ä¸º 5e-4 çš„ AdamOptimizerï¼ˆhttps://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizerï¼‰ã€‚



```
class MasterAgent():
  def __init__(self):
    self.game_name = 'CartPole-v0'
    save_dir = args.save_dir
    self.save_dir = save_dir
    if not os.path.exists(save_dir):
      os.makedirs(save_dir)

    env = gym.make(self.game_name)
    self.state_size = env.observation_space.shape[0]
    self.action_size = env.action_space.n
    self.opt = tf.train.AdamOptimizer(args.lr, use_locking=True)
    print(self.state_size, self.action_size)

    self.global_model = ActorCriticModel(self.state_size, self.action_size)  # global network
    self.global_model(tf.convert_to_tensor(np.random.random((1, self.state_size)), dtype=tf.float32))
```



ä¸»æ™ºèƒ½ä½“å°†è¿è¡Œè®­ç»ƒå‡½æ•°ä»¥å®ä¾‹åŒ–å¹¶å¯åŠ¨æ¯ä¸€ä¸ªæ™ºèƒ½ä½“ã€‚ä¸»æ™ºèƒ½ä½“è´Ÿè´£åè°ƒå’Œç›‘ç®¡æ¯ä¸€ä¸ªæ™ºèƒ½ä½“ã€‚æ¯ä¸€ä¸ªæ™ºèƒ½ä½“éƒ½å°†å¼‚æ­¥è¿è¡Œã€‚ï¼ˆå› ä¸ºè¿™æ˜¯åœ¨ Python ä¸­è¿è¡Œçš„ï¼Œä»æŠ€æœ¯ä¸Šè®²è¿™ä¸èƒ½ç§°ä¸ºçœŸæ­£çš„å¼‚æ­¥ï¼Œç”±äº GILï¼ˆå…¨å±€è§£é‡Šå™¨é”ï¼‰çš„åŸå› ï¼Œä¸€ä¸ªå•ç‹¬çš„ Python è¿‡ç¨‹ä¸èƒ½å¹¶è¡Œå¤šä¸ªçº¿ç¨‹ï¼ˆåˆ©ç”¨å¤šæ ¸ï¼‰ã€‚ä½†å¯ä»¥åŒæ—¶è¿è¡Œå®ƒä»¬ï¼ˆåœ¨ I/O å¯†é›†å‹æ“ä½œè¿‡ç¨‹ä¸­è½¬æ¢ä¸Šä¸‹æ–‡ï¼‰ã€‚æˆ‘ä»¬ç”¨çº¿ç¨‹ç®€å•è€Œæ¸…æ™°åœ°å®ç°äº†æ ·ä¾‹ã€‚



```
def train(self):
    if args.algorithm == 'random':
      random_agent = RandomAgent(self.game_name, args.max_eps)
      random_agent.run()
      return

    res_queue = Queue()

    workers = [Worker(self.state_size,
                      self.action_size,
                      self.global_model,
                      self.opt, res_queue,
                      i, game_name=self.game_name,
                      save_dir=self.save_dir) for i in range(multiprocessing.cpu_count())]

    for i, worker in enumerate(workers):
      print("Starting worker {}".format(i))
      worker.start()

    moving_average_rewards = []  # record episode reward to plot
    while True:
      reward = res_queue.get()
      if reward is not None:
        moving_average_rewards.append(reward)
      else:
        break
    [w.join() for w in workers]

    plt.plot(moving_average_rewards)
    plt.ylabel('Moving average ep reward')
    plt.xlabel('Step')
    plt.savefig(os.path.join(self.save_dir,
                             '{} Moving Average.png'.format(self.game_name)))
    plt.show()
```



**Memory ç±»â€”â€”å­˜å‚¨æˆ‘ä»¬çš„ç»éªŒ**



æ­¤å¤–ï¼Œä¸ºäº†æ›´ç®€å•åœ°è¿½è¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬ç”¨äº† Memory ç±»ã€‚è¯¥ç±»çš„åŠŸèƒ½æ˜¯è¿½è¸ªæ¯ä¸€æ­¥çš„è¡Œä¸ºã€å¥–åŠ±å’ŒçŠ¶æ€ã€‚



```
class Memory:
  def __init__(self):
    self.states = []
    self.actions = []
    self.rewards = []

  def store(self, state, action, reward):
    self.states.append(state)
    self.actions.append(action)
    self.rewards.append(reward)

  def clear(self):
    self.states = []
    self.actions = []
    self.rewards = []
```



ç°åœ¨æˆ‘ä»¬å·²ç»çŸ¥é“äº†ç®—æ³•çš„å…³é”®ï¼šå·¥ä½œæ™ºèƒ½ä½“ã€‚å·¥ä½œæ™ºèƒ½ä½“ç»§æ‰¿è‡ª threading ç±»ï¼Œæˆ‘ä»¬é‡å†™äº†æ¥è‡ª Thread çš„ run æ–¹æ³•ã€‚è¿™ä½¿æˆ‘ä»¬å¾—ä»¥å®ç° A3C ä¸­çš„ç¬¬ä¸€ä¸ª Aâ€”â€”å¼‚æ­¥ã€‚æˆ‘ä»¬å…ˆé€šè¿‡å®ä¾‹åŒ–å±€éƒ¨æ¨¡å‹å’Œè®¾ç½®ç‰¹å®šçš„è®­ç»ƒå‚æ•°å¼€å§‹ã€‚



```
class Worker(threading.Thread):
  # Set up global variables across different threads
  global_episode = 0
  # Moving average reward
  global_moving_average_reward = 0
  best_score = 0
  save_lock = threading.Lock()

  def __init__(self,
               state_size,
               action_size,
               global_model,
               opt,
               result_queue,
               idx,
               game_name='CartPole-v0',
               save_dir='/tmp'):
    super(Worker, self).__init__()
    self.state_size = state_size
    self.action_size = action_size
    self.result_queue = result_queue
    self.global_model = global_model
    self.opt = opt
    self.local_model = ActorCriticModel(self.state_size, self.action_size)
    self.worker_idx = idx
    self.game_name = game_name
    self.env = gym.make(self.game_name).unwrapped
    self.save_dir = save_dir
    self.ep_loss = 0.0
```

**è¿è¡Œç®—æ³•**



ä¸‹ä¸€æ­¥æ˜¯è¦å®ç° run å‡½æ•°ã€‚è¿™æ˜¯è¦çœŸæ­£è¿è¡Œæˆ‘ä»¬çš„ç®—æ³•äº†ã€‚æˆ‘ä»¬å°†é’ˆå¯¹ç»™å®šçš„å…¨å±€æœ€å¤§è¿è¡Œæ¬¡æ•°è¿è¡Œæ‰€æœ‰çº¿ç¨‹ã€‚è¿™æ˜¯ A3C ä¸­çš„ã€ŒåŠ¨ä½œã€æ‰€èµ·çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„æ™ºèƒ½ä½“ä¼šåœ¨ã€Œè¯„ä»·ã€åˆ¤æ–­è¡Œä¸ºæ—¶æ ¹æ®ç­–ç•¥å‡½æ•°é‡‡å–ã€Œè¡ŒåŠ¨ã€ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„ä»·å€¼å‡½æ•°ã€‚å°½ç®¡è¿™ä¸€èŠ‚çš„ä»£ç çœ‹èµ·æ¥å¾ˆå¤šï¼Œä½†å®é™…ä¸Šæ²¡æœ‰è¿›è¡Œå¤ªå¤šæ“ä½œã€‚åœ¨æ¯ä¸€ä¸ª episode ä¸­ï¼Œä»£ç åªç®€å•åœ°åšäº†è¿™äº›ï¼š



\1. åŸºäºç°æœ‰æ¡†æ¶å¾—åˆ°ç­–ç•¥ï¼ˆè¡Œä¸ºæ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚

\2. æ ¹æ®ç­–ç•¥é€‰æ‹©è¡ŒåŠ¨ã€‚

\3. å¦‚æœæ™ºèƒ½ä½“å·²ç»åšäº†ä¸€äº›æ“ä½œï¼ˆargs.update_freqï¼‰æˆ–è€…è¯´æ™ºèƒ½ä½“å·²ç»è¾¾åˆ°äº†ç»ˆç«¯çŠ¶æ€ï¼ˆç»“æŸï¼‰ï¼Œé‚£ä¹ˆï¼š

a. ç”¨ä»å±€éƒ¨æ¨¡å‹è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ›´æ–°å…¨å±€æ¨¡å‹ã€‚

\4. é‡å¤



```
def run(self):
    total_step = 1
    mem = Memory()
    while Worker.global_episode < args.max_eps:
      current_state = self.env.reset()
      mem.clear()
      ep_reward = 0.
      ep_steps = 0
      self.ep_loss = 0

      time_count = 0
      done = False
      while not done:
        logits, _ = self.local_model(
            tf.convert_to_tensor(current_state[None, :],
                                 dtype=tf.float32))
        probs = tf.nn.softmax(logits)

        action = np.random.choice(self.action_size, p=probs.numpy()[0])
        new_state, reward, done, _ = self.env.step(action)
        if done:
          reward = -1
        ep_reward += reward
        mem.store(current_state, action, reward)

        if time_count == args.update_freq or done:
          # Calculate gradient wrt to local model. We do so by tracking the
          # variables involved in computing the loss by using tf.GradientTape
          with tf.GradientTape() as tape:
            total_loss = self.compute_loss(done,
                                           new_state,
                                           mem,
                                           args.gamma)
          self.ep_loss += total_loss
          # Calculate local gradients
          grads = tape.gradient(total_loss, self.local_model.trainable_weights)
          # Push local gradients to global model
          self.opt.apply_gradients(zip(grads,
                                       self.global_model.trainable_weights))
          # Update local model with new weights
          self.local_model.set_weights(self.global_model.get_weights())

          mem.clear()
          time_count = 0

          if done:  # done and print information
            Worker.global_moving_average_reward = \
              record(Worker.global_episode, ep_reward, self.worker_idx,
                     Worker.global_moving_average_reward, self.result_queue,
                     self.ep_loss, ep_steps)
            # We must use a lock to save our model and to print to prevent data races.
            if ep_reward > Worker.best_score:
              with Worker.save_lock:
                print("Saving best model to {}, "
                      "episode score: {}".format(self.save_dir, ep_reward))
                self.global_model.save_weights(
                    os.path.join(self.save_dir,
                                 'model_{}.h5'.format(self.game_name))
                )
                Worker.best_score = ep_reward
            Worker.global_episode += 1
        ep_steps += 1

        time_count += 1
        current_state = new_state
        total_step += 1
    self.result_queue.put(None)
```



**å¦‚ä½•è®¡ç®—æŸå¤±ï¼Ÿ**



å·¥ä½œæ™ºèƒ½ä½“é€šè¿‡è®¡ç®—æŸå¤±å¾—åˆ°æ‰€æœ‰ç›¸å…³ç½‘ç»œå‚æ•°çš„æ¢¯åº¦ã€‚è¿™æ˜¯ A3C ä¸­æœ€åä¸€ä¸ª Aâ€”â€”advantageï¼ˆä¼˜åŠ¿ï¼‰æ‰€èµ·çš„ä½œç”¨ã€‚å°†è¿™äº›åº”ç”¨äºå…¨å±€ç½‘ç»œã€‚æŸå¤±è®¡ç®—å¦‚ä¸‹ï¼š



- ä»·å€¼æŸå¤±ï¼šL=âˆ‘(Râ€”V(s))Â²
- ç­–ç•¥æŸå¤±ï¼šL=-log(ğ…(s)) * A(s)



å¼ä¸­ R æ˜¯æŠ˜æ‰£å¥–åŠ±ï¼ŒV æ˜¯ä»·å€¼å‡½æ•°ï¼ˆè¾“å…¥çŠ¶æ€ï¼‰ï¼Œğ›‘ æ˜¯ç­–ç•¥å‡½æ•°ï¼ˆè¾“å…¥çŠ¶æ€ï¼‰ï¼ŒA æ˜¯ä¼˜åŠ¿å‡½æ•°ã€‚æˆ‘ä»¬ç”¨æŠ˜æ‰£å¥–åŠ±ä¼°è®¡ Q å€¼ï¼Œå› ä¸ºæˆ‘ä»¬ä¸èƒ½ç›´æ¥ç”¨ A3C å†³å®š Q å€¼ã€‚



```
def compute_loss(self,
                   done,
                   new_state,
                   memory,
                   gamma=0.99):
    if done:
      reward_sum = 0.  # terminal
    else:
      reward_sum = self.local_model(
          tf.convert_to_tensor(new_state[None, :],
                               dtype=tf.float32))[-1].numpy()[0]

    # Get discounted rewards
    discounted_rewards = []
    for reward in memory.rewards[::-1]:  # reverse buffer r
      reward_sum = reward + gamma * reward_sum
      discounted_rewards.append(reward_sum)
    discounted_rewards.reverse()

    logits, values = self.local_model(
        tf.convert_to_tensor(np.vstack(memory.states),
                             dtype=tf.float32))
    # Get our advantages
    advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],
                            dtype=tf.float32) - values
    # Value loss
    value_loss = advantage ** 2

    # Calculate our policy loss
    actions_one_hot = tf.one_hot(memory.actions, self.action_size, dtype=tf.float32)

    policy = tf.nn.softmax(logits)
    entropy = tf.reduce_sum(policy * tf.log(policy + 1e-20), axis=1)

    policy_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=actions_one_hot,
                                                             logits=logits)
    policy_loss *= tf.stop_gradient(advantage)
    policy_loss -= 0.01 * entropy
    total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))
    return total_loss
```



å·¥ä½œæ™ºèƒ½ä½“å°†é‡å¤åœ¨å…¨å±€ç½‘ç»œä¸­é‡ç½®ç½‘ç»œå‚æ•°å’Œä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€è®¡ç®—æŸå¤±å†å°†æ¢¯åº¦åº”ç”¨äºå…¨å±€ç½‘ç»œçš„è¿‡ç¨‹ã€‚é€šè¿‡è¿è¡Œä¸‹åˆ—å‘½ä»¤è®­ç»ƒç®—æ³•ï¼špython a3c_cartpole.pyâ€”trainã€‚



**æµ‹è¯•ç®—æ³•**



é€šè¿‡å¯ç”¨æ–°ç¯å¢ƒå’Œç®€å•éµå¾ªè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹å¾—åˆ°çš„ç­–ç•¥è¾“å‡ºæµ‹è¯•ç®—æ³•ã€‚è¿™å°†å‘ˆç°å‡ºæˆ‘ä»¬çš„ç¯å¢ƒå’Œæ¨¡å‹äº§ç”Ÿçš„ç­–ç•¥åˆ†å¸ƒä¸­çš„æ ·æœ¬ã€‚



```
 def play(self):
    env = gym.make(self.game_name).unwrapped
    state = env.reset()
    model = self.global_model
    model_path = os.path.join(self.save_dir, 'model_{}.h5'.format(self.game_name))
    print('Loading model from: {}'.format(model_path))
    model.load_weights(model_path)
    done = False
    step_counter = 0
    reward_sum = 0

    try:
      while not done:
        env.render(mode='rgb_array')
        policy, value = model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))
        policy = tf.nn.softmax(policy)
        action = np.argmax(policy)
        state, reward, done, _ = env.step(action)
        reward_sum += reward
        print("{}. Reward: {}, action: {}".format(step_counter, reward_sum, action))
        step_counter += 1
    except KeyboardInterrupt:
      print("Received Keyboard Interrupt. Shutting down.")
    finally:
      env.close()
```



ä½ å¯ä»¥åœ¨æ¨¡å‹è®­ç»ƒå¥½åè¿è¡Œä¸‹åˆ—å‘½ä»¤ï¼špython a3c_cartpole.pyã€‚



æ£€æŸ¥æ¨¡å‹æ‰€å¾—åˆ†æ•°çš„æ»‘åŠ¨å¹³å‡ï¼š



![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8z8b8pQrp0s9dStZDzKIyLGyqSejG8p0hed8JrUEibj1FVc2VP11bJ6icy5dfB4uiaToIzopV3uvT4g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



æˆ‘ä»¬åº”è¯¥çœ‹åˆ°å¾—åˆ† >200 åæ”¶æ•›äº†ã€‚è¯¥æ¸¸æˆè¿ç»­è¯•éªŒ 100 æ¬¡å¹³å‡è·å¾—äº† 195.0 çš„å¥–åŠ±ï¼Œè‡³æ­¤ç§°å¾—ä¸Šã€Œè§£å†³ã€äº†è¯¥æ¸¸æˆã€‚



åœ¨æ–°ç¯å¢ƒä¸­çš„è¡¨ç°ï¼š

![img](https://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW8z8b8pQrp0s9dStZDzKIyLo5KBbL3t5wJeU17Q7UdcpFzG6BibcbskT94db4ZgcTd8LibIHlKAGWQw/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)



**å…³é”®ç‚¹**



è¯¥æ•™ç¨‹æ¶µç›–çš„å†…å®¹ï¼š



- é€šè¿‡ A3C çš„å®ç°è§£å†³äº† CartPoleã€‚
- ä½¿ç”¨äº†è´ªå©ªæ‰§è¡Œã€æ¨¡å‹å­ç±»å’Œè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚
- Eager ä½¿å¼€å‘è®­ç»ƒå¾ªç¯å˜å¾—ç®€å•ï¼Œå› ä¸ºå¯ä»¥ç›´æ¥æ‰“å°å’Œè°ƒè¯•å¼ é‡ï¼Œè¿™ä½¿ç¼–ç å˜å¾—æ›´å®¹æ˜“ä¹Ÿæ›´æ¸…æ™°ã€‚
- é€šè¿‡ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œå¯¹å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€è¿›è¡Œäº†å­¦ä¹ ï¼Œå¹¶å°†å…¶ç»“åˆåœ¨ä¸€èµ·ä»¥å®ç° A3C
- é€šè¿‡åº”ç”¨ tf.gradient å¾—åˆ°çš„ä¼˜åŒ–å™¨æ›´æ–°è§„åˆ™è¿­ä»£æ›´æ–°äº†å…¨å±€ç½‘ç»œã€‚![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



*åŸæ–‡é“¾æ¥ï¼š**https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296*


# ç›¸å…³

- [æ•™ç¨‹ | å¦‚ä½•ä¿æŒè¿åŠ¨å°è½¦ä¸Šçš„æ——æ†å±¹ç«‹ä¸å€’ï¼ŸTensorFlowåˆ©ç”¨A3Cç®—æ³•è®­ç»ƒæ™ºèƒ½ä½“ç©CartPoleæ¸¸æˆ](https://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247489235&idx=1&sn=f44db346075c8912e51924037e250508&chksm=fbd27a72cca5f364a1f0e7430e2538a5cc80b9b0b28e0bf0e9ed7532ca43dc5b57e6723fbfee&mpshare=1&scene=1&srcid=0821Y3F0xc30KTMiodw6rvw6#rd)
