---
title: 1.02 有哪些深度学习模型压缩方法
toc: true
date: 2019-08-31
---


# 目前有哪些深度学习模型压缩方法？

主要是剪枝与量化。


目前深度学习模型压缩方法主要分为

- 网络剪枝
- 低秩分解
- 网络量化
- 知识迁移网络
- 紧凑网络设计

或者这样分：

- 网络剪枝（network pruning）
- 低秩近似（low-rank Approximation）
- 网络量化（network quantization）
- 知识蒸馏（knowledge distillation）
- 紧凑网络设计（compact Network design）


或者这样分：


- 线性或非线性量化：1/2bits, int8 和 fp16等；
- 结构或非结构剪枝：deep compression, channel pruning 和 network slimming等；
- 权重矩阵的低秩分解
- 知识蒸馏
- 网络结构简化（squeeze-net, mobile-net, shuffle-net）



而这些方法又可分为前端压缩和后端压缩。

### 17.4.1 前端压缩和后端压缩对比

|  对比项目  |                    前端压缩                    |                   后端压缩                   |
| :--------: | :--------------------------------------------: | :------------------------------------------: |
|    含义    |         不会改变原始网络结构的压缩技术         |     会大程度上改变原始网络结构的压缩技术     |
|  主要方法  | 知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝 | 低秩近似、未加限制的剪枝、参数量化、二值网络 |
|  实现难度  |                     较简单                     |                     较难                     |
|  是否可逆  |                      可逆                      |                    不可逆                    |
|  成熟应用  |                      剪枝                      |              低秩近似、参数量化              |
| 待发展应用 |                    知识蒸馏                    |                   二值网络                   |


### 17.4.5 前端压缩

（1）知识蒸馏

一个复杂模型可由多个简单模型或者强约束条件训练得到。复杂模型特点是性能好，但其参数量大，计算效率低。小模型特点是计算效率高，但是其性能较差。知识蒸馏是让复杂模型学习到的知识迁移到小模型当中，使其保持其快速的计算速度前提下，同时拥有复杂模型的性能，达到模型压缩的目的。
（2）紧凑的模型结构设计
紧凑的模型结构设计主要是对神经网络卷积的方式进行改进，比如使用两个 3x3 的卷积替换一个 5x5 的卷积、使用深度可分离卷积等等方式降低计算参数量。  目前很多网络基于模块化设计思想，在深度和宽度两个维度上都很大，导致参数冗余。因此有很多关于模型设计的研究，如 SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。
（3）滤波器层面的剪枝
滤波器层面的剪枝属于非结构花剪枝，主要是对较小的权重矩阵整个剔除，然后对整个神经网络进行微调。此方式由于剪枝过于粗放，容易导致精度损失较大，而且部分权重矩阵中会存留一些较小的权重造成冗余，剪枝不彻底。  具体操作是在训练时使用稀疏约束（加入权重的稀疏正则项，引导模型的大部分权重趋向于 0）。完成训练后，剪去滤波器上的这些 0 。

优点是简单，缺点是剪得不干净，非结构化剪枝会增加内存访问成本。

### 17.4.6 后端压缩
（1）低秩近似
在卷积神经网络中，卷积运算都是以矩阵相乘的方式进行。对于复杂网络，权重矩阵往往非常大，非常消耗存储和计算资源。低秩近似就是用若干个低秩矩阵组合重构大的权重矩阵，以此降低存储和计算资源消耗。

| 事项 | 特点                                                         |
| :--- | :----------------------------------------------------------- |
| 优点 | 可以降低存储和计算消耗；<br />一般可以压缩 2-3倍；精度几乎没有损失； |
| 缺点 | 模型越复杂，权重矩阵越大，利用低秩近似重构参数矩阵不能保证模型的性能 ；   <br />超参数的数量随着网络层数的增加呈线性变化趋势，例如中间层的特征通道数等等。 <br />随着模型复杂度的提升，搜索空间急剧增大。 |

（2）未加限制的剪枝

完成训练后，不加限制地剪去那些冗余参数。

| 事项 | 特点                                                         |
| ---- | ------------------------------------------------------------ |
| 优点 | 保持模型性能不损失的情况下，减少参数量 9-11倍； <br />剔除不重要的权重，可以加快计算速度，同时也可以提高模型的泛化能力； |
| 缺点 | 极度依赖专门的运行库和特殊的运行平台，不具有通用性；<br /> 压缩率过大时，破坏性能； |

（3）参数量化

神经网络的参数类型一般是 32 位浮点型，使用较小的精度代替 32 位所表示的精度。或者是将多个权重映射到同一数值，权重共享。**量化其实是一种权值共享的策略**。量化后的权值张量是一个高度稀疏的有很多共享权值的矩阵，对非零参数，我们还可以进行定点压缩，以获得更高的压缩率。

| 事项 | 特点                                                         |
| ---- | ------------------------------------------------------------ |
| 优点 | 模型性能损失很小，大小减少 8-16倍；                           |
| 缺点 | 压缩率大时，性能显著下降； <br />依赖专门的运行库，通用性较差； |
| 举例 | 二值化网络：XNORnet [13], ABCnet with Multiple Binary Bases [14], <br />Bin-net with High-Order Residual Quantization [15], Bi-Real Net [16]；<br/>三值化网络：Ternary weight networks [17], Trained Ternary Quantization [18]；
W1-A8 或 W2-A8量化： Learning Symmetric Quantization [19]；
INT8量化：TensorFlow-lite [20], TensorRT [21]；
其他（非线性）：Intel INQ [22], log-net, CNNPack [23] 等；
原文：https://blog.csdn.net/baidu_31437863/article/details/84474847 |
| 总结 | 最为典型就是二值网络、XNOR网络等。其主要原理就是采用 1bit 对网络的输入、权重、响应进行编码。减少模型大小的同时，原始网络的卷积操作可以被 bit-wise运算代替，极大提升了模型的速度。但是，如果原始网络结果不够复杂（模型描述能力），由于二值网络会较大程度降低模型的表达能力。因此现阶段有相关的论文开始研究 n-bit编码方式成为 n 值网络或者多值网络或者变 bit、组合 bit 量化来克服二值网络表达能力不足的缺点。 |

（4）二值网络

相对量化更为极致，对于 32bit 浮点型数用 1bit 二进制数-1或者 1 表示，可大大减小模型尺寸。

| 事项 | 特点                                                         |
| ---- | ------------------------------------------------------------ |
| 优点 | 网络体积小，运算速度快，有时可避免部分网络的 overfitting      |
| 缺点 | 二值神经网络损失的信息相对于浮点精度是非常大；<br />粗糙的二值化近似导致训练时模型收敛速度非常慢 |

（5）三值网络

| 事项 | 特点                                                         |
| ---- | ------------------------------------------------------------ |
| 优点 | 相对于二值神经网络，三值神经网络(Ternary Weight Networks)在同样的模型结构下可以达到成百上千倍的表达能力提升；并且，在计算时间复杂度上，三元网络和二元网络的计算复杂度是一样的。<br />例如，对于 ResNet-18层网络中最常出现的卷积核(3x3大小)，二值神经网络模型最多可以表达 2 的 3x3 次方(=512)种结构，而三元神经网络则可以表达 3 的 3x3 次方(=19683)种卷积核结构。在表达能力上，三元神经网络相对要高 19683/512 = 38倍。因此，三元神经网络模型能够在保证计算复杂度很低的情况下大幅的提高网络的表达能力，进而可以在精度上相对于二值神经网络有质的飞跃。另外，由于对中间信息的保存更多，三元神经网络可以极大的加快网络训练时的收敛速度，从而更快、更稳定的达到最优的结果。 |
|      |                                                              |
