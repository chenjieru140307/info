---
title: 2.21 核函数
toc: true
date: 2018-07-31 17:45:06
---

# 可以补充进来的

# 核函数

## 也许并不存在一个正确划分的超平面

在本章前面的讨论中，我们假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类。然而在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面。<span style="color:red;">是的。</span>

例如图 6.3中的“异或”问题就不是线性可分的：

<center>

![](http://images.iterate.site/blog/image/180627/FGa4JJDlEk.png?imageslim){ width=55% }


</center>

对这样的间题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。<span style="color:red;">我们怎么能保证在高维空间上一定是线性可分的呢？而且在高维空间上的线性划分为什么能说是原始空间上的好的划分呢？</span>

例如在图 6.3中，若将原始的二维空间映射到一个合适的三维空间，就能找到一个合适的划分超平面。幸运的是，如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分.<span style="color:red;">为什么一定存在呢？而且，我们怎么知道这个一定存在的高维空间是几维呢？这种映射为什么一定</span>

令 $\phi(\boldsymbol{x})$ 表示将 $\boldsymbol{x}$ 映射后的特征向量，于是，在特征空间中划分超平面所对应的模型可表示为：<span style="color:red;">是这么回事</span>

$$
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b\tag{6.19}
$$

其中 $\boldsymbol{w}$ 和 $b$ 是模型参数。类似式(6.6)，有

$$
\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \phi\left(\boldsymbol{x}_{i}\right)+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}\tag{6.20}
$$

其对偶问题是

$$
\begin{array}{l}{\max _{\alpha} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)} \\ {\text { s.t. } \sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\qquad \alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}\tag{6.21}
$$

求解式(6.21)涉及到计算 $\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)$ ，这是样本 $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{j}$ 映射到特征空间之后的内积。由于特征空间维数可能很高，甚至可能是无穷维，因此直接计算 $\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)$ 通常是困难的。为了避开这个障碍，可以设想这样一个函数：<span style="color:red;">为什么会是无穷维？</span>

$$
\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\langle\phi\left(\boldsymbol{x}_{i}\right), \phi\left(\boldsymbol{x}_{j}\right)\right\rangle=\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)\tag{6.22}
$$


即  $\boldsymbol{x}_{i}$ 与 $\boldsymbol{x}_{j}$ 在特征空间的内积等于它们在原始样本空间中通过函数 $\kappa(\cdot, \cdot)$ 计算的结果。<span style="color:red;">这句话没明白。为什么相等？</span>

有了这样的函数，我们就不必直接去计算高维甚至无穷维特征空间中的内积，于是式(6.21)可重写为

$$
\begin{array}{ll}{\max _{\alpha}} & {\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)} \\ {\text { s.t. }} & {\sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {} & {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}\tag{6.23}
$$

求解后即可得到

$$
\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)+b \end{aligned}\tag{6.24}
$$

这里的函数  $\kappa(\cdot, \cdot)$  就是“核函数”(kernel function)。<span style="color:red;">这就是核函数？</span>

式(6.24)显示出模型最优解可通过训练样本的核函数展开，这一展式亦称 “支持向量展式” (support vector expansion)。

显然，若已知合适映射 $\phi(\cdot)$ 的具体形式，则可写出核函数 $\kappa(\cdot,\cdot)$ 。<span style="color:red;">怎么写出？</span>但在现实任务中我们通常不知道 $\phi(\cdot)$ 是什么形式，那么，合适的核函数是否一定存在呢? 什么样的函数能做核函数呢？<span style="color:red;">是呀</span>

我们有下面的定理：

定理 6.1 (核函数) 令 $\mathcal{X}$ 为输入空间， $\kappa(\cdot,\cdot)$ 是定义在 $\mathcal{X}\times \mathcal{X}$ 上的对称函数，则 $\kappa$ 是核函数当且仅当对于任意数据 $D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$ ，“核矩阵” (kernel matrix) $\mathbf{K}$ 总是半正定的：

$$
\mathbf{K}=\left[\begin{array}{ccccc}{\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{1}\right)} & {\dots} & {\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{j}\right)} & {\dots} & {\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{m}\right)} \\ {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)} & {\dots} & {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{m}\right)} \\ {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{j}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{m}\right)}\end{array}\right]
$$

<span style="color:red;">没看懂，什么是半正定？再明确下。</span>

定理 6.1 表明，只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。<span style="color:red;">什么是半正定？</span>事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射 $\phi$。换言之，任何一个核函数都隐式地定义了一个称为 “再生核希尔伯特空间” (Reproducing Kernel Hilbert Space，简称 RKHS) 的特征空间。<span style="color:red;">什么是“再生核希尔伯特空间” ？为什么说核函数隐式的定义了一个再生核希尔伯特空间？</span>

> 这方面有一些基本的经验，例如对文本数据通常使用线性核，情况不明时可尝试高斯核。

通过前面的讨论可知，我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要。需注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间。

于是，“核函数选择”成为支持向量机的最大变数。若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。<span style="color:red;">嗯，是的，要怎么选择呢？</span>

表 6.1列出了几种常用的核函数.

<center>

![](http://images.iterate.site/blog/image/180627/g30aAkaHJF.png?imageslim){ width=55% }


</center>


此外，还可通过函数组合得到，例如：

- 若 $\kappa_1$ 和 $\kappa_2$ 为核函数，则对于任意正数 $\gamma_1$ 、 $\gamma_2$，其线性组合 $$
\gamma_{1} \kappa_{1}+\gamma_{2} \kappa_{2}\tag{6.25}
$$
也是核函数。
- 若 $\kappa_1$ 和 $\kappa_2$ 为核函数，则核函数的直积 $$
\kappa_{1} \otimes \kappa_{2}(\boldsymbol{x}, \boldsymbol{z})=\kappa_{1}(\boldsymbol{x}, \boldsymbol{z}) \kappa_{2}(\boldsymbol{x}, \boldsymbol{z})\tag{6.26}
$$
也是核函数；
- 若 $\kappa_1$ 为核函数，则对于任意函数 $g(x)$ ，$$
\kappa(\boldsymbol{x}, \boldsymbol{z})=g(\boldsymbol{x}) \kappa_{1}(\boldsymbol{x}, \boldsymbol{z}) g(\boldsymbol{z})\tag{6.27}
$$
也是核函数。


<span style="color:red;">这么多种核函数，要怎么选择呢？grid_search 也搜索不过来把？最常用的核函数是什么？核函数一般的选择取决于哪些条件？</span>






### 2.18.3 核函数特点及其作用

​
引入核函数目的：把原坐标系里线性不可分的数据用核函数 Kernel 投影到另一个空间，尽量使得数据在新的空间里线性可分。

核函数方法的广泛应用，与其特点是分不开的：

- 核函数的引入避免了“维数灾难”，大大减小了计算量。而输入空间的维数 $n$ 对核函数矩阵无影响。因此，核函数方法可以有效处理高维输入。<span style="color:red;">这么厉害吗？</span>
- 无需知道非线性变换函数 $\phi$ 的形式和参数。<span style="color:red;">为什么不需要知道？是自动定的吗？</span>
- 核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。
- 核函数方法可以和不同的算法相结合，形成多种不同的基于核函数技术的方法，且这两部分的设计可以单独进行，并可以为不同的应用选择不同的核函数和算法。<span style="color:red;">怎么和不同的算法结合的？</span>




# 相关

- 《机器学习》周志华
