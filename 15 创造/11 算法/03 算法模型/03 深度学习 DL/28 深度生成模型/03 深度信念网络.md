---
title: 03 深度信念网络
toc: true
date: 2019-06-05
---
# 可以补充进来的

- 这个是否要拆分出去？

# 深度信念网络

Deep belief networks

深度信念网络是第一批成功应用深度架构训练的非卷积模型之一。2006年深度信念网络的引入开始了当前深度学习的复兴。在引入深度信念网络之前，深度模型被认为太难以优化。具有凸目标函数的核机器引领了研究前沿。深度信念网络在 MNIST 数据集上表现超过内核化支持向量机，以此证明深度架构是能够成功的。尽管现在与其他无监督或生成学习算法相比，深度信念网络大多已经失去了青睐并很少使用，但它们在深度学习历史中的重要作用仍应该得到承认。

深度信念网络是具有若干潜变量层的生成模型。潜变量通常是二值的，而可见单元可以是二值或实数。尽管构造连接比较稀疏的 DBN~是可能的，但在一般的模型中，每层的每个单元连接到每个相邻层中的每个单元（没有层内连接）。顶部两层之间的连接是无向的。而所有其他层之间的连接是有向的，箭头指向最接近数据的层。见\fig?b的例子。


具有 $l$ 个隐藏层的 DBN~包含 $l$ 个权重矩阵：$\boldsymbol W^{(1)},\ldots, \boldsymbol W^{(l)}$。同时也包含 $l+1$ 个偏置向量：$\boldsymbol b^{(0)},\ldots,\boldsymbol b^{(l)}$，其中 $\boldsymbol b^{(0)}$ 是可见层的偏置。DBN~表示的概率分布由下式给出：



$$\begin{aligned}
 P(\boldsymbol h^{(l)}, \boldsymbol h^{(l-1)}) \propto& \exp \big( \boldsymbol b^{(l)^\top} \boldsymbol h^{(l)} +  \boldsymbol b^{(l-1)^\top} \boldsymbol h^{(l-1)}
 + \boldsymbol h^{(l-1)^\top} \boldsymbol W^{(l)} \boldsymbol h^{(l)} \big), \\
 P(h_i^{(k)} = 1  \mid  \boldsymbol h^{(k+1)}) &= \sigma \big( b_i^{(k)} + \boldsymbol W_{:,i}^{(k+1)^\top} \boldsymbol h^{(k+1)}
                                                          \big)~ \forall i,  \forall k \in 1, \ldots, l-2, \\
P(v_i = 1  \mid  \boldsymbol h^{(1)}) &=  \sigma \big( b_i^{(0)} + \boldsymbol W_{:,i}^{(1)^\top} \boldsymbol h^{(1)}
                                                          \big)~ \forall i.
\end{aligned}$$


在实值可见单元的情况下，替换


$$\begin{aligned}
 \mathbf v \sim \mathcal N \big( \boldsymbol v; \boldsymbol b^{(0)} + \boldsymbol W^{(1)^\top} \boldsymbol h^{(1)}, \boldsymbol \beta^{-1} \big)
\end{aligned}$$


为便于处理，$\boldsymbol \beta$ 为对角形式。至少在理论上，推广到其他指数族的可见单元是直观的。只有一个隐藏层的 DBN~只是一个 RBM。



为了从 DBN~中生成样本，我们先在顶部的两个隐藏层上运行几个 Gibbs采样步骤。这个阶段主要从 RBM（由顶部两个隐藏层定义）中采一个样本。然后，我们可以对模型的其余部分使用单次原始采样，以从可见单元绘制样本。

深度信念网络引发许多与有向模型和无向模型同时相关的问题。


由于每个有向层内的相消解释效应，并且由于无向连接的两个隐藏层之间的相互作用，深度信念网络中的推断是难解的。评估或最大化对数似然的标准证据下界也是难以处理的，因为证据下界基于大小等于网络宽度的团的期望。

评估或最大化对数似然，不仅需要面对边缘化潜变量时难以处理的推断问题，而且还需要处理顶部两层无向模型内难处理的配分函数问题。

为训练深度信念网络，我们可以先使用对比散度或随机最大似然方法训练 RBM 以最大化 $ \mathbb E_{\mathbf v \sim p_{\text{data}}} \log p(\boldsymbol v)$。\,RBM 的参数定义了 DBN~第一层的参数。然后，第二个 RBM 训练为近似最大化


$$\begin{aligned}
 \mathbb E_{\mathbf v \sim p_{\text{data}}}  \mathbb E_{\mathbf h^{(1)} \sim p^{(1)}(\boldsymbol h^{(1)}  \mid  \boldsymbol v)}  \log p^{(2)}(\boldsymbol h^{(1)}) ,
\end{aligned}$$


其中 $p^{(1)}$ 是第一个 RBM 表示的概率分布，$p^{(2)}$ 是第二个 RBM 表示的概率分布。换句话说，第二个 RBM 被训练为模拟由第一个 RBM 的隐藏单元采样定义的分布，而第一个 RBM 由数据驱动。这个过程能无限重复，从而向 DBN~添加任意多层，其中每个新的 RBM 对前一个 RBM 的样本建模。每个 RBM 定义 DBN~的另一层。这个过程可以被视为提高数据在 DBN~下似然概率的变分下界。


在大多数应用中，对 DBN~进行贪心逐层训练后，不需要再花功夫对其进行联合训练。然而，使用醒眠算法对其进行生成精调是可能的。



训练好的 DBN~可以直接用作生成模型，但是 DBN~的大多数兴趣来自于它们改进分类模型的能力。我们可以从 DBN~获取权重，并使用它们定义 MLP：


$$\begin{aligned}
 \boldsymbol h^{(1)} &= \sigma \big( b^{(1)} + \boldsymbol v^\top \boldsymbol W^{(1)} \big), \\
 \boldsymbol h^{(l)} &= \sigma \big( b_i^{(l)} + \boldsymbol h^{(l-1)^\top}\boldsymbol W^{(l)} \big) ~\forall l \in 2, \ldots, m.
\end{aligned}$$


利用 DBN~的生成训练后获得的权重和偏置初始化该 MLP~之后，我们可以训练该 MLP~来执行分类任务。这种 MLP~的额外训练是判别性精调的示例。


与\chap?中从基本原理导出的许多推断方程相比，这种特定选择的 MLP~有些随意。这个 MLP~是一个启发式选择，似乎在实践中效果不错，并在文献中一贯使用。许多近似推断技术是由它们在一些约束下，并在对数似然上找到最大\emph{紧}变分下界的能力所驱动的。我们可以使用 DBN~中 MLP~定义的隐藏单元的期望，构造对数似然的变分下界，但这对于隐藏单元上的\emph{任何}概率分布都是如此，并没有理由相信该 MLP~提供了一个特别的紧界。特别地，MLP~忽略了 DBN~图模型中许多重要的相互作用。MLP~将信息从可见单元向上传播到最深的隐藏单元，但不向下或侧向传播任何信息。DBN~图模型解释了同一层内所有隐藏单元之间的相互作用以及层之间的自顶向下的相互作用。


虽然 DBN~的对数似然是难处理的，但它可以使用~AIS~近似。通过近似，可以评估其作为生成模型的质量。


术语"深度信念网络"通常不正确地用于指代任意种类的深度神经网络，甚至没有潜变量意义的网络。这个术语应特指最深层中具有无向连接，而在所有其他连续层之间存在向下有向连接的模型。

这个术语也可能导致一些混乱，因为术语"信念网络"有时指纯粹的有向模型，而深度信念网络包含一个无向层。深度信念网络也与动态贝叶斯网络（dynamic Bayesian networks） 共享首字母缩写 DBN，动态贝叶斯网络表示马尔可夫链的贝叶斯网络。






# 相关

- 《深度学习》花书
