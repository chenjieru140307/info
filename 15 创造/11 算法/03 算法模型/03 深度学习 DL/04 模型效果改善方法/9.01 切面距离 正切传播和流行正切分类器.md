---
title: 9.01 切面距离 正切传播和流行正切分类器
toc: true
date: 2019-06-05
---

# 切面距离、正切传播和流形正切分类器

如流行学习所述，许多机器学习通过假设数据位于低维流形附近来克服维数灾难。

一个利用流形假设的早期尝试是切面距离算法。它是一种非参数的最近邻算法，其中使用的度量不是通用的欧几里德距离，而是根据邻近流形关于聚集概率的知识导出的。这个算法假设我们尝试分类的样本和同一流形上的样本具有相同的类别。由于分类器应该对局部因素（对应于流形上的移动）的变化保持不变，一种合理的度量是将点 $\boldsymbol x_1$ 和 $\boldsymbol x_2$ 各自所在流形 $M_1$ 和 $M_2$ 的距离作为点 $\boldsymbol x_1$ 和 $\boldsymbol x_2$ 之间的最近邻距离。然而这可能在计算上是困难的（它需要解决一个寻找 $M_1$ 和 $M_2$ 最近点对的优化问题），一种局部合理的廉价替代是使用 $\boldsymbol x_i$ 点处切平面近似 $M_i$，并测量两条切平面或一个切平面和点之间的距离。这可以通过求解一个低维线性系统（就流形的维数而言）来实现。当然，这种算法需要指定那些切向量。

受相关启发，正切传播算法（图 7.9 ）训练带有额外惩罚的神经网络分类器，使神经网络的每个输出 $f(\boldsymbol x)$ 对已知的变化因素是局部不变的。这些变化因素对应于沿着的相同样本聚集的流形的移动。这里实现局部不变性的方法是要求 $\nabla_{\boldsymbol x} f(\boldsymbol x)$ 与已知流形的切向 $\boldsymbol v^{(i)}$ 正交，或者等价地通过正则化惩罚 $\Omega$ 使 $f$ 在 $\boldsymbol x$ 的 $\boldsymbol v^{(i)}$ 方向的导数较小：

$$
\begin{aligned}
\Omega(f) = \sum_i \Big((\nabla_{\boldsymbol x} f(\boldsymbol x)^\top \boldsymbol v^{(i)}) \Big)^2 .
\end{aligned}\tag{7.67}
$$

这个正则化项当然可以通过适当的超参数缩放，并且对于大多数神经网络，我们需要对许多输出求和(此处为描述简单，$f(\boldsymbol x)$ 为唯一输出)。与切面距离算法一样，我们根据切向量推导先验，通常从变换（如平移、旋转和缩放图像）的效果获得形式知识。正切传播不仅用于监督学习，还在强化学习中有所应用。


<center>

![](http://images.iterate.site/blog/image/20190718/5LKRJrwAhbTG.png?imageslim){ width=55% }

</center>

> 7.9 正切传播算法和流形正切分类器主要思想的示意图，它们都正则化分类器的输出函数 $f(\boldsymbol x)$。每条曲线表示不同类别的流形，这里表示嵌入二维空间中的一维流形。在一条曲线上，我们选择单个点并绘制一个与类别流形（平行并接触流形）相切的向量以及与类别流形（与流形正交）垂直的向量。在多维情况下，可以存在许多切线方向和法线方向。我们希望分类函数在垂直于流形方向上快速改变，并且在类别流形的方向上保持不变。正切传播和流形正切分类器都会正则化 $f(\boldsymbol x)$，使其不随 $\boldsymbol x$ 沿流形的移动而剧烈变化。正切传播需要用户手动指定正切方向的计算函数（例如指定小平移后的图像保留在相同类别的流形中），而流形正切分类器通过训练自编码器拟合训练数据来估计流形的正切方向 。我们将在\chap?中讨论使用自编码器来估计流形。



正切传播与数据集增强密切相关。在这两种情况下，该算法的用户通过指定一组应当不会改变网络输出的转换，将其先验知识编码至算法中。不同的是在数据集增强的情况下，网络显式地训练正确分类这些施加大量变换后产生的不同输入。正切传播不需要显式访问一个新的输入点。取而代之，它解析地对模型正则化从而在指定转换的方向抵抗扰动。虽然这种解析方法是聪明优雅的，但是它有两个主要的缺点。首先，模型的正则化只能抵抗无穷小的扰动。显式的数据集增强能抵抗较大的扰动。其次，我们很难在基于整流线性单元的模型上使用无限小的方法。这些模型只能通过关闭单元或缩小它们的权重才能缩小它们的导数。它们不能像 sigmoid 或 tanh 单元一样通过较大权重在高值处饱和以收缩导数。数据集增强在整流线性单元上工作得很好，因为不同的整流单元会在每一个原始输入的不同转换版本上被激活。



正切传播也和双反向传播以及对抗训练有关联。双反向传播正则化使 Jacobian 矩阵偏小，而对抗训练找到原输入附近的点，训练模型在这些点上产生与原来输入相同的输出。正切传播和手动指定转换的数据集增强都要求模型在输入变化的某些特定的方向上保持不变。双反向传播和对抗训练都要求模型对输入所有方向中的变化（只要该变化较小）都应当保持不变。正如数据集增强是正切传播非无限小的版本，对抗训练是双反向传播非无限小的版本。

流形正切分类器无需知道切线向量的先验。我们将在自编码器看到，自编码器可以估算流形的切向量。流形正切分类器使用这种技术来避免用户指定切向量。如图 14.10 所示，这些估计的切向量不仅对图像经典几何变换（如转化、旋转和缩放）保持不变，还必须掌握对特定对象（如正在移动的身体某些部分）保持不变的因素。因此根据流形正切分类器提出的算法相当简单：

- （1）使用自编码器通过无监督学习来学习流形的结构，以及
- （2）如正切传播（式 7.67 ）一样使用这些切面正则化神经网络分类器。

图 14.10：


<center>

![mark](http://images.iterate.site/blog/image/20190828/9Angq5J99VAG.png?imageslim)

</center>


> 图 14.10: 通过局部 PCA 和收缩自编码器估计的流形切向量的图示。流形的位置由来自 CIFAR-10 数据集中狗的输入图像定义。切向量通过输入到代码映射的 Jacobian 矩阵 $\frac{\partial h}{\partial x}$ 的前导奇异向量估计。虽然局部 PCA 和 CAE 都可以捕获局部切方向，但 CAE 能够从有限训练数据形成更准确的估计，因为它利用了不同位置的参数共享（共享激活的隐藏单元子集）。CAE切方向通常对应于物体的移动或改变部分（例如头或腿）。


在本章中，我们已经描述了大多数用于正则化神经网络的通用策略。正则化是机器学习的中心主题，因此我们将不时在其余各章中重新回顾。机器学习的另一个中心主题是优化，我们将在下一章描述。

# 相关

- 《深度学习》花书
