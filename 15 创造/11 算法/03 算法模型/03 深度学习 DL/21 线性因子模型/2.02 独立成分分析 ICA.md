---
title: 2.02 独立成分分析 ICA
toc: true
date: 2019-06-05
---


# 独立成分分析



独立成分分析是最古老的表示学习算法之一。它是一种建模线性因子的方法，旨在将观察到的信号分离成许多潜在信号，这些潜在信号通过缩放和叠加可以恢复成观察数据。

这些信号是完全独立的，而不是仅仅彼此不相关。

> \sec?讨论了不相关变量和独立变量之间的差异。


许多不同的具体方法被称为 ICA。与我们本书中描述的其他生成模型最相似的 ICA 变种~训练了完全参数化的生成模型。潜在因子 $\boldsymbol h$ 的先验 $p(\boldsymbol h)$，必须由用户提前给出并固定。接着模型确定性地生成 $\boldsymbol x = \boldsymbol W \boldsymbol h$。我们可以通过非线性变化（使用\eqn?）来确定 $p(\boldsymbol x)$。然后通过一般的方法比如最大化似然进行学习。



这种方法的动机是，通过选择一个独立的 $p(\boldsymbol h)$，我们可以尽可能恢复接近独立的潜在因子。这是一种常用的方法，它并不是用来捕捉高级别的抽象因果因子，而是恢复已经混合在一起的低级别信号。在该设置中，每个训练样本对应一个时刻，每个 $x_i$ 是一个传感器对混合信号的观察值，并且每个 $h_i$ 是单个原始信号的一个估计。例如，我们可能有 $n$ 个人同时说话。如果我们在不同位置放置 $n$ 个不同的麦克风，则 ICA 可以检测每个麦克风的音量变化，并且分离信号，使得每个 $h_i$ 仅包含一个人清楚地说话。这通常用于脑电图的神经科学，这种技术可用于记录源自大脑的电信号。放置在受试者头部上的许多电极传感器用于测量来自身体的多种电信号。实验者通常仅对来自大脑的信号感兴趣，但是来自受试者心脏和眼睛的信号强到足以混淆在受试者头皮处的测量结果。信号到达电极，并且混合在一起，因此为了分离源于心脏与源于大脑的信号，并且将不同脑区域中的信号彼此分离，ICA 是必要的。



如前所述，ICA 存在许多变种。一些版本在 $\boldsymbol x$ 的生成中添加一些噪声，而不是使用确定性的解码器。大多数方法不使用最大似然准则，而是旨在使 $\boldsymbol h = \boldsymbol W^{-1}\boldsymbol x$ 的元素彼此独立。许多准则能够达成这个目标。\eqn?需要用到 $\boldsymbol W$ 的行列式，这可能是代价很高且数值不稳定的操作。ICA 的一些变种通过将 $\boldsymbol W$ 约束为正交来避免这个有问题的操作。


ICA 的所有变种均要求 $p(\boldsymbol h)$ 是非高斯的。这是因为如果 $p(\boldsymbol h)$ 是具有高斯分量的独立先验，则 $\boldsymbol W$ 是不可识别的。对于许多 $\boldsymbol W$ 值，我们可以在 $p(\boldsymbol x)$ 上获得相同的分布。这与其他线性因子模型有很大的区别，例如概率 PCA 和因子分析通常要求 $p(\boldsymbol h)$ 是高斯的，以便使模型上的许多操作具有闭式解。在用户明确指定分布的最大似然方法中，一个典型的选择是使用 $p(h_i) = \frac{d}{dh_i}\sigma(h_i)$。这些非高斯分布的典型选择在 $0$ 附近具有比高斯分布更高的峰值，因此我们也可以看到独立成分分析经常用于学习稀疏特征。




按照我们对生成模型这个术语的定义，ICA 的许多变种不是生成模型。在本书中，生成模型可以直接表示 $p(\boldsymbol x)$，也可以认为是从 $p(\boldsymbol x)$ 中抽取样本。ICA 的许多变种仅知道如何在 $\boldsymbol x$ 和 $\boldsymbol h$ 之间变换，而没有任何表示 $p(\boldsymbol h)$ 的方式，因此也无法在 $p(\boldsymbol x)$ 上施加分布。例如，许多 ICA 变量旨在增加 $\boldsymbol h = \boldsymbol W^{-1}\boldsymbol x$ 的样本峰度，因为高峰度说明了 $p(\boldsymbol h)$ 是非高斯的，但这是在没有显式表示 $p(\boldsymbol h)$ 的情况下完成的。这就是为什么 ICA 多被用作分离信号的分析工具，而不是用于生成数据或估计其密度。


正如 PCA 可以推广到\chap?中描述的非线性自编码器，ICA 也可以推广到非线性生成模型，其中我们使用非线性函数 $f$ 来生成观测数据。关于非线性 ICA 最初的工作可以参考 {hyvarinen1999nonlinear}，它和集成学习的成功结合可以参见 {roberts2001independent,lappalainen2000nonlinear}。ICA 的另一个非线性扩展是非线性独立成分估计方法~，这个方法堆叠了一系列可逆变换（在编码器阶段），其特性是能高效地计算每个变换的~Jacobian~行列式。这使得我们能够精确地计算似然，并且像 ICA一样，NICE 尝试将数据变换到具有因子的边缘分布的空间。由于非线性编码器的使用，这种方法更可能成功。<span style="color:red;">译者注：相比于 ICA</span>因为编码器和一个能进行完美逆变换的解码器相关联，所以可以直接从模型生成样本（首先从 $p(\boldsymbol h)$ 采样，然后使用解码器）。


ICA 的另一个推广是通过鼓励组内统计依赖关系、抑制组间依赖关系来学习特征组。当相关单元的组被选为不重叠时，这被称为独立子空间分析。我们还可以向每个隐藏单元分配空间坐标，并且空间上相邻的单元组形成一定程度的重叠。这能够鼓励相邻的单元学习类似的特征。当应用于自然图像时，这种地质 ICA 方法可以学习 Gabor 滤波器，从而使得相邻特征具有相似的方向、位置或频率。在每个区域内出现类似 Gabor 函数的许多不同相位存在抵消作用，使得在小区域上的池化产生了平移不变性。



# 相关

- 《深度学习》花书
