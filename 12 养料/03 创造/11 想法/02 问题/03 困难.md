---
title: 12 机器学习方法面临的困难
toc: true
date: 2019-06-05
---
# 促使深度学习发展的挑战

简单机器学习算法在很多不同的重要问题上效果都良好。

但是它们不能成功解决人工智能中的核心问题，如语音识别或者对象识别。

深度学习发展动机的一部分原因是传统学习算法在这类人工智能问题上泛化能力不足。

本节介绍为何处理高维数据时在新样本上泛化特别困难，以及为何在传统机器学习中实现泛化的机制不适合学习高维空间中复杂的函数。这些空间经常涉及巨大的计算代价。深度学习旨在克服这些以及其他一些难题。


## 维数灾难

当数据的维数很高时，很多机器学习问题变得相当困难。这种现象被称为维数灾难。特别值得注意的是，一组变量不同的可能配置数量会随着变量数目的增加而指数级增长。

维数灾难发生在计算机科学的许多地方，在机器学习中尤其如此。

由维数灾难带来的一个挑战是统计挑战。如图 5.9 所示，统计挑战产生于 $\boldsymbol{x}$ 的可能配置数目远大于训练样本的数目。为了充分理解这个问题，我们假设输入空间如图所示被分成单元格。空间是低维时，我们可以用由大部分数据占据的少量单元格去描述这个空间。泛化到新数据点时，通过检测和新输入点在相同单元格中的训练样本，我们可以判断如何处理新数据点。例如，如果要估计某点 $\boldsymbol{x}$ 处的概率密度，我们可以返回 $\boldsymbol{x}$ 处单位体积单元格内训练样本的数目除以训练样本的总数。如果我们希望对一个样本进行分类，我们可以返回相同单元格中训练样本最多的类别。如果我们是做回归分析，我们可以平均该单元格中样本对应的目标值。但是，如果该单元格中没有样本，该怎么办呢？因为在高维空间中参数配置数目远大于样本数目，大部分单元格中没有样本。我们如何能在这些新配置中找到一些有意义的东西呢？许多传统机器学习算法只是简单地假设在一个新点的输出应大致和最接近的训练点的输出相同。<span style="color:red;">嗯。</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190719/HuK9E3Gc6x9c.png?imageslim">
</p>

> 5.9 当数据的相关维度增大时（从左向右），我们感兴趣的配置数目会随之指数级增长。
> - (左) 在这个一维的例子中，我们用一个变量来区分所感兴趣的 $10$ 个区域。当每个区域都有足够的样本数时（每个区域对应图中的一个单元格），学习算法能够轻易地泛化得很好。泛化的一个直接方法是估计目标函数在每个区域的值（可能是在相邻区域之间插值）。
> - (中) 在二维情况下，对每个变量区分 $10$ 个不同的值更加困难。我们需要追踪 $10\times10=100$ 个区域，至少需要很多样本来覆盖所有的区域。
> - (右) 三维情况下，区域数量增加到了 $10^3=1000$，至少需要那么多的样本。对于需要区分的 $d$ 维以及 $v$ 个值来说，我们需要 $O(v^d)$ 个区域和样本。这就是维数灾难的一个示例。


## 局部不变性和平滑正则化

为了更好地泛化，机器学习算法需要由先验信念引导应该学习什么类型的函数。此前，我们已经看到过由模型参数的概率分布形成的先验。通俗地讲，我们也可以说先验信念直接影响函数本身，而仅仅通过它们对函数的影响来间接改变参数。<span style="color:red;">是呀，这个的确，先验的知识不是那么好获得的。</span>此外，我们还能通俗地说，先验信念还间接地体现在选择一些偏好某类函数的算法，尽管这些偏好并没有通过我们对不同函数置信程度的概率分布表现出来（也许根本没法表现）。<span style="color:red;">怎么表现？</span>



其中最广泛使用的隐式"先验"是平滑先验，或局部不变性先验。
这个先验表明我们学习的函数不应在小区域内发生很大的变化。<span style="color:red;">嗯嗯，是呀，那如果在小区域内发生了很大变化怎么办？如果不平滑怎么办？</span>

许多简单算法完全依赖于此先验达到良好的泛化，其结果是不能推广去解决人工智能级别任务中的统计挑战。<span style="color:red;">嗯，为啥？</span>本书中，我们将介绍深度学习如何引入额外的（显式或隐式的）先验去降低复杂任务中的泛化误差。这里，我们解释为什么仅依靠平滑先验不足以应对这类任务。<span style="color:red;">对呀，非常想知道怎么在深度学习中引入额外的先验？显示的或者隐式的。</span>

有许多不同的方法来显式或隐式地表示学习函数应该具有光滑或局部不变的先验。所有这些不同的方法都旨在鼓励学习过程能够学习出函数 $f^*$，对于大多数设置 $\boldsymbol{x}$ 和小变动 $\epsilon$，都满足条件


$$
f^{*}(\boldsymbol{x}) \approx f^{*}(\boldsymbol{x}+\epsilon)
$$


换言之，如果我们知道对应输入 $\boldsymbol{x}$ 的答案（例如， $\boldsymbol{x}$ 是个有标签的训练样本），那么该答案对于 $\boldsymbol{x}$ 的邻域应该也适用。<span style="color:red;">嗯。</span>如果在有些邻域中我们有几个好答案，那么我们可以组合它们（通过某种形式的平均或插值法）以产生一个尽可能和大多数输入一致的答案。<span style="color:red;">嗯。</span>

局部不变方法的一个极端例子是 $k$-最近邻系列的学习算法。当一个区域里的所有点 $\boldsymbol{x}$ 在训练集中的 $k$ 个最近邻是一样的，那么对这些点的预测也是一样的。当 $k=1$ 时，不同区域的数目不会比训练样本还多。<span style="color:red;">是的。</span>

虽然 $k$-最近邻算法复制了附近训练样本的输出，大部分核机器也是在和附近训练样本相关的训练集输出上插值。一类重要的核函数是局部核，<span style="color:red;">局部核之前好像没有看到过。</span>其核函数 $k(\boldsymbol u,\boldsymbol v)$ 在 $\boldsymbol u=\boldsymbol v$ 时很大，当 $\boldsymbol u$ 和 $\boldsymbol v$ 距离拉大时而减小。局部核可以看作是执行模版匹配的相似函数，用于度量测试样本  $\boldsymbol{x}$ 和每个训练样本 $\boldsymbol x^{(i)}$ 有多么相似。<span style="color:red;">对于局部核再补充下。</span>近年来深度学习的很多推动力源自研究局部模版匹配的局限性，以及深度学习如何克服这些局限性。<span style="color:red;">没明白。</span>

决策树也有平滑学习的局限性，因为它将输入空间分成和叶节点一样多的区间，并在每个区间使用单独的参数（或者有些决策树的拓展有多个参数）。如果目标函数需要至少拥有 $n$ 个叶节点的树才能精确表示，那么至少需要 $n$ 个训练样本去拟合。需要几倍于 $n$ 的样本去达到预测输出上的某种统计置信度。<span style="color:red;">嗯，是的。</span>


总的来说，区分输入空间中 $O(k)$ 个区间，所有的这些方法需要 $O(k)$ 个样本。通常会有 $O(k)$ 个参数，$O(1)$ 参数对应于 $O(k)$ 区间之一。最近邻算法中，每个训练样本至多用于定义一个区间，如图 5.10 所示。<span style="color:red;"> 嗯，每个训练样本至多用于定义一个区间。</span>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190719/hETP4ppPgj4U.png?imageslim">
</p>

> 5.10 最近邻算法如何划分输入空间的示例。每个区域内的一个样本（这里用圆圈表示）定义了区域边界（这里用线表示）。每个样本相关的 $y$ 值定义了对应区域内所有数据点的输出。由最近邻定义并且匹配几何模式的区域被称为 Voronoi 图。<span style="color:red;">这个图是啥？补充下。</span>这些连续区域的数量不会比训练样本的数量增加得更快。尽管此图具体说明了最近邻算法的效果，其他的单纯依赖局部光滑先验的机器学习算法也表现出了类似的泛化能力：每个训练样本仅仅能告诉学习者如何在其周围的相邻区域泛化。<span style="color:red;">嗯，是的，每个训练样本仅仅能高速学习者如何在其周围的相邻区域泛化。</span>


有没有什么方法能表示区间数目比训练样本数目还多的复杂函数？<span style="color:red;">是呀？</span>显然，只是假设函数的平滑性不能做到这点。例如，想象目标函数作用在西洋跳棋盘上。棋盘包含许多变化，但只有一个简单的结构。想象一下，如果训练样本数目远小于棋盘上的黑白方块数目，那么会发生什么。基于局部泛化和平滑性或局部不变性先验，如果新点和某个训练样本位于相同的棋盘方块中，那么我们能够保证正确地预测新点的颜色。但如果新点所在的方块没有训练样本，学习器不一定能举一反三。如果仅依靠这个先验，一个样本只能告诉我们它所在的方块的颜色。获得整个棋盘颜色的唯一方法是其上的每个方块至少要有一个样本。<span style="color:red;">嗯。</span>


只要在要学习的真实函数的峰值和谷值处有足够多的样本，那么平滑性假设和相关的无参数学习算法的效果都非常好。当要学习的函数足够平滑，并且只在少数几维变化，这样做一般没问题。在高维空间中，即使是非常平滑的函数，也会在不同维度上有不同的变化方式。如果函数在不同的区间中表现不一样，那么就非常难用一组训练样本去刻画函数。如果函数是复杂的（我们想区分多于训练样本数目的大量区间），有希望很好地泛化么？<span style="color:red;">是呀。</span>

这些问题，即是否可以有效地表示复杂的函数以及所估计的函数是否可以很好地泛化到新的输入，答案是有。关键观点是，只要我们通过额外假设生成数据的分布来建立区域间的依赖关系，那么 $O(k)$ 个样本足以描述多如 $O(2^k)$ 的大量区间。通过这种方式，我们确实能做到非局部的泛化。为了利用这些优势，许多不同的深度学习算法都提出了一些适用于多种 AI 任务的隐式或显式的假设。<span style="color:red;">没看懂。</span>


一些其他的机器学习方法往往会提出更强的，针对特定问题的假设。例如，假设目标函数是周期性的，我们很容易解决棋盘问题。通常，神经网络不会包含这些很强的（针对特定任务的）假设，因此神经网络可以泛化到更广泛的各种结构中。人工智能任务的结构非常复杂，很难限制到简单的、人工手动指定的性质，如周期性，因此我们希望学习算法具有更通用的假设。深度学习的核心思想是假设数据由因素或特征组合产生，这些因素或特征可能来自一个层次结构的多个层级。许多其他类似的通用假设进一步提高了深度学习算法。这些很温和的假设允许了样本数目和可区分区间数目之间的指数增益。<span style="color:red;">没看懂。</span>这类指数增益将在第 6.4.1 节、第 15.4 节和第 15.5 节中更详尽地介绍。深度的分布式表示带来的指数增益有效地解决了维数灾难带来的挑战。<span style="color:red;">没明白。</span>



## 流形学习

流形是一个机器学习中很多想法内在的重要概念。

流形指连接在一起的区域。数学上，它是指一组点，且每个点都有其邻域。给定一个任意的点，其流形局部看起来像是欧几里得空间。<span style="color:red;">流行局部看起来像是欧几里得空间 是什么意思？</span>日常生活中，我们将地球视为二维平面，但实际上它是三维空间中的球状流形。

每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位置。例如在地球表面这个流形中，我们可以朝东南西北走。

尽管术语"流形"有正式的数学定义，但是机器学习倾向于更松散地定义一组点，只需要考虑少数嵌入在高维空间中的自由度或维数就能很好地近似。每一维都对应着局部的变化方向。如\fig?所示，训练数据位于二维空间中的一维流形中。在机器学习中，我们允许流形的维数从一个点到另一个点有所变化。这经常发生于流形和自身相交的情况中。例如，数字"8"形状的流形在大多数位置只有一维，但在中心的相交处有两维。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190719/1EI77Rch4wL7.png?imageslim">
</p>

> 5.11 从一个二维空间的分布中抽取的数据样本，这些样本实际上聚集在一维流形附近，像一个缠绕的带子。实线代表学习器应该推断的隐式流形。

如果我们希望机器学习算法学习整个 $\mathbb{R}^{n}$ 上有趣变化的函数，那么很多机器学习问题看上去都是无望的。流形学习算法通过一个假设来克服这个障碍，该假设认为 $\mathbb{R}^{n}$ 中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。<span style="color:red;">嗯，是的。</span>流形学习最初用于连续数值和无监督学习的环境，尽管这个概率集中的想法也能够泛化到离散数据和监督学习的设定下：关键假设仍然是概率质量高度集中。<span style="color:red;">是的，关键假设仍然是概率质量高度集中。</span>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190719/RrRf0HJAT5vR.png?imageslim">
</p>

> 5.12 随机地均匀抽取图像（根据均匀分布随机地选择每一个像素）会得到噪声图像。尽管在人工智能应用中以这种方式生成一个脸或者其他物体的图像是非零概率的，但是实际上我们从来没有观察到这种现象。这也意味着人工智能应用中遇到的图像在所有图像空间中的占比可以是忽略不计的。<span style="color:red;">是的呀！</span>


数据位于低维流形的假设并不总是对的或者有用的。我们认为在人工智能的一些场景中，如涉及到处理图像、声音或者文本时，流形假设至少是近似对的。这个假设的支持证据包含两类观察结果。<span style="color:red;">嗯嗯。</span>

- 第一个支持流形假设的观察是现实生活中的图像、文本、声音的概率分布都是高度集中的。均匀的噪声从来不会与这类领域的结构化输入类似。图 5.12 显示均匀采样的点看上去像是没有信号时模拟电视上的静态模式。同样，如果我们均匀地随机抽取字母来生成文件，能有多大的概率得到一个有意义的英语文档？几乎是零。因为大部分字母长序列不对应着自然语言序列：自然语言序列的分布只占了字母序列的总空间里非常小的一部分。<span style="color:red;">是的。</span>
- 当然，集中的概率分布不足以说明数据位于一个相当小的流形中。我们还必须确保，我们遇到的样本和其他样本相互连接，每个样本被其他高度相似的样本包围，而这些高度相似的样本可以通过变换来遍历该流形得到。支持流形假设的第二个论点是，我们至少能够非正式地想象这些邻域和变换。在图像中，我们当然会认为有很多可能的变换仍然允许我们描绘出图片空间的流形：我们可以逐渐变暗或变亮光泽、逐步移动或旋转图中对象、逐渐改变对象表面的颜色等等。在大多数应用中很有可能会涉及到多个流形。例如，人脸图像的流形不太可能连接到猫脸图像的流形。<span style="color:red;">嗯，这段话有点理解的不是很深。</span>


这些支持流形假设的思维实验传递了一些支持它的直观理由。更严格的实验在人工智能中备受关注的一大类数据集上支持了这个假设。

当数据位于低维流形中时，使用流形中的坐标而非 $\mathbb{R}^{n}$ 中的坐标表示机器学习数据更为自然。日常生活中，我们可以认为道路是嵌入在三维空间的一维流形。我们用一维道路中的地址号码确定地址，而非三维空间中的坐标。提取这些流形中的坐标是非常具有挑战性的，但是很有希望改进许多机器学习算法。这个一般性原则能够用在很多情况中。图 5.13 展示了包含人脸的数据集的流形结构。在本书的最后，我们会介绍一些学习这样的流形结构的必备方法。在图 20.6 中，我们将看到机器学习算法如何成功完成这个目标。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190720/udhbIYYfEPrp.png?imageslim">
</p>

> 5.13 QMUL Multiview Face数据集中的训练样本，其中的物体是移动的从而覆盖对应两个旋转角度的二维流形。我们希望学习算法能够发现并且理出这些流形坐标。图 2.06 提供了这样一个示例。



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190720/e1C40vzn6nLq.png?imageslim">
</p>


> 图 20.6: 由变分自编码器学习的高维流形在 2 维坐标系中的示例 (Kingma and Welling, 2014a)。我们可以在纸上直接绘制两个可视化的维度，因此可以使用 2 维潜在编码训练模型来了解模型的工作原理（即使我们认为数据流形的固有维度要高得多）。<span style="color:red;">哪来的二维潜在编码训练模型？</span>图中所示的图像不是来自训练集的样本，而是仅仅通过改变 2 维 "编码" $\boldsymbol{z}$，由模型 $p(\boldsymbol{x} | \boldsymbol{z})$ 实际生成的图像 $\boldsymbol{x}$（每个图像对应于 "编码" $\boldsymbol{z}$ 位于 2 维均匀网格的不同选择）。<span style="color:red;">怎么生成的？</span>
>
> - (左) Frey 人脸流形的 2 维映射。其中一个维度（水平）已发现大致对应于面部的旋转，而另一个（垂直）对应于情绪表达。
> - (右) MNIST 流形的 2 维映射。

<span style="color:red;">简直太酷炫了，怎么画出来的？有例子吗？要补充下。</span>
