
# 可以补充进来的

- 泛函要不要单独拿出来讲？

# 变分



# 变分 没讲


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/fEdl1AdLHJ.png?imageslim">
</p>

另一个思路


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/2fdJAhD60h.png?imageslim">
</p>

变分的提法


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/mghj5kL31k.png?imageslim">
</p>

变分目标函数分析


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/DB44ABBf5D.png?imageslim">
</p>

新目标函数的可行性


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/1j4838AC3H.png?imageslim">
</p>

变分和 EM 的联系


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/DiAK936dJA.png?imageslim">
</p>

思考：目标函数的物理含义


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/j3idhlhD4J.png?imageslim">
</p>

思考：似然函数期望与目标函数


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/9eK6b54Bik.png?imageslim">
</p>

两个 KL 散度的区别


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/gFgiHd6jbC.png?imageslim">
</p>



### 两个 KL 散度的区别


绿色曲线是真实分布 p 的等高线；红色曲线
是使用近似 \(p(z_1,z_2)=p(z_1)p(z_2))得到的等高线


- 左：KL(p||q)：zero avoiding
- 右：KL(q||p)：zero forcing


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/5231c12A37.png?imageslim">
</p>

两个 KL 散度的区别

色曲线是真实分布 p 的等高线；红色曲线是单模型近似分布 q 的等高线。


* 左：KL(p||q)：q趋向于覆盖 p
* 中、右：KL(q||p)：q能够锁定某一个峰值



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/5B9D5b4gE4.png?imageslim">
</p>




### 两个 KL 散度之间的联系




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/5a3Jl7DlEC.png?imageslim">
</p>



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/LB4m7A0eih.png?imageslim">
</p>

Hellinger distance


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/0JcbiDdLA9.png?imageslim">
</p>

该距离满足三角不等式，是对称、非负距离


### 平均场方法(Mean field method)




<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/K2d9dgCF3a.png?imageslim">
</p>



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/bHgaI2dKAe.png?imageslim">
</p>

变分推导/似然下界 L


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/78bi0I0025.png?imageslim">
</p>

变分推导最终结论


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/86JjckgCac.png?imageslim">
</p>

Ising model


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/F2Kh6L221d.png?imageslim">
</p>

势函数的系数


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/KDIm1I4m10.png?imageslim">
</p>

使用变分做图像去噪


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/bLBfD85khd.png?imageslim">
</p>

后验概率


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/3H22197bl4.png?imageslim">
</p>

近似概率


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/D1B109GkFB.png?imageslim">
</p>

根据公式：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/c43abgc09g.png?imageslim">
</p>

更新方程：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/c2d8FG9G4f.png?imageslim">
</p>

迭代方程：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/Lj5iC4f2kB.png?imageslim">
</p>

实际效果：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/4dc2DmFKfD.png?imageslim">
</p>

变分贝叶斯(Variational Bayes,VB)


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/48JHgcD5G5.png?imageslim">
</p>

高斯分布的变分贝叶斯 Variational Bayes


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/bjAgk2H18H.png?imageslim">
</p>

未正则化的对数后验


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/LDAc98Hi21.png?imageslim">
</p>



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/IC6mHe4hfj.png?imageslim">
</p>



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/DhFcagEeH6.png?imageslim">
</p>



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/1b8j432EfD.png?imageslim">
</p>



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/JlAlaLK40b.png?imageslim">
</p>

变分参数估计实例


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/61i2Aa5BLC.png?imageslim">
</p>

变分总结




- 变分既能够推断隐变量的分布，也能推断未知参数的分布，是非常有力的参数学习工具。其难点在于公式演算略显复杂，和采样相对：一个容易计算但速度慢，一个不容易计算但运行效率高
- 平均场方法的变分推导，对离散和连续的隐变量都适用。在平均场方法的框架下，变分推导一次更新一个分布，其本质为坐标上升。可以使用模式搜索(pattern search)、基于参数的扩展(parameter expansion)等方案加速。
- 有时候，假定所有变量都是独立是不符合实际的，可以使用结构化平均场(structured mean field)，将变量分成若干组，每组之间是独立的。
- 变分除了能够和贝叶斯理论相配合得到 VB，还能进一步与 EM 算法结合，得到 VBEM，用于带隐变量和未知参数的推断。
    - 如 GMM、LDA





# 相关

1. [该怎么理解泛函以及变分？](https://www.zhihu.com/question/26527625)
2. [变分法 wiki](https://zh.wikipedia.org/wiki/%E5%8F%98%E5%88%86%E6%B3%95)
3. [变分算法](https://blog.csdn.net/u012771351/article/details/53095658)
4. [变分推断(variational inference)学习笔记(1)——概念介绍](https://blog.csdn.net/AiTODD1/article/details/41088131)
