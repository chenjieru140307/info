---
title: 23 信息论
toc: true
date: 2019-05-15
---
# 可以补充进来的

- <span style="color:red;">对于信息论还是要补充下的。现在知道的太少了。</span>

# 信息论


## 想要量化信息

信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。

它最初被发明是用来研究在一个含有噪声的信道上用离散的字母表来发送消息，例如通过无线电传输来通信。在这种情况下，信息论告诉我们如何对消息设计最优编码以及计算消息的期望长度，这些消息是使用多种不同编码机制、从特定的概率分布上采样得到的。<span style="color:red;">没明白，什么是使用多种不同编码机制，从特定的概率分布上采样得到的？</span>

在机器学习中，我们也可以把信息论应用于连续型变量，此时某些消息长度的解释不再适用。<span style="color:red;">怎么应用的？</span>

信息论是电子工程和计算机科学中许多领域的基础。在本书中，我们主要使用信息论的一些关键思想来描述概率分布或者量化概率分布之间的相似性。<span style="color:red;">嗯，怎么去量化概率分布之间的相似性呢？</span>

信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。

消息说：“今天早上太阳升起”，信息量是如此之少，以至于没有必要发送；但一条消息说：“今天早上有日食”，信息量就很丰富。<span style="color:red;">嗯嗯，是的。</span>

我们想要通过这种基本想法来量化信息。特别是：

- 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。
- 较不可能发生的事件具有更高的信息量。
- 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。<span style="color:red;">嗯，两倍，emmm ，好像是吧。</span>

## 自信息

为了满足上述 3 个性质，我们定义一个事件 $\mathrm{x}=x$ 的自信息(self-information)为：

$$
I(x)=-\log P(x)\tag{3.48}
$$

<span style="color:red;">什么是自信息？这个概念的出现是为了表达什么呢？为什么是 $log$ 呢？为什么自信息是这样的表达呢？</span>

在本书中，我们总是用 $log$ 来表示自然对数，其底数为 $e$。因此我们定义的 $I(x)$ 单位是奈特(nats)。<span style="color:red;">嗯，也就是 $ln$ 是吧。</span>

一奈特是以 $\frac{1}{e}$ 的概率观测到一个事件时获得的信息量。其他的材料中使用底数为 $2$ 的对数，单位是比特 (bit) 或者香农 (shannons) ；<span style="color:red;">嗯，这个之前知道，但是现在才知道以 $2$ 为底的是比特或者香浓，以 $e$ 为底的是奈特。嗯，一般我们用那个呢？而且，一般我们用什么呢？</span> 通过比特度量的信息只是通过奈特度量信息的常数倍。<span style="color:red;">是的，常数倍。</span>

当 $\mathrm{x}$ 是连续的，我们使用类似的关于信息的定义，但有些来源于离散形式的性质就丢失了。例如，一个具有单位密度的事件信息量仍然为 $0$，但是不能保证它一定发生。

自信息只处理单个的输出。

## 香农熵

我们可以用香农熵 (Shannon entropy) 来对整个概率分布中的不确定性总量进行量化：


$$
H(\mathrm{x})=\mathbb{E}_{\mathrm{x} \sim P}[I(x)]=-\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)]\tag{3.49}
$$

也记作 $H(P)$ 。换言之，一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。<span style="color:red;">遵循这个分布的事件所产生的期望信息总量</span>

它给出了对依据概率分布 $P$ 生成的符号进行编码所需的比特数在平均意义上的下界(当对数底数不是 $2$ 时，单位将有所不同)。<span style="color:red;">没有很明白这句话。</span>

那些接近确定性的分布(输出几乎可以确定)具有较低的熵；那些接近均匀分布的概率分布具有较高的熵。<span style="color:red;">嗯。</span>

图 3.5 给出了一个说明：当 $\mathrm{x}$ 是连续的，香农熵被称为微分熵(differential entropy)。<span style="color:red;">这句话没有怎么看明白，即使结合下图也还是有点没有很清楚。</span>


<center>

![](http://images.iterate.site/blog/image/20190515/NYQrxsFBuhBH.png?imageslim){ width=55% }


</center>

图 3.5　二值随机变量的香农熵。该图说明了更接近确定性的分布是如何具有较低的香农熵，而更接近均匀分布的分布是如何具有较高的香农熵。水平轴是 $p$，表示二值随机变量等于 $1$ 的概率。熵由 $(p-1) \log (1-p)-p \log p$ 给出。<span style="color:red;">嗯。</span>

当 $p$ 接近 $0$ 时，分布几乎是确定的，因为随机变量几乎总是 $0$。当 $p$ 接近 $1$ 时，分布也几乎是确定的，因为随机变量几乎总是 $1$。当 $p=0.5$ 时，熵是最大的，因为分布在两个结果( $0$ 和 $1$ )上是均匀的。

## KL 散度

如果对于同一个随机变量 $\mathrm{x}$ 有两个单独的概率分布 $P(\mathrm{x})$ 和 $Q(\mathrm{x})$，可以使用 KL 散度 (Kullback-Leibler(KL)divergence) 来衡量这两个分布的差异：<span style="color:red;">哇塞，大名鼎鼎的 KL 散度。</span>

$$
D_{\mathrm{KL}}(P \| Q)=\mathbb{E}_{\mathrm{x} \sim P}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]\tag{3.50}
$$


在离散型变量的情况下，KL 散度衡量的是，当我们使用一种被设计成能够使得概率分布 $Q$ 产生的消息的长度最小的编码，发送包含由概率分布 $P$ 产生的符号的消息时，所需要的额外信息量(如果我们使用底数为 $2$ 的对数时，信息量用比特衡量，但在机器学习中，我们通常用奈特和自然对数。)<span style="color:red;">哈哈，这个意义真的是，有些 nice。</span>


KL 散度有很多有用的性质，最重要的是，它是非负的。KL 散度为 $0$，当且仅当 $P$ 和 $Q$ 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的。<span style="color:red;">嗯，用到了 几乎处处</span>

因为 KL 散度是非负的并且衡量的是两个分布之间的差异，它经常被用作分布之间的某种距离。

然而，它并不是真的距离，因为它不是对称的：对于某些 $P$ 和 $Q$ ，$D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)$。这种非对称性意味着选择 $D_{\mathrm{KL}}(P \| Q)$ 还是 $D_{\mathrm{KL}}(Q \| P)$ 影响很大。<span style="color:red;">嗯嗯，KL 散度可以衡量分布之间的某种距离，但是因为它是不对称的，所以不是真正的距离，嗯，这个之前的一本书上也看到过。</span>

关于这种非对称的 KL 的选择，我们可以看下下图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190515/UmnAAIblPkxu.png?imageslim">
</p>

图 3.6　KL 散度是不对称的。假设我们有一个分布 $p(x)$，并且希望用另一个分布 $q(x)$ 来近似它。我们可以选择最小化 $D_{\mathrm{KL}}(p \| q)$ 或最小化 $D_{\mathrm{KL}}(q \| p)$ 。<span style="color:red;">嗯，有什么不同吗？</span><span style="color:blue;">嗯，下面有说。</span>为了说明每种选择的效果，我们令 $p$ 是两个高斯分布的混合，令 $q$ 为单个高斯分布。选择使用 KL 散度的哪个方向是取决于问题的。

一些应用需要这个近似分布 $q$ 在真实分布 $p$ 放置高概率的所有地方都放置高概率，而其他应用需要这个近似分布 $q$ 在真实分布 $p$ 放置低概率的所有地方都很少放置高概率。<span style="color:red;">没有很深的理解。</span>

KL 散度方向的选择反映了对于每种应用，优先考虑哪一种选择。

- (左)最小化 $D_{\mathrm{KL}}(p \| q)$ 的效果。在这种情况下，我们选择一个 $q$，使得它在 $p$ 具有高概率的地方具有高概率。当 $p$ 具有多个峰时，$q$ 选择将这些峰模糊到一起，以便将高概率质量放到所有峰上。
- (右)最小化 $D_{\mathrm{KL}}(q \| p)$ 的效果。在这种情况下，我们选择一个 $q$，使得它在 $p$ 具有低概率的地方具有低概率。当 $p$ 具有多个峰并且这些峰间隔很宽时，如该图所示，最小化 KL 散度会选择单个峰，以避免将概率质量放置在 $p$ 的多个峰之间的低概率区域中。

这里，我们说明当 $q$ 被选择成强调左边峰时的结果。我们也可以通过选择右边峰来得到 KL 散度相同的值。如果这些峰没有被足够强的低概率区域分离，那么 KL 散度的这个方向仍然可能选择模糊这些峰。

<span style="color:red;">上面这部分还是没怎么明白。</span>


## 交叉熵

一个和 KL 散度密切联系的量是交叉熵 (cross-entropy) ，即 $H(P, Q)=H(P)+D_{\mathrm{KL}}(P \| Q)$，它和 KL 散度很像，但是缺少左边一项：

$$
H(P, Q)=-\mathbb{E}_{\mathrm{x} \sim P} \log Q(x)\tag{3.51}
$$

针对 $Q$ 最小化交叉熵等价于最小化 KL 散度，因为 $Q$ 并不参与被省略的那一项。<span style="color:red;">嗯，这个地方再思考下。</span>



当我们计算这些量时，经常会遇到 0 $\log 0$ 这个表达式。按照惯例，在信息论中，我们将这个表达式处理为 $\lim _{x \rightarrow 0} x \log x=0$ 。



# 相关

- 《深度学习》花书
