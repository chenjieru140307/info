---
title: 1.12 推导 从最小平方误差的角度
toc: true
date: 2019-03-31
---
# 可以补充进来的

- 自己重新推导下。公式自己重新写下。


# PCA 从最小平方误差的角度来推导



## 仍然看二维空间中的点

<span style="color:red;">是呀。</span>


我们还是考虑二维空间中的样本点，如下图所示。

<center>

![](http://images.iterate.site/blog/image/20190331/Kbtnx61X9bCf.png?imageslim){ width=55% }


</center>

从求解直线的思路出发，很容易联想到数学中的线性回归问题：其目标也是求解一个线性函数使得对应直线能够更好地拟合样本点集合。

如果我们从这个角度定义 PCA 的目标，那么问题就会转化为一个回归问题。<span style="color:red;">是的，有些不错。</span>

顺着这个思路，在高维空间中，我们实际上是要找到一个 $d$ 维超平面，使得数据点到这个超平面的距离平方和最小。

以 $d=1$ 为例，超平面退化为直线，即把样本点投影到最佳直线，最小化的就是所有点到直线的距离平方之和，如图所示：

<center>

![](http://images.iterate.site/blog/image/20190331/iXYqzDMIX0gc.png?imageslim){ width=55% }


</center>


## 使用式子进行描述和求解

数据集中每个点 $x_k$ 到 $d$ 维超平面 $D$ 的距离为：

$$
\operatorname{distance}\left(\boldsymbol{x}_{k}, \boldsymbol{D}\right)=\left\|\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right\|_{2}\tag{4.6}
$$


其中 $\widetilde{x}_k$ 表示 $x_k$ 在超平面 D 上的投影向量。如果该超平面由 d 个标准正交基 $W=\{\omega_1,\omega_2,...,\omega_d\}$ 构成，根据线性代数理论 $\widetilde{x_k}$ 可以由这组基 线性表示，

$$
\widetilde{\boldsymbol{x}}_{k}=\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\tag{4.7}
$$


其中 $\omega_i^Tx_k$ 表示 $x_k$ 在 $\omega_i$ 方向上投影的长度。因此，实际上 $\widetilde{x_k}$ 就是 $x_k$ 在 $W$ 这组标准正交基下的坐标。而 PCA 要优化的目标为：
$$
\left\{\begin{matrix}
\underset{\omega_{1}, \ldots, \omega_{d}}{\arg \min } \sum_{k=1}^{n}\left\|\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right\|_{2}^{2}\\
s.t.\quad \underset{\forall i,j}{\omega_{i}^{\mathrm{T}} \omega_{j}}=\delta_{i j}=\left\{\begin{array}{l}{1, i=j} \\ {0, i \neq j}\end{array}\right.
\end{matrix}\right. \tag{4.8}
$$



由向量内积的性质，我们知道 $x_k^T\widetilde{x}_k=\widetilde{x}_k^Tx_k$，于是将式（4.8）中的每一个距离展开：

$$
\begin{aligned}\left\|\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right\|_{2}^{2} &=\left(\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right)^{\mathrm{T}}\left(\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}_{k}}\right) \\ &=\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}-\boldsymbol{x}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}_{k}}-\widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}+\widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}}_{k} \\ &=\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}-2 \boldsymbol{x}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}_{k}}+\widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}}_{k} \end{aligned}\tag{4.9}
$$


其中第一项 $x_k^Tx_k$ 与选取的 $W$ 无关，是个常数。将式（4.7）代入式（4.9）的第二项和第三项可得到，

$$
\begin{aligned}\boldsymbol{x}_{k}^{\mathbf{T}} \widetilde{\boldsymbol { x }}_{k} &=\boldsymbol{x}_{k}^{\mathrm{T}} \sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i} \\& =\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i} \\&=\sum_{i=1}^{d} \boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i}\end{aligned}\tag{4.10}
$$

$$
\begin{aligned}\widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}_{k}}&=\left(\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\right)^{\mathrm{T}}\left(\sum_{j=1}^{d}\left(\boldsymbol{\omega}_{j}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{j}\right) \\& =\sum_{i=1}^{d} \sum_{j=1}^{d}\left(\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\right)^{\mathrm{T}}\left(\left(\boldsymbol{\omega}_{j}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{j}\right)\end{aligned}\tag{4.11}
$$



注意到，其中 $\omega_i^Tx_k$ 和 $\omega_j^Tx_k$ 表示投影长度，都是数字。且当 $i\neq j$ 时，$\omega_i^T\omega_j=0$ ，因此式（4.11）的交叉项中只剩下 d 项：

$$
\begin{aligned} \widetilde{\boldsymbol{x}}_{k}^{\mathrm{T}} \widetilde{\boldsymbol{x}}_{k} &=\sum_{i=1}^{d}\left(\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\right)^{\mathrm{T}}\left(\left(\omega_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \boldsymbol{\omega}_{i}\right)\\&=\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right)\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \\ &=\sum_{i=1}^{d}\left(\boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k}\right)\left(\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i}\right)\\&=\sum_{i=1}^{d} \boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i} \end{aligned}\tag{4.12}
$$



注意到， $\sum_{i=1}^{d} \omega_{i}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i}$ 实际上就是矩阵 $\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}$ 的迹（对角线元素之和），于是可以将式（4.9）继续化简：

$$
\begin{aligned}\left\|\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right\|_{2}^{2}&=-\sum_{i=1}^{d} \boldsymbol{\omega}_{i}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{\omega}_{i}+\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k} \\ &=-\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}\right)+\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}\end{aligned}\tag{4.13}
$$


因此式（4.8）可以写成：

$$
\begin{aligned}\arg \min _{W} \sum_{k=1}^{n}\left\|\boldsymbol{x}_{k}-\widetilde{\boldsymbol{x}}_{k}\right\|_{2}^{2}&=\sum_{k=1}^{n}\left(-\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}\right)+\boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{x}_{k}\right) \\ &=-\sum_{k=1}^{n} \operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}\right)+C\end{aligned}\tag{4.14}
$$



根据矩阵乘法的性质 $\sum_{k} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}}=\boldsymbol{X} \boldsymbol{X}^{\mathbf{T}}$，因此优化问题可以转化为 $\arg \max _{W} \sum_{k=1}^{n} \operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{k} \boldsymbol{x}_{k}^{\mathrm{T}} \boldsymbol{W}\right)$，这等价于求解带约束的优化问题：


$$
\left\{\begin{array}{c}{\arg \max _{W} \operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{X} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{W}\right)} \\ {\text { s.t. } \boldsymbol{W}^{\mathrm{T}} \boldsymbol{W}=I}\end{array}\right.\tag{4.15}
$$

如果我们对 W 中的 d 个基 $\boldsymbol{\omega}_{1}, \boldsymbol{\omega}_{2}, \ldots, \boldsymbol{\omega}_{d}$ 依次求解，就会发现和最大方差理论的方法完全等价。比如当 d=1时，我们实际求解的问题是：

$$
\left\{\begin{array}{c}{\arg \max _{\omega} \boldsymbol{\omega}^{\mathrm{T}} \boldsymbol{X} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{\omega}} \\ {\text {s.t.} \quad \boldsymbol{\omega}^{\mathrm{T}} \boldsymbol{\omega}=1}\end{array}\right.\tag{4.16}
$$

最佳直线 $\omega$ 与最大方差法求解的最佳投影方向一致，即协方差矩阵的最大特征值所对应的特征向量，差别仅是协方差矩阵 $\sum$ 的一个倍数，以及常数 $\sum_{k=1}^{n}x_k^Tx_k$ 偏差，但这并不影响我们对最大值的优化。


## 总结

至此，我们从最小平方误差的角度解释了 PCA 的原理、目标函数和求解方法。不难发现，这与最大方差角度殊途同归，从不同的目标函数出发，得到了相同的求解方法。


# 相关

- 《百面机器学习》

