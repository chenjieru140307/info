# AlexNet


应用于图像分类。

它的出现证明了深层卷积神经网络在复杂模型下的有效性，使 CNN 在计算机视觉中流行开来。

## 结构


<center>

![](http://images.iterate.site/blog/image/20190722/mx3QHPzvlcL9.png?imageslim){ width=85% }

</center>

如图 所示，除去下采样（池化层）和局部响应规范化操作（Local Responsible Normalization, LRN），<span style="color:red;">嗯，也就是 局部归一化。</span>AlexNet 一共包含 8 层，前 5 层由卷积层组成，而剩下的 3 层为全连接层。

网络结构分为上下两层，分别对应两个 GPU 的操作过程，除了中间某些层（$C_3$ 卷积层和 $F_{6-8}$ 全连接层会有 GPU 间的交互），其他层两个 GPU 分别计算结果。最后一层全连接层的输出作为 $softmax$ 的输入，得到 1000 个图像分类标签对应的概率值。除去 GPU 并行结构的设计，AlexNet 网络结构与 LeNet 十分相似，其网络的参数配置如表所示。<span style="color:red;">这个 GPU 并行结构是怎么工作的？</span>

AlexNet 网络参数配置：

|         网络层         |               输入尺寸               |                  核尺寸                  |               输出尺寸               |              可训练参数量               |
|:----------------------:|:------------------------------------:|:----------------------------------------:|:------------------------------------:|:---------------------------------------:|
|   卷积层 $C_1$ $^*$    |        $224\times224\times3$         | $11\times11\times3/4,48(\times2_{GPU})$  | $55\times55\times48(\times2_{GPU})$  | $(11\times11\times3+1)\times48\times2$  |
| 下采样层 $S_{max}$$^*$ | $55\times55\times48(\times2_{GPU})$  |       $3\times3/2(\times2_{GPU})$        | $27\times27\times48(\times2_{GPU})$  |                    0                    |
|      卷积层 $C_2$      | $27\times27\times48(\times2_{GPU})$  | $5\times5\times48/1,128(\times2_{GPU})$  | $27\times27\times128(\times2_{GPU})$ | $(5\times5\times48+1)\times128\times2$  |
|   下采样层 $S_{max}$   | $27\times27\times128(\times2_{GPU})$ |       $3\times3/2(\times2_{GPU})$        | $13\times13\times128(\times2_{GPU})$ |                    0                    |
|   卷积层 $C_3$ $^*$    |  $13\times13\times128\times2_{GPU}$  | $3\times3\times256/1,192(\times2_{GPU})$ | $13\times13\times192(\times2_{GPU})$ | $(3\times3\times256+1)\times192\times2$ |
|      卷积层 $C_4$      | $13\times13\times192(\times2_{GPU})$ | $3\times3\times192/1,192(\times2_{GPU})$ | $13\times13\times192(\times2_{GPU})$ | $(3\times3\times192+1)\times192\times2$ |
|      卷积层 $C_5$      | $13\times13\times192(\times2_{GPU})$ | $3\times3\times192/1,128(\times2_{GPU})$ | $13\times13\times128(\times2_{GPU})$ | $(3\times3\times192+1)\times128\times2$ |
|   下采样层 $S_{max}$   | $13\times13\times128(\times2_{GPU})$ |       $3\times3/2(\times2_{GPU})$        |  $6\times6\times128(\times2_{GPU})$  |                    0                    |
|  全连接层 $F_6$  $^*$  |   $6\times6\times128\times2_{GPU}$   |     $9216\times2048(\times2_{GPU})$      | $1\times1\times2048(\times2_{GPU})$  |       $(9216+1)\times2048\times2$       |
|     全连接层 $F_7$     |  $1\times1\times2048\times2_{GPU}$   |     $4096\times2048(\times2_{GPU})$      | $1\times1\times2048(\times2_{GPU})$  |       $(4096+1)\times2048\times2$       |
|     全连接层 $F_8$     |  $1\times1\times2048\times2_{GPU}$   |             $4096\times1000$             |         $1\times1\times1000$         |       $(4096+1)\times1000\times2$       |

>卷积层 $C_1$ 输入为 $224\times224\times3$ 的图片数据，分别在两个 GPU 中经过核为 $11\times11\times3$、步长（stride）为 4 的卷积卷积后，分别得到两条独立的 $55\times55\times48$ 的输出数据。
>
>下采样层 $S_{max}$ 实际上是嵌套在卷积中的最大池化操作，但是为了区分没有采用最大池化的卷积层单独列出来。在 $C_{1-2}$ 卷积层中的池化操作之后（ReLU激活操作之前），还有一个 LRN 操作，用作对相邻特征点的归一化处理。<span style="color:red;">这个 LRN 是放在池化之后，ReLU 之前吗？如果是使用 BN 放在那里？这个查下有池化的网络怎么 BN 和 WN。</span>
>
>卷积层 $C_3$ 的输入与其他卷积层不同，$13\times13\times192\times2_{GPU}$ 表示汇聚了上一层网络在两个 GPU 上的输出结果作为输入，所以在进行卷积操作时通道上的卷积核维度为 384。
>
>全连接层 $F_{6-8}$ 中输入数据尺寸也和 $C_3$ 类似，都是融合了两个 GPU 流向的输出结果作为输入。


## 特性

- 所有卷积层都使用 ReLU 作为非线性映射函数，使模型收敛速度更快
- 在多个 GPU 上进行模型的训练，不但可以提高模型的训练速度，还能提升数据的使用规模
- 使用 LRN 对局部的特征进行归一化，结果作为 ReLU 激活函数的输入能有效降低错误率
- 重叠最大池化（overlapping max pooling），即池化范围 $z$ 与步长 $s$ 存在关系 $z>s$（如 $S_{max}$ 中核尺度为 $3\times3/2$），避免平均池化（average pooling）的平均效应。<span style="color:red;">一般用最大池化都是使用的重叠最大池化吗？</span>
- 使用随机丢弃技术（dropout）选择性地忽略训练中的单个神经元，避免模型的过拟合。<span style="color:red;">嗯。</span>


