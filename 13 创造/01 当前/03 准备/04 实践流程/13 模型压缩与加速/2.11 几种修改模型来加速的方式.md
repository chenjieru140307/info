---
title: 2.11 几种修改模型来加速的方式
toc: true
date: 2019-08-31
---
# 几种修改模型来加速的方式

## 输入输出的 channel 相同时，MAC 最小


**卷积层的输入和输出特征通道数相等时 MAC 最小，此时模型速度最快。**


假设 feature map的大小为 h*w，输入通道 $c_1$，输出通道 $c_2$。
已知：
$$
FLOPs = B = h * w * c1 * c2
=> c1 * c2 = \dfrac{B}{h * w}
$$

$$
MAC = h * w * (c1 + c2) + c1 * c2
$$

$$
=> MAC \geq 2 * h * w \sqrt{\dfrac{B}{h * w}} + \dfrac{B}{h * w}
$$

根据均值不等式得到(c1-c2)^2>=0，等式成立的条件是 c1=c2，也就是输入特征通道数和输出特征通道数相等时，在给定 FLOPs 前提下，MAC达到取值的下界。

### 17.8.4 减少组卷积的数量
**过多的 group 操作会增大 MAC，从而使模型速度变慢**
由以上公式可知，group卷积想比与传统的卷积可以降低计算量，提高模型的效率；如果在相同的 FLOPs 时，group卷积为了满足 FLOPs 会是使用更多 channels，可以提高模型的精度。但是随着 channel 数量的增加，也会增加 MAC。
FLOPs：
$$
B = \dfrac{h * w * c1 * c2}{g}
$$
MAC：
$$
MAC = h * w * (c1 + c2) + \dfrac{c1 * c2}{g}
$$
由 MAC，FLOPs可知：
$$
MAC = h * w * c1 + \dfrac{B*g}{c1} + \dfrac{B}{h * w}
$$
当 FLOPs 固定(B不变)时，g越大，MAC越大。

### 17.8.5 减少网络碎片化程度(分支数量)
**模型中分支数量越少，模型速度越快**
此结论主要是由实验结果所得。
以下为网络分支数和各分支包含的卷积数目对神经网络速度的影响。
<center>

![](http://images.iterate.site/blog/image/20190722/w4XxhO2DOebB.png?imageslim){ width=55% }

</center>

实验中使用的基本网络结构，分别将它们重复 10 次，然后进行实验。实验结果如下：
<center>

![](http://images.iterate.site/blog/image/20190722/xesRXfxdmDrn.png?imageslim){ width=55% }

</center>

由实验结果可知，随着网络分支数量的增加，神经网络的速度在降低。网络碎片化程度对 GPU 的影响效果明显，对 CPU 不明显，但是网络速度同样在降低。

### 17.8.7 减少元素级操作
**元素级操作所带来的时间消耗也不能忽视**
ReLU ，Tensor 相加，Bias相加的操作，分离卷积（depthwise convolution）都定义为元素级操作。
FLOPs大多数是对于卷积计算而言的，因为元素级操作的 FLOPs 相对要低很多。但是过的元素级操作也会带来时间成本。ShuffleNet作者对 ShuffleNet v1和 MobileNet v2的几种层操作的时间消耗做了分析，发现元素级操作对于网络速度的影响也很大。
<center>

![](http://images.iterate.site/blog/image/20190722/EChGU8JRbKSw.png?imageslim){ width=55% }

</center>
