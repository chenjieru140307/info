---
title: 1.11 SVM 与 逻辑回归的对比
toc: true
date: 2019-09-03
---

### 2.18.11 逻辑回归与 SVM 的异同

<span style="color:red;">嗯，总结的挺好的，但是排版有点不是很清晰。</span>

相同点：

- LR和 SVM 都是**分类**算法。
- LR和 SVM 都是**监督学习**算法。
- LR和 SVM 都是**判别模型**。
- 如果不考虑核函数，LR和 SVM 都是**线性分类**算法，也就是说他们的分类决策面都是线性的。说明：LR也是可以用核函数的。但 LR 通常不采用核函数的方法。（**计算量太大**）<span style="color:red;">嗯。想知道 LR 的核函数的使用。</span>

不同点：

**1、LR 采用 log 损失，SVM 采用合页(hinge) 损失。**

<span style="color:red;">SVM 的是合页损失吗？</span>

逻辑回归的损失函数：

$$
J(\theta)=-\frac{1}{m}\sum^m_{i=1}\left[y^{i}logh_{\theta}(x^{i})+ (1-y^{i})log(1-h_{\theta}(x^{i}))\right]
$$

支持向量机的目标函数:

$$
L(w,n,a)=\frac{1}{2}||w||^2-\sum^n_{i=1}\alpha_i \left( y_i(w^Tx_i+b)-1\right)
$$

​逻辑回归方法基于概率理论，假设样本为 $1$ 的概率可以用 sigmoid 函数来表示，然后通过**极大似然估计**的方法估计出参数的值。

​支持向量机基于几何**边界最大化**原理，认为存在最大几何边界的分类面为最优分类面。<span style="color:red;">边界最大化原理，之前好像没看到过。</span>

**2、LR 对异常值敏感，SVM 对异常值不敏感**。

​支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。LR 模型找到的那个超平面，是尽量让所有点都远离他，而 SVM 寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。

支持向量机改变非支持向量样本并不会引起决策面的变化。

逻辑回归中改变任何样本都会引起决策面的变化。

**3、计算复杂度不同。对于海量数据，SVM 的效率较低，LR 效率比较高**

- 当样本较少，特征维数较低时，SVM 和 LR 的运行时间均比较短，SVM 较短一些。准确率的话，LR 明显比 SVM 要高。
- 当样本稍微增加些时，SVM 运行时间开始增长，但是准确率赶超了 LR。SVM 时间虽长，但在可接受范围内。
- 当数据量增长到 20000 时，特征维数增长到 200 时，SVM 的运行时间剧烈增加，远远超过了 LR 的运行时间。但是准确率却和 LR 相差无几。(这其中主要原因是大量非支持向量参与计算，造成 SVM 的二次规划问题)<span style="color:red;">这里面的运行时间指的是训练时间吧？</span><span style="color:blue;">嗯，是的，因为预测的时候这些都是提供的一个分类面。</span><span style="color:red;">造成 SVM 的二次规划问题是什么意思？</span>

**4、对非线性问题的处理方式不同**

LR 主要靠特征构造，必须组合交叉特征，特征离散化。SVM 也可以这样，还可以通过核函数 kernel（因为只有支持向量参与核计算，计算复杂度不高）。由于可以利用核函数，SVM 则可以通过对偶求解高效处理。LR 则在特征空间维度很高时，表现较差。<span style="color:red;">嗯，很高大概是多高。</span>

**5、SVM的损失函数就自带正则**。

损失函数中的 $\frac{1}{2}||w||^2$ 项，这就是为什么 SVM 是结构风险最小化算法的原因！！！而 LR 必须另外在损失函数上添加正则项！！！<span style="color:red;">为什么呢？这个怎么能算作正则项呢？</span>

6、SVM自带**结构风险最小化**，LR则是**经验风险最小化**。<span style="color:red;">为什么说 LR 是经验风险最小化？</span>

7、SVM 会用核函数而 LR 一般不用核函数。<span style="color:red;">有使用带核函数的 LR 吗？</span>
