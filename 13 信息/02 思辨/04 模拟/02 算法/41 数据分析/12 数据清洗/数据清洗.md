
##   **Data Preprocessing**

大部分情况下，在构造 Feature 之前，我们需要对比赛提供的数据集进行一些处理。通常的步骤有：

- 有时数据会分散在几个不同的文件中，需要 Join 起来。
- 处理 **Missing Data**。
- 处理 **Outlier**。
- 必要时转换某些 **Categorical Variable** 的表示方式。
- 有些 Float 变量可能是从未知的 Int 变量转换得到的，这个过程中发生精度损失会在数据中产生不必要的 **Noise**，即两个数值原本是相同的却在小数点后某一位开始有不同。这对 Model 可能会产生很负面的影响，需要设法去除或者减弱 Noise。

这一部分的处理策略多半依赖于在前一步中探索数据集所得到的结论以及创建的可视化图表。在实践中，我建议使用 **iPython Notebook** 进行对数据的操作，并熟练掌握常用的 pandas 函数。这样做的好处是可以随时得到结果的反馈和进行修改，也方便跟其他人进行交流（在 Data Science 中 Reproducible Results 是很重要的)。

下面给两个例子。

#### **Outlier**

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20191018/epT27yAgjLz5.png?imageslim">
</p>

这是经过 Scaling 的坐标数据。可以发现右上角存在一些离群点，去除以后分布比较正常。

#### **Dummy Variables**

对于 Categorical Variable，常用的做法就是 One-hot encoding。即对这一变量创建一组新的伪变量，对应其所有可能的取值。这些变量中只有这条数据对应的取值为 1，其他都为 0。

如下，将原本有 7 种可能取值的 `Weekdays` 变量转换成 7 个 Dummy Variables。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20191018/PRgzYmirFY1R.png?imageslim">
</p>

要注意，当变量可能取值的范围很大（比如一共有成百上千类）时，这种简单的方法就不太适用了。这时没有有一个普适的方法，但我会在下一小节描述其中一种。


# 相关


- [如何在 Kaggle 首战中进入前 10%](https://blog.csdn.net/sqiu_11/article/details/74332642)
