

## 6.10 如何解决 RNN 中的梯度消失问题？

上节描述的梯度消失是在无限的利用历史数据而造成，但是 RNN 的特点本来就是能利用历史数据获取更多的可利用信息，解决 RNN 中的梯度消失方法主要有：

- 选取更好的激活函数，如 Relu 激活函数。ReLU函数的左侧导数为 0，右侧导数恒为 1，这就避免了“梯度消失“的发生。但恒为 1 的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。<span style="color:red;">恒为 1 的导数为什么会导致梯度爆炸？别的影响因素吗？那么合适的阈值要设定为多少？</span>
- 加入 BN 层，其优点包括可加速收敛、控制过拟合，可以少用或不用 Dropout 和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。<span style="color:red;">RNN 不是不使用 BN 的吗？奇怪了？突然想起一个问题，加了 BN 之后还可以用 dropout 吗？</span>
- 改变传播结构，LSTM 结构可以有效解决这个问题。<span style="color:red;">LSTM 是解决梯度消失的问题的吗？不是的吧？</span><span style="color:blue;">哦，是的，就是因为 RNN在处理长期依赖时有困难才有的 LSTM。</span>








# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
