

# 采样和蒙特卡罗方法


机器学习中的许多重要工具都基于从某种分布中采样以及用这些样本对目标量做一个蒙特卡罗估计。


## 为什么需要采样？


有许多原因使我们希望从某个分布中采样。当我们需要以较小的代价近似许多项的和或某个积分时，采样是一种很灵活的选择。有时候，我们使用它加速一些很费时却易于处理的求和估计，就像我们使用小批量对整个训练代价进行子采样一样。在其他情况下，我们需要近似一个难以处理的求和或积分，例如估计一个无向模型中配分函数对数的梯度时。在许多其他情况下，抽样实际上是我们的目标，例如我们想训练一个可以从训练分布采样的模型。



## 蒙特卡罗采样的基础


当无法精确计算和或积分（例如，和具有指数数量个项，且无法被精确简化）时，通常可以使用蒙特卡罗采样来近似它。这种想法把和或者积分视作某分布下的期望，然后\emph{通过估计对应的平均值来近似这个期望}。令


$$\begin{aligned}
s = \sum_{\boldsymbol x} p(\boldsymbol x)f(\boldsymbol x) = E_p[f(\mathbf x)]
\end{aligned}$$


或者


$$\begin{aligned}
s = \int p(\boldsymbol x)f(\boldsymbol x)d\boldsymbol x = E_p[f(\mathbf x)]
\end{aligned}$$


为我们所需要估计的和或者积分，写成期望的形式，$p$ 是一个关于随机变量 $\mathbf x$ 的概率分布（求和时）或者概率密度函数（求积分时）。


我们可以通过从 $p$ 中抽取 $n$ 个样本 $\boldsymbol x^{(1)},\ldots,\boldsymbol x^{(n)}$ 来近似 $s$ 并得到一个经验平均值


$$\begin{aligned}
\hat{s}_n = \frac{1}{n}\sum_{i=1}^{n}f(\boldsymbol x^{(i)}).
\end{aligned}$$


下面几个性质表明了这种近似的合理性。首先很容易观察到 $\hat{s}$ 这个估计是无偏的，由于


$$\begin{aligned}
\mathbb E[\hat{s}_n] = \frac{1}{n}\sum_{i=1}^{n}\mathbb E[f(\boldsymbol x^{(i)})] = \frac{1}{n}\sum_{i=1}^{n}s = s.
\end{aligned}$$



此外，根据大数定理，如果样本 $\boldsymbol x^{(i)}$ 是独立同分布的，那么其平均值几乎必然收敛到期望值，即


$$\begin{aligned}
\lim_{n\xrightarrow{}\infty} \hat{s}_n = s,
\end{aligned}$$


只需要满足各个单项的方差 $\text{Var}[f(\boldsymbol x^{(i)})]$ 有界。详细地说，我们考虑当 $n$ 增大时 $\hat{s}_n$ 的方差。只要满足 $\text{Var}[f(\mathbf x^{(i)})]<\infty$，方差 $\text{Var}[\hat{s}_n]$ 就会减小并收敛到 $0$：


$$\begin{aligned}
\text{Var}[\hat{s}_n] & = \frac{1}{n^2}\sum_{i=1}^{n}\text{Var}[f(\mathbf x)]\\
&  = \frac{\text{Var}[f(\mathbf x)]}{n}.
\end{aligned}$$


这个简单有用的结果启迪我们如何估计蒙特卡罗均值中的不确定性，或者等价地说是蒙特卡罗估计的期望误差。我们计算了 $f(\boldsymbol x^{(i)})$ 的经验均值和方差(注：通常我们会倾向于计算方差的无偏估计，它由偏差的平方和除以 $n-1$ 而非 $n$ 得到。)，然后将估计的方差除以样本数 $n$ 来得到 $\text{Var}[\hat{s}_n]$ 的估计。中心极限定理告诉我们 $\hat{s}_n$ 的分布收敛到以 $s$ 为均值以 $\frac{\text{Var}[f(\mathbf x)]}{n}$ 为方差的正态分布。这使得我们可以利用正态分布的累积函数来估计 $\hat{s}_n$ 的置信区间。



以上的所有结论都依赖于我们可以从基准分布 $p(\mathbf x)$ 中轻易地采样，但是这个假设并不是一直成立的。当我们无法从 $p$ 中采样时，一个备选方案是用\sec?讲到的重要采样。一种更加通用的方式是构建一个收敛到目标分布的估计序列。这就是马尔可夫链蒙特卡罗方法（见\sec?）。




# 相关

- 《深度学习》花书
