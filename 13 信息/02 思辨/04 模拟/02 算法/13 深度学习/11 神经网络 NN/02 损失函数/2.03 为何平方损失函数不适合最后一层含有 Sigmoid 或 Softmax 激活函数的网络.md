

### 为何平方损失函数不适合最后一层含有 Sigmoid 或 Softmax 激活函数的神经网络呢？

为何平方损失函数不适合最后一层含有 Sigmoid 或 Softmax 激活函数的神经网络呢？可以回顾上一问推导出的平方误差损失函数相对于输出层的导数：

$$
\delta^{(L)}=-\left(y-a^{(L)}\right) f^{\prime}\left(z^{(L)}\right)\tag{9.28}
$$


其中最后一项 $f^{\prime}\left(z^{(L)}\right)$ 为激活函数的导数。当激活函数为 Sigmoid 函数时，如果 $z^{(L)}$ 的绝对值较大，函数的梯度会趋于饱和，即 $f^{\prime}\left(z^{(L)}\right)$ 的绝对值非常小，导致 $\delta^{(L)}$ 的取值也非常小，使得基于梯度的学习速度非常缓慢。<span style="color:red;">哇塞！是呀！牛逼！嗯，但是还有个问题，这个地方如果使用 ReLU 或者 ReLU 的变体，是不是就可以了？因为只会稀疏化，不会饱和吧？</span>

当使用交叉熵损失函数时，相对于输出层的导数（也可以被认为是残差）为：

$$
\delta^{(L)}=a_{\tilde{k}}^{(L)}-1\tag{9.29}
$$

此时的导数是线性的，因此不会存在学习速度过慢的问题。<span style="color:red;">嗯，有些厉害！很好。</span>

<span style="color:red;">嗯，想知道，最后一层不含，前几层含，会对平方差损失函数的使用有影响吗？嗯，再理解下。 </span>
