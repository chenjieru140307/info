

# 半监督解释因果关系




表示学习的一个重要问题是"什么原因能够使一个表示比另一个表示更好？"一种假设是，理想表示中的特征对应到观测数据的潜在成因，特征空间中不同的特征或方向对应着不同的原因，从而表示能够区分这些原因。这个假设促使我们去寻找表示 $p(\boldsymbol x)$ 的更好方法。如果 $\boldsymbol y$ 是 $\boldsymbol x$ 的重要成因之一，那么这种表示也可能是计算 $p(\boldsymbol y \mid \boldsymbol x)$ 的一种良好表示。从 20 世纪 90 年代以来，这个想法已经指导了大量的深度学习研究工作~。关于半监督学习可以超过纯监督学习的其他论点，请读者参考{Chapelle-2006}的第 1.2节。



在表示学习的其他方法中，我们大多关注易于建模的表示——例如，数据稀疏或是各项之间相互独立的情况。能够清楚地分离出潜在因素的表示可能并不一定易于建模。然而，该假设促使半监督学习使用无监督表示学习的一个更深层原因是，对于很多人工智能任务而言，有两个相随的特点：一旦我们能够获得观察结果基本成因的解释，那么将会很容易分离出个体属性。具体来说，如果表示向量 $\boldsymbol h$ 表示观察值 $\boldsymbol x$ 的很多潜在因素，并且输出向量 $\boldsymbol y$ 是最为重要的原因之一，那么从 $\boldsymbol h$ 预测 $\boldsymbol y$ 会很容易。



首先，让我们看看 $p(\mathbf x)$ 的无监督学习无助于学习 $p(\mathbf y\mid\mathbf x)$ 时，半监督学习为何失败。例如，考虑一种情况，$p(\mathbf x)$ 是均匀分布的，我们希望学习 $f(\boldsymbol x) = \mathbb E[\mathbf y \mid \boldsymbol x]$。显然，仅仅观察训练集的值 $\boldsymbol x$ 不能给我们关于 $p(\mathbf y \mid \mathbf x)$ 的任何信息。



接下来，让我们看看半监督学习成功的一个简单例子。考虑这样的情况，$\mathbf x$ 来自一个混合分布，每个 $\mathbf y$ 值具有一个混合分量，如\fig?所示。如果混合分量很好地分出来了，那么建模 $p(\mathbf x)$ 可以精确地指出每个分量的位置，每个类一个标注样本的训练集足以精确学习 $p(\mathbf y \mid \mathbf x)$。但是更一般地，什么能将 $p(\mathbf y \mid \mathbf x)$ 和 $p(\mathbf x)$ 关联在一起呢？



<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/aQMlyfDq09UM.png?imageslim">
</p>

> 15.4 混合模型。具有三个混合分量的 $x$ 上混合密度示例。混合分量的内在本质是潜在解释因子 $y$。因为混合分量（例如，图像数据中的自然对象类别）在统计学上是显著的，所以仅仅使用未标注样本无监督建模 $p(x)$ 也能揭示解释因子 $y$。



如果 $\mathbf y$ 与 $\mathbf x$ 的成因之一非常相关，那么 $p(\mathbf x)$ 和 $p(\mathbf y \mid \mathbf x)$ 也会紧密关联，试图找到变化潜在因素的无监督表示学习可能像半监督学习一样有用。



假设 $\mathbf y$ 是 $\mathbf x$ 的成因之一，让 $\mathbf h$ 代表所有这些成因。真实的生成过程可以被认为是根据这个有向图模型结构化出来的，其中 $\mathbf h$ 是 $\mathbf x$ 的父节点：

$$
p(\mathbf h, \mathbf x) = p(\mathbf x \mid \mathbf h) p(\mathbf h).
$$


因此，数据的边缘概率是

$$
p(\boldsymbol x) = \mathbb E_{\mathbf h} p(\boldsymbol x \mid \boldsymbol h).
$$

从这个直观的观察中，我们得出结论，$\mathbf x$ 最好可能的模型（从广义的观点）是会表示上述"真实"结构的，其中 $\boldsymbol h$ 作为潜变量解释 $\boldsymbol x$ 中可观察的变化。上文讨论的"理想"的表示学习应该能够反映出这些潜在因子。如果 $\mathbf y$ 是其中之一（或是紧密关联于其中之一），那么将很容易从这种表示中预测 $\mathbf y$。我们会看到给定 $\mathbf x$ 下 $\mathbf y$ 的条件分布通过贝叶斯规则关联到上式中的分量：

$$
p(\mathbf y \mid \mathbf x) = \frac{ p(\mathbf x \mid \mathbf y) p(\mathbf y) }{p(\mathbf x)}.
$$

因此边缘概率 $p(\mathbf x)$ 和条件概率 $p(\mathbf y \mid \mathbf x)$ 密切相关，前者的结构信息应该有助于学习后者。因此，在这些假设情况下，半监督学习应该能提高性能。


关于这个事实的一个重要的研究问题是，大多数观察是由极其大量的潜在成因形成的。假设 $\mathbf y = \mathrm h_i$，但是无监督学习器并不知道是哪一个 $\mathrm h_i$。对于一个无监督学习器暴力求解就是学习一种表示，这种表示能够捕获\emph{所有}合理的重要生成因子 $\mathrm h_j$，并将它们彼此区分开来，因此不管 $\mathrm h_i$ 是否关联于 $\mathbf y$，从 $\mathbf h$ 预测 $\mathbf y$ 都是容易的。


在实践中，暴力求解是不可行的，因为不可能捕获影响观察的所有或大多数变化因素。例如，在视觉场景中，表示是否应该对背景中的所有最小对象进行编码？根据一个有据可查的心理学现象，人们不会察觉到环境中和他们所在进行的任务并不立刻相关的变化，具体例子可以参考 {simons1998failure}。半监督学习的一个重要研究前沿是确定每种情况下要编码\emph{什么}。目前，处理大量潜在原因的两个主要策略是，同时使用无监督学习和监督学习信号，从而使得模型捕获最相关的变动因素，或是使用纯无监督学习学习更大规模的表示。



无监督学习的另一个思路是选择一个更好的确定哪些潜在因素最为关键的定义。之前，自编码器和生成模型被训练来优化一个类似于均方误差的固定标准。这些固定标准确定了哪些因素是重要的。例如，图像像素的均方误差隐式地指定，一个潜在因素只有在其显著地改变大量像素的亮度时，才是重要影响因素。如果我们希望解决的问题涉及到小对象之间的相互作用，那么这将有可能遇到问题。如\fig?所示，在机器人任务中，自编码器未能学习到编码小乒乓球。同样是这个机器人，它可以成功地与更大的对象进行交互（例如棒球，均方误差在这种情况下很显著）。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/jgYQBFiR6rmU.png?imageslim">
</p>
> 15.5 机器人任务上，基于均方误差训练的自编码器不能重构乒乓球。乒乓球的存在及其所有空间坐标，是生成图像且与机器人任务相关的重要潜在因素。不幸的是，自编码器具有有限的容量，基于均方误差的训练没能将乒乓球作为显著物体识别出来编码。以上图像由 Chelsea Finn提供。





还有一些其他的显著性的定义。例如，如果一组像素具有高度可识别的模式，那么即使该模式不涉及到极端的亮度或暗度，该模式还是会被认为非常显著。实现这样一种定义显著的方法是使用最近提出的生成式对抗网络。在这种方法中，生成模型被训练来愚弄前馈分类器。前馈分类器尝试将来自生成模型的所有样本识别为假的，并将来自训练集的所有样本识别为真的。在这个框架中，前馈网络能够识别出的任何结构化模式都是非常显著的。生成式对抗网络会在\sec?中更详细地介绍。为了叙述方便，知道它能\emph{学习}出如何决定什么是显著的就可以了。{lotter2015unsupervised}表明，生成人类头部头像的模型在使用均方误差训练时往往会忽视耳朵，但是对抗式框架学习能够成功地生成耳朵。因为耳朵与周围的皮肤相比不是非常明亮或黑暗，所以根据均方误差损失它们不是特别突出，但是它们高度可识别的形状和一致的位置意味着前馈网络能够轻易地学习出如何检测它们，从而使得它们在生成式对抗框架下是高度突出的。\fig?给了一些样例图片。生成式对抗网络只是确定应该表示哪些因素的一小步。我们期望未来的研究能够发现更好的方式来确定表示哪些因素，并且根据任务来开发表示不同因素的机制。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190718/rlHphargwtl2.png?imageslim">
</p>

> 15.6 预测生成网络是一个学习哪些特征显著的例子。在这个例子中，预测生成网络已被训练成在特定视角预测人头的 3D 模型。\emph{(左)}真实情况。这是一张网络应该生成的正确图片。\emph{(中)}由具有均方误差的预测生成网络生成的图片。因为与相邻皮肤相比，耳朵不会引起亮度的极大差异，所以它们的显著性不足以让模型学习表示它们。\emph{(右)}由具有均方误差和对抗损失的模型生成的图片。使用这个学成的代价函数，由于耳朵遵循可预测的模式，因此耳朵是显著重要的。学习哪些原因对于模型而言是足够重要和相关的，是一个重要的活跃研究领域。




正如 {Janzing-et-al-ICML2012}指出，学习潜在因素的好处是，如果真实的生成过程中 $\mathbf x$ 是结果，$\mathbf y$ 是原因，那么建模 $p(\mathbf x \mid \mathbf y)$ 对于 $p(\mathbf y)$ 的变化是鲁棒的。如果因果关系被逆转，这是不对的，因为根据贝叶斯规则，$p(\mathbf x \mid \mathbf y)$ 将会对 $p(\mathbf y)$ 的变化十分敏感。很多时候，我们考虑分布的变化（由于不同领域、时间不稳定性或任务性质的变化）时，\emph{因果机制是保持不变的}（"宇宙定律不变"），而潜在因素的边缘分布是会变化的。因此，通过学习试图恢复成因向量 $\mathbf h$ 和 $p(\mathbf x \mid \mathbf h)$ 的生成模型，我们可以期望最后的模型对所有种类的变化有更好的泛化和鲁棒性。






# 相关

- 《深度学习》花书
