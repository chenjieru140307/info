---
title: 决策树3
toc: true
date: 2019-03-30
---
# 可以补充进来的

- 决策树之前看的还是比较明白的，嗯这段时间没看了，再理解下
- 感觉决策树的几个例子可以合并一下，基本的意思都是相同的。要进行融合下。而且，要自己手动计算下，详细写出每一步的思考过程。


# 决策树

信息论，树形数据结构，优化理论


## 决策树有哪些常用的启发函数？

<span style="color:red;">为什么叫启发函数？</span>



## 常用的决策树算法有 ID3、C4.5、CART，它们构建树所使用的启发式函数各是什么？

<span style="color:red;">为什么叫做启发式函数？</span>

首先，我们回顾一下这几种决策树构造时使用的准则。

**ID3—— 最大信息增益**


对于样本集合 D，类别数为 K，数据集 D 的经验熵表示为：<span style="color:red;">其实一直想知道经验熵的式子为什么是这样的？</span>
$$
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}\tag{3.18}
$$

其中 $C_k$ 是样本集合 $D$ 中属于第 $k$ 类的样本子集，$|C_k|$ 表示该子集的元素个数，$|D|$ 表示样本集合的元素个数。


然后计算某个特征 $A$ 对于数据集 $D$ 的经验条件熵 $H(D|A)$ 为：

$$
H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{D |}\left(-\sum_{k=1}^{k} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}\right)\tag{3,19}
$$



其中，$D_i$ 表示 $D$ 中特征 $A$ 取第 $i$ 个值的样本子集，$D_{ik}$ 表示 $D_i$ 中属于第 k类的样本子集。


于是信息增益 $
g(D, A)
$ 可以表示为二者之差，可得：

$$g(D,A)=H(D)-H(D|A)\tag{3.20}$$

<span style="color:red;">嗯。</span>

这些定义听起来有点像绕口令，不妨我们用一个例子来简单说明下计算过程。假设共有 5 个人追求场景中的女孩，年龄有两个属性（老，年轻），长相有三个属性（帅，一般，丑），工资有三个属性（高，中等，低），会写代码有两个属性（会，不会），最终分类结果有两类（见，不见）。我们根据女孩有监督的主观意愿可以得到表 3.1：

![](http://images.iterate.site/blog/image/20190330/9AR6phO6GiN3.png?imageslim){ width=55% }


在这个问题中，

$$
H(D)=-\frac{3}{5} \log _{2} \frac{3}{5}-\frac{2}{5} \log _{2} \frac{2}{5}=0.971
$$

根据式（3.19）可计算出 4 个分支结点的信息熵为：

![](http://images.iterate.site/blog/image/20190330/EczaTKd1jH2m.png?imageslim){ width=55% }
![](http://images.iterate.site/blog/image/20190330/y8zrVkIz2ew0.png?imageslim){ width=55% }
![](http://images.iterate.site/blog/image/20190330/f3vIn1RpjrWb.png?imageslim){ width=55% }
![](http://images.iterate.site/blog/image/20190330/JLDAEjKCuqaX.png?imageslim){ width=55% }


于是，根据式（3.20）可计算出各个特征的信息增益为：

![](http://images.iterate.site/blog/image/20190330/7QTUNbALR947.png?imageslim){ width=55% }

显然，特征“写代码”的信息增益最大，所有的样本根据此特征，可以直接被分到叶结点（即见或不见）中，完成决策树生长。当然，在实际应用中，决策树往往不能通过一个特征就完成构建，需要在经验熵非 0 的类别中继续生长。


**C4.5——最大信息增益比**


特征 A 对于数据集 D 的信息增益比定义为：

$$
g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}\tag{3.21}
$$

其中：

$$
H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}\tag{3.22}
$$

称为数据集 D 关于 A 的取值熵。针对上述问题，我们可以根据式（3.22）求出数据集关于每个特征的取值熵为：

$$
H_{年龄}(D)=-\frac{1}{5} \log _{2} \frac{1}{5}-\frac{4}{5} \log _{2} \frac{4}{5}=0.722
$$
$$
H_{长相}(D)=-\frac{1}{5} \log _{2} \frac{1}{5}-\frac{3}{5} \log _{2} \frac{3}{5}-\frac{1}{5} \log _{2} \frac{1}{5}=1.371
$$
$$
H_{工资}(D)=-\frac{3}{5} \log _{2} \frac{3}{5}-\frac{1}{5} \log _{2} \frac{1}{5}-\frac{1}{5} \log _{2} \frac{1}{5}=1.371
$$
$$
H_{写代码}(D)=-\frac{3}{5} \log _{2} \frac{3}{5}-\frac{2}{5} \log _{2} \frac{2}{5}=0.971
$$


于是，根据式（3.21）可计算出各个特征的信息增益比为：

$$
g_{R}(D，年龄)=0.236,g_{R}(D，长相)=0.402\\
g_{R}(D，工资)=0.402,g_{R}(D，写代码)=1
$$

信息增益比最大的仍是特征“写代码”，但通过信息增益比，特征“年龄”对应的指标上升了，而特征“长相”和特征“工资”却有所下降。


**CART——最大基尼指数（Gini）**


Gini描述的是数据的纯度，与信息熵含义类似。

$$
\operatorname{Gini}(D)=1-\sum_{k=1}^{n}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}\tag{3.23}
$$


CART在每一次迭代中选择基尼指数最小的特征及其对应的切分点进行分类。但与 ID3、C4.5不同的是，CART是一颗二叉树，采用二元切割法，每一步将数据按特征 A 的取值切成两份，分别进入左右子树。特征 A 的 Gini 指数定义为

$$
\operatorname{Gini}(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \operatorname{Gini}\left(D_{i}\right)\tag{3.24}
$$


还是考虑上述的例子，应用 CART 分类准则，根据式（3.24）可计算出各个特征的 Gini 指数为：


$$\operatorname{Gini}(D|年龄=老)=0.4，\operatorname{Gini}(D|年龄=年轻)=0.4，\\
\operatorname{Gini}(D|长相=帅)=0.\operatorname{Gini}(D|长相=丑)=0.4，\\
\operatorname{Gini}(D|写代码=会)=0，\operatorname{Gini}(D|写代码=不会)=0，\\
\operatorname{Gini}(D|工资=高)=0.47，\operatorname{Gini}(D|工资=中等)=0.3，\operatorname{Gini}(D|工资=低)=0.4．
$$

在“年龄”“长相”“工资”“写代码”四个特征中，我们可以很快地发现特征“写代码”的 Gini 指数最小为 0，因此选择特征“写代码”作为最优特征，“写代码=会”为最优切分点。按照这种切分，从根结点会直接产生两个叶结点，基尼指数降为 0，完成决策树生长。


## 如何对决策树进行剪枝？


一棵完全生长的决策树会面临一个很严重的问题，即过拟合。假设我们真的需要考虑 DNA 特征，由于每个人的 DNA 都不同，完全生长的决策树所对应的每个叶结点中只会包含一个样本，这就导致决策树是过拟合的。用它进行预测时，在测试集上的效果将会很差。

因此我们需要对决策树进行剪枝，剪掉一些枝叶，提升模型的泛化能力。

决策树的剪枝通常有两种方法：

- 预剪枝（Pre-Pruning）
- 后剪枝（Post-Pruning）

那么这两种方法是如何进行的呢？它们又各有什么优缺点？


- 预剪枝，即在生成决策树的过程中提前停止树的增长
- 后剪枝，是在已生成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树。


**预剪枝**


预剪枝的核心思想是在树中结点进行扩展之前，先计算当前的划分是否能带来模型泛化能力的提升，如果不能，则不再继续生长子树。

此时可能存在不同类别的样本同时存于结点中，按照多数投票的原则判断该结点所属类别。<span style="color:red;">嗯，是的，多数投票来对此时有各种类别样本存在的样本进行判定。</span>

预剪枝对于何时停止决策树的生长有以下几种方法：<span style="color:red;">一般是用哪种方法？</span>

1. 当树到达一定深度的时候，停止树的生长。
2. 当到达当前结点的样本数量小于某个阈值的时候，停止树的生长。
3. 计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，不再继续扩展。


预剪枝具有思想直接、算法简单、效率高等特点，适合解决大规模问题。

但如何准确地估计何时停止树的生长（即上述方法中的深度或阈值），针对不同问题会有很大差别，需要一定经验判断。<span style="color:red;">好吧，这个经验值要怎么定？</span>

且 **预剪枝存在一定局限性，有欠拟合的风险，虽然当前的划分会导致测试集准确率降低，但在之后的划分中，准确率可能会有显著上升。** <span style="color:red;">是呀，这个之前好像看到过具体的计算的，要融合下。</span>


**后剪枝**


后剪枝的核心思想是让算法生成一棵完全生长的决策树，然后从最底层向上计算是否剪枝。

剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样按照多数投票的原则进行判断。

同样地，后剪枝也可以通过在测试集上的准确率进行判断，如果剪枝过后准确率有所提升，则进行剪枝。

相比于预剪枝，后剪枝方法通常可以得到泛化能力更强的决策树，但时间开销会更大。<span style="color:red;">是的。</span>

常见的后剪枝方法包括：<span style="color:red;">这些最好都看下，具体是什么，为什么需要创造出这么多的后剪枝方法？在什么情况下使用的？</span>

- 错误率降低剪枝（Reduced Error Pruning，REP）
- 悲观剪枝（Pessimistic Error Pruning，PEP）
- 代价复杂度剪枝（Cost Complexity Pruning，CCP）
- 最小误差剪枝（Minimum Error Pruning，MEP）
- CVP（Critical Value Pruning）
- OPP（Optimal Pruning）
- 等方法

这些剪枝方法各有利弊，关注不同的优化角度，本文选取著名的 CART 剪枝方法 CCP 进行介绍。

代价复杂剪枝 CCP 主要包含以下两个步骤：


1. 从完整决策树 $T_0$ 开始，生成一个子树序列 $\{T_0,T_1,T_2,...,T_n\}$，其中 $T_{i+1}$ 由 $T_i$ 生成，$T_n$ 为树的根结点。
2. 在子树序列中，根据真实误差选择最佳的决策树。


步骤（1）从 $T_0$ 开始，裁剪 $T_i$ 中关于训练数据集合误差增加最小的分支以得到 $T_{i+1}$ 。具体地，当一棵树 $T$ 在结点 $t$ 处剪枝时，它的误差增加可以用 $R(t)−R(T_t)$ 表示，其中 $R(t)$ 表示进行剪枝之后的该结点误差，$R(T_t)$ 表示未进行剪枝时子树 $T_t$ 的误差。考虑到树的复杂性因素，我们用 $|L(T_t)|$ 表示子树 $T_t$ 的叶子结点个数，则树在结点 $t$ 处剪枝后的误差增加率为：

$$\alpha=\frac{R(t)-R(T_t)}{|L(T_t)|-1} \tag{3.25}$$


在得到 $T_i$ 后，我们每步选择 $α$ 最小的结点进行相应剪枝。

用一个例子简单地介绍生成子树序列的方法。假设把场景中的问题进行一定扩展，女孩需要对 80 个人进行见或不见的分类。

假设根据某种规则，已经得到了一棵 CART 决策树 T0，如图 3.15所示：

![](http://images.iterate.site/blog/image/20190330/hIlxGUGupd5M.png?imageslim){ width=55% }


此时共 5 个内部结点可供考虑，其中：

$$
a\left(t_{0}\right)=\frac{25-5}{6-1}=4
$$
$$
\alpha\left(t_{1}\right)=\frac{10-(1+2+0+0)}{4-1}=2.33
$$
$$
\alpha\left(t_{2}\right)=\frac{5-(1+1)}{2-1}=3
$$
$$
\alpha\left(t_{3}\right)=\frac{4-(1+2)}{2-1}=1
$$
$$
\alpha\left(t_{4}\right)=\frac{4-0}{2-1}=4
$$


可见 $α(t_3)$ 最小，因此对 $t_3$ 进行剪枝，得到新的子树 $T_1$，如图 3.16所示：

![](http://images.iterate.site/blog/image/20190330/Php52wP7Vn57.png?imageslim){ width=55% }

而后继续计算所有结点对应的误差增加率，分别为α（t1）=3，α（t2）=3，α（t4）=4。因此对 t1 进行剪枝，得到 T2，如图所示：

![](http://images.iterate.site/blog/image/20190330/vw6LtPcglhVe.png?imageslim){ width=55% }

此时α（t0）=6.5，α（t2）=3，选择 t2 进行剪枝，得到 T3。于是只剩下一个内部结点，即根结点，得到 T4。


在步骤（2）中，我们需要从子树序列中选出真实误差最小的决策树。CCP给出了两种常用的方法：

- 一种是基于独立剪枝数据集，该方法与 REP 类似，但由于其只能从子树序列 ${T0，T1，T2，……，Tn}$ 中选择最佳决策树，而非像 REP 能在所有可能的子树中寻找最优解，因此性能上会有一定不足。
- 另一种是基于 k 折交叉验证，将数据集分成 k 份，前 k−1份用于生成决策树，最后一份用于选择最优的剪枝树。重复进行 N 次，再从这 N 个子树中选择最优的子树。


代价复杂度剪枝使用交叉验证策略时，不需要测试数据集，精度与 REP 差不多，但形成的树复杂度小。而从算法复杂度角度，由于生成子树序列的时间复杂度与原始决策树的非叶结点个数呈二次关系，导致算法相比 REP、PEP、MEP等线性复杂度的后剪枝方法，运行时间开销更大。<span style="color:red;">嗯，要更仔细的总结。</span>


剪枝过程在决策树模型中占据着极其重要的地位。有很多研究表明，剪枝比树的生成过程更为关键。

对于不同划分标准生成的过拟合决策树，在经过剪枝之后都能保留最重要的属性划分，因此最终的性能差距并不大。<span style="color:red;">嗯，这个没想到。</span>

理解剪枝方法的理论，在实际应用中根据不同的数据类型、规模，决定使用何种决策树以及对应的剪枝策略，灵活变通，找到最优选择，是本节想要传达给读者的思想。<span style="color:red;">是呀，这些要看看有什么经验可以总结的。要补充下。</span>


## 逸闻趣事奥卡姆剃刀定律（Occam’s Razor，Ockham’s Razor）

<span style="color:red;">这个要不要放到别的地方？比如机器学习了解里面？</span>

14世纪，逻辑学家、圣方济各会修士奥卡姆威廉（William of Occam）提出奥卡姆剃刀定律。这个原理最简单的描述是“如无必要，勿增实体”，即“简单有效原理”。

很多人误解了奥卡姆剃刀定律，认为简单就一定有效，但奥卡姆剃刀定律从来没有说“简单”的理论就是“正确”的理论，通常表述为“当两个假说具有完全相同的解释力和预测力时，我们以那个较为简单的假说作为讨论依据”。

奥卡姆剃刀的思想其实与机器学习消除过拟合的思想是一致的。特别是在决策树剪枝的过程中，我们正是希望在预测力不减的同时，用一个简单的模型去替代原来复杂的模型。而在 ID3 决策树算法提出的过程中，模型的创建者 Ross Quinlan也确实参照了奥卡姆剃刀的思想。类似的思想还同样存在于神经网络的 Dropout 的方法中，我们降低模型复杂度，为的是提高模型的泛化能力。


严格讲，奥卡姆剃刀定律不是一个定理，而是一种思考问题的方式。我们面对任何工作的时候，如果有一个简单的方法和一个复杂的方法能够达到同样的效果，我们应该选择简单的那个。因为简单的选择是巧合的几率更小，更有可能反应事物的内在规律。<span style="color:red;">为什么简单的更有可能反应事物的内在规律？这个能说明复杂宇宙是搭建在非常简单的原理上的吗？</span>




# 原文与相关

- 《百面机器学习》


