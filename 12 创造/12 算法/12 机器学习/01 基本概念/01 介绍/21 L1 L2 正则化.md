
# 引入

## 线性回归的损失函数

给定数据集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$ ，其中 $\boldsymbol{x} \in \mathbb{R}^{d}$ ，$y \in \mathbb{R}$ 。我们考虑最简单的线性回归模型，以平方误差为损失函数，则优化目标为

$$
\min _{\boldsymbol{w}} \sum_{i=1}^{m}\left(y_{i}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}\right)^{2}
$$


## 引入 $\mathrm{L}_{2}$ 正则化

为了缓解过拟合问题，对线性回归的损失函数引入正则化项。若使用 $\mathrm{L}_{2}$ 范数正则化，则有

$$
\min _{\boldsymbol{w}} \sum_{i=1}^{m}\left(y_{i}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}\right)^{2}+\lambda\|\boldsymbol{w}\|_{2}^{2}
$$


其中正则化参数 $\lambda>0$ 。

这样的 L2 正则化后的线性回归也被称为“岭回归” (ridge regression) 。

通过引入 $\mathrm{L}_{2}$ 范数正则化，确能显著降低过拟合的风险。


## 引入 $\mathrm{L}_{1}$ 正则化

能否将正则化项中的 $\mathrm{L}_{2}$ 范数替换为 $\mathrm{L}_{p}$ 范数呢？

答案是肯定的。若令 $p=1$ ，即采用 $\mathrm{L}_{1}$ 范数，则有

$$
\min _{\boldsymbol{w}} \sum_{i=1}^{m}\left(y_{i}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}\right)^{2}+\lambda\|\boldsymbol{w}\|_{1}
$$

其中正则化参数  $\lambda>0$ 。


这样的 L1 正则化后的线性回归也被称为 LASSO 回归 (Least Absolute Shrinkage and Selection Operator) 。



$\mathrm{L}_{1}$ 范数和 $\mathrm{L}_{2}$ 范数正则化都有助于降低过拟合风险，但前者还会带来一个额外的好处：它比后者更易于获得“稀疏”(sparse)解，即它求得的 $\boldsymbol{w}$ 会有更少的非零分量。



# L1 正则化使得模型参数具有稀疏性的原理

## 角度 1：解空间形状

我们可以通过 KKT 条件给出一种解释。


事实上，“带正则项”和“带约束条件”是等价的。为了约束 $w$ 的可能取值空间从而防止过拟合，我们为该最优化问题加上一个约束，就是 $w$ 的 L2 范数的平方不能大于 $m$：<span style="color:red;">嗯。</span>


$$
\left\{\begin{array}{l}{\min \sum_{i=1}^{N}\left(y_{i}-w^{\mathrm{T}} x_{i}\right)^{2}} \\ {\text {s.t.}\|w\|_{2}^{2} \leqslant m}\end{array}\right. \tag{7.55}
$$


为了求解带约束条件的凸优化问题，写出拉格朗日函数：<span style="color:red;">嗯。</span>

$$
\sum_{i=1}^{N}\left(y_{i}-w^{\mathrm{T}} x_{i}\right)^{2}+\lambda\left(\|\boldsymbol{w}\|_{2}^{2}-m\right)\tag{7.56}
$$

若 $w^{*}$ 和 $\lambda^{*}$ 分别是原问题和对偶问题的最优解，则根据 KKT 条件，它们应满足；


$$
\left\{\begin{array}{l}{0=\nabla_{w}\left(\sum_{i=1}^{N}\left(y_{i}-w^{* \mathrm{T}} x_{i}\right)^{2}+\lambda^{*}\left(\left\|w^{*}\right\|_{2}^{2}-m\right)\right.} \\ {0 \leqslant \lambda^{*}}\end{array}\right.\tag{7.57}
$$

仔细一看，第一个式子不就是 $w^{*}$ 为带 L2 正则项的优化问题的最优解的条件嘛，而 $\lambda^{*}$ 就是 L2 正则项前面的正则参数。<span style="color:red;">嗯。好像有些明白。</span>

这时回头再看开头的问题就清晰了：L2正则化相当于为参数定义了一个圆形的解空间（因为必须保证 L2 范数不能大于 m），而 L1 正则化相当于为参数定义了一个棱形的解空间。L1 和 L2 的解空间是不同的。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180629/943jCmBh5K.png?imageslim">
</p>


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190413/xSb9wKReRwmR.png?imageslim">
</p>




在二维的情况下，黄色的部分是 L2 和 L1 正则项约束后的解空间，绿色的等高线是凸优化问题中目标函数的等高线，如图所示。

由图可知，L2正则项约束后的解空间是圆形，而 L1 正则项约束的解空间是多边形。显然，多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。<span style="color:red;">为什么呢？</span>

采用 $\mathrm{L}_{1}$ 范数时平方误差项等值线与正则化项等值线的交点常出现在坐标轴上，即 $w_1$ 或 $w_2$ 为 0，<span style="color:red;">为什么呢？</span>而在采用 $\mathrm{L}_{2}$ 范数时，两者的交点常出现在某个象限中，即 $w_1$ 或 $w_2$ 均非 0；

换言之，采用范数 $\mathrm{L}_{1}$ 比 $\mathrm{L}_{2}$ 范数更易于得到稀疏解。

如果原问题目标函数的最优解不是恰好落在解空间内，那么约束条件下的最优解一定是在解空间的边界上，而 L1“棱角分明”的解空间显然更容易与目标函数等高线在角点碰撞，从而产生稀疏解。<span style="color:red;">原问题目标函数的最优解不是恰好落在解空间内，那么约束条件下的最优解一定是在解空间边界上。没有很懂。</span>

## 角度 2：函数叠加

第二个角度试图用更直观的图示来解释 L1 产生稀疏性这一现象。

仅考虑一维的情况，多维情况是类似的，如图下图所示。假设棕线是原始目标函数 $L(w)$ 的曲线图，显然最小值点在蓝点处，且对应的 $w^{\star}$ 值非 0。

<center>

![](http://images.iterate.site/blog/image/20190413/FsAL4pqvbiv4.png?imageslim){ width=55% }

</center>

- 首先，考虑加上 L2 正则化项，目标函数变成 $L(w)+C w^{2}$ ，其函数曲线为黄色。此时，最小值点在黄点处，对应的 $w^{\star}$ 的绝对值减小了，但仍然非 0。
- 然后，考虑加上 L1 正则化项，目标函数变成 $L(w)+C|w|$ ，其函数曲线为绿色。此时，最小值点在红点处，对应的 $w$ 是 0，产生了稀疏性。<span style="color:red;">为什么这个就产生了稀疏性？</span>

产生上述现象的原因也很直观：

- 加入 L1 正则项后，对带正则项的目标函数求导，正则项部分产生的导数在原点左边部分是 $−C$，在原点右边部分是 $C$，因此，只要原目标函数的导数绝对值小于 $C$，那么带正则项的目标函数在原点左边部分始终是递减的，在原点右边部分始终是递增的，最小值点自然在原点处。

- 相反，L2 正则项在原点处的导数是 0，只要原目标函数在原点处的导数不为 0，那么最小值点就不会在原点，所以 L2 只有减小 $w$ 绝对值的作用，对解空间的稀疏性没有贡献。<span style="color:red;">嗯。</span>

在一些在线梯度下降算法中，往往会采用截断梯度法来产生稀疏性，这同 L1 正则项产生稀疏性的原理是类似的。<span style="color:red;">什么意思？截断梯度法是什么方法？怎么使用？什么时候使用？为什么要使用？有类似的场景吗？</span>


## 角度 3：贝叶斯先验

从贝叶斯的角度来理解 L1 正则化和 L2 正则化，简单的解释是：

- L1正则化相当于对模型参数 $w$ 引入了拉普拉斯先验，
- L2正则化相当于引入了高斯先验
- 拉普拉斯先验使参数为 $0$ 的可能性更大。

<span style="color:red;">什么是拉普拉斯先验和高斯先验？</span>

高斯分布曲线图：

<center>

![](http://images.iterate.site/blog/image/20190413/51BFsCjfJORQ.png?imageslim){ width=55% }

</center>

由图可见，高斯分布在极值点（0点）处是平滑的，也就是高斯先验分布认为 $w$ 在极值点附近取不同值的可能性是接近的。这就是 L2 正则化只会让 $w$ 更接近 0 点，但不会等于 0 的原因。<span style="color:red;">多理解下。</span>

拉普拉斯分布曲线图：

<center>

![](http://images.iterate.site/blog/image/20190413/WPsOPJULiBK0.png?imageslim){ width=55% }

</center>

由图可见，拉普拉斯分布在极值点（0点）处是一个尖峰，所以拉普拉斯先验分布中参数 $w$ 取值为 $0$ 的可能性要更高。

在此我们不再给出 L1 和 L2 正则化分别对应拉普拉斯先验分布和高斯先验分布的详细证明。<span style="color:red;">还是要补充下的。感觉没有特别清楚。每一点都要弄明白。</span>


# $\mathrm{L}_{1}$ 正则化得到稀疏解的意义

注意到 $\boldsymbol{w}$ 取得稀疏解意味着初始的 $d$ 个特征中仅有对应着 $\boldsymbol{w}$ 的非零分量的特征才会出现在最终模型中，于是，求解 $\mathrm{L}_{1}$ 范数正则化的结果是得到了仅采用一部分初始特征的模型；

换言之，**基于 $\mathrm{L}_{1}$ 正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成。**<span style="color:red;">厉害</span>


# 可以补充进来的

- 近端梯度下降
- L-Lipschitz 条件

# 求解 $\mathrm{L}_{1}$ 正则化后的解


$\mathrm{L}_{1}$ 正则化问题的求解可使用近端梯度下降(Proximal Gradient Descent, 简称 **PGD**)。

具体来说，令 $\nabla$ 表示微分算子，对优化目标

$$
\min _{\boldsymbol{x}} f(\boldsymbol{x})+\lambda\|\boldsymbol{x}\|_{1}
$$

若 $f(\boldsymbol{x})$ 可导，且 $\nabla f$ 满足 L-Lipschitz 条件，即存在常数 $L>0$ 使得

$$
\left\|\nabla f\left(\boldsymbol{x}^{\prime}\right)-\nabla f(\boldsymbol{x})\right\|_{2}^{2} \leqslant L\left\|\boldsymbol{x}^{\prime}-\boldsymbol{x}\right\|_{2}^{2} \quad\left(\forall \boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\tag{11.9}
$$

> $$\left \| \nabla f(\boldsymbol x{}')-\nabla  f(\boldsymbol x) \right \|_{2}^{2} \leqslant L\left \| \boldsymbol x{}'-\boldsymbol x \right \|_{2}^{2} 　(\forall \boldsymbol x,\boldsymbol x{}'),$$
> [解析]：*L-Lipschitz* 条件定义为：对于函数 $f(\boldsymbol x)$，若其任意定义域中的 **x1**,**x2**，都存在 L>0，使得 $|f(\boldsymbol x1)-f(\boldsymbol x2)|≤L|\boldsymbol x1-\boldsymbol x2|$。通俗理解就是，对于函数上的每一对点，都存在一个实数 L，使得它们连线斜率的绝对值不大于这个 L，其中最小的 L 称为*Lipschitz*常数。
> 　　　将公式变形可以更好的理解：$$\frac{\left \| \nabla f(\boldsymbol x{}')-\nabla f(\boldsymbol x) \right \|_{2}^{2}}{\left \| \boldsymbol x{}'-\boldsymbol x \right \|_{2}^{2}}\leqslant L 　(\forall \boldsymbol x,\boldsymbol x{}'),$$
> 　　　进一步，如果 $\boldsymbol x{}'\to  \boldsymbol x$，即 $$\lim_{\boldsymbol x{}'\to \boldsymbol x}\frac{\left \| \nabla f(\boldsymbol x{}')-\nabla f(\boldsymbol x)\right \|_{2}^{2}}{\left \| \boldsymbol x{}'-\boldsymbol x \right \|_{2}^{2}}$$
> 　　　“ *Lipschitz* 连续”很常见，知乎有一个问答(https://www.zhihu.com/question/51809602) 对*Lipschitz*连续的解释很形象：以陆地为例， 连续就是说这块地上没有特别陡的坡；其中最陡的地方有多陡呢？这就是所谓的*Lipschitz*常数。



则在 $\boldsymbol{x}_{k}$ 附近可将 $f(\boldsymbol{x})$ 通过二阶泰勒展式近似为

$$
\begin{aligned} \hat{f}(\boldsymbol{x}) & \simeq f\left(\boldsymbol{x}_{k}\right)+\left\langle\nabla f\left(\boldsymbol{x}_{k}\right), \boldsymbol{x}-\boldsymbol{x}_{k}\right\rangle+\frac{L}{2}\left\|\boldsymbol{x}-\boldsymbol{x}_{k}\right\|^{2} \\ &=\frac{L}{2}\left\|\boldsymbol{x}-\left(\boldsymbol{x}_{k}-\frac{1}{L} \nabla f\left(\boldsymbol{x}_{k}\right)\right)\right\|_{2}^{2}+\mathrm{const} \end{aligned}\tag{11.10}
$$



> $$
> \hat{f}(x) \simeq f(x_{k})+\langle \nabla f(x_{k}),x-x_{k} \rangle + \frac{L}{2}\left \| x-x_{k} \right\|^{2}
> $$
>
> [推导]：
> $$
> \begin{aligned}
> \hat{f}(x) &\simeq f(x_{k})+\langle \nabla f(x_{k}),x-x_{k} \rangle + \frac{L}{2}\left \| x-x_{k} \right\|^{2} \\
> &= f(x_{k})+\langle \nabla f(x_{k}),x-x_{k} \rangle + \langle\frac{L}{2}(x-x_{k}),x-x_{k}\rangle \\
> &= f(x_{k})+\langle \nabla f(x_{k})+\frac{L}{2}(x-x_{k}),x-x_{k} \rangle \\
> &= f(x_{k})+\frac{L}{2}\langle\frac{2}{L}\nabla f(x_{k})+(x-x_{k}),x-x_{k} \rangle \\
> &= f(x_{k})+\frac{L}{2}\langle x-x_{k}+\frac{1}{L}\nabla f(x_{k})+\frac{1}{L}\nabla f(x_{k}),x-x_{k}+\frac{1}{L}\nabla f(x_{k})-\frac{1}{L}\nabla f(x_{k}) \rangle \\
> &= f(x_{k})+\frac{L}{2}\left\| x-x_{k}+\frac{1}{L}\nabla f(x_{k}) \right\|_{2}^{2} -\frac{1}{2L}\left\|\nabla f(x_{k})\right\|_{2}^{2} \\
> &= \frac{L}{2}\left\| x-(x_{k}-\frac{1}{L}\nabla f(x_{k})) \right\|_{2}^{2} + const \qquad (因为 f(x_{k})和\nabla f(x_{k})是常数)
> \end{aligned}
> $$


其中 const 是与 $\boldsymbol{x}$ 无关的常数，$\langle\cdot, \cdot\rangle$ 表示内积。显然，式(11.10)的最小值在如下 $\boldsymbol{x}_{k+1}$ 获得：

$$
\boldsymbol{x}_{k+1}=\boldsymbol{x}_{k}-\frac{1}{L} \nabla f\left(\boldsymbol{x}_{k}\right)\tag{11.11}
$$

于是，若通过梯度下降法对 $f(\boldsymbol{x})$ 进行最小化，则每一步梯度下降迭代实际 上等价于最小化二次函数 $\hat{f}(\boldsymbol{x})$ 。将这个思想推广到式(11.8)，则能类似地得到 其每一步迭代应为

$$
\boldsymbol{x}_{k+1}=\underset{\boldsymbol{x}}{\arg \min } \frac{L}{2}\left\|\boldsymbol{x}-\left(\boldsymbol{x}_{k}-\frac{1}{L} \nabla f\left(\boldsymbol{x}_{k}\right)\right)\right\|_{2}^{2}+\lambda\|\boldsymbol{x}\|_{1}\tag{11.12}
$$

即在每一步对 $f(\boldsymbol{x})$ 进行梯度下降迭代的同时考虑 $\mathrm{L}_{1}$ 范数最小化。

对于式(11.12)，可先计算 $\boldsymbol{z}=\boldsymbol{x}_{k}-\frac{1}{L} \nabla f\left(\boldsymbol{x}_{k}\right)$ ，然后求解

$$
\boldsymbol{x}_{k+1}=\underset{\boldsymbol{x}}{\arg \min } \frac{L}{2}\|\boldsymbol{x}-\boldsymbol{z}\|_{2}^{2}+\lambda\|\boldsymbol{x}\|_{1}\tag{11.13}
$$



> $$\boldsymbol x_{\boldsymbol k+\boldsymbol 1}=\underset{\boldsymbol x}{argmin}\frac{L}{2}\left \| \boldsymbol x -\boldsymbol z\right \|_{2}^{2}+\lambda \left \| \boldsymbol x \right \|_{1}$$
> [推导]：假设目标函数为 $g(\boldsymbol x)$，则
> $$
> \begin{aligned}
> g(\boldsymbol x)
> & =\frac{L}{2}\left \|\boldsymbol  x \boldsymbol -\boldsymbol z\right \|_{2}^{2}+\lambda \left \| \boldsymbol x \right \|_{1}\\
> & =\frac{L}{2}\sum_{i=1}^{d}\left \| x^{i} -z^{i}\right \|_{2}^{2}+\lambda \sum_{i=1}^{d}\left \| x^{i} \right \|_{1} \\
> & =\sum_{i=1}^{d}(\frac{L}{2}(x^{i}-z^{i})^{2}+\lambda \left | x^{i}\right |)&
> \end{aligned}
> $$
> 由上式可见， $g(\boldsymbol x)$ 可以拆成 d个独立的函 数，求解式(11.13)可以分别求解 d 个独立的目标函数。
> 针对目标函数 $g(x^{i})=\frac{L}{2}(x^{i}-z^{i})^{2}+\lambda \left | x^{i}\right |$，通过求导求解极值：
> $$\frac{dg(x^{i})}{dx^{i}}=L(x^{i}-z^{i})+\lambda sgn(x^{i})$$
> 其中 $$sgn(x^{i})=\left\{\begin{matrix}
> 1, &x^{i}>0\\
>  -1,& x^{i}<0
> \end{matrix}\right.$$
> 令导数为 0，可得：$$x^{i}=z^{i}-\frac{\lambda }{L}sgn(x^{i})$$ 可分为三种情况：
> 1. 当 $z^{i}>\frac{\lambda }{L}$ 时：
>     (1）假设此时的根 $x^{i}<0$，则 $sgn(x^{i})=-1$，所以 $x^{i}=z^{i}+\frac{\lambda }{L}>0$，与假设矛盾。
>     (2）假设此时的根 $x^{i}>0$，则 $sgn(x^{i})=1$，所以 $x^{i}=z^{i}-\frac{\lambda }{L}>0$，成立。
> 2. 当 $z^{i}<-\frac{\lambda }{L}$ 时：
>     (1）假设此时的根 $x^{i}>0$，则 $sgn(x^{i})=1$，所以 $x^{i}=z^{i}-\frac{\lambda }{L}<0$，与假设矛盾。
>     (2）假设此时的根 $x^{i}<0$，则 $sgn(x^{i})=-1$，所以 $x^{i}=z^{i}+\frac{\lambda }{L}<0$，成立。
> 3. 当 $\left |z^{i}  \right |<\frac{\lambda }{L}$ 时：
>     (1）假设此时的根 $x^{i}>0$，则 $sgn(x^{i})=1$，所以 $x^{i}=z^{i}-\frac{\lambda }{L}<0$，与假设矛盾。
>     (2）假设此时的根 $x^{i}<0$，则 $sgn(x^{i})=-1$，所以 $x^{i}=z^{i}+\frac{\lambda }{L}>0$，与假设矛盾，此时 $x^{i}=0$ 为函数的极小值。
> 综上所述可得函数闭式解如下：
> $$x_{k+1}^{i}=\left\{\begin{matrix}
> z^{i}-\frac{\lambda }{L}, &\frac{\lambda }{L}< z^{i}\\
> 0, & \left |z^{i}  \right |\leqslant \frac{\lambda }{L}\\
> z^{i}+\frac{\lambda }{L}, & z^{i}<-\frac{\lambda }{L}
> \end{matrix}\right.$$




令 $x^i$ 表示 $\boldsymbol{x}$ 的第 $i$ 个分量，将式(11.13)按分量展开可看出，其中不存在 $x^ix^j(i\neq j)$ 这样的项，即 $\boldsymbol{x}$ 的各分量互不影响，于是式(11.13)有闭式解

$$
x_{k+1}^{i}=\left\{\begin{array}{ll}{z^{i}-\lambda / L,} & {\lambda / L<z^{i}} \\ {0,} & {\left|z^{i}\right| \leqslant \lambda / L} \\ {z^{i}+\lambda / L,} & {z^{i}<-\lambda / L}\end{array}\right.\tag{11.14}
$$


其中 $x_{k+1}^i$ 与 $z^i$ 分别是 $\boldsymbol{x}_{k+1}$ 与 $\boldsymbol{z}$ 的第 $i$ 个分量。因此，通过 PGD 能使 LASSO 和其他基于 $\mathrm{L}_{1}$ 范数最小化的方法得以快速求解。


