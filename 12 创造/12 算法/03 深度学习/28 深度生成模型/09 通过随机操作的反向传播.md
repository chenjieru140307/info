---
title: 09 通过随机操作的反向传播
toc: true
date: 2019-06-05
---

# 通过随机操作的反向传播



传统的神经网络对一些输入变量 $\boldsymbol x$ 施加确定性变换。当开发生成模型时，我们经常希望扩展神经网络以实现 $\boldsymbol x$ 的随机变换。这样做的一个直接方法是使用额外输入 $\boldsymbol z$（从一些简单的概率分布采样得到，如均匀或高斯分布）来增强神经网络。神经网络在内部仍可以继续执行确定性计算，但是函数 $f(\boldsymbol x,\boldsymbol z)$ 对于不能访问 $\boldsymbol z$ 的观察者来说将是随机的。假设 $f$ 是连续可微的，我们可以像往常一样使用反向传播计算训练所需的梯度。

作为示例，让我们考虑从均值 $\mu$ 和方差 $\sigma^2$ 的高斯分布中采样 $\mathrm y$ 的操作：


$$\begin{aligned}
 \mathrm y \sim \mathcal N(\mu, \sigma^2).
\end{aligned}$$


因为 $\mathrm y$ 的单个样本不是由函数产生的，而是由一个采样过程产生，它的输出会随我们的每次查询变化，所以取 $\mathrm y$ 相对于其分布的参数 $\mu$ 和 $\sigma^2$ 的导数似乎是违反直觉的。然而，我们可以将采样过程重写，对基本随机变量 $\mathrm z \sim \mathcal N(z;0,1)$ 进行转换以从期望的分布获得样本：


$$\begin{aligned}
 y = \mu + \sigma z.
\end{aligned}$$

现在我们将其视为具有额外输入 $\mathrm z$ 的确定性操作，可以通过采样操作来反向传播。至关重要的是，额外输入是一个随机变量，其分布不是任何我们想对其计算导数的变量的函数。如果我们可以用相同的 $\mathrm z$ 值再次重复采样操作，结果会告诉我们 $\mu$ 或 $\sigma$ 的微小变化将会如何改变输出。



能够通过该采样操作反向传播允许我们将其并入更大的图中。我们可以在采样分布的输出之上构建图元素。例如，我们可以计算一些损失函数 $J(y)$ 的导数。我们还可以构建这样的图元素，其输出是采样操作的输入或参数。例如，我们可以通过 $\mu = f(\boldsymbol x; \boldsymbol \theta)$ 和 $\sigma = g(\boldsymbol x; \boldsymbol \theta)$ 构建更大的图。在这个增强图中，我们可以通过这些函数的反向传播导出 $\nabla_{\boldsymbol \theta} J(y)$。

在该高斯采样示例中使用的原理能更广泛地应用。我们可以将任何形为 $p(\mathrm y;\boldsymbol \theta)$ 或 $p(\mathrm y \mid \boldsymbol x;\boldsymbol \theta)$ 的概率分布表示为 $p(\mathrm y \mid \boldsymbol \omega)$，其中 $\boldsymbol \omega$ 是同时包含参数 $\boldsymbol \theta$ 和输入 $\boldsymbol x$ 的变量(如果适用的话)。给定从分布 $p(\mathrm y \mid \boldsymbol \omega)$ 采样的值 $\mathrm y$（其中 $\boldsymbol \omega$ 可以是其他变量的函数），我们可以将


$$\begin{aligned}
 \mathbf y \sim p(\mathbf y  \mid  \boldsymbol \omega)
\end{aligned}$$


重写为


$$\begin{aligned}
 \boldsymbol y = f(\boldsymbol z; \boldsymbol \omega),
\end{aligned}$$


其中 $\boldsymbol z$ 是随机性的来源。只要 $f$ 是几乎处处连续可微的，我们就可以使用传统工具（例如应用于 $f$ 的反向传播算法）计算 $\mathrm y$ 相对于 $\boldsymbol \omega$ 的导数。至关重要的是，$\boldsymbol \omega$ 不能是 $\boldsymbol z$ 的函数，且 $\boldsymbol z$ 不能是 $\boldsymbol \omega$ 的函数。这种技术通常被称为重参数化技巧、**随机反向传播**(stochastic back-propagation)或 **扰动分析**(perturbation analysis)。


要求 $f$ 是连续可微的，当然需要 $\boldsymbol y$ 是连续的。如果我们希望通过产生离散值样本的采样过程进行反向传播，则可以使用强化学习算法（如~REINFORCE~算法的变体）来估计 $\boldsymbol \omega$ 上的梯度，这将在\sec?中讨论。

在神经网络应用中，我们通常选择从一些简单的分布中采样 $\boldsymbol z$，如单位均匀分布或单位高斯分布，并通过网络的确定性部分重塑其输入来实现更复杂的分布。

通过随机操作扩展梯度或优化的想法可追溯到二十世纪中叶~，并且首先在强化学习~的情景下用于机器学习。最近，它已被应用于变分近似~ 和随机生成神经网络~。许多网络，如去噪自编码器或使用 Dropout 的正则化网络，也被自然地设计为将噪声作为输入，而不需要任何特殊的重参数化就能使噪声独立于模型。




## 通过离散随机操作的反向传播


当模型发射离散变量 $\boldsymbol y$ 时，重参数化技巧不再适用。假设模型采用输入 $\boldsymbol x$ 和参数 $\boldsymbol \theta$，两者都封装在向量 $\boldsymbol \omega$ 中，并且将它们与随机噪声 $\boldsymbol z$ 组合以产生 $\boldsymbol y$：


$$\begin{aligned}
 \boldsymbol y = f(\boldsymbol z;\boldsymbol \omega).
\end{aligned}$$


因为 $\boldsymbol y$ 是离散的，$f$ 必须是一个阶跃函数。阶跃函数的导数在任何点都是没用的。在每个阶跃边界， 导数是未定义 的，但这是一个小问题。大问题是导数在阶跃边界之间的区域几乎处处为零。因此，任何代价函数 $J(\boldsymbol y)$ 的导数无法给出如何更新模型参数 $\boldsymbol \theta$ 的任何信息。

REINFORCE~算法（REward Increment $=$ nonnegative Factor $\times$ Offset Reinforcement $\times$ Characteristic Eligibility）提供了定义一系列简单而强大解决方案的框架。其核心思想是，即使 $J(f(\boldsymbol z;\boldsymbol \omega))$ 是具有无用导数的阶跃函数，期望代价 $\mathbb E_{\mathbf z \sim p(\mathbf z)} J(f(\boldsymbol z;\boldsymbol \omega))$ 通常是服从梯度下降的光滑函数。虽然当 $\boldsymbol y$ 是高维（或者是许多离散随机决策组合的结果）时，该期望通常是难解的，但我们可以使用蒙特卡罗平均进行无偏估计。梯度的随机估计可以与 SGD 或其他基于随机梯度的优化技术一起使用。

通过简单地微分期望成本，我们可以推导出~REINFORCE~最简单的版本：


$$\begin{aligned}
 \mathbb E_{\boldsymbol z}[J(\boldsymbol y)] &= \sum_{\boldsymbol y} J(\boldsymbol y) p(\boldsymbol y), \\
 \frac{\partial \mathbb E[J(\boldsymbol y)]}{\partial \boldsymbol \omega} &= \sum_{\boldsymbol y} J(\boldsymbol y)
 &=  \sum_{\boldsymbol y} J(\boldsymbol y) p(\boldsymbol y) \frac{\partial\log p(\boldsymbol y)}{\partial \boldsymbol \omega}
 & \approx \frac{1}{m} \sum_{\boldsymbol y^{(i)} \sim p(\boldsymbol y), i=1}^m
\end{aligned}$$


\eqn?依赖于 $J$ 不直接引用 $\boldsymbol \omega$ 的假设。放松这个假设来扩展该方法是简单的。\eqn?利用对数的导数规则，$\frac{\partial\log p(\boldsymbol y)}{\partial \boldsymbol \omega} = \frac{1}{p(\boldsymbol y)}\frac{\partial p(\boldsymbol y)}{\partial \boldsymbol \omega}$。\eqn?给出了该梯度的无偏蒙特卡罗估计。



在本节中我们写的 $p(\boldsymbol y)$，可以等价地写成 $p(\boldsymbol y  \mid  \boldsymbol x)$。这是因为 $p(\boldsymbol y)$ 由 $\boldsymbol \omega$ 参数化，并且如果 $\boldsymbol x$ 存在，$\boldsymbol \omega$ 包含 $\boldsymbol \theta$ 和 $\boldsymbol x$ 两者。

简单~REINFORCE~估计的一个问题是其具有非常高的方差，需要采 $\boldsymbol y$ 的许多样本才能获得对梯度的良好估计，或者等价地，如果仅绘制一个样本，SGD 将收敛得非常缓慢并将需要较小的学习率。通过使用方差减小方法~，可以地减少该估计的方差。想法是修改估计量，使其预期值保持不变，但方差减小。在~REINFORCE~的情况下提出的方差减小方法，涉及计算用于偏移 $J(\boldsymbol y)$ 的 **基线**(baseline)。注意，不依赖于 $\boldsymbol y$ 的任何偏移 $b(\boldsymbol w)$ 都不会改变估计梯度的期望，因为


$$\begin{aligned}
 E_{p(\boldsymbol y)} \Bigg[ \frac{\partial\log p(\boldsymbol y)}{\partial \boldsymbol \omega}  \Bigg] &=
 \sum_{\boldsymbol y} p(\boldsymbol y) \frac{\partial\log p(\boldsymbol y)}{\partial \boldsymbol \omega} \\
 &= \sum_{\boldsymbol y} \frac{\partial p(\boldsymbol y)}{\partial \boldsymbol \omega} \\
 &= \frac{\partial}{\partial \boldsymbol \omega} \sum_{\boldsymbol y} p(\boldsymbol y) =
 \frac{\partial}{\partial \boldsymbol \omega} 1 = 0,
\end{aligned}$$

这意味着


$$\begin{aligned}
 E_{p(\boldsymbol y)} \Bigg[ (J(\boldsymbol y) - b(\boldsymbol \omega))\frac{\partial\log p(\boldsymbol y)}{\partial \boldsymbol \omega}  \Bigg] &=
 E_{p(\boldsymbol y)} \Bigg[ J(\boldsymbol y) \frac{\partial\log p(\boldsymbol y)}{\partial \boldsymbol \omega} \Bigg]
 - b(\boldsymbol \omega) E_{p(\boldsymbol y)} \Bigg[ \frac{\partial\log p(\boldsymbol y)}{\partial \boldsymbol \omega}  \Bigg] \\
 &= E_{p(\boldsymbol y)} \Bigg[ J(\boldsymbol y) \frac{\partial\log p(\boldsymbol y)}{\partial \boldsymbol \omega} \Bigg] .
\end{aligned}$$


此外，我们可以通过计算 $(J(\boldsymbol y) - b(\boldsymbol \omega))\frac{\partial\log p(\boldsymbol y)}{\partial \boldsymbol \omega} $ 关于 $p(\boldsymbol y)$ 的方差，并关于 $b(\boldsymbol \omega)$ 最小化获得最优 $b(\boldsymbol \omega)$。我们发现这个最佳基线 $b^*(\boldsymbol \omega)_i$ 对于向量 $\boldsymbol \omega$ 的每个元素 $\omega_i$ 是不同的：


$$\begin{aligned}
 b^*(\boldsymbol \omega)_i = \frac{E_{p(\boldsymbol y)} \Big[ J(\boldsymbol y)
 \frac{\partial\log p(\boldsymbol y)^2}{\partial \omega_i}  \Big]}
{E_{p(\boldsymbol y)} \Big[\frac{\partial\log p(\boldsymbol y)^2}{\partial \omega_i}\Big] }.
\end{aligned}$$


相对于 $\omega_i$ 的梯度估计则变为


$$\begin{aligned}
 (J(\boldsymbol y) - b(\boldsymbol \omega)_i)\frac{\partial\log p(\boldsymbol y)}{\partial \omega_i},
\end{aligned}$$


其中 $b(\boldsymbol \omega)_i$ 估计上述 $b^*(\boldsymbol \omega)_i$。获得估计 $b$ 通常需要将额外输出添加到神经网络，并训练新输出对 $\boldsymbol \omega$ 的每个元素估计 $E_{p(\boldsymbol y)}[J(\boldsymbol y)\frac{\partial\log p(\boldsymbol y)^2}{\partial \omega_i}]$ 和 $E_{p(\boldsymbol y)}[\frac{\partial\log p(\boldsymbol y)^2}{\partial \omega_i}]$。这些额外的输出可以用均方误差目标训练，对于给定的 $\boldsymbol \omega$，从 $p(\boldsymbol y)$ 采样 $\boldsymbol y$ 时，分别用 $J(\boldsymbol y)\frac{\partial\log p(\boldsymbol y)^2}{\partial \omega_i}$ 和 $\frac{\partial\log p(\boldsymbol y)^2}{\partial \omega_i}$ 作目标。然后可以将这些估计代入\eqn?就能恢复估计 $b$。{Mnih+Gregor-ICML2014} 倾向于使用通过目标 $J(\boldsymbol y)$ 训练的单个共享输出（跨越 $\boldsymbol \omega$ 的所有元素 $i$），并使用 $b(\boldsymbol \omega) \approx E_{p(\boldsymbol y)} [J(\boldsymbol y)]$ 作为基线。



在强化学习背景下引入的方差减小方法， {Dayan-1990} 推广了二值奖励的前期工作。可以参考 {bengio2013estimating}、{Mnih+Gregor-ICML2014}、{Ba+Mnih-arxiv2014}、{Mnih2014}或 {Xu-et-al-ICML2015} 中在深度学习的背景下使用减少方差的~REINFORCE~算法的现代例子。除了使用与输入相关的基线 $b(\boldsymbol \omega)$，{Mnih+Gregor-ICML2014} 发现可以在训练期间调整 $(J(\boldsymbol y) - b(\boldsymbol \omega))$ 的尺度（即除以训练期间的移动平均估计的标准差 ），即作为一种适应性学习率，可以抵消训练过程中该量大小发生的重要变化的影响。{Mnih+Gregor-ICML2014} 称之为启发式 **方差归一化**(variance normalization)。

基于~REINFORCE~的估计器可以被理解为将 $\boldsymbol y$ 的选择与 $J(\boldsymbol y)$ 的对应值相关联来估计梯度。如果在当前参数化下不太可能出现 $\boldsymbol y$ 的良好值，则可能需要很长时间来偶然获得它，并且获得所需信号的配置应当被加强。




# 相关

- 《深度学习》花书
