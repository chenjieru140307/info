---
title: 2.11 欠完备自编码器
toc: true
date: 2019-06-05
---

# 欠完备自编码器

将输入复制到输出听起来没什么用，但我们通常不关心解码器的输出。相反，我们希望通过训练自编码器对输入进行复制而使 $\boldsymbol h$ 获得有用的特性。


从自编码器获得有用特征的一种方法是限制 $\boldsymbol h$ 的维度比 $\boldsymbol x$ 小，这种编码维度小于输入维度的自编码器称为欠完备自编码器。

学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。<span style="color:red;">嗯。</span>


学习过程可以简单地描述为最小化一个损失函数


$$\begin{aligned}
    L(\boldsymbol x, g(f(\boldsymbol x))),
\end{aligned}$$


其中 $L$ 是一个损失函数，惩罚 $g(f(\boldsymbol x))$ 与 $\boldsymbol x$ 的差异，如均方误差。


当解码器是线性的且 $L$ 是均方误差，欠完备的自编码器会学习出与 PCA 相同的生成子空间。这种情况下，自编码器在训练来执行复制任务的同时学到了训练数据的主元子空间。



因此，拥有非线性编码器函数 $f$ 和非线性解码器函数 $g$ 的自编码器能够学习出更强大的 PCA 非线性推广。不幸的是，如果编码器和解码器被赋予过大的容量，自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息。从理论上说，我们可以设想这样一个自编码器，它只有一维编码，但它具有一个非常强大的非线性编码器，能够将每个训练数据 $\boldsymbol x^{(i)}$ 表示为编码 $i$。而解码器可以学习将这些整数索引映射回特定训练样本的值。这种特定情形不会在实际情况中发生，但它清楚地说明，如果自编码器的容量太大，那训练来执行复制任务的自编码器可能无法学习到数据集的任何有用信息。



# 相关

- 《深度学习》花书
