

### 3.8.3 初始化为小的随机数

将权重初始化为很小的数字是一个普遍的打破网络对称性的解决办法。这个想法是，神经元在一开始都是随机的、独一无二的，所以它们会计算出不同的更新，并将自己整合到整个网络的各个部分。

一个权重矩阵的实现可能看起来像 $W=0.01∗np.random.randn(D,H)$，<span style="color:red;">D,H 是什么？</span>其中 randn 是从均值为 $0$ 的单位标准高斯分布进行取样。通过这个公式(函数)，每个神经元的权重向量初始化为一个从多维高斯分布取样的随机向量，所以神经元在输入空间中指向随机的方向(so the neurons point in random direction in the input space. 应该是指输入空间对于随机方向有影响)。其实也可以从均匀分布中来随机选取小数，但是在实际操作中看起来似乎对最后的表现并没有太大的影响。<span style="color:red;">嗯，两种分布差不多吗？</span>

备注：并不是数字越小就会表现的越好。比如，如果一个神经网络层的权重非常小，那么在反向传播算法就会计算出很小的梯度(因为梯度 gradient 是与权重成正比的)。在网络不断的反向传播过程中将极大地减少“梯度信号”，并可能成为深层网络的一个需要注意的问题。<span style="color:red;">嗯。</span>





### 3.8.4 用 $1/\sqrt n$ 校准方差

<span style="color:red;">没明白为什么要校准。</span>

上述建议的一个问题是，随机初始化神经元的输出的分布有一个随输入量增加而变化的方差。结果证明，我们可以通过将其权重向量按其输入的平方根(即输入的数量)进行缩放，从而将每个神经元的输出的方差标准化到 $1$。也就是说推荐的启发式方法 (heuristic) 是将每个神经元的权重向量按下面的方法进行初始化:<span style="color:red;">什么是启发式算法？</span>

$$
w=np.random.randn(n)/\sqrt n
$$

其中 $n$ 表示输入的数量。这保证了网络中所有的神经元最初的输出分布大致相同，并在经验上提高了收敛速度。<span style="color:red;"> n 指的是什么？跟上面的 D,H 有什么不同吗？</span>

### 3.8.5 稀疏初始化(Sparse Initialazation)

另一种解决未校准方差问题的方法是把所有的权重矩阵都设为零，但是为了打破对称性，每个神经元都是随机连接地(从如上面所介绍的一个小的高斯分布中抽取权重)到它下面的一个固定数量的神经元。一个典型的神经元连接的数目可能是小到 10 个。<span style="color:red;">这样可以吗？这样不是永久改变了网络的链接？数目怎么定的？效果怎么样？</span>

### 3.8.6 初始化偏差

将偏差初始化为零是可能的，也是很常见的，因为非对称性破坏是由权重的小随机数导致的。因为 ReLU 具有非线性特点，所以有些人喜欢使用将所有的偏差设定为小的常数值如 0.01，因为这样可以确保所有的 ReLU 单元在最开始就激活触发(fire)并因此能够获得和传播一些梯度值。然而，这是否能够提供持续的改善还不太清楚(实际上一些结果表明这样做反而使得性能更加糟糕)，所以更通常的做法是简单地将偏差初始化为 $0$。<span style="color:red;">嗯。</span>



# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
