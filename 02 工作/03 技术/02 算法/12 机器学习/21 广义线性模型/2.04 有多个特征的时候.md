

## $d$ 个特征的时候

### 当有多个特征的时候的距离

此时我们试图学得：

$$
f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b, 使得 f\left(\boldsymbol{x}_{i}\right) \simeq y_{i}
$$

这称为"多元线性回归" (multivariate linear regression）。

同样的，可以使用最小二乘法来对 $w$ 和 $b$ 进行估计。为便于讨论，我们把 $w$ 和 $b$ 吸收入向量形式 $\hat{w}=(w;b)$ ，相应的，把数据集 $D$ 表示为一个 $m \times (d + 1)$ 大小的矩阵 $X$ ，其中每行对应于一个示例，该行前 $d$ 个元素对应于示例的 $d$ 个属性值，最后一个元素恒置为 1 :

$$
\mathbf{X}=\left(\begin{array}{ccccc}{x_{11}} & {x_{12}} & {\dots} & {x_{1 d}} & {1} \\ {x_{21}} & {x_{22}} & {\dots} & {x_{2 d}} & {1} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {\vdots} \\ {x_{m 1}} & {x_{m 2}} & {\dots} & {x_{m d}} & {1}\end{array}\right)=\left(\begin{array}{cc}{\boldsymbol{x}_{1}^{\mathrm{T}}} & {1} \\ {\boldsymbol{x}_{2}^{\mathrm{T}}} & {1} \\ {\vdots} & {\vdots} \\ {\boldsymbol{x}_{m}^{\mathrm{T}}} & {1}\end{array}\right)
$$

再把标记也写成向量形式 $\boldsymbol{y}=\left(y_{1} ; y_{2} ; \ldots ; y_{m}\right)$， 则有：

$$
\hat{\boldsymbol{w}}^{*}=\underset{\boldsymbol{\hat { w }}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})\tag{3.9}
$$

<span style="color:red;">是的，这个没问题，右边是距离的平方。仍然是求使距离最小的时候的参数。</span>

### 这时候的求导

<span style="color:red;">这里的推导没看懂，包括下面对满秩矩阵和非满秩的分析。</span>

令 $E_{\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$ ，对 $\hat{\boldsymbol{w}}$ 求导得到：

$$
\frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}}=2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})\tag{3.10}
$$


> $$ \cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol w}}=2\mathbf{X}^T(\mathbf{X}\hat{\boldsymbol w}-\boldsymbol{y}) $$
>
> [推导]：将 $ E_{\hat{\boldsymbol w}}=(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol w})^T(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol w}) $ 展开可得：
> $$ E_{\hat{\boldsymbol w}}= \boldsymbol{y}^T\boldsymbol{y}-\boldsymbol{y}^T\mathbf{X}\hat{\boldsymbol w}-\hat{\boldsymbol w}^T\mathbf{X}^T\boldsymbol{y}+\hat{\boldsymbol w}^T\mathbf{X}^T\mathbf{X}\hat{\boldsymbol w} $$
> 对 $ \hat{\boldsymbol w} $ 求导可得：
> $$ \cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol w}}= \cfrac{\partial \boldsymbol{y}^T\boldsymbol{y}}{\partial \hat{\boldsymbol w}}-\cfrac{\partial \boldsymbol{y}^T\mathbf{X}\hat{\boldsymbol w}}{\partial \hat{\boldsymbol w}}-\cfrac{\partial \hat{\boldsymbol w}^T\mathbf{X}^T\boldsymbol{y}}{\partial \hat{\boldsymbol w}}+\cfrac{\partial \hat{\boldsymbol w}^T\mathbf{X}^T\mathbf{X}\hat{\boldsymbol w}}{\partial \hat{\boldsymbol w}} $$
> 由向量的求导公式可得：
> $$ \cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol w}}= 0-\mathbf{X}^T\boldsymbol{y}-\mathbf{X}^T\boldsymbol{y}+(\mathbf{X}^T\mathbf{X}+\mathbf{X}^T\mathbf{X})\hat{\boldsymbol w} $$
> $$ \cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol w}}=2\mathbf{X}^T(\mathbf{X}\hat{\boldsymbol w}-\boldsymbol{y}) $$

<span style="color:red;">怎么对矩阵求导的？一直不知道</span>

令上式为零可得 $\hat{\boldsymbol{w}}$ 最优解的闭式解，但由于涉及矩阵逆的计算，比单变量情形要复杂一些。下面我们做一个简单的讨论.


#### $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 为满秩矩阵的时候


当 $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 为满秩矩阵 (full-rank matrix)或正走矩阵 (positive definite matrix) 时，令上式为 0 可得：

$$
\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}\tag{3.11}
$$

其中：$\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1}$ 是矩阵 $\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)$ 的逆矩阵，令 $\hat{\boldsymbol{x}}_{i}=\left(\boldsymbol{x}_{i}, 1\right)$ ，则最终学得的多元线性回归模型为

$$
f\left(\hat{\boldsymbol{x}}_{i}\right)=\hat{\boldsymbol{x}}_{i}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}\tag{3.12}
$$

#### $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 为不满秩怎么办？

然而，现实任务中 $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 往往不是满秩矩阵。例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致 $\mathbf{X}$ 的列数多于行数 ，$\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 显然不满秩。此时可解出多个 $\hat{w}$，它们都能使均方误差最小化。

选择哪一个解作为输出 ，将由学习算法的归纳偏好决定，常见的做法是引入正则化 (regularization) 项。<span style="color:red;">什么叫学习算法的归纳偏好？为什么正则化可以调整？正则化不是为了降低模型复杂度的过拟合问题的吗？</span>



# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
