

# PCA 可以看做一种学习数据表示的无监督学习算法

可以将 PCA 看做一种学习数据表示的无监督学习算法。

因为：

- PCA 学习一种比原始输入维数更低的表示。
- 它也学习了一种元素之间彼此没有线性相关的表示。这是学习表示中元素统计独立标准的第一步。要实现完全独立性，表示学习算法也必须去掉变量间的非线性关系。<span style="color:red;">嗯嗯，要怎么去掉变量之间的非线性关系呢？要怎么知道变量之间的非线性关系呢？还是说去除几种可能的非线性关系？而且，为什么要去除呢？</span>





- 对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。<span style="color:red;">详细过程想知道。</span>
- 完成 PCA 的关键是 - 协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。<span style="color:red;">嗯嗯，协方差矩阵度量的是维度与维度之间的关系，而不是样本与样本之间的关系。</span>
- 之所以对角化，因为对角化之后非对角上的元素都是 0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。<span style="color:red;">嗯。</span>




# 相关

- 《百面机器学习》
